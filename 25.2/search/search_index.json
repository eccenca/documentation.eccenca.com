{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to documentation.eccenca.com \ud83e\udd13","text":"<ul> <li> <p> Release Notes</p> <p>Documentation of changes and enhancements for each version.</p> </li> <li> <p> Tutorials</p> <p>Learn by example and step-by-step guidelines to achieve a concrete goal fast.</p> </li> <li> <p> Getting Started</p> <p>This page describes how to work with Corporate Memory and shortly outlines all functionalities of the user interface.</p> </li> <li> <p> Build</p> <p>Lift your data by integrating multiple datasets into a Knowledge Graph.</p> </li> <li> <p> Explore and Author</p> <p>Explore, author and interact with your Knowledge Graph.</p> </li> <li> <p> Consume</p> <p>This section outlines how to consume data from the Knowledge Graph.</p> </li> <li> <p> Deploy and Configure</p> <p>Deploy in your own environment.</p> </li> <li> <p> Automate</p> <p>Setup processes and automate activities based on and towards your Knowledge Graph.</p> </li> <li> <p> Develop</p> <p>API documentation and programming recipes.</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:advancedtutorial","title":"AdvancedTutorial","text":"<ul> <li>            Build Knowledge Graphs from Kafka Topics          </li> <li>            Connect to Snowflake          </li> <li>            Evaluate Jinja Template and Send an Email Message          </li> <li>            Lift data from JSON and XML source          </li> <li>            Populate Data to Neo4j          </li> </ul>"},{"location":"tags/#tag:api","title":"API","text":"<ul> <li>            Build (DataIntegration) APIs          </li> <li>            Explore backend APIs          </li> <li>            Provide Data in any Format via a Custom API          </li> <li>            cmempy - Python API          </li> </ul>"},{"location":"tags/#tag:automate","title":"Automate","text":"<ul> <li>            Populate Graphs to Apache Kafka          </li> <li>            Processing Data with Variable Input Workflows          </li> <li>            Scheduling Workflows          </li> <li>            cmemc - Command Line Interface          </li> <li>            cmemc - Python Scripts          </li> <li>            cmemc: Command Group - workflow scheduler          </li> <li>            cmemc: SPARQL Scripts          </li> <li>            cmemc: Using Github Actions          </li> <li>            cmemc: Using Gitlab Pipelines          </li> <li>            cmemc: Workflow Execution and Orchestration          </li> </ul>"},{"location":"tags/#tag:beginnerstutorial","title":"BeginnersTutorial","text":"<ul> <li>            Active Learning of Linking Rules          </li> <li>            Getting Started          </li> <li>            Lift data from tabular data such as CSV, XSLX or database tables          </li> </ul>"},{"location":"tags/#tag:bestpractice","title":"BestPractice","text":"<ul> <li>            Cool IRIs          </li> <li>            Define Prefixes / Namespaces          </li> </ul>"},{"location":"tags/#tag:cmemc","title":"cmemc","text":"<ul> <li>            Access Conditions          </li> <li>            Configuration          </li> <li>            Invocation          </li> <li>            Processing Data with Variable Input Workflows          </li> <li>            cmemc - Command Line Interface          </li> <li>            cmemc - Python Scripts          </li> <li>            cmemc: Certificate handling and SSL verification          </li> <li>            cmemc: Command Group - admin          </li> <li>            cmemc: Command Group - admin acl          </li> <li>            cmemc: Command Group - admin client          </li> <li>            cmemc: Command Group - admin metrics          </li> <li>            cmemc: Command Group - admin migration          </li> <li>            cmemc: Command Group - admin store          </li> <li>            cmemc: Command Group - admin user          </li> <li>            cmemc: Command Group - admin workspace          </li> <li>            cmemc: Command Group - admin workspace python          </li> <li>            cmemc: Command Group - config          </li> <li>            cmemc: Command Group - dataset          </li> <li>            cmemc: Command Group - dataset resource          </li> <li>            cmemc: Command Group - graph          </li> <li>            cmemc: Command Group - graph imports          </li> <li>            cmemc: Command Group - graph validation          </li> <li>            cmemc: Command Group - project          </li> <li>            cmemc: Command Group - project variable          </li> <li>            cmemc: Command Group - query          </li> <li>            cmemc: Command Group - vocabulary          </li> <li>            cmemc: Command Group - vocabulary cache          </li> <li>            cmemc: Command Group - workflow          </li> <li>            cmemc: Command Group - workflow scheduler          </li> <li>            cmemc: Command Reference          </li> <li>            cmemc: Command-Line Completion          </li> <li>            cmemc: Environment-based Configuration          </li> <li>            cmemc: File-based Configuration          </li> <li>            cmemc: Getting Credentials from External Processes          </li> <li>            cmemc: Installation          </li> <li>            cmemc: SPARQL Scripts          </li> <li>            cmemc: Troubleshooting and Caveats          </li> <li>            cmemc: Using Github Actions          </li> <li>            cmemc: Using Gitlab Pipelines          </li> <li>            cmemc: Using the Docker Image          </li> <li>            cmemc: Workflow Execution and Orchestration          </li> </ul>"},{"location":"tags/#tag:configuration","title":"Configuration","text":"<ul> <li>            Build (DataIntegration)          </li> <li>            Caveats          </li> <li>            Changing Passwords and Keys          </li> <li>            Configure Corporate Memory with an external Keycloak          </li> <li>            Docker Orchestration          </li> <li>            Explore          </li> <li>            Explore backend          </li> <li>            General          </li> <li>            GraphDB          </li> <li>            HTTP          </li> <li>            In-Memory          </li> <li>            Keycloak          </li> <li>            Neptune          </li> <li>            OAuth          </li> <li>            Production-Ready Settings          </li> <li>            Quad Store Configuration          </li> <li>            Reverse Proxy          </li> <li>            Virtuoso          </li> <li>            cmemc: Command Group - config          </li> </ul>"},{"location":"tags/#tag:dashboards","title":"Dashboards","text":"<ul> <li>            Charts Catalog and Charts Integration          </li> <li>            Consuming Graphs in Power BI          </li> <li>            Consuming Graphs in Redash          </li> <li>            Embedding Services via the Integrations Module          </li> </ul>"},{"location":"tags/#tag:docker","title":"Docker","text":"<ul> <li>            Caveats          </li> <li>            Docker Orchestration          </li> <li>            HTTP          </li> <li>            cmemc: Using the Docker Image          </li> </ul>"},{"location":"tags/#tag:evaluatetemplate","title":"EvaluateTemplate","text":"<ul> <li>            Evaluate Jinja Template and Send an Email Message          </li> </ul>"},{"location":"tags/#tag:experttutorial","title":"ExpertTutorial","text":"<ul> <li>            Consuming Graphs with SQL Databases          </li> <li>            Extracting data from a Web API          </li> <li>            How to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)          </li> <li>            Loading JDBC datasets incrementally          </li> <li>            Processing Data with Variable Input Workflows          </li> <li>            Provide Data in any Format via a Custom API          </li> </ul>"},{"location":"tags/#tag:filesystem","title":"Filesystem","text":"<ul> <li>            Caveats          </li> </ul>"},{"location":"tags/#tag:java","title":"Java","text":"<ul> <li>            Accessing Graphs with Java Applications          </li> </ul>"},{"location":"tags/#tag:keycloak","title":"Keycloak","text":"<ul> <li>            Changing Passwords and Keys          </li> <li>            Configure Corporate Memory with an external Keycloak          </li> <li>            Keycloak          </li> <li>            cmemc: Command Group - admin client          </li> <li>            cmemc: Command Group - admin user          </li> </ul>"},{"location":"tags/#tag:knowledgegraph","title":"KnowledgeGraph","text":"<ul> <li>            Build Knowledge Graphs from Kafka Topics          </li> <li>            Building a Customized User Interface          </li> <li>            Business Knowledge Editor Module          </li> <li>            Charts Catalog and Charts Integration          </li> <li>            Connect to Snowflake          </li> <li>            Consuming Graphs in Power BI          </li> <li>            Consuming Graphs in Redash          </li> <li>            Cool IRIs          </li> <li>            Define Prefixes / Namespaces          </li> <li>            Graph Exploration          </li> <li>            Lift data from JSON and XML source          </li> <li>            Lift data from tabular data such as CSV, XSLX or database tables          </li> <li>            Populate Graphs to Apache Kafka          </li> <li>            Project and Global Build Variables          </li> <li>            Query Module          </li> <li>            Statement Annotations          </li> <li>            Versioning of Graph Changes          </li> <li>            Vocabulary Catalog          </li> <li>            Workspaces          </li> <li>            cmemc: Command Group - graph          </li> <li>            cmemc: Command Group - graph imports          </li> <li>            cmemc: Command Group - graph validation          </li> </ul>"},{"location":"tags/#tag:load-balancer","title":"Load Balancer","text":"<ul> <li>            Caveats          </li> </ul>"},{"location":"tags/#tag:plugin","title":"Plugin","text":"<ul> <li>            Python Plugins: Installation and Usage          </li> </ul>"},{"location":"tags/#tag:project","title":"Project","text":"<ul> <li>            cmemc: Command Group - project          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            Python Plugins          </li> <li>            Python Plugins: Development          </li> <li>            Python Plugins: Installation and Usage          </li> <li>            Python Plugins: Setup and Configuration          </li> <li>            cmemc - Python Scripts          </li> <li>            cmemc: Command Group - admin workspace python          </li> <li>            cmempy - Python API          </li> </ul>"},{"location":"tags/#tag:pythonplugin","title":"PythonPlugin","text":"<ul> <li>            Build Knowledge Graphs from Kafka Topics          </li> <li>            Populate Graphs to Apache Kafka          </li> </ul>"},{"location":"tags/#tag:reference","title":"Reference","text":"<ul> <li>            DataIntegration: Activity Reference          </li> <li>            DataIntegration: Plugin Reference          </li> <li>            Datatypes          </li> <li>            Graph Resource Pattern          </li> <li>            Node Shape Reference          </li> <li>            Property Shapes          </li> <li>            Rule Operators          </li> <li>            cmemc: Command Reference          </li> </ul>"},{"location":"tags/#tag:releasenote","title":"ReleaseNote","text":"<ul> <li>            Corporate Memory 19.10          </li> <li>            Corporate Memory 20.03          </li> <li>            Corporate Memory 20.06          </li> <li>            Corporate Memory 20.10          </li> <li>            Corporate Memory 20.12          </li> <li>            Corporate Memory 21.02          </li> <li>            Corporate Memory 21.04          </li> <li>            Corporate Memory 21.06          </li> <li>            Corporate Memory 21.11          </li> <li>            Corporate Memory 22.1          </li> <li>            Corporate Memory 22.2.3          </li> <li>            Corporate Memory 23.1.3          </li> <li>            Corporate Memory 23.2.1          </li> <li>            Corporate Memory 23.3.2          </li> <li>            Corporate Memory 24.1.3          </li> <li>            Corporate Memory 24.2.1          </li> <li>            Corporate Memory 24.3.2          </li> <li>            Corporate Memory 25.1.2          </li> <li>            Corporate Memory 25.2.0          </li> </ul>"},{"location":"tags/#tag:security","title":"Security","text":"<ul> <li>            Access Conditions          </li> <li>            Changing Passwords and Keys          </li> <li>            Configure Corporate Memory with an external Keycloak          </li> <li>            Keycloak          </li> <li>            OAuth          </li> <li>            Production-Ready Settings          </li> <li>            cmemc: Certificate handling and SSL verification          </li> <li>            cmemc: Command Group - admin acl          </li> <li>            cmemc: Command Group - admin client          </li> <li>            cmemc: Command Group - admin user          </li> <li>            cmemc: Getting Credentials from External Processes          </li> </ul>"},{"location":"tags/#tag:sparql","title":"SPARQL","text":"<ul> <li>            Provide Data in any Format via a Custom API          </li> <li>            Query Module          </li> <li>            cmemc: Command Group - admin store          </li> <li>            cmemc: Command Group - query          </li> <li>            cmemc: SPARQL Scripts          </li> </ul>"},{"location":"tags/#tag:validation","title":"Validation","text":"<ul> <li>            cmemc: Command Group - graph validation          </li> </ul>"},{"location":"tags/#tag:variables","title":"Variables","text":"<ul> <li>            Project and Global Build Variables          </li> <li>            cmemc: Command Group - project variable          </li> </ul>"},{"location":"tags/#tag:video","title":"Video","text":"<ul> <li>            Scheduling Workflows          </li> <li>            cmemc: Command-Line Completion          </li> </ul>"},{"location":"tags/#tag:vocabulary","title":"Vocabulary","text":"<ul> <li>            Datatypes          </li> <li>            Node Shape Reference          </li> <li>            Property Shapes          </li> <li>            Thesauri Management          </li> <li>            Vocabulary Catalog          </li> <li>            cmemc: Command Group - vocabulary          </li> <li>            cmemc: Command Group - vocabulary cache          </li> </ul>"},{"location":"tags/#tag:volume","title":"Volume","text":"<ul> <li>            Caveats          </li> </ul>"},{"location":"tags/#tag:workflow","title":"Workflow","text":"<ul> <li>            Processing Data with Variable Input Workflows          </li> <li>            Scheduling Workflows          </li> <li>            Workflow Reconfiguration          </li> <li>            Workflow Trigger          </li> <li>            cmemc: Command Group - workflow          </li> <li>            cmemc: Workflow Execution and Orchestration          </li> </ul>"},{"location":"automate/","title":"Automate","text":"<p>Setup processes and automate activities based on and towards your Knowledge Graph.</p> <p> Intended audience: Linked Data Experts and Deployment Engineers</p> <ul> <li> <p> cmemc - Command Line Interface</p> <p>cmemc is intended for system administrators and Linked Data experts, who want to automate and control activities on eccenca Corporate Memory remotely.</p> </li> <li> <p> Processing data with variable input workflows</p> <p>This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (i.e., without registering datasets).</p> </li> <li> <p> Scheduling Workflows</p> <p>For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator.</p> </li> <li> <p> Continuous Integration and Delivery</p> <p>Setup processes which continuously integrate data artifacts such as vocabularies and shapes with your Corporate Memory instances.</p> </li> </ul>"},{"location":"automate/cmemc-command-line-interface/","title":"cmemc - Command Line Interface","text":"<ul> <li> <p> Command Line interface for eccenca Corporate Memory</p> <p>Developed with  Python and publicly available as a pypi.org Package and a  Docker Image under the  Apache 2 License.</p> <p>Next Steps:  </p> <p> </p> </li> <li> <p> Intended for Administrators and Linked Data Expert</p> <p>Battle tested in many projects to   Automate Activities and  Remote Control eccenca Corporate Memory instances.</p> Example: List datasets with a specific tag and project.<pre><code>$ cmemc -c prod.knowledge.company.org # (1)!\n  dataset list \\\n  --filter project crm-graph \\\n  --filter tag velocity-daily\n</code></pre> <ol> <li><ul> <li>The option <code>-c</code> is short for <code>--connection</code> and references to a remote Corporate Memory instance.</li> <li>The <code>list</code> command in the <code>dataset</code> command group shows all datasets of an instance.</li> <li>In order to manipulate output dataset list, the <code>--filter</code> option takes two parameter, a filter type (<code>tag</code>, <code>project</code>, \u2026) and a value.</li> </ul> </li> </ol> </li> <li> <p> Fast ad-hoc Execution with Command Completion</p> <p> Create Build Project and Dataset </p> </li> <li> <p> Main Features:</p> <p>Manage (<code>list</code>, <code>inspect</code>, <code>import</code>, <code>export</code>, \u2026) and Manipulate (<code>create</code>, <code>delete</code>, <code>execute</code>, \u2026) Vocabularies, Datasets,  Knowledge Graphs,  Workflows, Projects, Queries, Scheduler, Configurations and much more.</p> Example: Backup the query catalog including imports.<pre><code>$ cmemc graph export \\\n  https://ns.eccenca.com/data/queries/ \\\n  --include-imports --output-dir queries \\\n  --filename-template \"{{date}}-{{iriname}}\"\n</code></pre> </li> </ul>","tags":["cmemc","Automate"]},{"location":"automate/cmemc-command-line-interface/command-reference/","title":"Command Reference","text":"<p>Info</p> <p>cmemc is organized as a tree of command groups, each with a set of commands. You can access the command groups in the table as well as in the navigation on the left. You can access the commands directly from the table or by visiting a command group page first.</p> Command Group Command Description admin status Output health and version information. admin token Fetch and output an access token. admin acl list List access conditions. admin acl inspect Inspect an access condition. admin acl create Create an access condition. admin acl update Update an access condition. admin acl delete Delete access conditions. admin acl review Review grants for a given account. admin client list List client accounts. admin client secret Get or generate a new secret for a client account. admin client open Open clients in the browser. admin metrics get Get sample data of a metric. admin metrics inspect Inspect a metric. admin metrics list List metrics for a specific job. admin migration list List migration recipies. admin migration execute Execute needed migration recipes. admin store showcase Create showcase data. admin store bootstrap Update/Import or remove bootstrap data. admin store export Backup all knowledge graphs to a ZIP archive. admin store import Restore graphs from a ZIP archive. admin store migrate Migrate configuration resources to the current version. admin user list List user accounts. admin user create Create a user account. admin user update Update a user account. admin user delete Delete a user account. admin user password Change the password of a user account. admin user open Open user in the browser. admin workspace export Export the complete workspace (all projects) to a ZIP file. admin workspace import Import the workspace from a file. admin workspace reload Reload the workspace from the backend. admin workspace python install Install a python package to the workspace. admin workspace python uninstall Uninstall a python packages from the workspace. admin workspace python list List installed python packages. admin workspace python list-plugins List installed workspace plugins. admin workspace python open Open a package pypi.org page in the browser. admin workspace python reload Reload / Register all installed plugins. config list List configured connections. config edit Edit the user-scope configuration file. config get Get the value of a known cmemc configuration key. config eval Export all configuration values of a configuration for evaluation. dataset list List available datasets. dataset delete Delete datasets. dataset download Download the resource file of a dataset. dataset upload Upload a resource file to a dataset. dataset inspect Display metadata of a dataset. dataset create Create a dataset. dataset open Open datasets in the browser. dataset update Update a dataset. dataset resource list List available file resources. dataset resource delete Delete file resources. dataset resource inspect Display all metadata of a file resource. dataset resource usage Display all usage data of a file resource. graph count Count triples in graph(s). graph tree (Hidden) Deprecated: use \u2018graph imports tree\u2019 instead. graph list List accessible graphs. graph export Export graph(s) as NTriples to stdout (-), file or directory. graph delete Delete graph(s) from the store. graph import Import graph(s) to the store. graph open Open / explore a graph in the browser. graph imports tree Show graph tree(s) of the imports statement hierarchy. graph imports list List accessible graph imports statements. graph imports create Add statement to import a TO_GRAPH into a FROM_GRAPH. graph imports delete Delete statement to import a TO_GRAPH into a FROM_GRAPH. graph validation execute Start a new validation process. graph validation list List running and finished validation processes. graph validation inspect List and inspect errors found with a validation process. graph validation cancel Cancel a running validation process. graph validation export Export a report of finished validations. project open Open projects in the browser. project list List available projects. project export Export projects to files. project import Import a project from a file or directory. project delete Delete projects. project create Create projects. project reload Reload projects from the workspace provider. project variable list List available project variables. project variable get Get the value or other data of a project variable. project variable delete Delete a project variable. project variable create Create a new project variable. project variable update Update data of an existing project variable. query execute Execute queries which are loaded from files or the query catalog. query list List available queries from the catalog. query open Open queries in the editor of the query catalog in your browser. query status Get status information of executed and running queries. query replay Re-execute queries from a replay file. query cancel Cancel a running query. vocabulary open Open / explore a vocabulary graph in the browser. vocabulary list Output a list of vocabularies. vocabulary install Install one or more vocabularies from the catalog. vocabulary uninstall Uninstall one or more vocabularies. vocabulary import Import a turtle file as a vocabulary. vocabulary cache update Reload / updates the data integration cache for a vocabulary. vocabulary cache list Output the content of the global vocabulary cache. workflow execute Execute workflow(s). workflow io Execute a workflow with file input/output. workflow list List available workflow. workflow status Get status information of workflow(s). workflow open Open a workflow in your browser. workflow scheduler open Open scheduler(s) in the browser. workflow scheduler list List available scheduler. workflow scheduler inspect Display all metadata of a scheduler. workflow scheduler disable Disable scheduler(s). workflow scheduler enable Enable scheduler(s).","tags":["Reference","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/","title":"admin Command Group","text":"<p>Import bootstrap data, backup/restore workspace or get status.</p> <p>This command group consists of commands for setting up and configuring eccenca Corporate Memory.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/#admin-status","title":"admin status","text":"<p>Output health and version information.</p> Usage<pre><code>$ cmemc admin status [OPTIONS]\n</code></pre> <p>This command outputs version and health information of the selected deployment. If the version information cannot be retrieved, UNKNOWN is shown.</p> <p>Additionally, this command informs you in one of these cases: (1) A warning, if the target version of your cmemc client is newer than the version of your backend. (2) A warning, if the ShapeCatalog has a different version than your Explore component. (3) An error, if your Corporate Memory license is expired (grace period). (4) A warning, if your Graph DB license will expire in less than a month.</p> <p>To get status information of all configured deployments use this command in combination with parallel.</p> Example<pre><code>$ cmemc config list | parallel --ctag cmemc -c {} admin status\n</code></pre> Options <pre><code>--key TEXT                     Get only specific key(s) from the status /\n                               info output. There are two special keys\n                               available: 'all' will list all available keys\n                               in the table, 'overall.healthy' with result\n                               in  UP in case all health flags are UP as\n                               well (otherwise DOWN).\n--exit-1 [never|error|always]  Specify, when this command returns with exit\n                               code 1. Available options are 'never' (exit 0\n                               on errors and warnings), 'error' (exit 1 on\n                               errors, exit 0 on warnings), 'always' (exit 1\n                               on errors and warnings).  [default: never]\n--enforce-table                A single value with --key will be returned as\n                               plain text instead of a table with one row\n                               and the header. This default behaviour allows\n                               for more easy integration with scripts. This\n                               flag enforces the use of tabular output, even\n                               for single row tables.\n--raw                          Outputs combined raw JSON output of the\n                               health/info endpoints.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/#admin-token","title":"admin token","text":"<p>Fetch and output an access token.</p> Usage<pre><code>$ cmemc admin token [OPTIONS]\n</code></pre> <p>This command can be used to check for correct authentication as well as to use the token with wget / curl or similar standard tools:</p> Example<pre><code>$ curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug\n</code></pre> <p>Please be aware that this command can reveal secrets which you might not want to be present in log files or on the screen.</p> Options <pre><code>--raw       Outputs raw JSON. Note that this option will always try to fetch\n            a new JSON token response. In case you are working with\n            OAUTH_GRANT_TYPE=prefetched_token, this may lead to an error.\n--decode    Decode the access token and outputs the raw JSON. Note that the\n            access token is only decoded and esp. not validated.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/","title":"admin acl Command Group","text":"<p>List, create, delete and modify and review access conditions.</p> <p>With this command group, you can manage and inspect access conditions in eccenca Corporate Memory. Access conditions are identified by a URL. They grant access to knowledge graphs or actions to user or groups.</p>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/#admin-acl-list","title":"admin acl list","text":"<p>List access conditions.</p> Usage<pre><code>$ cmemc admin acl list [OPTIONS]\n</code></pre> <p>This command retrieves and lists all access conditions, which are manageable by the current account.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only URIs. This is useful for piping the IDs into other\n            commands.\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/#admin-acl-inspect","title":"admin acl inspect","text":"<p>Inspect an access condition.</p> Usage<pre><code>$ cmemc admin acl inspect [OPTIONS] ACCESS_CONDITION_ID\n</code></pre> <p>Note</p> <p>access conditions can be listed by using the <code>acl list</code> command.</p> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/#admin-acl-create","title":"admin acl create","text":"<p>Create an access condition.</p> Usage<pre><code>$ cmemc admin acl create [OPTIONS]\n</code></pre> <p>With this command, new access conditions can be created.</p> <p>An access condition captures information about WHO gets access to WHAT. In order to specify WHO gets access, use the <code>--user</code> and / or <code>--group</code> options. In order to specify WHAT an account get access to, use the <code>--read-graph</code>, <code>--write-graph</code> and <code>--action</code> options.`</p> <p>In addition to that, you can specify a name, a description and an ID (all optional).</p> <p>A special case are dynamic access conditions, based on a SPARQL query: Here you have to provide a query with the projection variables <code>user</code>, <code>group</code> <code>readGraph</code> and <code>writeGraph</code> to create multiple grants at once. You can either provide a query file or a query URL from the query catalog.</p> <p>Note</p> <p>Queries for dynamic access conditions are copied into the ACL, so changing the query in the query catalog does not change it in the access condition.</p> Example<pre><code>$ cmemc admin acl create --group local-users --write-graph https://example.org/\n</code></pre> Options <pre><code>--user TEXT         A specific user account required by the access\n                    condition.\n--group TEXT        A membership in a user group required by the access\n                    condition.\n--read-graph TEXT   Grants read access to a graph.\n--write-graph TEXT  Grants write access to a graph (includes read access).\n--action TEXT       Grants usage permissions to an action / functionality.\n--query TEXT        Dynamic access condition query (file or the query\n                    catalog IRI).\n--id TEXT           An optional ID (will be an UUID otherwise).\n--name TEXT         A optional name.\n--description TEXT  An optional description.\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/#admin-acl-update","title":"admin acl update","text":"<p>Update an access condition.</p> Usage<pre><code>$ cmemc admin acl update [OPTIONS] ACCESS_CONDITION_ID\n</code></pre> <p>Given an access condition URL, you can change specific options to new values.</p> Options <pre><code>--name TEXT         A optional name.\n--description TEXT  An optional description.\n--user TEXT         A specific user account required by the access\n                    condition.\n--group TEXT        A membership in a user group required by the access\n                    condition.\n--read-graph TEXT   Grants read access to a graph.\n--write-graph TEXT  Grants write access to a graph (includes read access).\n--action TEXT       Grants usage permissions to an action / functionality.\n--query TEXT        Dynamic access condition query (file or the query\n                    catalog IRI).\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/#admin-acl-delete","title":"admin acl delete","text":"<p>Delete access conditions.</p> Usage<pre><code>$ cmemc admin acl delete [OPTIONS] [ACCESS_CONDITION_IDS]...\n</code></pre> <p>This command deletes existing access conditions from the account.</p> <p>Note</p> <p>Access conditions can be listed by using the <code>cmemc admin acs list</code> command.</p> Options <pre><code>-a, --all   Delete all access conditions. This is a dangerous option, so use\n            it with care.\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/acl/#admin-acl-review","title":"admin acl review","text":"<p>Review grants for a given account.</p> Usage<pre><code>$ cmemc admin acl review [OPTIONS] USER\n</code></pre> <p>This command has two working modes: (1) You can review the access conditions of an actual account, (2) You can review the access conditions of an imaginary account with a set of freely added groups (what-if-scenario).</p> <p>The output of the command is a list of grants the account has based on your input and all access conditions loaded in the store. In addition to that, some metadata of the account is shown.</p> Options <pre><code>--raw         Outputs raw JSON.\n--group TEXT  Add groups to the review request (what-if-scenario).\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/client/","title":"admin client Command Group","text":"<p>List client accounts, get or generate client account secrets.</p> <p>This command group is an opinionated interface to the Keycloak realm of your Corporate Memory instance. In order to be able to use the commands in this group, the configured cmemc connection account needs to be equipped with the <code>manage-clients</code> role in the used realm.</p> <p>Client accounts are identified by a client ID which is unique in the scope of the used realm.</p> <p>In case your Corporate Memory deployment does not use the default deployment layout, the following additional config variables can be used in your connection configuration: <code>KEYCLOAK_BASE_URI</code> defaults to <code>{</code>CMEM_BASE_URI<code>}/auth</code> and locates your Keycloak deployment; <code>KEYCLOAK_REALM_ID</code> defaults to <code>cmem</code> and identifies the used realm.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/client/#admin-client-list","title":"admin client list","text":"<p>List client accounts.</p> Usage<pre><code>$ cmemc admin client list [OPTIONS]\n</code></pre> <p>Outputs a list of client accounts, which can be used to get an overview as well as a reference for the other commands of the <code>admin client</code> command group.</p> <p>Note</p> <p>The list command only outputs clients which have a client secret.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only Client ID. This is useful for piping the IDs into\n            other commands.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/client/#admin-client-secret","title":"admin client secret","text":"<p>Get or generate a new secret for a client account.</p> Usage<pre><code>$ cmemc admin client secret [OPTIONS] CLIENT_ID\n</code></pre> <p>This command retrieves or generates a new secret for a client account from a realm.</p> Options <pre><code>--generate  Generate a new secret\n--output    Display client secret\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/client/#admin-client-open","title":"admin client open","text":"<p>Open clients in the browser.</p> Usage<pre><code>$ cmemc admin client open [CLIENT_IDS]...\n</code></pre> <p>With this command, you can open a client in the keycloak web interface in your browser.</p> <p>The command accepts multiple client IDs which results in opening multiple browser tabs.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/","title":"admin metrics Command Group","text":"<p>List and get metrics.</p> <p>This command group consists of commands for reading and listing internal monitoring metrics of eccenca Corporate Memory. A deployment consists of multiple jobs (e.g. DP, DI), which provide multiple metric families for an endpoint.</p> <p>Each metric family can consist of different samples identified by labels with a name and a value (dimensions). A metric has a specific type (counter, gauge, summary and histogram) and additional metadata.</p> <p>Please have a look at https://prometheus.io/docs/concepts/data_model/ for further details.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/#admin-metrics-get","title":"admin metrics get","text":"<p>Get sample data of a metric.</p> Usage<pre><code>$ cmemc admin metrics get [OPTIONS] METRIC_ID\n</code></pre> <p>A metric of a specific job is identified by a metric ID. Possible metric IDs of a job can be retrieved with the <code>metrics list</code> command. A metric can contain multiple samples. These samples are distinguished by labels (name and value).</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  A set of label name/value pairs in order to filter\n                         the samples of the requested metric family. Each\n                         metric has a different set of labels with different\n                         values. In order to get a list of possible label\n                         names and values, use the command without this\n                         option. The label names are then shown as column\n                         headers and label values as cell values of this\n                         column.\n--enforce-table          A single sample value will be returned as plain\n                         text instead of the normal table. This allows for\n                         more easy integration with scripts. This flag\n                         enforces the use of tabular output, even for single\n                         row tables.\n--raw                    Outputs raw prometheus sample classes.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/#admin-metrics-inspect","title":"admin metrics inspect","text":"<p>Inspect a metric.</p> Usage<pre><code>$ cmemc admin metrics inspect [OPTIONS] METRIC_ID\n</code></pre> <p>This command outputs the data of a metric. The first table includes basic metadata about the metric. The second table includes sample labels and values.</p> Options <pre><code>--raw       Outputs raw JSON of the table data.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/#admin-metrics-list","title":"admin metrics list","text":"<p>List metrics for a specific job.</p> Usage<pre><code>$ cmemc admin metrics list [OPTIONS]\n</code></pre> <p>For each metric, the output table shows the metric ID, the type of the metric, a count of how many labels (label names) are describing the samples (L) and a count of how many samples are currently available for a metric (S).</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter metrics by one of the following filter names\n                         and a corresponding value: job, name, type, id.\n--id-only                Lists metric identifier only. This is useful for\n                         piping the IDs into other commands.\n--raw                    Outputs (sorted) JSON dict, parsed from the metrics\n                         API output.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/migration/","title":"admin migration Command Group","text":"<p>List and apply migration recipes.</p> <p>With this command group, you can check your instance for needed migration activities as well as apply them to your data.</p> <p>Beside an ID and a description, migration recipes have the following metadata: \u2018First Version\u2019 is the first Corporate Memory version, where this recipe is maybe applicable. The recipe will never be applied to a version below this version. \u2018Tags\u2019 is a classification of the recipe with regard to the target data, it migrates.</p> <p>The following tags are important: <code>system</code> recipes target data structures which are needed to run the most basic functionality properly. These recipes can and should be applied after each version upgrade. <code>user</code> recipes can change user and / or customizing data. <code>acl</code> recipes migrate access condition data. <code>shapes</code> recipes migrate shape data. <code>config</code> recipes migrate configuration data.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/migration/#admin-migration-list","title":"admin migration list","text":"<p>List migration recipies.</p> Usage<pre><code>$ cmemc admin migration list [OPTIONS]\n</code></pre> <p>This command lists all available migration recipies</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter migration recipes by one of the following\n                         filter names and a corresponding value: id,\n                         description, first_version, tag.\n--id-only                Lists only IDs. This is useful for piping the IDs\n                         into other commands.\n--raw                    Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/migration/#admin-migration-execute","title":"admin migration execute","text":"<p>Execute needed migration recipes.</p> Usage<pre><code>$ cmemc admin migration execute [OPTIONS] [MIGRATION_ID]\n</code></pre> <p>This command executes one or more migration recipes. Each recipe has a check method to determine if a migration is needed. In addition to that, the current component version needs to match the specified first-last-version range of the recipe.</p> <p>Recipes are executed ordered by first_version.</p> <p>Here are some argument examples, in order to see how to use this command: execute <code>--all</code> <code>--test-only</code> will list all needed migrations (but not execute them), execute <code>--filter</code> tag system will apply all migrations which target system data, execute bootstrap-data will apply bootstrap-data migration if needed.</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter migration recipes by one of the following\n                         filter names and a corresponding value: id,\n                         description, first_version, tag.\n-a, --all                Execute all needed migrations.\n--test-only              Only test, do not execute migrations.\n--id-only                Lists only recipe identifier.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/","title":"admin store Command Group","text":"<p>Import, export and bootstrap the knowledge graph store.</p> <p>This command group consist of commands to administrate the knowledge graph store as a whole.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-showcase","title":"admin store showcase","text":"<p>Create showcase data.</p> Usage<pre><code>$ cmemc admin store showcase [OPTIONS]\n</code></pre> <p>This command creates a showcase scenario of multiple graphs including integration graphs, shapes, statement annotations, etc.</p> <p>Note</p> <p>There is currently no deletion mechanism for the showcase data, and you need to remove the showcase graphs manually (or just remove all graphs).</p> Options <pre><code>--scale INTEGER  The scale factor provides a way to set the target size of\n                 the scenario. A value of 10 results in around 40k triples,\n                 a value of 50 in around 350k triples.  [default: 10]\n--create         Delete old showcase data if present and create new showcase\n                 databased on the given scale factor.\n--delete         Delete existing showcase data if present.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-bootstrap","title":"admin store bootstrap","text":"<p>Update/Import or remove bootstrap data.</p> Usage<pre><code>$ cmemc admin store bootstrap [OPTIONS]\n</code></pre> <p>Use <code>--import</code> to import the bootstrap data needed for managing shapes and configuration objects. This will remove the old data first.</p> <p>Use <code>--remove</code> to delete bootstrap data.</p> <p>Note</p> <p>The removal of existing bootstrap data will search for resources which are flagged with the isSystemResource property.</p> <p>Note</p> <p>The import part of this command is equivalent to the \u2018bootstrap-data\u2019 migration recipe</p> Options <pre><code>--import    Delete existing bootstrap data if present and import bootstrap\n            data which was delivered with Corporate Memory.\n--remove    Delete existing bootstrap data if present.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-export","title":"admin store export","text":"<p>Backup all knowledge graphs to a ZIP archive.</p> Usage<pre><code>$ cmemc admin store export [OPTIONS] [BACKUP_FILE]\n</code></pre> <p>The backup file is a ZIP archive containing all knowledge graphs (one Turtle file + configuration file per graph).</p> <p>This command will create lots of load on the server. It can take a long time to complete.</p> Options <pre><code>--replace   Replace existing files. This is a dangerous option, so use it\n            with care.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-import","title":"admin store import","text":"<p>Restore graphs from a ZIP archive.</p> Usage<pre><code>$ cmemc admin store import BACKUP_FILE\n</code></pre> <p>The backup file is a ZIP archive containing all knowledge graphs  (one Turtle file + configuration file per graph).</p> <p>The command will load a single backup ZIP archive into the triple store by replacing all graphs with the content of the Turtle files in the archive and deleting all graphs which are not in the archive.</p> <p>This command will create lots of load on the server. It can take a long time to complete. The backup file will be transferred to the server, then unzipped and imported graph by graph. After the initial transfer the network connection is not used anymore and may be closed by proxies. This does not mean that the import failed.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-migrate","title":"admin store migrate","text":"<p>Migrate configuration resources to the current version.</p> Usage<pre><code>$ cmemc admin store migrate [OPTIONS]\n</code></pre> <p>This command serves two purposes: (1) When invoked without an option, it lists all migrateable configuration resources. (2) When invoked with the <code>--workspaces</code> option, it migrates the workspace configurations to the current version.</p> Options <pre><code>--workspaces  Migrate workspace configurations to the current version.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/","title":"admin user Command Group","text":"<p>List, create, delete and modify user accounts.</p> <p>This command group is an opinionated interface to the Keycloak realm of your Corporate Memory instance. In order to be able to manage user data, the configured cmemc connection account needs to be equipped with the <code>manage-users</code> role in the used realm.</p> <p>User accounts are identified by a username which unique in the scope of the used realm.</p> <p>In case your Corporate Memory deployment does not use the default deployment layout, the following additional config variables can be used in your connection configuration: <code>KEYCLOAK_BASE_URI</code> defaults to <code>/auth</code> on <code>CMEM_BASE_URI</code> and locates your Keycloak deployment; <code>KEYCLOAK_REALM_ID</code> defaults to <code>cmem</code> and identifies the used realm.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-list","title":"admin user list","text":"<p>List user accounts.</p> Usage<pre><code>$ cmemc admin user list [OPTIONS]\n</code></pre> <p>Outputs a list of user accounts, which can be used to get an overview as well as a reference for the other commands of the <code>admin user</code> command group.</p> Options <pre><code>--raw                    Outputs raw JSON.\n--filter &lt;TEXT TEXT&gt;...  Filter users by one of the following filter names\n                         and a corresponding value: enabled, email,\n                         username.\n--id-only                Lists only username. This is useful for piping the\n                         IDs into other commands.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-create","title":"admin user create","text":"<p>Create a user account.</p> Usage<pre><code>$ cmemc admin user create USERNAME\n</code></pre> <p>This command creates a new user account.</p> <p>Note</p> <p>The created user account has no metadata such as personal data or group assignments. In order to add these details to a user account, use the <code>admin user update</code> command.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-update","title":"admin user update","text":"<p>Update a user account.</p> Usage<pre><code>$ cmemc admin user update [OPTIONS] USERNAME\n</code></pre> <p>This command updates metadata and group assignments of a user account.</p> <p>For each data value, a separate option needs to be used. All options can be combined in a single execution.</p> <p>Note</p> <p>In order to assign a group to a user account, the group need to be added or imported to the realm upfront.</p> Options <pre><code>--first-name TEXT      Set a new first name.\n--last-name TEXT       Set a new last name.\n--email TEXT           Set a new email.\n--assign-group TEXT    Assign a group.\n--unassign-group TEXT  Unassign a group.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-delete","title":"admin user delete","text":"<p>Delete a user account.</p> Usage<pre><code>$ cmemc admin user delete USERNAME\n</code></pre> <p>This command deletes a user account from a realm.</p> <p>Note</p> <p>The deletion of a user account does not delete the assigned groups of this account, only the assignments to these groups.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-password","title":"admin user password","text":"<p>Change the password of a user account.</p> Usage<pre><code>$ cmemc admin user password [OPTIONS] USERNAME\n</code></pre> <p>With this command, the password of a user account can be changed. The default execution mode of this command is an interactive prompt which asks for the password (twice). In order automate password changes, you can use the <code>--value</code> option.</p> <p>Warning</p> <p>Providing passwords on the command line can be dangerous (e.g. due to a potential exploitation in the shell history). A suggested more save way for automation is to provide the password in a variable first (e.g. with <code>NEW_PASS=$(pwgen -1 40)</code>) and use it afterward in the cmemc call: <code>cmemc admin user password max --value ${NEW_PASS}</code>.</p> Options <pre><code>--value TEXT      With this option, the new password can be set in a non-\n                  interactive way.\n--temporary       If enabled, the user must change the password on next\n                  login.\n--request-change  If enabled, will send a email to user to reset the\n                  password.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-open","title":"admin user open","text":"<p>Open user in the browser.</p> Usage<pre><code>$ cmemc admin user open [USERNAMES]...\n</code></pre> <p>With this command, you can open a user in the keycloak console in your browser to change them.</p> <p>The command accepts multiple usernames which results in opening multiple browser tabs.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/","title":"admin workspace Command Group","text":"<p>Import, export and reload the project workspace.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/#admin-workspace-export","title":"admin workspace export","text":"<p>Export the complete workspace (all projects) to a ZIP file.</p> Usage<pre><code>$ cmemc admin workspace export [OPTIONS] [FILE]\n</code></pre> <p>Depending on the requested export type, this ZIP file contains either one Turtle file per project (type <code>rdfTurtle</code>) or a substructure of resource files and XML descriptions (type <code>xmlZip</code>).</p> <p>The file name is optional and will be generated with by the template if absent.</p> Options <pre><code>--replace                     Replace existing files. This is a dangerous\n                              option, so use it with care.\n--type TEXT                   Type of the exported workspace file.\n                              [default: xmlZip]\n-t, --filename-template TEXT  Template for the export file name. Possible\n                              placeholders are (Jinja2): {{connection}}\n                              (from the --connection option) and {{date}}\n                              (the current date as YYYY-MM-DD). The file\n                              suffix will be appended. Needed directories\n                              will be created.  [default:\n                              {{date}}-{{connection}}.workspace]\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/#admin-workspace-import","title":"admin workspace import","text":"<p>Import the workspace from a file.</p> Usage<pre><code>$ cmemc admin workspace import [OPTIONS] FILE\n</code></pre> Options <pre><code>--type TEXT  Type of the exported workspace file.  [default: xmlZip]\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/#admin-workspace-reload","title":"admin workspace reload","text":"<p>Reload the workspace from the backend.</p> Usage<pre><code>$ cmemc admin workspace reload\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/","title":"admin workspace python Command Group","text":"<p>List, install, or uninstall python packages.</p> <p>Python packages are used to extend the Build (DataIntegration) workspace with python plugins. To get a list of installed packages, execute the list command.</p> <p>Warning</p> <p>Installing packages from unknown sources is not recommended. Plugins are not verified for malicious code.</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-install","title":"admin workspace python install","text":"<p>Install a python package to the workspace.</p> Usage<pre><code>$ cmemc admin workspace python install PACKAGE\n</code></pre> <p>This command is essentially a <code>pip install</code> in the remote python environment.</p> <p>You can install a package by uploading a source distribution .tar.gz file, by uploading a build distribution .whl file, or by specifying a package name, i.e., a pip requirement specifier with a package name available on pypi.org (e.g. <code>requests==2.27.1</code>).</p> <p>Note</p> <p>The tab-completion of this command lists only public packages from pypi.org and not from additional or changed python package repositories you may have configured on the server.</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-uninstall","title":"admin workspace python uninstall","text":"<p>Uninstall a python packages from the workspace.</p> Usage<pre><code>$ cmemc admin workspace python uninstall [OPTIONS] [PACKAGE_NAME]...\n</code></pre> <p>This command is essentially a <code>pip uninstall</code> in the remote python environment.</p> Options <pre><code>-a, --all   This option removes all installed packages from the system,\n            leaving only the pre-installed mandatory packages in the\n            environment.\n</code></pre>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-list","title":"admin workspace python list","text":"<p>List installed python packages.</p> Usage<pre><code>$ cmemc admin workspace python list [OPTIONS]\n</code></pre> <p>This command is essentially a <code>pip list</code> in the remote python environment.</p> <p>It outputs a table of python package identifiers with version information.</p> Options <pre><code>--raw        Outputs raw JSON.\n--id-only    Lists only package identifier. This is useful for piping the\n             IDs into other commands.\n--available  Instead of listing installed packages, this option lists\n             installable packages from pypi.org, which are prefixed with\n             'cmem-plugin-' and so are most likely Corporate Memory plugin\n             packages.\n</code></pre>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-list-plugins","title":"admin workspace python list-plugins","text":"<p>List installed workspace plugins.</p> Usage<pre><code>$ cmemc admin workspace python list-plugins [OPTIONS]\n</code></pre> <p>This commands lists all discovered plugins.</p> <p>Note</p> <p>The plugin discovery is restricted to package prefix (<code>cmem-</code>).</p> Options <pre><code>--raw              Outputs raw JSON.\n--id-only          Lists only plugin identifier.\n--package-id-only  Lists only plugin package identifier.\n</code></pre>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-open","title":"admin workspace python open","text":"<p>Open a package pypi.org page in the browser.</p> Usage<pre><code>$ cmemc admin workspace python open PACKAGE\n</code></pre> <p>With this command, you can open the pypi.org page of a published package in your browser. From there, you can follow links, review the version history as well as the origin of the package, and read the provided documentation.</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-reload","title":"admin workspace python reload","text":"<p>Reload / Register all installed plugins.</p> Usage<pre><code>$ cmemc admin workspace python reload\n</code></pre> <p>This command will register all installed plugins into the Build (DataIntegration) workspace. This command is useful, when you are installing packages into the Build Python environment without using the provided cmemc commands (e.g. by mounting a prepared filesystem in the docker container).</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/","title":"config Command Group","text":"<pre><code>List and edit configs as well as get config values.\n\nConfigurations are identified by the section identifier in the\nconfig file. Each configuration represent a Corporate Memory deployment\nwith its specific access method as well as credentials.\n\nA minimal configuration which uses client credentials has the following\nentries:\n\n\b\n[example.org]\nCMEM_BASE_URI=https://cmem.example.org/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=my-secret-account-pass\n\nNote that OAUTH_GRANT_TYPE can be either client_credentials, password or\nprefetched_token.\n\nIn addition to that, the following config parameters can be used as well:\n\n\b\nSSL_VERIFY=False    - for ignoring certificate issues (not recommended)\nDP_API_ENDPOINT=URL - to point to a non-standard Explore backend (DataPlatform) location\nDI_API_ENDPOINT=URL - to point to a non-standard Build (DataIntegration) location\nOAUTH_TOKEN_URI=URL - to point to an external IdentityProvider location\nOAUTH_USER=username - only if OAUTH_GRANT_TYPE=password\nOAUTH_PASSWORD=password - only if OAUTH_GRANT_TYPE=password\nOAUTH_ACCESS_TOKEN=token - only if OAUTH_GRANT_TYPE=prefetched_token\n\nIn order to get credential information from an external process, you can\nuse the parameter OAUTH_PASSWORD_PROCESS, OAUTH_CLIENT_SECRET_PROCESS and\nOAUTH_ACCESS_TOKEN_PROCESS to set up an external executable.\n\n\b\nOAUTH_CLIENT_SECRET_PROCESS=/path/to/getpass.sh\nOAUTH_PASSWORD_PROCESS=[\"getpass.sh\", \"parameter1\", \"parameter2\"]\n\nThe credential executable can use the cmemc environment for fetching the\ncredential (e.g. CMEM_BASE_URI and OAUTH_USER).\nIf the credential executable is not given with a full path, cmemc\nwill look into your environment PATH for something which can be executed.\nThe configured process needs to return the credential on the first line\nof stdout. In addition to that, the process needs to exit with exit\ncode 0 (without failure). There are examples available in the online\nmanual.\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-list","title":"config list","text":"<p>List configured connections.</p> Usage<pre><code>$ cmemc config list\n</code></pre> <p>This command lists all configured connections from the currently used config file.</p> <p>The connection identifier can be used with the <code>--connection</code> option in order to use a specific Corporate Memory instance.</p> <p>In order to apply commands on more than one instance, you need to use typical unix gear such as xargs or parallel.</p> Example<pre><code>$ cmemc config list | xargs -I % sh -c 'cmemc -c % admin status'\n</code></pre> Example<pre><code>$ cmemc config list | parallel --jobs 5 cmemc -c {} admin status\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-edit","title":"config edit","text":"<p>Edit the user-scope configuration file.</p> Usage<pre><code>$ cmemc config edit\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-get","title":"config get","text":"<p>Get the value of a known cmemc configuration key.</p> Usage<pre><code>$ cmemc config get {CMEM_BASE_URI|SSL_VERIFY|REQUESTS_CA_BUNDLE|DP_API_END\n             POINT|DI_API_ENDPOINT|KEYCLOAK_BASE_URI|KEYCLOAK_REALM_ID|OAUTH_T\n             OKEN_URI|OAUTH_GRANT_TYPE|OAUTH_USER|OAUTH_PASSWORD|OAUTH_CLIENT_\n             ID|OAUTH_CLIENT_SECRET|OAUTH_ACCESS_TOKEN}\n</code></pre> <p>In order to automate processes such as fetching custom API data from multiple Corporate Memory instances, this command provides a way to get the value of a cmemc configuration key for the selected deployment.</p> Example<pre><code>$ curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug\n</code></pre> <p>The commands return with exit code 1 if the config key is not used in the current configuration.</p>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-eval","title":"config eval","text":"<p>Export all configuration values of a configuration for evaluation.</p> Usage<pre><code>$ cmemc config eval [OPTIONS]\n</code></pre> <p>The output of this command is suitable to be used by a shell\u2019s <code>eval</code> command. It will output the complete configuration as <code>export key=\"value\"</code> statements, which allow for the preparation of a shell environment.</p> Example<pre><code>$ eval $(cmemc -c my config eval)\n</code></pre> <p>Warning</p> <p>Please be aware that credential details are shown in cleartext with this command.</p> Options <pre><code>--unset     Instead of exporting all configuration keys, this option will\n            unset all keys.\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/","title":"dataset Command Group","text":"<p>List, create, delete, inspect, up-/download or open datasets.</p> <p>This command group allows for managing workspace datasets as well as dataset file resources. Datasets can be created and deleted. File resources can be uploaded and downloaded. Details of dataset parameters can be listed with inspect.</p> <p>Datasets are identified by a combined key of the <code>PROJECT_ID</code> and a <code>DATASET_ID</code> (e.g: <code>my-project:my-dataset</code>).</p> <p>Note</p> <p>To get a list of existing datasets, execute the <code>dataset list</code> command or use tab-completion.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-list","title":"dataset list","text":"<p>List available datasets.</p> Usage<pre><code>$ cmemc dataset list [OPTIONS]\n</code></pre> <p>Output and filter a list of available datasets. Each dataset is listed with its ID, type and label.</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter datasets based on metadata. First parameter\n                         can be one of the following values: project, regex,\n                         tag, type. The options for the second parameter\n                         depend on the first parameter.\n--raw                    Outputs raw JSON objects of the dataset search API\n                         response.\n--id-only                Lists only dataset IDs and no labels or other\n                         metadata. This is useful for piping the IDs into\n                         other cmemc commands.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-delete","title":"dataset delete","text":"<p>Delete datasets.</p> Usage<pre><code>$ cmemc dataset delete [OPTIONS] [DATASET_IDS]...\n</code></pre> <p>This command deletes existing datasets in integration projects from Corporate Memory. The corresponding dataset resources will not be deleted.</p> <p>Warning</p> <p>Datasets will be deleted without prompting.</p> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Options <pre><code>-a, --all                Delete all datasets. This is a dangerous option, so\n                         use it with care.\n--project TEXT           In combination with the '--all' flag, this option\n                         allows for deletion of all datasets of a certain\n                         project. The behaviour is similar to the 'dataset\n                         list --project' command.\n--filter &lt;TEXT TEXT&gt;...  Delete datasets based on metadata. First parameter\n                         --filter CHOICE can be one of ['project', 'regex',\n                         'tag', 'type']. The second parameter is based on\n                         CHOICE.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-download","title":"dataset download","text":"<p>Download the resource file of a dataset.</p> Usage<pre><code>$ cmemc dataset download [OPTIONS] DATASET_ID OUTPUT_PATH\n</code></pre> <p>This command downloads the file resource of a dataset to your local file system or to standard out (<code>-</code>). Note that this is not possible for dataset types such as Knowledge Graph (<code>eccencaDataplatform</code>) or SQL endpoint (<code>sqlEndpoint</code>).</p> <p>Without providing an output path, the output file name will be the same as the remote file resource.</p> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Options <pre><code>--replace   Replace existing files. This is a dangerous option, so use it\n            with care!\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-upload","title":"dataset upload","text":"<p>Upload a resource file to a dataset.</p> Usage<pre><code>$ cmemc dataset upload DATASET_ID INPUT_PATH\n</code></pre> <p>This command uploads a file to a dataset. The content of the uploaded file replaces the remote file resource. The name of the remote file resource will not be changed.</p> <p>Warning</p> <p>If the remote file resource is used in more than one dataset, all of these datasets are affected by this command.</p> <p>Warning</p> <p>The content of the uploaded file is not tested, so uploading a JSON file to an XML dataset will result in errors.</p> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Example<pre><code>$ cmemc dataset upload cmem:my-dataset new-file.csv\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-inspect","title":"dataset inspect","text":"<p>Display metadata of a dataset.</p> Usage<pre><code>$ cmemc dataset inspect [OPTIONS] DATASET_ID\n</code></pre> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-create","title":"dataset create","text":"<p>Create a dataset.</p> Usage<pre><code>$ cmemc dataset create [OPTIONS] [DATASET_FILE]\n</code></pre> <p>Datasets are created in projects and can have associated file resources. Each dataset has a type (such as <code>csv</code>) and a list of parameters which can alter or specify the dataset behaviour.</p> <p>To get more information about available dataset types and associated parameters, use the <code>--help-types</code> and <code>--help-parameter</code> options.</p> Example<pre><code>$ cmemc dataset create --project my-project --type csv my-file.csv\n</code></pre> Options <pre><code>-t, --type TEXT                 The dataset type of the dataset to create.\n                                Example types are 'csv','json' and\n                                'eccencaDataPlatform' (-&gt; Knowledge Graph).\n--project TEXT                  The project, where you want to create the\n                                dataset in. If there is only one project in\n                                the workspace, this option can be omitted.\n-p, --parameter &lt;TEXT TEXT&gt;...  A set of key/value pairs. Each dataset type\n                                has different parameters (such as charset,\n                                arraySeparator, ignoreBadLines, ...). In\n                                order to get a list of possible parameter,\n                                use the'--help-parameter' option.\n--replace                       Replace remote file resources in case there\n                                already exists a file with the same name.\n--id TEXT                       The dataset ID of the dataset to create. The\n                                dataset ID will be automatically created in\n                                case it is not present.\n--help-types                    Lists all possible dataset types on given\n                                Corporate Memory instance. Note that this\n                                option already needs access to the instance.\n--help-parameter                Lists all possible (optional and mandatory)\n                                parameter for a dataset type. Note that this\n                                option already needs access to the instance.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-open","title":"dataset open","text":"<p>Open datasets in the browser.</p> Usage<pre><code>$ cmemc dataset open DATASET_IDS...\n</code></pre> <p>With this command, you can open a dataset in the workspace in your browser.</p> <p>The command accepts multiple dataset IDs which results in opening multiple browser tabs.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-update","title":"dataset update","text":"<p>Update a dataset.</p> Usage<pre><code>$ cmemc dataset update [OPTIONS] DATASET_ID\n</code></pre> <p>With this command, you can update the configuration of an existing dataset. Similar to the <code>dataset create</code> command, you need to use configuration key/value pairs on the <code>--parameter</code> option.</p> <p>To get more information about the available configuration parameters on a dataset, use the <code>--help-parameter</code> option.</p> Example<pre><code>$ cmemc dataset update my-project:my-csv -p separator \";\"\n</code></pre> Options <pre><code>-p, --parameter &lt;TEXT TEXT&gt;...  A configuration parameter key/value pair.\n                                Each dataset type has different parameters\n                                (such as charset, arraySeparator,\n                                ignoreBadLines, ...). In order to get a list\n                                of possible parameter, use the'--help-\n                                parameter' option.\n--help-parameter                Lists all possible (optional and mandatory)\n                                configuration parameter for a given dataset.\n                                Note that this option already needs access\n                                to the instance.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/","title":"dataset resource Command Group","text":"<p>List, inspect or delete dataset file resources.</p> <p>File resources are identified by their paths and project IDs.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-list","title":"dataset resource list","text":"<p>List available file resources.</p> Usage<pre><code>$ cmemc dataset resource list [OPTIONS]\n</code></pre> <p>Outputs a table or a list of dataset resources (files).</p> Options <pre><code>--raw                    Outputs raw JSON.\n--id-only                Lists only resource names and no other metadata.\n                         This is useful for piping the IDs into other\n                         commands.\n--filter &lt;TEXT TEXT&gt;...  Filter file resources based on metadata. First\n                         parameter CHOICE can be one of ['project',\n                         'regex']. The second parameter is based on CHOICE,\n                         e.g. a project ID or a regular expression string.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-delete","title":"dataset resource delete","text":"<p>Delete file resources.</p> Usage<pre><code>$ cmemc dataset resource delete [OPTIONS] [RESOURCE_IDS]...\n</code></pre> <p>There are three selection mechanisms: with specific IDs, only those specified resources will be deleted; by using <code>--filter</code>, resources based on the filter type and value will be deleted; using <code>--all</code> will delete all resources.</p> Options <pre><code>--force                  Delete resource even if in use by a task.\n-a, --all                Delete all resources. This is a dangerous option,\n                         so use it with care.\n--filter &lt;TEXT TEXT&gt;...  Filter file resources based on metadata. First\n                         parameter CHOICE can be one of ['project',\n                         'regex']. The second parameter is based on CHOICE,\n                         e.g. a project ID or a regular expression string.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-inspect","title":"dataset resource inspect","text":"<p>Display all metadata of a file resource.</p> Usage<pre><code>$ cmemc dataset resource inspect [OPTIONS] RESOURCE_ID\n</code></pre> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-usage","title":"dataset resource usage","text":"<p>Display all usage data of a file resource.</p> Usage<pre><code>$ cmemc dataset resource usage [OPTIONS] RESOURCE_ID\n</code></pre> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/","title":"graph Command Group","text":"<p>List, import, export, delete, count, tree or open graphs.</p> <p>Graphs are identified by an IRI.</p> <p>Note</p> <p>The get a list of existing graphs, execute the <code>graph list</code> command or use tab-completion.</p>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-count","title":"graph count","text":"<p>Count triples in graph(s).</p> Usage<pre><code>$ cmemc graph count [OPTIONS] [IRIS]...\n</code></pre> <p>This command lists graphs with their triple count. Counts do not include imported graphs.</p> Options <pre><code>-a, --all        Count all graphs\n-s, --summarize  Display only a sum of all counted graphs together\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-tree","title":"graph tree","text":"<p>(Hidden) Deprecated: use \u2018graph imports tree\u2019 instead.</p> Usage<pre><code>$ cmemc graph tree [OPTIONS] [IRIS]...\n</code></pre> Options <pre><code>-a, --all   Show tree of all (readable) graphs.\n--raw       Outputs raw JSON of the graph importTree API response.\n--id-only   Lists only graph identifier (IRIs) and no labels or other\n            metadata. This is useful for piping the IRIs into other\n            commands. The output with this option is a sorted, flat, de-\n            duplicated list of existing graphs.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-list","title":"graph list","text":"<p>List accessible graphs.</p> Usage<pre><code>$ cmemc graph list [OPTIONS]\n</code></pre> Options <pre><code>--raw                      Outputs raw JSON of the graphs list API response.\n--id-only                  Lists only graph identifier (IRIs) and no labels\n                           or other metadata. This is useful for piping the\n                           IRIs into other commands.\n--filter &lt;CHOICE TEXT&gt;...  Filter graphs based on effective access\n                           conditions or import closure. First parameter\n                           CHOICE can be 'access' or 'imported-by'. The\n                           second parameter can be 'readonly' or 'writeable'\n                           in case of 'access' or any readable graph in case\n                           of 'imported-by'.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-export","title":"graph export","text":"<p>Export graph(s) as NTriples to stdout (-), file or directory.</p> Usage<pre><code>$ cmemc graph export [OPTIONS] [IRIS]...\n</code></pre> <p>In case of file export, data from all selected graphs will be concatenated in one file. In case of directory export, .graph and .ttl files will be created for each graph.</p> Options <pre><code>-a, --all                       Export all readable graphs.\n--include-imports               Export selected graph(s) and all graphs\n                                which are imported from these selected\n                                graph(s).\n--include-import-statements     Save graph imports information from other\n                                graphs to the exported graphs and write\n                                *.imports files.\n--create-catalog                In addition to the .ttl and .graph files,\n                                cmemc will create an XML catalog file\n                                (catalog-v001.xml) which can be used by\n                                applications such as Prot\u00e9g\u00e9.\n--output-dir DIRECTORY          Export to this directory.\n--output-file FILE              Export to this file.  [default: -]\n-t, --filename-template TEXT    Template for the export file name(s). Used\n                                together with --output-dir. Possible\n                                placeholders are (Jinja2): {{hash}} - sha256\n                                hash of the graph IRI, {{iriname}} - graph\n                                IRI converted to filename, {{connection}} -\n                                from the --connection option and {{date}} -\n                                the current date as YYYY-MM-DD. The file\n                                suffix will be appended. Needed directories\n                                will be created.  [default: {{hash}}]\n--mime-type [application/n-triples|text/turtle|application/rdf+xml]\n                                Define the requested mime type  [default:\n                                text/turtle]\n--compress [gzip]               Compress the exported graph files.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-delete","title":"graph delete","text":"<p>Delete graph(s) from the store.</p> Usage<pre><code>$ cmemc graph delete [OPTIONS] [IRIS]...\n</code></pre> Options <pre><code>-a, --all                    Delete all writeable graphs.\n--include-imports            Delete selected graph(s) and all writeable\n                             graphs which are imported from these selected\n                             graph(s).\n--include-import-statements  Delete import reference of deleted graphs\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-import","title":"graph import","text":"<p>Import graph(s) to the store.</p> Usage<pre><code>$ cmemc graph import [OPTIONS] INPUT_PATH [IRI]\n</code></pre> <p>If input is a file, content will be uploaded to the graph identified with the IRI.</p> <p>If input is a directory and NO IRI is given, it scans for file-pairs such as <code>xyz.ttl</code> and <code>xyz.ttl.graph</code>, where <code>xyz.ttl</code> is the actual triples file and <code>xyz.ttl.graph</code> contains the graph IRI in the first line: <code>https://mygraph.de/xyz/</code>.</p> <p>If input is a directory AND a graph IRI is given, it scans for all <code>*.ttl</code> files in the directory and imports all content to the graph, ignoring the <code>*.ttl.graph</code> files.</p> <p>If the <code>--replace</code> flag is set, the data in the graphs will be overwritten, if not, it will be added.</p> <p>Note</p> <p>Directories are scanned on the first level only (not recursively).</p> Options <pre><code>--replace                    Replace / overwrite the graph(s), instead of\n                             just adding the triples to the graph.\n--skip-existing              Skip importing a file if the target graph\n                             already exists in the store. Note that the\n                             graph list is fetched once at the beginning of\n                             the process, so that you can still add multiple\n                             files to one single graph (if it does not\n                             exist).\n--include-import-statements  Use *.imports files to re-apply the graph\n                             imports of the imported graphs.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-open","title":"graph open","text":"<p>Open / explore a graph in the browser.</p> Usage<pre><code>$ cmemc graph open IRI\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/imports/","title":"graph imports Command Group","text":"<p>List, create, delete and show graph imports.</p> <p>Graphs are identified by an IRI. Statement imports are managed by creating owl:imports statements such as \u201c<code>FROM_GRAPH</code> owl:imports <code>TO_GRAPH</code>\u201d in the <code>FROM_GRAPH</code>. All statements in the <code>TO_GRAPH</code> are then available in the <code>FROM_GRAPH</code>.</p> <p>Note</p> <p>The get a list of existing graphs, execute the <code>graph list</code> command or use tab-completion.</p>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/imports/#graph-imports-tree","title":"graph imports tree","text":"<p>Show graph tree(s) of the imports statement hierarchy.</p> Usage<pre><code>$ cmemc graph imports tree [OPTIONS] [IRIS]...\n</code></pre> <p>You can output one or more trees of the import hierarchy.</p> <p>Imported graphs which do not exist are shown as <code>[missing: IRI]</code>. Imported graphs which will result in an import cycle are shown as <code>[ignored: IRI]</code>. Each graph is shown with label and IRI.</p> Options <pre><code>-a, --all   Show tree of all (readable) graphs.\n--raw       Outputs raw JSON of the graph importTree API response.\n--id-only   Lists only graph identifier (IRIs) and no labels or other\n            metadata. This is useful for piping the IRIs into other\n            commands. The output with this option is a sorted, flat, de-\n            duplicated list of existing graphs.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/imports/#graph-imports-list","title":"graph imports list","text":"<p>List accessible graph imports statements.</p> Usage<pre><code>$ cmemc graph imports list [OPTIONS]\n</code></pre> <p>Graphs are identified by an IRI. Statement imports are managed by creating owl:imports statements such as \u201c<code>FROM_GRAPH</code> owl:imports <code>TO_GRAPH</code>\u201d in the <code>FROM_GRAPH</code>. All statements in the <code>TO_GRAPH</code> are then available in the <code>FROM_GRAPH</code>.</p> Options <pre><code>--raw                    Outputs raw JSON response.\n--filter &lt;TEXT TEXT&gt;...  Filter imports by one of the following filter names\n                         and a corresponding value: from-graph, to-graph.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/imports/#graph-imports-create","title":"graph imports create","text":"<p>Add statement to import a TO_GRAPH into a FROM_GRAPH.</p> Usage<pre><code>$ cmemc graph imports create FROM_GRAPH TO_GRAPH\n</code></pre> <p>Graphs are identified by an IRI. Statement imports are managed by creating owl:imports statements such as \u201c<code>FROM_GRAPH</code> owl:imports <code>TO_GRAPH</code>\u201d in the <code>FROM_GRAPH</code>. All statements in the <code>TO_GRAPH</code> are then available in the <code>FROM_GRAPH</code>.</p> <p>Note</p> <p>The get a list of existing graphs, execute the <code>graph list</code> command or use tab-completion.</p>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/imports/#graph-imports-delete","title":"graph imports delete","text":"<p>Delete statement to import a TO_GRAPH into a FROM_GRAPH.</p> Usage<pre><code>$ cmemc graph imports delete FROM_GRAPH TO_GRAPH\n</code></pre> <p>Graphs are identified by an IRI. Statement imports are managed by creating owl:imports statements such as \u201c<code>FROM_GRAPH</code> owl:imports <code>TO_GRAPH</code>\u201d in the <code>FROM_GRAPH</code>. All statements in the <code>TO_GRAPH</code> are then available in the <code>FROM_GRAPH</code>.</p> <p>Note</p> <p>The get a list of existing graph imports, execute the <code>graph imports list</code> command or use tab-completion.</p>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/validation/","title":"graph validation Command Group","text":"<p>Validate resources in a graph.</p> <p>This command group is dedicated to the management of resource validation processes. A validation process verifies, that resources in a specific graph are valid according to the node shapes in a shape catalog graph.</p> <p>Note</p> <p>Validation processes are identified with a random ID and can be listed with the <code>graph validation list</code> command. To start or cancel validation processes, use the <code>graph validation execute</code> and <code>graph validation cancel</code> command. To inspect the found violations of a validation process, use the <code>graph validation inspect</code> command.</p>","tags":["KnowledgeGraph","Validation","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/validation/#graph-validation-execute","title":"graph validation execute","text":"<p>Start a new validation process.</p> Usage<pre><code>$ cmemc graph validation execute [OPTIONS] IRI\n</code></pre> <p>Validation is performed on all typed resources of the data / context graph (and its sub-graphs). Each resource is validated against all applicable node shapes from the shape catalog.</p> Options <pre><code>--wait                          Wait until the process is finished. When\n                                using this option without the `--id-only`\n                                flag, it will enable a progress bar and a\n                                summary view.\n--shape-graph TEXT              The shape catalog used for validation.\n                                [default: https://vocab.eccenca.com/shacl/]\n--query TEXT                    SPARQL query to select the resources which\n                                you want to validate from the data graph.\n                                Can be provided as a local file or as a\n                                query catalog IRI. [default: all typed\n                                resources]\n--result-graph TEXT             (Optionally) write the validation results to\n                                a Knowledge Graph. [default: None]\n--replace                       Replace the result graph instead of just\n                                adding the new results. This is a dangerous\n                                option, so use it with care!\n--ignore-graph TEXT             A set of data graph IRIs which are not\n                                queried in the resource selection. This\n                                option is useful for validating only parts\n                                of an integration graph which imports other\n                                graphs.\n--id-only                       Return the validation process identifier\n                                only. This is useful for piping the ID into\n                                other commands.\n--inspect                       Return the list of violations instead of the\n                                summary (includes --wait).\n--polling-interval INTEGER RANGE\n                                How many seconds to wait between status\n                                polls. Status polls are cheap, so a higher\n                                polling interval is most likely not needed.\n                                [default: 1; x&gt;=1]\n</code></pre>","tags":["KnowledgeGraph","Validation","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/validation/#graph-validation-list","title":"graph validation list","text":"<p>List running and finished validation processes.</p> Usage<pre><code>$ cmemc graph validation list [OPTIONS]\n</code></pre> <p>This command provides a filterable table or identifier list of validation processes. The command operates on the process summary and provides some statistics.</p> <p>Note</p> <p>Detailed information on the found violations can be listed with the <code>graph validation inspect</code> command.</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter validation processes by one of the following\n                         filter names and a corresponding value: status,\n                         context-graph, shape-graph, more-resources-than,\n                         more-violations-than, more-violated-resources-than.\n--id-only                List validation process identifier only. This is\n                         useful for piping the IDs into other commands.\n--raw                    Outputs raw JSON of the validation list.\n</code></pre>","tags":["KnowledgeGraph","Validation","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/validation/#graph-validation-inspect","title":"graph validation inspect","text":"<p>List and inspect errors found with a validation process.</p> Usage<pre><code>$ cmemc graph validation inspect [OPTIONS] PROCESS_ID\n</code></pre> <p>This command provides detailed information on the found violations of a validation process.</p> <p>Use the <code>--filter</code> option to limit the output based on different criteria such as constraint name (<code>constraint</code>), origin node shape of the rule (<code>node-shape</code>), or the validated resource (<code>resource</code>).</p> <p>Note</p> <p>Validation processes IDs can be listed with the <code>graph validation list</code> command, or by utilizing the tab completion of this command.</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter violations by one of the following filter\n                         names and a corresponding value: constraint,\n                         severity, resource, node-shape, source.\n--id-only                Return violated resource identifier only. This is\n                         useful for piping the ID into other commands.\n--summary                Outputs the summary of the graph validation instead\n                         of the violations list (not filterable).\n--raw                    Outputs raw JSON of the validation result.\n</code></pre>","tags":["KnowledgeGraph","Validation","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/validation/#graph-validation-cancel","title":"graph validation cancel","text":"<p>Cancel a running validation process.</p> Usage<pre><code>$ cmemc graph validation cancel PROCESS_ID\n</code></pre> <p>Note</p> <p>In order to get the process IDs of all currently running validation processes, use the <code>graph validation list</code> command with the option <code>--filter status running</code>, or utilize the tab completion of this command.</p>","tags":["KnowledgeGraph","Validation","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/validation/#graph-validation-export","title":"graph validation export","text":"<p>Export a report of finished validations.</p> Usage<pre><code>$ cmemc graph validation export [OPTIONS] [PROCESS_IDS]...\n</code></pre> <p>This command exports a jUnit XML or JSON report in order to process them somewhere else (e.g. a CI pipeline).</p> <p>You can export a single report of multiple validation processes.</p> <p>For jUnit XML: Each validation process result will be transformed to a single test suite. All violations of one resource in a result will be collected and attached to a single test case in that test suite.</p> <p>Note</p> <p>Validation processes IDs can be listed with the <code>graph validation list</code> command, or by utilizing the tab completion of this command.</p> Options <pre><code>--output-file FILE      Export the report to this file. Existing files will\n                        be overwritten.  [default: report.xml]\n--exit-1 [never|error]  Specify, when this command returns with exit code 1.\n                        Available options are 'never' (exit 0, even if there\n                        are violations in the reports), 'error' (exit 1 if\n                        there is at least one violation in a report).),\n                        [default: error]\n--format [JSON|XML]     Export either the plain JSON report or a distilled\n                        jUnit XML report.  [default: XML]\n</code></pre>","tags":["KnowledgeGraph","Validation","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/","title":"project Command Group","text":"<p>List, import, export, create, delete or open projects.</p> <p>Projects are identified by a <code>PROJECT_ID</code>.</p> <p>Note</p> <p>To get a list of existing projects, execute the <code>project list</code> command or use tab-completion.</p>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-open","title":"project open","text":"<p>Open projects in the browser.</p> Usage<pre><code>$ cmemc project open PROJECT_IDS...\n</code></pre> <p>With this command, you can open a project in the workspace in your browser to change them.</p> <p>The command accepts multiple project IDs which results in opening multiple browser tabs.</p>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-list","title":"project list","text":"<p>List available projects.</p> Usage<pre><code>$ cmemc project list [OPTIONS]\n</code></pre> <p>Outputs a list of project IDs which can be used as reference for the project create, delete, export and import commands.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only project identifier and no labels or other metadata.\n            This is useful for piping the IDs into other commands.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-export","title":"project export","text":"<p>Export projects to files.</p> Usage<pre><code>$ cmemc project export [OPTIONS] [PROJECT_IDS]...\n</code></pre> <p>Projects can be exported with different export formats. The default type is a zip archive which includes metadata as well as dataset resources. If more than one project is exported, a file is created for each project. By default, these files are created in the current directory with a descriptive name (see <code>--template</code> option default).</p> <p>Note</p> <p>Projects can be listed by using the <code>project list</code> command.</p> <p>You can use the template string to create subdirectories.</p> Example<pre><code>$ cmemc config list | parallel -I% cmemc -c % project export --all -t \"dump/{{connection}}/{{date}}-{{id}}.project\"\n</code></pre> Options <pre><code>-a, --all                     Export all projects.\n--replace                     Replace existing files. This is a dangerous\n                              option, so use it with care.\n--output-dir DIRECTORY        The base directory, where the project files\n                              will be created. If this directory does not\n                              exist, it will be silently created.  [default:\n                              .]\n--type TEXT                   Type of the exported project file(s). Use the\n                              --help-types option or tab completion to see a\n                              list of possible types.  [default: xmlZip]\n-t, --filename-template TEXT  Template for the export file name(s). Possible\n                              placeholders are (Jinja2): {{id}} (the project\n                              ID), {{connection}} (from the --connection\n                              option) and {{date}} (the current date as\n                              YYYY-MM-DD). The file suffix will be appended.\n                              Needed directories will be created.  [default:\n                              {{date}}-{{connection}}-{{id}}.project]\n--extract                     Export projects to a directory structure\n                              instead of a ZIP archive. Note that the\n                              --filename-template option is ignored here.\n                              Instead, a sub-directory per exported project\n                              is created under the output directory. Also\n                              note that not all export types are\n                              extractable.\n--help-types                  Lists all possible export types.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-import","title":"project import","text":"<p>Import a project from a file or directory.</p> Usage<pre><code>$ cmemc project import [OPTIONS] PATH [PROJECT_ID]\n</code></pre> Example<pre><code>$ cmemc project import my_project.zip my_project\n</code></pre> Options <pre><code>--replace   Replace an existing project. This is a dangerous option, so use\n            it with care.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-delete","title":"project delete","text":"<p>Delete projects.</p> Usage<pre><code>$ cmemc project delete [OPTIONS] [PROJECT_IDS]...\n</code></pre> <p>This command deletes existing data integration projects from Corporate Memory.</p> <p>Warning</p> <p>Projects will be deleted without prompting!</p> <p>Note</p> <p>Projects can be listed with the <code>project list</code> command.</p> Options <pre><code>-a, --all   Delete all projects. This is a dangerous option, so use it with\n            care.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-create","title":"project create","text":"<p>Create projects.</p> Usage<pre><code>$ cmemc project create [OPTIONS] PROJECT_IDS...\n</code></pre> <p>This command creates one or more new projects. Existing projects will not be overwritten.</p> <p>Note</p> <p>Projects can be listed by using the <code>project list</code> command.</p> Options <pre><code>--from-transformation TEXT  This option can be used to explicitly create the\n                            link specification, which is internally executed\n                            when using the mapping suggestion of a\n                            transformation task. You need the task ID of the\n                            transformation task.\n--label TEXT                Give the label of the project. You can give more\n                            than one label if you create more than one\n                            project.\n--description TEXT          Give the description of the project. You can\n                            give more than one description if you create\n                            more than one project.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-reload","title":"project reload","text":"<p>Reload projects from the workspace provider.</p> Usage<pre><code>$ cmemc project reload [OPTIONS] [PROJECT_IDS]...\n</code></pre> <p>This command reloads all tasks of a project from the workspace provider. This is similar to the <code>workspace reload</code> command, but for a single project only.</p> <p>Note</p> <p>You need this in case you changed project data externally or loaded a project which uses plugins which are not installed yet. In this case, install the plugin(s) and reload the project afterward.</p> <p>Warning</p> <p>Depending on the size your datasets esp. your Knowledge Graphs, reloading a project can take a long time to re-create the path caches.</p> Options <pre><code>-a, --all   Reload all projects\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/variable/","title":"project variable Command Group","text":"<p>List, create, delete or get data from project variables.</p> <p>Project variables can be used in dataset and task parameters, and in the template transform operator. Variables are either based on a static value or based on a template. They may use templates that access globally configured variables or other preceding variables from the same project.</p> <p>Variables are identified by a <code>VARIABLE_ID</code>. To get a list of existing variables, execute the list command or use tab-completion. The <code>VARIABLE_ID</code> is a concatenation of a <code>PROJECT_ID</code> and a <code>VARIABLE_NAME</code>, such as <code>my-project:my-variable</code>.</p>","tags":["Variables","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/variable/#project-variable-list","title":"project variable list","text":"<p>List available project variables.</p> Usage<pre><code>$ cmemc project variable list [OPTIONS]\n</code></pre> <p>Outputs a table or a list of project variables.</p> Options <pre><code>--raw                    Outputs raw JSON.\n--id-only                Lists only variables names and no other metadata.\n                         This is useful for piping the IDs into other\n                         commands.\n--filter &lt;TEXT TEXT&gt;...  Filter variables based on metadata. First parameter\n                         CHOICE can be one of ['project', 'regex']. The\n                         second parameter is based on CHOICE, e.g. a project\n                         ID or a regular expression string.\n</code></pre>","tags":["Variables","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/variable/#project-variable-get","title":"project variable get","text":"<p>Get the value or other data of a project variable.</p> Usage<pre><code>$ cmemc project variable get [OPTIONS] VARIABLE_ID\n</code></pre> <p>Use the <code>--key</code> option to specify which information you want to get.</p> <p>Note</p> <p>Only the <code>value</code> key is always available on a project variable. Static value variables have no <code>template</code> key, and the <code>description</code> key is optional for both types of variables.</p> Options <pre><code>--key [value|template|description]\n                                Specify the name of the value you want to\n                                get.  [default: value]\n--raw                           Outputs raw json.\n</code></pre>","tags":["Variables","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/variable/#project-variable-delete","title":"project variable delete","text":"<p>Delete a project variable.</p> Usage<pre><code>$ cmemc project variable delete VARIABLE_ID\n</code></pre> <p>Note</p> <p>You can not delete a variable which is used by another (template based) variable. In order to do so, delete the template based variable first.</p>","tags":["Variables","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/variable/#project-variable-create","title":"project variable create","text":"<p>Create a new project variable.</p> Usage<pre><code>$ cmemc project variable create [OPTIONS] VARIABLE_NAME\n</code></pre> <p>Variables need to be created with a value or a template (not both). In addition to that, a project ID and a name are mandatory.</p> Example<pre><code>$ cmemc project variable create my_var --project my_project --value abc\n</code></pre> <p>Note</p> <p>cmemc is currently not able to manage the order of the variables in a project. This means you have to create plain value variables in advance, before you can create template based variables, which access these values.</p> Options <pre><code>--value TEXT        The value of the new project variable.\n--template TEXT     The template of the new project variable. You can use\n                    Jinja template syntax, e.g. use '{{global.myVar}}' for\n                    accessing global variables, or '{{project.myVar}}' for\n                    accessing variables from the same project.\n--description TEXT  The optional description of the new project variable.\n--project TEXT      The project, where you want to create the variable in.\n                    If there is only one project in the workspace, this\n                    option can be omitted.\n</code></pre>","tags":["Variables","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/variable/#project-variable-update","title":"project variable update","text":"<p>Update data of an existing project variable.</p> Usage<pre><code>$ cmemc project variable update [OPTIONS] VARIABLE_ID\n</code></pre> <p>With this command you can update the value or the template, as well as the description of a project variable.</p> <p>Note</p> <p>If you update the template of a static variable, it will be transformed to a template based variable. If you want to change the value of a template based variable, an error will be shown.</p> Options <pre><code>--value TEXT        The new value of the project variable.\n--template TEXT     The new template of the project variable. You can use\n                    Jinja template syntax, e.g. use '{{global.myVar}}' for\n                    accessing global variables, or '{{project.myVar}}' for\n                    accessing variables from the same project.\n--description TEXT  The new description of the project variable.\n</code></pre>","tags":["Variables","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/","title":"query Command Group","text":"<p>List, execute, get status or open SPARQL queries.</p> <p>Queries are identified either by a file path, a URI from the query catalog or a shortened URI (qname, using a default namespace).</p> <p>One or more queries can be executed one after the other with the execute command. With open command you can jump to the query editor in your browser.</p> <p>Queries can use a mustache like syntax to specify placeholder for parameter values (e.g. <code>{{resourceUri}}</code>). These parameter values need to be given as well, before the query can be executed (use the<code>-p</code> option).</p> <p>Note</p> <p>In order to get a list of queries from the query catalog, execute the <code>query list</code> command or use tab-completion.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-execute","title":"query execute","text":"<p>Execute queries which are loaded from files or the query catalog.</p> Usage<pre><code>$ cmemc query execute [OPTIONS] QUERIES...\n</code></pre> <p>Queries are identified either by a file path, a URI from the query catalog, or a shortened URI (qname, using a default namespace).</p> <p>If multiple queries are executed one after the other, the first failing query stops the whole execution chain.</p> <p>Limitations: All optional parameters (e.g. accept, base64, \u2026) are provided for ALL queries in an execution chain. If you need different parameters for each query in a chain, run cmemc multiple times and use the logical operators &amp;&amp; and || of your shell instead.</p> Options <pre><code>--catalog-graph TEXT            The used query catalog graph.  [default:\n                                https://ns.eccenca.com/data/queries/]\n--accept TEXT                   Accept header for the HTTP request(s).\n                                Setting this to 'default' means that cmemc\n                                uses an appropriate output for terminals.\n                                [default: default]\n--no-imports                    Graphs which include other graphs (using\n                                owl:imports) will be queried as merged\n                                overall-graph. This flag disables this\n                                default behaviour. The flag has no effect on\n                                update queries.\n--base64                        Enables base64 encoding of the query\n                                parameter for the SPARQL requests (the\n                                response is not touched). This can be useful\n                                in case there is an aggressive firewall\n                                between cmemc and Corporate Memory.\n-p, --parameter &lt;TEXT TEXT&gt;...  In case of a parameterized query\n                                (placeholders with the '{{key}}' syntax),\n                                this option fills all placeholder with a\n                                given value before the query is\n                                executed.Pairs of placeholder/value need to\n                                be given as a tuple 'KEY VALUE'. A key can\n                                be used only once.\n--limit INTEGER                 Override or set the LIMIT in the executed\n                                SELECT query. Note that this option will\n                                never give you more results than the LIMIT\n                                given in the query itself.\n--offset INTEGER                Override or set the OFFSET in the executed\n                                SELECT query.\n--distinct                      Override the SELECT query by make the result\n                                set DISTINCT.\n--timeout INTEGER               Set max execution time for query evaluation\n                                (in milliseconds).\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-list","title":"query list","text":"<p>List available queries from the catalog.</p> Usage<pre><code>$ cmemc query list [OPTIONS]\n</code></pre> <p>Outputs a list of query URIs which can be used as reference for the query execute command.</p> Options <pre><code>--catalog-graph TEXT  The used query catalog graph.  [default:\n                      https://ns.eccenca.com/data/queries/]\n--id-only             Lists only query identifier and no labels or other\n                      metadata. This is useful for piping the ids into other\n                      cmemc commands.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-open","title":"query open","text":"<p>Open queries in the editor of the query catalog in your browser.</p> Usage<pre><code>$ cmemc query open [OPTIONS] QUERIES...\n</code></pre> <p>With this command, you can open (remote) queries from the query catalog in the query editor in your browser (e.g. in order to change them). You can also load local query files into the query editor, in order to import them into the query catalog.</p> <p>The command accepts multiple query URIs or files which results in opening multiple browser tabs.</p> Options <pre><code>--catalog-graph TEXT  The used query catalog graph.  [default:\n                      https://ns.eccenca.com/data/queries/]\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-status","title":"query status","text":"<p>Get status information of executed and running queries.</p> Usage<pre><code>$ cmemc query status [OPTIONS] [QUERY_ID]\n</code></pre> <p>With this command, you can access the latest executed SPARQL queries on the Explore backend (DataPlatform). These queries are identified by UUIDs and listed ordered by starting timestamp.</p> <p>You can filter queries based on status and runtime in order to investigate slow queries. In addition to that, you can get the details of a specific query by using the ID as a parameter.</p> Options <pre><code>--id-only                Lists only query identifier and no labels or other\n                         metadata. This is useful for piping the ids into\n                         other cmemc commands.\n--raw                    Outputs raw JSON response of the query status API.\n--filter &lt;TEXT TEXT&gt;...  Filter queries by one of the following filter names\n                         and a corresponding value: status, type, trace-id,\n                         user, graph, slower-than, regex.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-replay","title":"query replay","text":"<p>Re-execute queries from a replay file.</p> Usage<pre><code>$ cmemc query replay [OPTIONS] REPLAY_FILE\n</code></pre> <p>This command reads a <code>REPLAY_FILE</code> and re-executes the logged queries. A <code>REPLAY_FILE</code> is a JSON document which is an array of JSON objects with at least a key <code>queryString</code> holding the query text OR a key <code>iri</code> holding the IRI of the query in the query catalog. It can be created with the <code>query status</code> command.</p> Example<pre><code>$ query status --raw &gt; replay.json\n</code></pre> <p>The output of this command shows basic query execution statistics.</p> <p>The queries are executed one after another in the order given in the input <code>REPLAY_FILE</code>. Query placeholders / parameters are ignored. If a query results in an error, the duration is not counted.</p> <p>The optional output file is the same JSON document which is used as input, but each query object is annotated with an additional <code>replays</code> object, which is an array of JSON objects which hold values for the replay|loop|run IDs, start and end time as well as duration and other data.</p> Options <pre><code>--raw               Output the execution statistic as raw JSON.\n--loops INTEGER     Number of loops to run the replay file.  [default: 1]\n--wait INTEGER      Number of seconds to wait between query executions.\n                    [default: 0]\n--output-file FILE  Save the optional output to this file. Input and output\n                    of the command can be the same file. The output is\n                    written at the end of a successful command execution.\n                    The output can be stdout ('-') - in this case, the\n                    execution statistic output is oppressed.\n--run-label TEXT    Optional label of this replay run.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-cancel","title":"query cancel","text":"<p>Cancel a running query.</p> Usage<pre><code>$ cmemc query cancel QUERY_ID\n</code></pre> <p>With this command, you can cancel a running query. Depending on the backend triple store, this will result in a broken result stream (stardog, neptune and virtuoso) or a valid result stream with incomplete results (graphdb)</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/","title":"vocabulary Command Group","text":"<p>List, (un-)install, import or open vocabs / manage cache.</p>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-open","title":"vocabulary open","text":"<p>Open / explore a vocabulary graph in the browser.</p> Usage<pre><code>$ cmemc vocabulary open IRI\n</code></pre> <p>Vocabularies are identified by their graph IRI. Installed vocabularies can be listed with the <code>vocabulary list</code> command.</p>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-list","title":"vocabulary list","text":"<p>Output a list of vocabularies.</p> Usage<pre><code>$ cmemc vocabulary list [OPTIONS]\n</code></pre> <p>Vocabularies are graphs (see <code>graph</code> command group) which consists of class and property descriptions.</p> Options <pre><code>--id-only                       Lists only vocabulary identifier (IRIs) and\n                                no labels or other metadata. This is useful\n                                for piping the ids into other cmemc\n                                commands.\n--filter [all|installed|installable]\n                                Filter list based on status.  [default:\n                                installed]\n--raw                           Outputs raw JSON.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-install","title":"vocabulary install","text":"<p>Install one or more vocabularies from the catalog.</p> Usage<pre><code>$ cmemc vocabulary install [OPTIONS] [IRIS]...\n</code></pre> <p>Vocabularies are identified by their graph IRI. Installable vocabularies can be listed with the vocabulary list command.</p> Options <pre><code>-a, --all   Install all vocabularies from the catalog.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-uninstall","title":"vocabulary uninstall","text":"<p>Uninstall one or more vocabularies.</p> Usage<pre><code>$ cmemc vocabulary uninstall [OPTIONS] [IRIS]...\n</code></pre> <p>Vocabularies are identified by their graph IRI. Already installed vocabularies can be listed with the vocabulary list command.</p> Options <pre><code>-a, --all   Uninstall all installed vocabularies.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-import","title":"vocabulary import","text":"<p>Import a turtle file as a vocabulary.</p> Usage<pre><code>$ cmemc vocabulary import [OPTIONS] FILE\n</code></pre> <p>With this command, you can import a local ontology file as a named graph and create a corresponding vocabulary catalog entry.</p> <p>The uploaded ontology file is analysed locally in order to discover the named graph and the prefix declaration. This requires an OWL ontology description which correctly uses the <code>vann:preferredNamespacePrefix</code> and <code>vann:preferredNamespaceUri</code> properties.</p> Options <pre><code>--namespace &lt;TEXT TEXT&gt;...  In case the imported vocabulary file does not\n                            include a preferred namespace prefix, you can\n                            manually add a namespace prefix with this\n                            option. Example: --namespace ex\n                            https://example.org/\n--replace                   Replace (overwrite) existing vocabulary, if\n                            present.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/cache/","title":"vocabulary cache Command Group","text":"<p>List und update the vocabulary cache.</p>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/cache/#vocabulary-cache-update","title":"vocabulary cache update","text":"<p>Reload / updates the data integration cache for a vocabulary.</p> Usage<pre><code>$ cmemc vocabulary cache update [OPTIONS] [IRIS]...\n</code></pre> Options <pre><code>-a, --all   Update cache for all installed vocabularies.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/cache/#vocabulary-cache-list","title":"vocabulary cache list","text":"<p>Output the content of the global vocabulary cache.</p> Usage<pre><code>$ cmemc vocabulary cache list [OPTIONS]\n</code></pre> Options <pre><code>--id-only   Lists only vocabulary term identifier (IRIs) and no labels or\n            other metadata. This is useful for piping the ids into other\n            cmemc commands.\n--raw       Outputs raw JSON.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/","title":"workflow Command Group","text":"<p>List, execute, status or open (io) workflows.</p> <p>Workflows are identified by a <code>WORKFLOW_ID</code>. The get a list of existing workflows, execute the list command or use tab-completion. The <code>WORKFLOW_ID</code> is a concatenation of a <code>PROJECT_ID</code> and a <code>TASK_ID</code>, such as <code>my-project:my-workflow</code>.</p>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-execute","title":"workflow execute","text":"<p>Execute workflow(s).</p> Usage<pre><code>$ cmemc workflow execute [OPTIONS] [WORKFLOW_IDS]...\n</code></pre> <p>With this command, you can start one or more workflows at the same time or in a sequence, depending on the result of the predecessor.</p> <p>Executing a workflow can be done in two ways: Without <code>--wait</code> just sends the starting signal and does not look for the workflow and its result (fire and forget). Starting workflows in this way, starts all given workflows at the same time.</p> <p>The optional <code>--wait</code> option starts the workflows in the same way, but also polls the status of a workflow until it is finished. In case of an error of a workflow, the next workflow is not started.</p> Options <pre><code>-a, --all                       Execute all available workflows.\n--wait                          Wait until workflows are completed.\n--progress                      Wait until workflows are completed and show\n                                a progress bar.\n--polling-interval INTEGER RANGE\n                                How many seconds to wait between status\n                                polls. Status polls are cheap, so a higher\n                                polling interval is most likely not needed.\n                                [default: 1; 0&lt;=x&lt;=60]\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-io","title":"workflow io","text":"<p>Execute a workflow with file input/output.</p> Usage<pre><code>$ cmemc workflow io [OPTIONS] WORKFLOW_ID\n</code></pre> <p>With this command, you can execute a workflow that uses replaceable datasets as input, output or for configuration. Use the input parameter to feed data into the workflow. Likewise, use output for retrieval of the workflow result. Workflows without a replaceable dataset will throw an error.</p> <p>Note</p> <p>Regarding the input dataset configuration - the following rules apply: If autoconfig is enabled (\u2018\u2013autoconfig\u2019, the default), the dataset configuration is guessed. If autoconfig is disabled (\u2018\u2013no-autoconfig\u2019) and the type of the dataset file is the same as the replaceable dataset in the workflow, the configuration from this dataset is copied. If autoconfig is disabled and the type of the dataset file is different from the replaceable dataset in the workflow, the default config is used.</p> Options <pre><code>-i, --input FILE                From which file the input is taken. If the\n                                workflow has no defined variable input\n                                dataset, this option is not allowed.\n-o, --output FILE               To which file the result is written to. Use\n                                '-' in order to output the result to stdout.\n                                If the workflow has no defined variable\n                                output dataset, this option is not allowed.\n                                Please note that the io command will not\n                                warn you on overwriting existing output\n                                files.\n--input-mimetype [application/x-plugin-file|application/x-plugin-file|application/x-plugin-csv|application/x-plugin-json|application/x-plugin-xml|application/x-plugin-text|application/x-plugin-text|application/x-plugin-excel|application/x-plugin-multiCsv|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/json|application/xml|text/csv|application/octet-stream|guess]\n                                Which input format should be processed: If\n                                not given, cmemc will try to guess the mime\n                                type based on the file extension or will\n                                fail.\n--output-mimetype [application/x-plugin-file|application/x-plugin-file|application/x-plugin-csv|application/x-plugin-json|application/x-plugin-xml|application/x-plugin-text|application/x-plugin-text|application/x-plugin-excel|application/x-plugin-multiCsv|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/x-plugin-binaryFile|application/json|application/xml|application/n-triples|application/vnd.openxmlformats-officedocument.spreadsheetml.sheet|text/csv|application/octet-stream|guess]\n                                Which output format should be requested: If\n                                not given, cmemc will try to guess the mime\n                                type based on the file extension or will\n                                fail. In case of an output to stdout, a\n                                default mime type will be used (JSON).\n--autoconfig / --no-autoconfig  Setup auto configuration of input datasets,\n                                e.g. in order to process CSV files with\n                                semicolon- instead of comma-separation.\n                                [default: autoconfig]\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-list","title":"workflow list","text":"<p>List available workflow.</p> Usage<pre><code>$ cmemc workflow list [OPTIONS]\n</code></pre> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  List workflows based on metadata. First parameter\n                         --filter CHOICE can be one of ['io', 'project',\n                         'regex', 'tag']. The second parameter is based on\n                         CHOICE.\n--id-only                Lists only workflow identifier and no labels or\n                         other metadata. This is useful for piping the IDs\n                         into other commands.\n--raw                    Outputs raw JSON objects of workflow task search\n                         API response.\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-status","title":"workflow status","text":"<p>Get status information of workflow(s).</p> Usage<pre><code>$ cmemc workflow status [OPTIONS] [WORKFLOW_IDS]...\n</code></pre> Options <pre><code>--project TEXT                  The project, from which you want to list the\n                                workflows. Project IDs can be listed with\n                                the 'project list' command.\n--raw                           Output raw JSON info.\n--filter [Idle|Not executed|Finished|Cancelled|Failed|Successful|Canceling|Running|Waiting]\n                                Show only workflows of a specific status.\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-open","title":"workflow open","text":"<p>Open a workflow in your browser.</p> Usage<pre><code>$ cmemc workflow open WORKFLOW_ID\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/","title":"workflow scheduler Command Group","text":"<p>List, inspect, enable/disable or open scheduler.</p> <p>Schedulers execute workflows in specified intervals. They are identified with a <code>SCHEDULER_ID</code>. To get a list of existing schedulers, execute the list command or use tab-completion.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-open","title":"workflow scheduler open","text":"<p>Open scheduler(s) in the browser.</p> Usage<pre><code>$ cmemc workflow scheduler open [OPTIONS] SCHEDULER_IDS...\n</code></pre> <p>With this command, you can open a scheduler in the workspace in your browser to change it.</p> <p>The command accepts multiple scheduler IDs which results in opening multiple browser tabs.</p> Options <pre><code>--workflow  Instead of opening the scheduler page, open the page of the\n            scheduled workflow.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-list","title":"workflow scheduler list","text":"<p>List available scheduler.</p> Usage<pre><code>$ cmemc workflow scheduler list [OPTIONS]\n</code></pre> <p>Outputs a table or a list of scheduler IDs which can be used as reference for the scheduler commands.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only task identifier and no labels or other metadata. This\n            is useful for piping the IDs into other commands.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-inspect","title":"workflow scheduler inspect","text":"<p>Display all metadata of a scheduler.</p> Usage<pre><code>$ cmemc workflow scheduler inspect [OPTIONS] SCHEDULER_ID\n</code></pre> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-disable","title":"workflow scheduler disable","text":"<p>Disable scheduler(s).</p> Usage<pre><code>$ cmemc workflow scheduler disable [OPTIONS] [SCHEDULER_IDS]...\n</code></pre> <p>The command accepts multiple scheduler IDs which results in disabling them one after the other.</p> Options <pre><code>-a, --all   Disable all scheduler.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-enable","title":"workflow scheduler enable","text":"<p>Enable scheduler(s).</p> Usage<pre><code>$ cmemc workflow scheduler enable [OPTIONS] [SCHEDULER_IDS]...\n</code></pre> <p>The command accepts multiple scheduler IDs which results in enabling them one after the other.</p> Options <pre><code>-a, --all   Enable all scheduler.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/","title":"Configuration","text":"<p>In order to work with cmemc, you have to configure it according to your needs.</p> <ul> <li> <p> File-based Configuration</p> <p>The most common way to configure cmemc is with a central configuration file.</p> </li> <li> <p> Environment-based Configuration</p> <p>In addition to configuration files, cmemc can be widely configured and parameterized with environment variables.</p> </li> <li> <p> Completion Setup</p> <p>Setting up command completion is optional but highly recommended and will greatly speed up your cmemc terminal sessions.</p> </li> <li> <p> Security Considerations</p> <p>cmemc can be configured to fetch your credentials from external processes, such as password stores. In addition to that, cmemc can work with custom certificates.</p> </li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/","title":"Certificate handling and SSL verification","text":"","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/#introduction","title":"Introduction","text":"<p>In a reasonable production deployment, all client-accessible Corporate Memory APIs will be securely available as HTTPS endpoints. This document clarifies how to deal with certificates. cmemc will validate the certificates of your HTTPS endpoints and indicate validation errors. If the certificates of your Corporate Memory deployment are based on a common and publicly available Certificate Authority (such as Let\u2019s Encrypt), cmemc is able to validate your certificates out of the box.</p> <p>However, in some cases you need to do one of the following:</p> <ul> <li>provide your own certificates CA bundle in order to allow cmem to trust your servers</li> <li>disable SSL verification entirely (for debugging and testing purpose only)</li> </ul>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/#provide-your-own-ca-bundle","title":"Provide your own CA bundle","text":"<p>cmemc will validate all used certificates of your HTTPS API endpoints by using a built-in CA bundle that comes from python\u2019s certifi package.</p> <p>Certifi is a carefully curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the identity of TLS hosts.</p> <p>If you need to configure a custom CA bundle to use with a specific connection, you can do so by using the <code>REQUESTS_CA_BUNDLE</code> key in the config or as an environment variable.</p> <p>You can validate which CA bundle is used by switching on debugging (<code>\u2013debug</code>) and watch for a CA bundle debug line (here, line 10).</p> using the debug mode to watch for the CA bundle<pre><code>$ cmemc --debug -c ssltest.eccenca.com graph list\n[2020-03-11 17:50:59.135898] Set config to /home/user/Library/Application Support/cmemc/config.ini\n[2020-03-11 17:50:59.136284] Config loaded: /home/user/Library/Application Support/cmemc/config.ini\n[2020-03-11 17:50:59.137476] Use connection config: ssltest.eccenca.com\n[2020-03-11 17:50:59.137564] CMEM_BASE_URI set by config to https://ssltest.eccenca.com\n[2020-03-11 17:50:59.137611] REQUESTS_CA_BUNDLE set by config to cacert.pem\n[2020-03-11 17:50:59.137718] OAUTH_GRANT_TYPE set by config to client_credentials\n[2020-03-11 17:50:59.137760] OAUTH_CLIENT_ID set by config to cmem-service-account\n[2020-03-11 17:50:59.137804] OAUTH_CLIENT_SECRET set by config\n[2020-03-11 17:50:59.137978] CA bundle loaded from /home/user/cacert.pem\n...\n</code></pre> <p>The CA bundle must be available in PEM format. You can use the openssl command line tool to fetch all certificates from an HTTPS URL and create a PEM CA Bundle out of it.</p> <p>Here is an example line producing the cacert.pem file used in the example above:</p> <pre><code>$ openssl s_client -showcerts -connect ssltest.eccenca.com:443 &lt;/dev/null 2&gt;/dev/null | openssl x509 -outform PEM &gt;cacert.pem\n$ cat cacert.pem\n-----BEGIN CERTIFICATE-----\nMIIFyzCCA7MCFDoiAY9Ry8dfH0rS/rINUb6inlvGMA0GCSqGSIb3DQEBCwUAMIGh\n[...]\nmiGId7jMXd24bpfYZSiniC0+SHiCwEmzN818Ss9aIMChymAnV3RRB/UqKLlOMnA=\n-----END CERTIFICATE-----\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/#disabling-ssl-verification-at-all","title":"Disabling SSL Verification at all","text":"<p>You can also disable SSL Verification completely by setting the <code>SSL_VERIFY</code> key in the config or environment to <code>false</code>.</p> <p>However, this will lead to warnings: <pre><code>$ cmemc -c ssltest.eccenca.com graph list\nSSL verification is disabled (SSL_VERIFY=False).\n...\n</code></pre></p>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/completion-setup/","title":"Command-Line Completion","text":"","tags":["cmemc","Video"]},{"location":"automate/cmemc-command-line-interface/configuration/completion-setup/#introduction","title":"Introduction","text":"<p>In case you are using bash or zsh as your terminal shell, you should enable Command-line tab completion for cmemc.</p> <p>Tab completion is a powerful feature and will save you a lot of typing work. Furthermore, it will help you to learn the different commands, parameters and options and will auto-complete parameter values taken live from your Corporate Memory instance (such as graph IRIs, project IDs, etc.).</p> <p></p> <p>We suggest using zsh so you can take advantage of its advanced menu-completion feature.</p>","tags":["cmemc","Video"]},{"location":"automate/cmemc-command-line-interface/configuration/completion-setup/#installation","title":"Installation","text":"<p>Info</p> <p>The installation commands for the completion recently changed. Use the following lines for the completion setup of cmemc &gt;= 23.3. If using an older version, look at the old documenation.</p> <p>In order to enable tab completion with zsh run the following command:</p> completion setup for zsh<pre><code>$ eval \"$(_CMEMC_COMPLETE=zsh_source cmemc)\"\n</code></pre> <p>To enable the interactive menu as seen above in zsh run the following command:</p> interactive menu for zsh<pre><code>$ zstyle ':completion:*' menu select\n</code></pre> <p>In order to enable tab completion with bash run the following command:</p> completion setup for bash<pre><code>$ eval \"$(_CMEMC_COMPLETE=bash_source cmemc)\"\n</code></pre> <p>You may want to add this line to your <code>.bashrc</code> or <code>.zshrc</code>.</p>","tags":["cmemc","Video"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/","title":"Environment-based Configuration","text":"","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#introduction","title":"Introduction","text":"<p>In addition to using configuration files, cmemc can also be widely configured and parameterized with environment variables.</p> <p>Typical use cases for when you may want to do this include:</p> <ul> <li>set a default connection (see below)</li> <li>enable session-wide debugging output</li> <li>control cmemc with variables from a calling process</li> <li>avoid having client and user credentials lying around in a file</li> </ul> <p>There are two major categories of environment variables you can use.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#environment-variables-for-configuration","title":"Environment variables for configuration","text":"<p>For these variables the rules are simple: You can use any variable from the config file in the same way as an environment variable.</p> <p>The following commands provide the same result as given in the basic example for a config file:</p> <pre><code>$ export CMEM_BASE_URI=http://localhost/\n$ export OAUTH_GRANT_TYPE=client_credentials\n$ export OAUTH_CLIENT_ID=cmem-service-account\n$ export OAUTH_CLIENT_SECRET=...\n</code></pre> <p>Info</p> <p>When you combine file-based and environment-based configuration, the config file always overwrites the environment.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#environment-variables-for-parameters-or-options","title":"Environment variables for parameters or options","text":"<p>The general pattern for parameter and option settings via environment variables is:</p> <ul> <li>all variables start with the prefix <code>CMEMC_</code></li> <li>command group and command follow the prefix in uppercase and separated by <code>_</code></li> <li>the option is in uppercase at the end.</li> <li>The naming scheme is: <code>CMEM[_&lt;COMMAND-GROUP&gt;_&lt;COMMAND&gt;][_&lt;OPTION&gt;]</code></li> </ul> <p>The next sections demonstrate this pattern with examples.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#example-set-a-default-connection","title":"Example: Set a default connection","text":"<p>We first run a cmemc command via command line parameter:</p> <pre><code>$ cmemc --config-file cmemc.ini --connection mycmem graph list --raw\n[\n  {\n    \"iri\": \"https://ns.eccenca.com/data/userinfo/\",\n... more JSON output ...\n</code></pre> <p>As a next step, we replace all connection parameters with environment variables:</p> <pre><code>$ export CMEMC_CONFIG_FILE=cmemc.ini\n$ export CMEMC_CONNECTION=mycmem\n</code></pre> <p>This alone allows us to save a lot of typing for a series of commands on the same Corporate Memory instance.</p> <pre><code>$ cmemc graph list --raw\n[... same output as above ...]\n</code></pre> <p>However, you can also pre-define command options in the same way:</p> <pre><code>$ export CMEMC_GRAPH_LIST_RAW=true\n</code></pre> <p>Again, the same command but <code>--raw</code> is set per default.</p> <pre><code>$ cmemc graph list\n[... same output as above ...]\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#example-enable-session-wide-debugging-output","title":"Example: enable session wide debugging output","text":"<p>Since there is a top level <code>--debug</code> option, the corresponding variable name is <code>CMEMC_DEBUG</code>:</p> <pre><code>$ export CMEMC_DEBUG=true\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#configuration-environment-export-from-the-config-file","title":"Configuration environment export from the config file","text":"<p>Beginning with v21.11, cmemc can export a configuration environment from a configuration file to set up an environment for later use with the <code>config eval</code> command.</p> <pre><code>$ cmemc -c my-cmem.example.org config eval\nexport CMEM_BASE_URI=\"https://my-cmem.example.org\"\nexport DI_API_ENDPOINT=\"https://my-cmem.example.org/dataintegration\"\nexport DP_API_ENDPOINT=\"https://my-cmem.example.org/dataplatform\"\nunset OAUTH_ACCESS_TOKEN\nexport OAUTH_CLIENT_ID=\"cmem-service-account\"\nexport OAUTH_CLIENT_SECRET=\"...\"\nexport OAUTH_GRANT_TYPE=\"client_credentials\"\nunset OAUTH_PASSWORD\nexport OAUTH_TOKEN_URI=\"https://my-cmem.example.org/auth/realms/cmem/protocol/openid-connect/token\"\nunset OAUTH_USER\nexport REQUESTS_CA_BUNDLE=\".../certifi/cacert.pem\"\nexport SSL_VERIFY=\"True\"\n</code></pre> <p>This can be used to export a full <code>config.env</code> or to <code>eval</code> it in an environment for other processes:</p> <pre><code>$ cmemc -c my-cmem.example.org config eval &gt; config.env\n$ eval $(cmemc -c my-cmem.example.org config eval)\n</code></pre> <p>Please note that the following command has the same effect but needs the <code>cmemc.ini</code> for evaluating the <code>config</code> values for the config section <code>my-cmem.example.org</code>:</p> <pre><code>$ export CMEMC_CONNECTION=\"my-cmem.example.org\"\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/","title":"File-based Configuration","text":"","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#introduction","title":"Introduction","text":"<p>This page documents how to configure cmemc via configuration files.</p> <p>cmemc looks for a default configuration file in a location depending on your operating system:</p> <ul> <li>For Linux, this is <code>$HOME/.config/cmemc/config.ini</code><sup>1</sup>.</li> <li>For Windows, this is <code>%APPDATA%\\cmemc\\config.ini</code>.</li> <li>For MacOS, this is <code>$HOME/Library/Application Support/cmemc/config.ini</code>.</li> </ul> <p>If you need to change this location and want to use another config file, you have two options:</p> <ul> <li>run cmemc with the <code>--config-file path/to/your/config.ini</code> option</li> <li>set a new config file with the environment variable <code>CMEMC_CONFIG_FILE</code></li> </ul> <p>However, once you start cmemc the first time without any command or option, it will create an empty configuration file at this location and will output a general introduction.</p> First cmemc run \u2026 <pre><code>$ cmemc\nEmpty config created: /home/user/.config/cmemc/config.ini\nUsage: cmemc [OPTIONS] COMMAND [ARGS]...\n\n  eccenca Corporate Memory Control (cmemc).\n\n  cmemc is the eccenca Corporate Memory Command Line Interface (CLI).\n\n  Available commands are grouped by affecting resource type (such as graph,\n  project and query). Each command and group has a separate --help screen\n  for detailed documentation. In order to see possible commands in a group,\n  simply execute the group command without further parameter (e.g. cmemc\n  project).\n\n  If your terminal supports colors, these coloring rules are applied: Groups\n  are colored in white; Commands which change data are colored in red; all\n  other commands as well as options are colored in green.\n\n  Please also have a look at the cmemc online documentation:\n\n                      https://eccenca.com/go/cmemc\n\n  cmemc is \u00a9 2023 eccenca GmbH, licensed under the Apache License 2.0.\n\nOptions:\n  -c, --connection TEXT  Use a specific connection from the config file.\n  --config-file FILE     Use this config file instead of the default one.\n                         [default: /Users/seebi/Library/Application\n                         Support/cmemc/config.ini]\n\n  -q, --quiet            Suppress any non-error info messages.\n  -d, --debug            Output debug messages and stack traces after errors.\n  --version              Show the version and exit.\n  -h, --help             Show this message and exit.\n\nCommands:\n  admin       Import bootstrap data, backup/restore workspace or get status.\n  config      List and edit configs as well as get config values.\n  dataset     List, create, delete, inspect, up-/download or open datasets.\n  graph       List, import, export, delete, count, tree or open graphs.\n  project     List, import, export, create, delete or open projects.\n  query       List, execute, get status or open SPARQL queries.\n  vocabulary  List, (un-)install, import or open vocabs / manage cache.\n  workflow    List, execute, status or open (io) workflows.\n</code></pre> <p>You can now edit your configuration file and add credentials and URL parameters for your Corporate Memory deployment. You either search for the configuration manually in your home directory or you can use the <code>config edit</code> command, which opens the configuration file in your default text editor (specified by the <code>EDITOR</code> variable).</p> <pre><code>$ cmemc config edit\nOpen editor for config file /home/user/.config/cmemc/config.ini\n</code></pre> <p>The rules for the configuration file are similar to a Windows INI file and are explained in detail at docs.python.org.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#examples","title":"Examples","text":"<p>Below is a minimal example using the <code>client_credentials</code> grant type.</p> <p>Example</p> <pre><code>[my-local]\nCMEM_BASE_URI=http://localhost/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=...\n</code></pre> <p>This creates a named section (connection identifier) <code>my-local</code> which is a connection to a Corporate Memory deployment on <code>http://localhost/</code>. In order to use this connection, you need to use the <code>--connection / -c</code> option with this identifier or set the default connection to this configuration. The authorization will be done with a system account <code>cmem-service-account</code> and the given client secret. Using this combination of configuration parameters is based on a typical installation where all components are available under the same host name.</p> <p>Another example using <code>password</code> grant type.</p> <p>Example</p> <pre><code>[my-local]\nCMEM_BASE_URI=http://localhost/\nOAUTH_GRANT_TYPE=password\nOAUTH_CLIENT_ID=cmemc\nOAUTH_USER=user\nOAUTH_PASSWORD=...\n</code></pre> <p>This creates a named section <code>my-local</code>, which is a connection to a Corporate Memory deployment on <code>http://localhost/</code>. The authorization will be done with the given <code>OAUTH_USER</code> and <code>OAUTH_PASSWORD</code>.</p> <p>Info</p> <p>The OAuth 2.0 token endpoint location (<code>OAUTH_TOKEN_URI</code>) defaults to <code>$KEYCLOAK_BASE_URI/realms/$KEYCLOAK_REALM_ID/protocol/openid-connect/token</code>. If Keycloak is exposed to a different domain than Corporate Memory, make sure the variables <code>KEYCLOAK_BASE_URI</code> and <code>KEYCLOAK_REALM_ID</code> are configured correctly. Please refer to Configure Corporate Memory with an external Keycloak for more information.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#configuration-variables","title":"Configuration Variables","text":"<p>The above example provides access to an installation where all components (including Keycloak) are deployed with the default URL base. However, if you need to fine-tune all locations or want to use special functionality, the following configuration parameters can be used.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#location-related","title":"Location related","text":"<p>The following configuration variables specify where cmemc can find the relevant HTTP endpoints. Most of them are optional.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#cmem_base_uri","title":"CMEM_BASE_URI","text":"<p>This is the base location (HTTP(S) URL) of your eccenca Corporate Memory deployment.</p> <p>You always have to set this configuration variable.</p> <p>This variable defaults to <code>http://docker.localhost/</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#di_api_endpoint","title":"DI_API_ENDPOINT","text":"<p>This is the base location (HTTP(S) URL) of all Data Integration APIs.</p> <p>This variable defaults to <code>$CMEM_BASE_URI/dataintegration/</code> and usually does not need to be set.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#dp_api_endpoint","title":"DP_API_ENDPOINT","text":"<p>This is the base location (HTTP(S) URL) of all Data Platform APIs.</p> <p>This variable defaults to <code>$CMEM_BASE_URI/dataplatform/</code> and usually does not need to be set.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#keycloak_base_uri","title":"KEYCLOAK_BASE_URI","text":"<p>This is the base location (HTTP(S) URL) of all Keycloak APIs.</p> <p>This variable defaults to <code>$CMEM_BASE_URI/auth/</code> and usually does not need to be set.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#keycloak_realm_id","title":"KEYCLOAK_REALM_ID","text":"<p>This is the identifier of your  Keycloak Realm.</p> <p>This variable defaults to <code>cmem</code> and usually does not need to be set.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_token_uri","title":"OAUTH_TOKEN_URI","text":"<p>This is the OpenID Connect (OIDC) OAuth 2.0 token endpoint location (HTTP(S) URL).</p> <p>This variable defaults to <code>$KEYCLOAK_BASE_URI/realms/$KEYCLOAK_REALM_ID/protocol/openid-connect/token</code> and usually does not need to be set.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#authentication-related","title":"Authentication related","text":"<p>The following configuration variables specify how cmemc can fetch a token to authenticate on the endpoints.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_grant_type","title":"OAUTH_GRANT_TYPE","text":"<p>This configures the OAuth Grant Type used to specify how cmemc is able to get a valid token for accessing the Corporate Memory APIs.</p> <p>Depending on the value of this variable, other authentication-related variables will become mandatory or obsolete. The following values can be used:</p> <ul> <li><code>client_credentials</code> - this refers to the OAuth 2.0 Client Credentials Grant Type. Mandatory variables for this grant type are <code>OAUTH_CLIENT_ID</code>, <code>OAUTH_CLIENT_SECRET</code> or <code>OAUTH_CLIENT_SECRET_PROCESS</code>.</li> <li><code>password</code> - this refers to the OAuth 2.0 Password Grant Type. Mandatory variables for this grant type are <code>OAUTH_CLIENT_ID</code>, <code>OAUTH_USER</code>, <code>OAUTH_PASSWORD</code> or <code>OAUTH_PASSWORD_PROCESS</code>.</li> <li><code>prefetched_token</code> - this value can be used in case you can provide a token that was fetched outside of cmemc. Mandatory variables for this grant type are <code>OAUTH_ACCESS_TOKEN</code> or <code>OAUTH_ACCESS_TOKEN_PROCESS</code>.</li> </ul> <p>This variable defaults to <code>client_credentials</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_client_id","title":"OAUTH_CLIENT_ID","text":"<p>This configures the used client ID. Usually, the following cmemc related clients are configured in the standard Corporate Memory realm:</p> <ul> <li><code>cmem-service-account</code> is the client which is configured to be used with the <code>client_credentials</code> grant type.</li> <li><code>cmemc</code> is the client which is configured to be used with the <code>password</code> grant type.</li> </ul> <p>You usually have to set this configuration variable (exception: you use the <code>prefetched_token</code> grant type).</p> <p>This variable defaults to <code>cmem-service-account</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_user","title":"OAUTH_USER","text":"<p>This variable specifies your user account.</p> <p>You only need to set this configuration variable if you use the <code>password</code> grant type.</p> <p>This variable defaults to <code>admin</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_password","title":"OAUTH_PASSWORD","text":"<p>This variable specifies your user password.</p> <p>You only need to set this configuration variable if you use the <code>password</code> grant type.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_client_secret","title":"OAUTH_CLIENT_SECRET","text":"<p>This variable specifies your client secret (password).</p> <p>You only need to set this configuration variable if you use the <code>client_credentials</code> grant type.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_access_token","title":"OAUTH_ACCESS_TOKEN","text":"<p>This variable specifies a pre-fetched access token.</p> <p>You only need to set this configuration variable if you use the <code>prefetched_token</code> grant type.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_password_process","title":"OAUTH_PASSWORD_PROCESS","text":"<p>In order to avoid saving credentials in configuration files you can use this optional configuration variable instead of the <code>OAUTH_PASSWORD</code> variable.</p> <p>Please refer to Getting Credentials from external Processes for more information.</p> <p>This variable defaults to <code>none</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_client_secret_process","title":"OAUTH_CLIENT_SECRET_PROCESS","text":"<p>In order to avoid saving credentials in configuration files you can use this optional configuration variable instead of the <code>OAUTH_CLIENT_SECRET</code> variable.</p> <p>Please refer to Getting Credentials from external Processes for more information.</p> <p>This variable defaults to <code>none</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_access_token_process","title":"OAUTH_ACCESS_TOKEN_PROCESS","text":"<p>In order to avoid saving credentials in configuration files you can use this optional configuration variable instead of the <code>OAUTH_ACCESS_TOKEN</code> variable.</p> <p>Please refer to Getting Credentials from external Processes for more information.</p> <p>This variable defaults to <code>none</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#network-related","title":"Network related","text":"","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#ssl_verify","title":"SSL_VERIFY","text":"<p>Setting this to <code>false</code> will disable certification verification (not recommended).</p> <p>Please refer to Certificate handling and SSL verification for more information.</p> <p>This variable defaults to <code>true</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#requests_ca_bundle","title":"REQUESTS_CA_BUNDLE","text":"<p>Setting this to a PEM file allows for using private Certificate Authorities for certificate validation.</p> <p>Please refer to Certificate handling and SSL verification for more information.</p> <p>This variable defaults to <code>$PYTHON_HOME/site-packages/certifi/cacert.pem</code>.</p> <ol> <li> <p>More precisely, it is <code>$XDG_CONFIG_HOME/cmemc/config.ini</code>, see also the XDG Base Directory Specification.\u00a0\u21a9</p> </li> </ol>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/","title":"Getting Credentials from External Processes","text":"","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#introduction","title":"Introduction","text":"<p>This page discusses how to avoid passwords in configuration files by using configured credential processes or environment variables. This is particularly useful when credentials often change and / or are stored in central infrastructures such as personal or company wide password managers. Moreover, you might find it useful when working with cmemc in CI/CD pipelines.</p>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#environment-variables","title":"Environment Variables","text":"<p>As described in the Configuration with Environment Variables document, cmemc can be configured with environment variables. The following code snippet demonstrates the behaviour:</p> <pre><code>$ export CMEM_BASE_URI=\"https://your-cmem.eccenca.dev/\"\n$ export OAUTH_GRANT_TYPE=\"client_credentials\"\n$ export OAUTH_CLIENT_ID=\"cmem-service-account\"\n$ export OAUTH_CLIENT_SECRET=\"...secret...\"\n$ cmemc graph list\n</code></pre> <p>In the context of a CI/CD pipeline, e.g., on github, these credentials can be taken from the repository secrets:</p> <pre><code>jobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: run cmemc\n    env:\n        CMEM_BASE_URI: https://your-cmem.eccenca.dev/\n        OAUTH_GRANT_TYPE: client_credentials\n        OAUTH_CLIENT_ID: cmem-service-account\n        OAUTH_CLIENT_SECRET: ${{ secrets.OAUTH_CLIENT_SECRET }}\n    run: |\n            cmemc graph list\n</code></pre> <p>In shell context, you can fetch the secret from an external process to the variable:</p> <pre><code>$ export OAUTH_CLIENT_SECRET=$(get-my-secret.sh)\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#external-processes","title":"External Processes","text":"<p>Another option, which is interesting when working with multiple Corporate Memory instances, is the configuration of an external process in your cmemc configuration file.</p> <p>In order to get credential information from an external process you need to use the following configuration variables to set up an external executable:</p> <ul> <li><code>OAUTH_PASSWORD_PROCESS</code>, to set up the process to get the user password when using the <code>password</code> grant type.</li> <li><code>OAUTH_CLIENT_SECRET_PROCESS</code>, to set up the process to get the client secret when using <code>client_credentials</code> grant type .</li> <li><code>OAUTH_ACCESS_TOKEN_PROCESS</code>, to set up the process to get the direct access token (<code>prefetched_token</code>).</li> </ul> <p>The credential executable can use the other cmemc environment keys of the configuration block for fetching the credentials (e.g. <code>CMEM_BASE_URI</code> and <code>OAUTH_USER</code>).</p> <p>If the credential executable is not given with a a full path, cmemc will look into your environment <code>PATH</code> for something that can be executed.</p> <p>The configured process needs to return the credentials on the first line of <code>stdout</code>. In addition to that, the process needs to exit with exit code 0 (without failure).</p> <p>The following config section demonstrates this behaviour:</p> <pre><code>[your-cmem]\nCMEM_BASE_URI=https://your-cmem.eccenca.dev/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET_PROCESS=get-my-secret.sh\n</code></pre> <p>If you need to add options to the call, you can write the call as a list:</p> <pre><code>[your-cmem]\nCMEM_BASE_URI=https://your-cmem.eccenca.dev/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET_PROCESS=[\"getpass.sh\", \"parameter1\", \"parameter2\"]\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#example-macos-keychain","title":"Example: MacOS Keychain","text":"<p>Here is a working example with the MacOS Keychain, which can be queried with the command line tool <code>security</code>.</p> <p>This example fetches a password for the account <code>cmem-service-account</code> for the service <code>https://your-cmem.eccenca.dev/</code>.</p> <pre><code>OAUTH_CLIENT_SECRET_PROCESS=[\"security\", \"find-generic-password\", \"-w\", \"-a\", \"cmem-service-account\", \"-s\", \"https://your-cmem.eccenca.dev/\" ]\n</code></pre> <p>The corresponding keychain entry looks like this:</p> <p></p> <p>In order to avoid repeating this long line in a cmemc configuration with lots of entries, it can be wrapped in a shell script like this:</p> <pre><code>#!/usr/bin/env bash\n\nif [ \"${OAUTH_GRANT_TYPE}\" = \"client_credentials\" ]; then\n    security find-generic-password -w -a \"${OAUTH_CLIENT_ID}\" -s \"${CMEM_BASE_URI}\" || exit 1\n    exit 0\nfi\nif [ \"${OAUTH_GRANT_TYPE}\" = \"password\" ]; then\n    security find-generic-password -w -a \"${OAUTH_USER}\" -s \"${CMEM_BASE_URI}\" || exit 1\n    exit 0\nfi\nexit 1\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/","title":"Installation","text":"<p>cmemc can be installed using the python package from pypi.org, the release package or by pulling the docker image.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/#via-pypiorg","title":"\u2026 via pypi.org","text":"<p>cmemc is available as an official pypi package so installation can be done with pip or pipx (preferred):</p> <pre><code>$ pipx install cmem-cmemc\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/#via-docker-image","title":"\u2026 via docker image","text":"<p>This topic is described on a stand-alone page.</p> <p>Note</p> <p>Once you have installed cmemc, you need to configure a connection with a config file or learn how to use environment variables to control cmemc.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/","title":"Invocation","text":"<p>cmemc is intended to be useful for administrators and Linked Data experts.</p> <p>Besides the plain ad-hoc invocation from a users terminal, the following recipes show additional invocation methods.</p> <ul> <li> <p> Executing cmemc as a Docker Container.</p> </li> <li> <p> Running cmemc jobs as part of Github Actions.</p> </li> <li> <p> Running cmemc jobs as part of Gitlab Pipelines.</p> </li> <li> <p> Preparing SPARQL Scripts to fetch data from your Knowledge Graphs.</p> </li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/","title":"Using the Docker Image","text":"","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/#introduction","title":"Introduction","text":"<p>In addition to the cmemc distribution package, you can use the eccenca cmemc docker image, which is based on the Red Hat Universal Base Image 9 Minimal. This is especially needed if you want to use cmemc in orchestrations.</p>","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/#image-and-tags","title":"Image and Tags","text":"<p>The following image - tag combinations are available for public use:</p> <ul> <li><code>docker-registry.eccenca.com/eccenca-cmemc:v24.3.0</code> - a specific release</li> <li><code>docker-registry.eccenca.com/eccenca-cmemc:latest</code> - same as the latest release</li> </ul> Image retrieval and check cmemc version<pre><code>$ docker run -it --rm docker-registry.eccenca.com/eccenca-cmemc:v24.3.0 --version\nUnable to find image 'docker-registry.eccenca.com/eccenca-cmemc:v24.3.0' locally\nv24.3.0: Pulling from eccenca-cmemc\n...\nDigest: sha256:....\nStatus: Downloaded newer image for docker-registry.eccenca.com/eccenca-cmemc:v24.3.0\ncmemc, version 24.3.0, running under python 3.11.9\n</code></pre>","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/#volumes","title":"Volumes","text":"<p>cmemc processes a configuration file and can import and export files which represent graph, project or workspace payloads. These files need to be mounted via docker volumes to be accessible for the dockerized cmemc.</p> <ul> <li><code>/config/cmemc.ini</code> (file) - the loaded configuration file</li> <li><code>/data</code> (directory) - the working directory</li> </ul> Using a volume to mount the config.<pre><code>$ cat cmemc.ini\n[my-deployment]\nCMEM_BASE_URI=https://data.example.org/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=credentialshere\n\n$ docker run -it --rm -v \"$(pwd)\"/cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v24.1 config list\nmy-deployment\n</code></pre> Using a volume to additionally mount the data directory.<pre><code>$ cat list-graphs.sparql\nSELECT DISTINCT ?graph (COUNT(?graph) AS ?triples)\nWHERE {\n    GRAPH ?graph\n    {\n        ?s ?p ?o\n    }\n}\nGROUP BY ?graph\nORDER BY DESC(?triples)\n\n$ docker run -it --rm -v $(pwd):/data -v $(pwd)/cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v24.3.0 -c my-deployment query execute ./list-graphs.sparql\ngraph,triples\nhttp://schema.org/,8809\nhttps://vocab.eccenca.com/shacl/,1752\n[...]\n</code></pre>","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/","title":"Using Github Actions","text":"","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#introduction","title":"Introduction","text":"<p>Github Actions allow for the automation and execution of workflows based on pushes, merge requests and other trigger events to your git repository. In order to control eccenca Corporate Memory instances from within Github Action based workflows, you need to provide cmemc as well as credentials for your instance to the workflow.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#runner-provisioning","title":"Runner Provisioning","text":"<p>Providing a working cmemc command is simple. You just need to install a python environment suitable to run cmemc (currently <code>3.11</code>). This can be done with the setup-python action. After that, simply use <code>pip</code> to install cmemc:</p> Partial github action yaml showing cmemc provisioning<pre><code>      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n      - name: install cmemc\n        run: |\n          pip install -q cmem-cmemc\n          cmemc --version\n</code></pre> <p>Adding the above to your workflow yaml description will provide a cmemc command which can be used in all subsequent steps of the same workflow.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#credentials-and-usage","title":"Credentials and Usage","text":"<p>Since we should never save credentials in your repository, we need to provide them as an encrypted secret managed outside of the repository. Github provides you with an Encrypted Secrets interface where you can add secrets for your repository, which in turn can be used in your workflows.</p> <p>Given the following workflow step, you need to add <code>MY_CMEM_BASE_URI</code>, <code>MY_OAUTH_GRANT_TYPE</code>, <code>MY_OAUTH_CLIENT_ID</code> and <code>MY_OAUTH_CLIENT_SECRET</code> as encrypted secrets to your repository:</p> Partial github action yaml showing credential provisioning<pre><code>      - name: use cmemc\n        run: |\n          cmemc graph import graph.ttl $GRAPH\n          cmemc graph count $GRAPH\n          cmemc graph delete $GRAPH\n        env:\n          GRAPH: \"https://github.com/eccenca/cmemc-workflow\"\n          CMEM_BASE_URI: ${{ secrets.MY_CMEM_BASE_URI }}\n          OAUTH_GRANT_TYPE: ${{ secrets.MY_OAUTH_GRANT_TYPE }}\n          OAUTH_CLIENT_ID: ${{ secrets.MY_OAUTH_CLIENT_ID }}\n          OAUTH_CLIENT_SECRET: ${{ secrets.MY_OAUTH_CLIENT_SECRET }}\n</code></pre> <p>The above snippet also demonstrates how you can map your encrypted secrets to cmemc\u2019s configuration variables.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#example-project","title":"Example Project","text":"<p>The Github project eccenca/cmemc-workflow provides an example workflow description which uses cmemc to import a graph, count the triples and removes the graph afterwards. Here is an example output:</p> <p></p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/","title":"Using Gitlab Pipelines","text":"","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#introduction","title":"Introduction","text":"<p>Gitlab CI/CD allows for the automation and execution of workflows based on pushes, merge requests and other trigger events to your git repository. In order to control eccenca Corporate Memory instances from within Gitlab CI/CD based workflows you need to provide cmemc and the credentials for your instance to the Gitlab CI/CD pipeline.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#runner-provisioning","title":"Runner Provisioning","text":"<p>In order to use cmemc in Gitlab pipelines you can use the cmemc docker image.</p> Partial .gitlab-ci.yml showing cmemc provisioning<pre><code>test:\n    image:\n      name: docker-registry.eccenca.com/eccenca-cmemc:latest\n      entrypoint: [\"\"]\n    script:\n        - cmemc --version\n</code></pre> <p>Adding the above to your pipeline description will provide a cmemc command which can be used in the workflow step.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#credentials-and-usage","title":"Credentials and Usage","text":"<p>Since we should never commit credentials in your repository, we need to provide them as an encrypted secret managed outside of the repository. You can add CI/CD variables to a project\u2019s settings directly in Gitlab.</p> <p>Given the following pipeline step, you need to add <code>MY_CMEM_BASE_URI</code>, <code>MY_OAUTH_GRANT_TYPE</code>, <code>MY_OAUTH_CLIENT_ID</code> and <code>MY_OAUTH_CLIENT_SECRET</code> as encrypted secrets to your repository:</p> Partial .gitlab-ci.yml showing credential provisioning<pre><code>test:\n    image:\n      name: docker-registry.eccenca.com/eccenca-cmemc:latest\n      entrypoint: [\"\"]\n    stage: test\n    script:\n        - cmemc --version\n        - cmemc graph import graph.ttl $GRAPH\n        - cmemc graph count $GRAPH\n        - cmemc graph delete $GRAPH\n    variables:\n      GRAPH: \"https://github.com/seebi/cmemc-workflow\"\n      CMEM_BASE_URI: $MY_CMEM_BASE_URI\n      OAUTH_GRANT_TYPE: $MY_OAUTH_GRANT_TYPE\n      OAUTH_CLIENT_ID: $MY_OAUTH_CLIENT_ID\n      AUTH_CLIENT_SECRET: $MY_OAUTH_CLIENT_SECRET\n</code></pre> <p>The above snippet also demonstrates how you can map your project variables to cmemc\u2019s configuration variables.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#example-project","title":"Example Project","text":"<p>The Github project eccenca/cmemc-workflow provides an example gitlab pipeline description which uses cmemc to import a graph, count the triples and removes the graph afterwards. Here is an example output:</p> <p></p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/sparql-scripts/","title":"SPARQL Scripts","text":"<p>By prepending a Shebang line to a SPARQL query file and making the file executable, it can be treated as an executable script. To give an example, below is a simple text file with a generic SPARQL query that counts all triples in all graphs and outputs an ordered list.</p> count-graphs.sh<pre><code>SELECT DISTINCT ?graph (COUNT(?graph) AS ?triples)\nWHERE {\n    GRAPH ?graph\n    {\n        ?s ?p ?o\n    }\n}\nGROUP BY ?graph\nORDER BY DESC(?triples)\n</code></pre> <p>In order to convert this text file into a SPARQL script you need to add the following line to the top of the file:</p> shebang line for SPARQL scripts<pre><code>#!/usr/bin/env -S cmemc query execute --accept text/csv\n</code></pre> <p>This will set cmemc as an interpreter for the rest of the file, and by using the query execute command, the rest of the file will be used as a SPARQL query.</p> <p>Now you need to define your SPARQL file as executable and run it:</p> <pre><code>$ chmod a+x ./count-graphs.sh\n</code></pre> <pre><code>$ ./count-graphs.sh\ngraph,triples\nhttps://vocab.eccenca.com/shacl/,4510\nhttps://vocab.eccenca.com/auth/,240\nhttps://ns.eccenca.com/example/data/vocabs/,169\nhttps://ns.eccenca.com/data/ac/,66\nhttps://ns.eccenca.com/data/queries/,39\nhttps://ns.eccenca.com/data/config/,4\nhttps://ns.eccenca.com/data/userinfo/,4\n</code></pre>","tags":["Automate","SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/","title":"Troubleshooting and Caveats","text":"<p>This page lists and documents possible issues and warnings when working with cmemc.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#proxy-is-in-the-way","title":"Proxy is in the way","text":"<p>If you feel that your system\u2019s proxy configuration negatively impacts the communication between cmemc and Corporate Memory, you can disable using any proxy by setting this variable:</p> <pre><code>export no_proxy='*'\n</code></pre> <p>This is due to the python requests library proxy handling.</p> <p>The no_proxy environment variable can be used to specify hosts which shouldn\u2019t be reached via proxy; if set, it should be a comma-separated list of hostname suffixes, optionally with :port appended, for example cern.ch,ncsa.uiuc.edu,some.host:8080.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#gateway-time-out","title":"Gateway Time-out","text":"<p>A gateway timeout occurs if your Corporate Memory infrastructure is not setup correctly.</p> <pre><code>$ cmemc -c my-cmem project import my-project.zip my-project\nImport file my-project.zip to project my-project ... 504 Server Error: Gateway Time-out for url: https://my-cmem/dataintegration/workspace/projects\n</code></pre> <p>This can have multiple reasons - please check in the following order:</p> <ul> <li><code>application.yaml</code> of DataIntegration</li> <li>reverse proxy configuration</li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/","title":"Workflow Execution and Orchestration","text":"","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#introduction","title":"Introduction","text":"<p>In some cases, you need to automate a complete graph of integration workflows which depend on each other and can sometimes run in parallel or consecutively. Although cmemc is not a workflow orchestration tool, you can easily use it do some basic workflow orchestration. This page describes how you can execute and orchestrate workflows. For simplicity, all the given examples do not select a specific connection (<code>--connection your-cmem</code>). We simply assume that you selected your instance via an environment variable (<code>export CMEMC_CONNECTION=your-cmem</code>).</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#simple-execution","title":"Simple Execution","text":"<p>cmemc allows for execution of workflows with the <code>workflow execute</code> command. To start a workflow, simply use this command:</p> workflow execute command<pre><code>$ cmemc workflow execute cmem:my-workflow\ncmem:my-workflow ... Started\n</code></pre> <p>Workflow identifier can be extended with command-line completion on the command line but you can also get a list of workflows with the <code>workflow list</code> command:</p> workflow list command<pre><code>$ cmemc workflow list\ncmem:my-workflow\ncmem:second-workflow\n</code></pre> <p>The default working mode of the <code>workflow execute</code> command starts a workflow without waiting for a response. In order to wait until the workflow is finished you need to use the <code>--wait</code> option:</p> workflow execute command with wait option<pre><code>$ cmemc workflow execute cmem:my-workflow --wait\ncmem:my-workflow ... Started ... Finished (Finished in 32.931s, just now)\n</code></pre> <p>For a reference of the <code>workflow execute</code> command, please have a look at the Command Reference or the command-specific help (<code>cmemc workflow execute --help</code>).</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#retrieve-status-information","title":"Retrieve Status Information","text":"<p>At any time, you can retrieve status information of a workflow with the <code>workflow status</code> command:</p> workflow status command<pre><code>$ cmemc workflow status cmem:my-workflow\ncmem:my-workflow ... Finished (Finished in 32.931s, 4 minutes ago)\n</code></pre> <p>Additionally, you can retrieve raw JSON data about a workflow, which can be used for post-processing:</p> workflow status command with JSON output<pre><code>$ cmemc -workflow status cmem:my-workflow --raw\n{\n  \"activity\": \"ExecuteLocalWorkflow\",\n  \"runtime\": 32931,\n  \"project\": \"cmem\",\n  \"failed\": false,\n  \"message\": \"Finished in 32.931s\",\n  \"task\": \"my-workflow\",\n  \"isRunning\": false,\n  \"statusName\": \"Finished\",\n  \"progress\": 100,\n  \"cancelled\": false,\n  \"startTime\": 1593679211989,\n  \"exceptionMessage\": null,\n  \"lastUpdateTime\": 1593679244920\n}\n</code></pre> <p>For a reference of the <code>workflow status</code> command, please have a look at the Command Reference or the command-specific help (<code>cmemc workflow status --help</code>).</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#serial-execution","title":"Serial Execution","text":"<p>The <code>workflow execute</code> command is able to start multiple workflows in a chain, waiting for each of the workflows to finish and exiting if there is an error with one of the workflows.</p> <p>To do this, use the <code>--wait</code> option and simply add more than one workflow identifier as parameters to the the command:</p> workflow execute command<pre><code>$ cmemc workflow execute cmem:my-workflow cmem:second-workflow --wait\ncmem:my-workflow ... Started ... Finished (Finished in 30.984s, just now)\ncmem:second-workflow ... Started ... Finished (Finished in 50.579s, just now)\n</code></pre> <p>Warning</p> <p>Starting workflows in this way means that cmemc exits with an error code 1 at the moment a workflow throws an error. None of the following workflows will be executed.</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#parallel-execution","title":"Parallel Execution","text":"<p>Sometimes you may want to execute workflows in parallel, because they do not depend on each other and it fastens up the overall runtime.</p> <p>To do this, there is currently a little bit of extra scripting needed. The main idea is to start the parallel workflows without waiting and then poll the status information until they are not running anymore.</p> <p>Here is an example script which does exactly this:</p> cmemc-parallel-workflows.sh<pre><code>#!/usr/bin/env bash\n# @(#) Example: execute two workflows in parallel and wait for the results (exit 1 on failure)\n# Use the unofficial bash strict mode: http://redsymbol.net/articles/unofficial-bash-strict-mode/\nset -euo pipefail; export FS=$'\\n\\t'\n\n# setup the used instance\nexport CMEMC_CONNECTION=your-cmem\n\n# check SSL config\nif [ \"$(cmemc config get SSL_VERIFY 2&gt;&amp;1)\" == True ]\nthen\n    NUM=0\nelse\n    NUM=1\nfi\n\n# start the given set of workflows\nWORKFLOW_IDS=\"cmem:my-workflow cmem:second-workflow\"\ncmemc workflow execute $WORKFLOW_IDS\n\n# loop until they are not running anymore\nRUNNING=-1\nuntil [ $RUNNING == $NUM ]\ndo\n    # wait 5 seconds - polling time\n    sleep 5\n    # use the the filter option to show only running workflows\n    RUNNING=$(cmemc workflow status $WORKFLOW_IDS --filter running 2&gt;&amp;1 | wc -l)\n    if [ $RUNNING != $NUM ]; then\n        echo \"We still have $RUNNING running workflows ...\"\n    fi\ndone\n\n# look for failed workflows\nFAILED=$(cmemc workflow status $WORKFLOW_IDS --filter failed 2&gt;&amp;1 | wc -l)\nif [ $FAILED != $NUM ]; then\n    echo \"Some workflows failed :-(\"\n    exit 1\nelse\n    echo \"All workflows finished successfully :-)\"\n    exit 0\nfi\n</code></pre>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/continuous-integration/","title":"Continuous Integration and Delivery","text":""},{"location":"automate/continuous-integration/#introduction","title":"Introduction","text":"<p>Project teams often manage crucial parts of their work assets inside of git repositories. This includes Corporate Memory related files such as ontologies, shapes or project configurations. Given such a project setup, it is often wanted to start activities with these Corporate Memory files. Continuous integration (CI) is the practice of automating the integration of changes from multiple contributors into a single project. Originated from software projects, CI can (and should) be applied to Knowledge Graph projects as well.</p>"},{"location":"automate/continuous-integration/#example-project","title":"Example Project","text":"<p>The following depiction shows an example project setup which applies CI/CD principles to a Knowledge Graph project.</p> <p></p> <p>The project has two Corporate Memory stages, DEV and PROD. The DEV Stage produces Knowledge Graphs based on artefacts from one or more Git repositories. Changes on the artefacts are tested by a Build Server. The Build Server pushes tested artefacts (Green Builds) to the DEV stage as well as produces other artefacts such as Visualisations, Documentation and Test Result artefacts. The automated activities in the Build Plan are executed with cmemc on the Corporate Memory instances. Once a release is ready, artefacts are transferred to the PROD Stage.</p>"},{"location":"automate/continuous-integration/#integration-recipes","title":"Integration Recipes","text":"<p>Complex build plans are compiled of simple steps, each executing a specific activity. cmemc is a building block to create complex build plans for your Knowledge Graph project. The following pages provide recipes for different CI/CD solutions:</p> <ul> <li> <p> Github Actions</p> </li> <li> <p> Gitlab Pipelines</p> </li> </ul>"},{"location":"automate/processing-data-with-variable-input-workflows/","title":"Processing Data with Variable Input Workflows","text":"","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#introduction","title":"Introduction","text":"<p>This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (i.e., without registering datasets). The concept to achieve this is named a Variable Dataset. A variable dataset is created and used inside of a workflow as an input for other tasks (e.g. a transformation) at the place where a \u201cregular\u201d dataset (such as register CSV file) would be placed.</p> <p>The workflow is then called via an HTTP REST call (or via\u00a0cmemc), thus uploading the payload data \u201cat the place\u201d of this variable input dataset and executing all following parts of the workflow.</p> <p>This allows for solving all kinds of \u2606 Automation tasks when you need to process lots of small data snippets or similar.</p> <p>Abstract</p> <p>The complete tutorial is available as a\u00a0project file. You can import this project</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li> <p>by using the command line interface</p> <pre><code>$ cmemc -c my-cmem project import tutorial-varinput.project.zip varinput\n</code></pre> </li> </ul>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#1-install-the-required-vocabularies","title":"1 Install the required vocabularies","text":"<p>First, install all required ontologies/vocabularies which are needed for mappings later in the tab VOCABULARIES.</p> <p>In this tutorial, we need the Schema.org and the RDFS vocabulary. Press the (toggle switch) button on the right to install them.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#2-create-a-new-project","title":"2 Create a new project","text":"<p>Second, create in the tab DATA INTEGRATION a new project. Provide it with a Title and Description.</p> <p>The project will include everything you need to build a workflow for extracting Feed XML data, transforming it into RDF, and loading it into a Knowledge Graph.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#3-create-an-example-feed-dataset-and-target-graph-dataset","title":"3 Create an (example) feed dataset and target graph dataset","text":"<p>Upload a sample XML dataset (feed data) into your project: Create \u2192 XML \u2192 Upload new file.</p> <p>For this tutorial, you may take this file: feed.xml(1)</p> <ol> <li>Original feed source was: <code>https://www.ecdc.europa.eu/en/taxonomy/term/2942/feed</code></li> </ol> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#4-create-the-feed-transformation","title":"4 Create the feed transformation","text":"<p>Based on the added sample feed XML Dataset, create a mapping to generate RDF triples. The screenshot provides an example mapping to generate WebPages, which includes a label, a URL, a text, and the date they were published in the feed. The mappings are based on classes and properties defined by the Schema.org and RDFS vocabulary.</p> <p>In case you need help with mapping data from XML to RDF, feel free to visit your respective tutorial: Lift data from JSON and XML sources.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#5-create-the-variable-input-and-workflow","title":"5 Create the variable input and workflow","text":"<p>Create a new workflow in your project. Move the input XML feed dataset and the Feed Data Graph into the workflow editor and connect them with your created Transform feed.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#6-use-cmemc-to-feed-data-into-the-workflow","title":"6 Use <code>cmemc</code> to feed data into the workflow","text":"<p>Finally, you can process all the feeds you want by executing the created workflow with a dynamic XML payload.</p> <p>For this, you need to use the <code>workflow io</code> command:</p> <pre><code># process one specific feed xml document\n$ cmemc workflow io varinput:process-feed -i feed.xml\n</code></pre> <p>You can easily automate this for a list of feeds like this:</p> <pre><code>$ cat feeds.txt\nhttps://feeds.npr.org/500005/podcast.xml\nhttp://rss.cnn.com/rss/cnn_topstories.rss\nhttps://lifehacker.com/rss\nhttp://feeds.bbci.co.uk/news/rss.xml\n\u2026\n\n# fetch the list of urls one by one and feed the content to the corporate memory workflow\n$ cat feeds.txt | xargs -I % sh -c '{ echo %; curl -s % -o feed.xml; cmemc workflow io varinput:process-feed -i feed.xml; rm feed.xml; }'\nhttps://feeds.npr.org/500005/podcast.xml\nhttp://rss.cnn.com/rss/cnn_topstories.rss\nhttps://lifehacker.com/rss\nhttp://feeds.bbci.co.uk/news/rss.xml\n\u2026\n</code></pre>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#7-explore-the-fetched-knowledge-graph","title":"7 Explore the fetched Knowledge Graph","text":"<p>In EXPLORATION, you can study the ingested feed data in your Knowledge Graph.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/scheduling-workflows/","title":"Scheduling Workflows","text":"","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#introduction","title":"Introduction","text":"<p>For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator. Please note that, in case you want to schedule workflows externally, cmemc can be used for that.</p>","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#create-a-scheduler","title":"Create a scheduler","text":"<ol> <li>Navigate to Build \u2192 Projects section in the workspace and Click Create.</li> <li>Select the Item type Scheduler.</li> <li>Click Add - then the Create new item of type Scheduler dialog box appears.</li> <li>Set the properties of the Scheduler:<ol> <li>Select the target project.</li> <li>Define the label of your scheduler</li> <li>Specify the workflow (task) to be executed.</li> <li>Define the interval for the scheduler to be executed again.     Example: <code>PT15MD</code> (Every 15 minutes)</li> <li>Define the start time for the scheduler to be executed for the first time.</li> <li>Click Enable to enable the scheduler.</li> <li>Click Stop on error to stop the scheduler on after a failed run.</li> </ol> </li> </ol> <p>Once you are ready with the configurations, click Create button. Now, the scheduler will be executed with the given settings.</p> <p></p>","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#modify-enable-or-disable-a-scheduler","title":"Modify, enable or disable a scheduler","text":"<ol> <li>Navigate to Build \u2192 Projects section in the workspace.</li> <li>Search the scheduler you want to modify.</li> <li>Select it or click on Open Details Page in the context menu.</li> <li>Click on the Configure button in the Configuration section.</li> <li>Change the values according to your needs.</li> </ol>","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#time-interval-specification","title":"Time Interval Specification","text":"<p>The scheduler interval is represented an ISO-8601 time duration string .</p> <p>The following values are possible:</p> <ul> <li><code>P</code> is the duration designator (referred to as \u201cperiod\u201d), and is always placed at the beginning of the duration.</li> <li><code>Y</code> for defining the number of years.</li> <li><code>M</code> for defining the number of months.</li> <li><code>W</code> for defining the number of weeks.</li> <li><code>D</code> for defining the number of days.</li> <li><code>T</code> is the time designator that precedes the time components.</li> <li><code>H</code> for defining the number of hours.</li> <li><code>M</code> for defining the number of minutes.</li> <li><code>S</code> for defining the number of seconds.</li> </ul> <p>A duration with all values being used: <code>P2Y6M4DT12H30M10S</code> (defines a a period of 2 years, 6 months, 4 days, 12 hours, 30 minutes and 10 seconds).</p> <p>More common examples:</p> <ul> <li><code>PT30M</code> - every half hour</li> <li><code>PT1H</code> - every hour</li> <li><code>P1D</code> - every day</li> </ul>","tags":["Workflow","Automate","Video"]},{"location":"build/","title":"Build","text":""},{"location":"build/#build","title":"Build","text":"<p>The Build stage is used to turn your legacy data points from existing datasets into an Enterprise Knowledge Graph structure. The subsections introduce the features of Corporate Memory that support this stage and provide guidance through your first lifting activities.</p> <p> Intended audience: Linked Data Experts</p> <ul> <li> <p> Introduction and Best Practices</p> <ul> <li>Introduction to the User Interface \u2014 a short introduction to the Build workspace incl. projects and tasks management.</li> <li>Rule Operators \u2014 Overview on operators that can be used to build linkage and transformation rules.</li> <li>Cool IRIs\u00a0\u2014 URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company.</li> <li>Define Prefixes / Namespaces\u00a0\u2014 Define Prefixes / Namespaces \u2014 Namespace declarations allow for abbreviation of IRIs by using a prefixed name instead of an IRI, in particular when writing SPARQL queries or Turtle.</li> </ul> </li> <li> <p> Tutorials</p> <ul> <li>Lift Data from Tabular Data \u2014 Build a Knowledge Graph from from Tabular Data such as CSV, XSLX or Database Tables.</li> <li>Lift data from JSON and XML sources\u00a0\u2014 Build a Knowledge Graph based on input data from hierarchical sources such as JSON and XML files.</li> <li>Extracting data from a Web API\u00a0\u2014 Build a Knowledge Graph based on input data from a Web API.</li> <li>Reconfigure Workflow Tasks\u00a0\u2014 During its execution, new parameters can be loaded from any source, which overwrites originally set parameters.</li> <li>Incremental Database Loading \u2014 Load data incrementally from a JDBC Dataset (relational database Table) into a Knowledge Graph.</li> </ul> </li> </ul>"},{"location":"build/active-learning/","title":"Active Learning of Linking Rules","text":"","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#introduction","title":"Introduction","text":"<p>Active learning infuses expert knowledge and creates new relationships between properties of two datasets. We can learn new rules and refine existing rules.</p>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#usage","title":"Usage","text":"<p>Active learning is a special case of machine learning in which a learning algorithm interactively queries a user to label new data points with the desired outputs. [wikipedia]</p> <p>In Corporate Memory we apply this approach to the process of learning a linking rule by interactively label records from the configured source and target dataset. Labeling in this case means to indicate if the pair of resources (from source and target) should be connected with the configured property. The labeling process creates reference links against which the linking rule can be created and further refined as more input is given by the user.</p> <p>Our active (link) learning is a three step process:</p> <ol> <li>It starts by define properties to compare between entities of the selected datasets.</li> <li>We continue with the interactive labeling process where user feedback is asked, the golden record build and a rule (automatically) calculated and refined (as more reference links are entered)</li> <li>Saving the learned rule and/or the reference links only</li> </ol>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#start-the-learning-dialog","title":"Start the learning dialog","text":"<p>Link learning is a feature available on a link rule in a Build project. See the Lift data from tabular data such as CSV, XSLX or database tables tutorial to learn how to setup a project. Use the Create  button in your project and select Linking to create a new linking rule. In the configuration dialog of your linking rule and setup source and target datasets as well as the linking property that should be yielded. Start the learning dialog by clicking the \u201cLearning\u201d tab in the linking view.</p> <p>The examples process below uses the movies example project which can be added to your workspace with the  Add \u201cmovies\u201d example project button to be found in the  user menu in the top right corner.</p>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#creating-an-automatic-link-rule","title":"Creating an automatic link rule","text":"<ul> <li> <p>Choose properties to compare.     Select from the suggestions or search them by specifying property paths for both entities.</p> <p></p> </li> </ul> <p>Note</p> <p>Based on the dataset suggestions for comparison are produced.</p>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#add-property-paths-for-both-entities","title":"Add property paths for both entities","text":"<ul> <li> <p>Click on the Source path and select a path.</p> <p></p> </li> <li> <p>Click on the Target path and select a corresponding path.</p> <p></p> </li> <li> <p>Click on the  icon to add the path pair to be examined in the learning algorithm.</p> <p></p> <p>Success</p> <p>Step Result: Both entities\u2019 paths were added.</p> <p></p> </li> <li> <p>Click on  icon to remove the paths.</p> <p></p> </li> <li> <p>Click on Start learning.</p> <p></p> <p>Note</p> <p>Clicking on the  icon uses the property value as the entity label, unselecting the  icon removes the property value from the entity label. Multiple properties can be starred to use them as a combined label.</p> <p></p> <p></p> <p>Success</p> <p>Step Result: The comparison in both paths will be reflected as shown below.</p> <p></p> <p>Cross-check the similarity between the source and target path data with regards to the configured link property (<code>owl:sameAs</code> in this example).</p> <p> Confirm: If the source and target title names are the same, click on Confirm and it is shown in dark blue colour.</p> <p></p> <p>Uncertain: If the title names differ slightly, we can consider the link uncertain. As it might be a spelling mistake, we cannot ensure it is the same nor can we say it is different. If the title names are different it is displayed in light blue colour.</p> <p></p> <p> Decline: If the title names of source and target path are different,click on decline and it displayed in light blue colour.</p> <p></p> </li> <li> <p>On the right side of the page click on the 3 dots, then click on show entity\u2019s URI.</p> <p></p> </li> </ul> <p>Success</p> <p>Step Result: It shows the link entity URIs along with rows numbers in both the dataset files.</p> <p></p> <ul> <li>Click on Save based on our input confirm, uncertain and decline the link rule will get generated automatically and the score changes for these entities in the score bar.</li> </ul> <p></p> <ul> <li>Switch on the save best learned rule, then click on save.</li> </ul> <p></p> <p>Success</p> <p>The new automatically created linking rule based on the input training data consisting of confirmed, uncertain and declined links is shown below. It tokenize all the input values from the connected source path and compares the data with target paths.</p> <p></p>","tags":["BeginnersTutorial"]},{"location":"build/cool-iris/","title":"Cool IRIs","text":"","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/cool-iris/#introduction","title":"Introduction","text":"<p>URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company.</p> <p>RFC 3986\u00a0defines a generic syntax for URIs:</p> <ul> <li><code>&lt;scheme&gt;:&lt;scheme-specific-part&gt;</code></li> <li>The scheme-specific part is often structured: <code>&lt;authority&gt;/&lt;path&gt;?&lt;query&gt;</code></li> </ul> <p>URIs are limited to ASCII characters. IRIs (Internationalized Resource Identifiers) allow Unicode (RFC 3987).</p> <p>The following list of example IRIs demonstrate the broad scope of this concept:</p> <ul> <li><code>ftp://ftp.is.co.za/rfc/rfc1808.txt</code></li> <li><code>http://www.ietf.org/rfc/rfc2396.txt</code></li> <li><code>ldap://[2001:db8::7]/c=GB?objectClass?one</code></li> <li><code>mailto:John.Doe@example.com</code></li> <li><code>news:comp.infosystems.www.servers.unix</code></li> <li><code>tel:+1-816-555-1212</code></li> <li><code>telnet://192.0.2.16:80/</code></li> <li><code>urn:oasis:names:specification:docbook:dtd:xml:4.1.2</code></li> </ul>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/cool-iris/#best-practices-in-corporate-memory","title":"Best practices in Corporate Memory","text":"<p>A good IRI is unique, stable, simple and manageable.</p> <p>Define a useful IRI-Scheme that can be used for resources.</p> <ul> <li>Define a Base URI which is the common authority for all resources in your graph.<ul> <li>Example:\u00a0<code>https://data.company.org/</code></li> </ul> </li> <li>Define subspaces where necessary, e.g. for each subproject or domain. Provide a\u00a0prefix\u00a0for each subspace. Examples:<ul> <li><code>https://data.company.org/hardware/</code>\u00a0for hardware artifacts</li> <li><code>https://data.company.org/software/</code>\u00a0for software artifacts</li> <li><code>PREFIX cohw: &lt;https://data.company.org/hardware/&gt;</code></li> <li><code>PREFIX cosw: &lt;https://data.company.org/software/&gt;</code></li> </ul> </li> <li>Based on these, build consistent schemes that define how your IRIs have to be build. Examples:<ul> <li><code>https://data.company.org/hardware/&lt;ProductClass&gt;/&lt;Serialnumber&gt;</code> to identify an individual product</li> <li><code>https://data.company.org/hardware/&lt;ProductClass&gt;/&lt;Modelnumber&gt;</code> to identify a product model</li> </ul> </li> </ul> <p>Warning</p> <p>Do not put a trailing slash at the end of resource IRIs as these cannot be used with prefix definitions in Turtle or SPARQL, which makes them more difficult to use.</p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/cool-iris/#more-information","title":"More information","text":"<ul> <li>Spanish Government, URIs for Open Data resources</li> <li>European Union, URIs for Legal Resources</li> <li>UK, \u201cDesigning URI sets for the UK public sector\u201d</li> <li>Other Resources<ul> <li>https://www.w3.org/TR/cooluris/</li> <li>https://www.w3.org/Provider/Style/URI.html</li> <li>https://www.w3.org/wiki/GoodURIs</li> <li>https://www.w3.org/TR/dwbp/</li> <li>https://www.w3.org/TR/ld-bp/</li> </ul> </li> </ul>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/","title":"Define Prefixes / Namespaces","text":"","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#introduction","title":"Introduction","text":"<p>A namespace declaration consists of a prefix name and a namespace IRI. Namespace declarations allow for the abbreviation of IRIs by using a prefixed resource name instead of a full IRI.</p> <p>For example, after defining a namespace with the values</p> <ul> <li>prefix name = <code>cohw</code>, and the</li> <li>namespace IRI = <code>https://data.company.org/hardware/</code></li> </ul> <p>you can use the term\u00a0<code>cohw:test</code>\u00a0as an abbreviation for the full IRI\u00a0<code>https://data.company.org/hardware/test</code>.</p> <p>This is particularly useful when you have to write source code in Turtle and SPARQL.</p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#managing-namespace-declarations","title":"Managing Namespace Declarations","text":"","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#using-the-vocabulary-catalog","title":"Using the Vocabulary Catalog","text":"<p>After installing a vocabulary from the\u00a0Vocabulary Catalog, the vocabulary namespace declaration is automatically added to all integration projects.</p> <p>In order to get the prefix name and the namespace IRI from the vocabulary graph, the following terms from the VANN vocabulary need to be used on the Ontology resource.</p> <ul> <li>vann:preferredNamespacePrefix - to specify the prefix name</li> <li>vann:preferredNamespaceUri - to specify the namespace IRI</li> </ul> <p>In the Explore area, an Ontology with a correct namespace declaration looks like this:</p> <p></p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#using-cmemc","title":"Using cmemc","text":"<p>The <code>vocabulary</code> command group of cmemc has an <code>import</code> command that you can use to install arbitrary vocabulary documents and register them as vocabularies in Corporate Memory.</p> <p>Beginning with v22.2, this command has an additional option <code>--namespace</code> which you can use to set a vocabulary namespace even if the vocabulary does not include the data needed for autodiscovery:</p> <pre><code>cmemc vocabulary import my-ont.ttl --namespace myo https//example.org/my/`\n</code></pre>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#using-the-project-configuration","title":"Using the Project Configuration","text":"<p>In addition to the used vocabulary namespace declarations, you may want to add well-known namespaces for organizing the Knowledge Graphs.</p> <p>Such organization use cases include:</p> <ul> <li>Namespaces per class / resource type:<ul> <li>prefix name = <code>persons</code>, namespace IRI = <code>https://example.org/data/persons/</code></li> </ul> </li> <li>Namespaces per data owner or origin:<ul> <li>prefix name = <code>sales</code>, namespace IRI = <code>https://example.org/data/sales/</code></li> </ul> </li> </ul> <p>Prefixes in Data Integration are defined on a project basis. When creating a new project, a list of well-know prefixes is already declared.</p> <p>After selecting a project from the search results the prefix management is available in the project configuration in the lower right area:</p> <p></p> <p>By using the Edit Prefix Settings button in this Configuration area, you will see the Manage Prefixes dialog:</p> <p></p> <p>In this dialog, you are able to</p> <ul> <li>Delete a namespace declaration\u00a0\u2192 Delete Prefix</li> <li>Add a new namespace\u00a0declaration \u2192 Add</li> </ul>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#validating-namespace-declarations","title":"Validating Namespace Declarations","text":"<p>After adding namespace declarations to a project you are able to use the abbreviated IRIs in the user interface, for instance, in the mapping editor, the Turtle editor or the Query editor:</p> <p></p> <p></p> <p></p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/evaluate-template/","title":"Evaluate Jinja Template and Send an Email Message","text":"","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#introduction","title":"Introduction","text":"<p>In this tutorial we dynamically produce text with a Jinja template and send it in an email after the execution of a workflow. The email message contains information retrieved from a graph. The graph dataset is attached to the email as an N-triples file.</p> <p>Tip</p> <p>The complete tutorial is available as a\u00a0project file. You can import the projects:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li> <p>by using the command line interface</p> <pre><code>cmemc -c my-cmem project import tutorial-template.project.zip tutorial-evaluate-template\n</code></pre> </li> </ul> <p>Warning</p> <p>In the imported project, enter valid email credentials in the send eMail task to successfully execute the included workflow (see \u00a710.3).</p> <p>The following material is used in this tutorial:</p> <ul> <li> <p>RDF graph containing company information regarding employees, products and services: company.ttl</p> <pre><code>&lt;http://ld.company.org/prod-instances/hw-A181-1118563&gt; a prod:Hardware ;\nrdfs:label \"A181-1118563 - Compensator Switch\" ;\nprod:compatibleProduct &lt;http://ld.company.org/prod-instances/hw-M558-2275045&gt; ;\nprod:depth_mm 14 ;\nprod:hasCategory &lt;http://ld.company.org/prod-instances/prod-cat-Switch&gt; ;\nprod:hasProductManager &lt;http://ld.company.org/prod-instances/empl-Adolfina.Hoch%40company.org&gt; ;\nprod:height_mm 32 ;\nprod:id \"A181-1118563\" ;\nprod:name \"Compensator Switch\" ;\nprod:price &lt;http://ld.company.org/prod-instances/price-hw-A181-1118563-EUR&gt; ;\nprod:weight_g 5 ;\nprod:width_mm 22 .\n...\n</code></pre> </li> </ul> <p>The tutorial consists of the following steps, which are described in detail below.</p>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#1-upload-the-company-graph","title":"1 Upload the Company Graph","text":"<p>Info</p> <p>The vocabulary contains the classes and properties needed to map the source data into entities in the Knowledge Graph.</p> <ol> <li> <p>In Corporate Memory, click Knowledge Graphs in the\u00a0navigation under\u00a0Explore on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0on the + symbol next to the search field on the top left side of the page.</p> <p></p> </li> <li> <p>In the dialog box, click New graph from File.</p> <p></p> </li> <li> <p>Drop the file company.ttl onto the dialog box, or click on browse to navigate to the file.</p> <p></p> </li> <li> <p>In the Target graph URI field, enter http://ld.company.org/prod-inst-jinja/ and click\u00a0Create option \u2018http://ld.company.org/prod-inst-jinja/\u2018</p> <p></p> </li> <li> <p>Tick the Add new graph checkbox and click Upload.</p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#2-create-a-project","title":"2 Create a Project","text":"<ol> <li> <p>Click Projects in the\u00a0navigation under\u00a0Build on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Create\u00a0at the top of the page.\u202f\u00a0</p> </li> <li> <p>Select\u00a0Project\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details (Title) and click Create.</p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#3-create-a-knowledge-graph-dataset","title":"3 Create a Knowledge Graph Dataset","text":"<p>Info</p> <p>The Knowledge graph dataset holds the Company graph we uploaded earlier.</p> <ol> <li> <p>Click Create\u00a0at the top of the page.</p> <p></p> </li> <li> <p>Select\u00a0Knowledge Graph\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details, such as Label and, for Graph, the IRI of the company graph http://ld.company.org/prod-inst-jinja/.     When finished, click Create.</p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#4-create-a-sparql-select-query-task-item","title":"4 Create a SPARQL Select Query Task Item","text":"<p>Info</p> <p>The SPARQL select query is used to retrieve the data from the company graph that we want to include in our email. It counts the instances of employees, managers, departments, products, and products that have other compatible products in the database.</p> <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0SPARQL Select query\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details such as Title and Description, then copy the above query and paste in the Select query field.     When finished, click Create.</p> <pre><code>PREFIX pv: &lt;http://ld.company.org/prod-vocab/&gt;\n\nSELECT ?employees\n    ?managers\n    ?departments\n    ?products\n    ?product_compatibility\n    ?currentDateTime\nFROM &lt;http://ld.company.org/prod-inst-jinja/&gt;\nWHERE {\n    { SELECT ( COUNT( DISTINCT ?employee ) AS ?employees ) {\n        ?employee a pv:Employee }\n    }\n    { SELECT ( COUNT( DISTINCT ?manager ) AS ?managers ) {\n        ?manager a pv:Manager }\n    }\n    { SELECT ( COUNT( DISTINCT ?department ) AS ?departments ) {\n        ?department a pv:Department }\n    }\n    { SELECT ( COUNT( DISTINCT ?product_) AS ?products ) {\n        ?product_ a pv:Hardware }\n    }\n    { SELECT ( COUNT( DISTINCT ?comp_product_ ) AS ?product_compatibility ) {\n        ?comp_product_  pv:compatibleProduct ?prod }\n    }\n    BIND( now() AS ?currentDateTime )\n}\n</code></pre> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#5-create-an-evaluate-template-task-item","title":"5 Create an Evaluate Template Task Item","text":"<p>Info</p> <p>The Jinja template in this item acts as the template for our email message.</p> <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0Evaluate template\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details, such as Label and Template.     In the Template field enter the following Jinja template.     Select jinja in the Language field.     When finished, click Create.</p> <pre><code>Hi,\n\nattached is the workflow result as an N-Triples file.\nTimestamp: {{ currentDateTime }}\n\nProduct compatibility:\n{{ product_compatibility }} out of {{ products }} products have compatible alternatives.\n\nOrganization information:\nThere are {{ managers }} managers and {{ employees }} employees in {{ departments }} departments.\n</code></pre> <p></p> </li> </ol> <p>Note</p> <p>The variable names correspond to those in the SPARQL query we previously created.</p>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#6-create-a-text-dataset","title":"6 Create a Text Dataset","text":"<p>Info</p> <p>The text dataset holds a text file that will contain the evaluated Jinja template that will be our email message.</p> <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0Text\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details, such as Label and FILE.     Under FILE, select Create empty file and enter the filename in the New file name field.     When finished, click Create.</p> <p></p> </li> </ol> <p>Note</p> <p>The Evaluate template operator can also be connected directly to the Transform. In this case, skip this section and enter output instead of text for the Value path of the value mapping in the Transform (see \u00a77.6).</p>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#7-create-a-transform","title":"7 Create a Transform","text":"<p>Info</p> <p>The Transform retrieves the text from the Text dataset to be sent as our email message.</p> <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.\u202f\u00a0</p> </li> <li> <p>Select\u00a0Transform\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required fields, such as Label and INPUT TASK Dataset.</p> <p></p> </li> <li> <p>Expand the \u00a0menu by clicking the arrow on the right side of the page\u00a0to expand the menu.</p> </li> <li> <p>Click the circular blue\u00a0+ icon on the lower right and select Add value mapping.</p> <p></p> </li> <li> <p>In the Target property field enter message (the parameter name for the email message) and in the Value path field enter text (the path for the text in the Text dataset).     When finished, click Save.</p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#8-create-a-request-rdf-triples-task-item","title":"8 Create a Request RDF Triples Task Item","text":"<p>Info</p> <p>The Request RDF triples task is used to write all tripled from the company graph into an RDF dataset in NTriples serialization.</p> <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0Request RDF triples\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the Label field and click Create.</p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#9-create-an-rdf-dataset","title":"9 Create an RDF Dataset","text":"<p>Info</p> <p>The RDF dataset holds an NTriples file that contains the triples requested by the Request RDF triples task, which we will send as the email attachment.</p> <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0RDF\u00a0and click\u00a0Add..</p> <p></p> </li> <li> <p>Fill in the required details, such as Label and FILE.     Under FILE, select Create empty file and enter a filename for the NTriples file in the New file name field.     When finished, click Create.</p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#10-create-a-send-email-task-item","title":"10 Create a Send Email Task Item","text":"<ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0Send eMail\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details, such as Label, your email credentials for sending, and the recipient email address(es).     When finished, click Create.</p> <ul> <li>Host: The SMTP host, e.g, mail.myProvider.com</li> <li>Port: The SMTP port</li> <li>User: The username for the email account</li> <li>Password: The password for the email account</li> <li>To: The recipient email address(es)</li> </ul> <p></p> <p></p> </li> </ol>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#11-create-the-workflow","title":"11 Create the Workflow","text":"<ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>Select\u00a0Workflow\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Fill in the required details (Label) and click Create.</p> <p></p> </li> <li> <p>In the workflow editor, arrange and connect the items as shown below.     Items can be dragged from the list of items on the left side onto the canvas.     To connect the outputs and inputs, click and hold the output on the right side of an item and drag it to the input on the left side of another item.</p> <ul> <li>The Knowledge Graph dataset connects to the Request RDF triples task and the SPARQL Select query task.</li> <li>The Request RDF triples task connects to the RDF dataset.     It requests all triples from the products graph and sends them to the dataset.</li> <li>The RDF dataset connects to the Send eMail task.     It holds the NTriples file that will be attached to the email.</li> <li>The SPARQL Select query task connects to the Evaluate template task.     Note that the graph to be queried is specified in the SPARQL query itself with the FROM clause, while the input only triggers its execution.     The query results are sent to its output.</li> <li>The Evaluate template task connects to the Text dataset.     It receives the SPARQL query results and sends the evaluated Jinja template to its output.</li> <li>The Text dataset connects to the Transform.     It holds the text file with the evaluated Jinja template and acts as input for the Transform.</li> </ul> <p></p> <p>Note</p> <p>The Evaluate template operator can also be connected directly to the Transform. In this case, skip \u00a76 and enter output instead of text for the Value path of the value mapping in the Transform (see \u00a77.6).</p> </li> <li> <p>Click on three dots of the Send eMail task, select Config and tick the check box to enable the config port.</p> <p></p> </li> <li> <p>Connect the output of the Transform to the config port located on the top of the Send eMail task.     When finished, click Save.     The complete workflow now looks as shown below.</p> <p></p> </li> </ol> <p>Info</p> <p>The Transform sends the message parameter with our message text as its value to the Send eMail task.</p>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/evaluate-template/#12-execute-the-workflow","title":"12 Execute the Workflow","text":"<ol> <li> <p>Execute the Workflow by clicking the play button (\u25b6).</p> <p></p> </li> </ol> <p>Info</p> <p>After the workflow has finished you can find an email in the mailbox of the recipient address you specified for the Send eMail task.</p>","tags":["AdvancedTutorial","EvaluateTemplate"]},{"location":"build/extracting-data-from-a-web-api/","title":"Extracting data from a Web API","text":"","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#introduction","title":"Introduction","text":"<p>This tutorial demonstrates how you can build a Knowledge Graph based on input data from a Web API. The tutorial is based on the GitHub API (v3), which we will use to fetch repository data of a certain organization and create a Knowledge Graph from the response.</p> <p>Abstract</p> <p>The complete tutorial is available as a\u00a0project file. You can import this project:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li>by using the command line interface</li> </ul> <pre><code>$ cmemc -c my-cmem project import tutorial-webapi.project.zip web-api\n</code></pre> <p>In order to get familiar with the API, simply fetch an example response with this command:</p> <pre><code>$ curl https://api.github.com/orgs/vocol/repos\n</code></pre> <p>The HTTP Get request retrieves all repositories of a GitHub organization named vocol.</p> <p>The JSON response includes the data for all repositories (mobivoc, vocol, \u2026). You can also download the response file here:\u00a0repos.json.</p> <pre><code>[\n    {\n        ...\n        \"id\": 22646219,\n        \"name\": \"mobivoc\",\n        ...\n    },\n    {\n        ...\n        \"id\": 22646629,\n        \"name\": \"vocol\",\n        ...\n    },\n    {\n        ...\n        \"id\": 30964669,\n        \"name\": \"scor\",\n        ...\n    },\n    ...\n]\n</code></pre>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#1-register-a-web-api","title":"1 Register a Web API","text":"<ol> <li> <p>Click the\u00a0Create\u00a0button (top right) in the data integration workspace and select the type\u00a0REST request.</p> <p></p> </li> <li> <p>Define a Label, Description and the URL of the Web API. Example input:\u00a0<code>https://api.github.com/orgs/vocol/repos</code>.</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#2-create-a-json-parser","title":"2 Create a JSON parser","text":"<p>As we are only interested in the HTTP Message Body which holds the JSON repository data, we first have to parse the body from the entire HTTP response.</p> <ol> <li> <p>Click the\u00a0Create\u00a0button (top right) in the data integration workspace and select the type\u00a0Parse JSON.</p> <p></p> </li> <li> <p>Define a Label, a Description, and the Input path. All other fields can keep the default settings. The default input path is always:\u00a0<code>&lt;http://silkframework.org/vocab/taskSpec/RestTaskResult/responseBody&gt;</code></p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#3-create-a-json-dataset","title":"3 Create a JSON Dataset","text":"<p>To create a JSON-to-RDF-mapping within Corporate Memory, we have to first register an example response from the API (repos.json). Based on the schema of the response, we can then define step-by-step the mappings, which are used to build the Knowledge Graph.</p> <ol> <li> <p>Click the Create button (top right) in the data integration workspace and select the type\u00a0JSON.</p> <p></p> </li> <li> <p>Upload the JSON file\u00a0repos.json\u00a0(API response) as a Dataset into Corporate Memory.</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#4-create-a-knowledge-graph","title":"4 Create a Knowledge Graph","text":"<p>The Knowledge Graph will be used to integrate all data coming from one or more APIs. The Knowledge Graph receives RDF triples from the defined Transformations for each API.</p> <ol> <li> <p>Click the Create button (top right) in the data integration workspace and select the type Knowledge Graph.</p> <p></p> </li> <li> <p>Provide the Knowledge Graph with a\u00a0Label\u00a0and\u00a0Description, as well as the following (example)\u00a0Graph\u00a0URI:\u00a0<code>http://ld.company.org/repository-data/</code></p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#5-create-a-transformation","title":"5 Create a Transformation","text":"<p>In order to transform the input data from the API, in our example structured in JSON, we have to define a mapping to create RDF triples which are then written to the Knowledge Graph.</p> <ol> <li> <p>Click the Create button (top right) in the data integration workspace and select the type\u00a0Transform.</p> <p></p> </li> <li> <p>Provide the Transformation with a\u00a0Label\u00a0and\u00a0Description, configure the\u00a0Input Dataset\u00a0(Repos.json) and the\u00a0Output Dataset\u00a0(Repository Knowledge Graph).</p> <p></p> </li> <li> <p>Click the\u00a0Mapping Editor\u00a0button in the previously defined Transformation.</p> <p></p> </li> <li> <p>In the following screenshots, we provide an example mapping for the data received by the GitHub API. For more complex mappings, we recommend the Tutorial\u00a0Lift data from JSON and XML sources.</p> <p></p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#6-create-a-workflow","title":"6 Create a Workflow","text":"<p>To build a workflow that combines all the elements we previously built, we now define a workflow for (1) requesting the data from the GitHub API, (2) parsing the HTTP response we receive, (3) transforming the JSON data into RDF triples and finally (4) writing the RDF triples into the Knowledge Graph.</p> <ol> <li> <p>Click the\u00a0Create\u00a0button (top right) in the data integration workspace and select the type\u00a0Workflow.</p> <p></p> </li> <li> <p>Provide the Transformation with a\u00a0Label\u00a0and a\u00a0Description.</p> <p></p> </li> <li> <p>Click the\u00a0Workflow Editor\u00a0button in the menu of the created workflow.</p> <p></p> </li> <li> <p>Drag and drop the different items into the Workflow Editor and combine them with one another (see example screenshot).\u00a0Save\u00a0the workflow, and press the\u00a0run symbol\u00a0to execute the workflow.</p> <p></p> </li> <li> <p>Validate the result by clicking on the\u00a0Workflow Report\u00a0tab and see the result of your execution. In this example, 15x repositories were found from the GitHub API request.</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/introduction-to-the-user-interface/","title":"Introduction to the user interface","text":"<p>This page provides a short introduction to the BUILD / Data Integration workspace incl. projects and different tasks.</p>"},{"location":"build/introduction-to-the-user-interface/#workbench","title":"Workbench","text":"<p>The workbench is the main entry point to the BUILD interface and provides a list view of your work, structured in projects.</p> <p></p> <p>On the left-hand side, a facet list provides an overview of different item types:</p> <ul> <li>Projects are the main structure and consist of datasets, workflows and different tasks (transformations, linking and other tasks)</li> <li>Datasets are registered data sources from files or endpoints, or internally managed.</li> <li>Transform tasks take an input dataset and execute a (hierarchical) set of mapping and transformation rules, and generate data for an output dataset.</li> <li>Linking tasks compare entities from two datasets according to (hierarchical) sets of comparators and generate links between these datasets in an output (link) dataset.</li> <li>Workflows can combine all items in a project in order to create a structured work plan which can be executed on demand, controlled remotely (e.g. via\u00a0cmemc) or executed\u00a0from a\u00a0scheduler.</li> </ul> <p>The interface can be used to browse from a global level to a project level as well as to search for items globally and project wide.</p> <p>On the top right side in the header, there is a big blue Create (+) button which allows for creation of all possible data integration items:</p> <p></p>"},{"location":"build/introduction-to-the-user-interface/#projects","title":"Projects","text":"<p>Selecting a project from the search results opens the project details view (see above).</p> <p>The project is the main concept to structure your work. In the workspace, you can review your projects, create new or delete old ones, and import or export them.</p> <p>Each project is self-contained and holds all relevant parts, such as the following:</p> <ul> <li>the raw data resources (local or remote data files) that form the starting point for your work</li> <li>the datasets that provide uniform access to different kinds of data resources</li> <li>transformation tasks to transform datasets (e.g., from one format or schema to another)</li> <li>linking tasks to integrate different datasets</li> <li>workflows to orchestrate the different tasks</li> <li>definitions of URI prefixes used in the project</li> </ul> <p>Resources, datasets, tasks and other parts of a project can be added and configured from within the workspace. By clicking the <code>Show Details</code>\u00a0button on any item in a project, you can \u2018look inside\u2019 and define the actual transformation or linking rules, or assemble the actual workflow.</p>"},{"location":"build/introduction-to-the-user-interface/#datasets","title":"Datasets","text":"<p>Selecting a dataset in a project or from the search results opens the dataset details view.</p> <p></p> <p>Here you have the following options:</p> <ul> <li>Change the meta data of the dataset (Summary - label and description, top left main area),</li> <li>Change the Configuration parameters of the dataset (lower right side area),</li> <li>Browse to other Related Items, such as transforms, linking or workflow tasks which use this dataset (top right side area),</li> <li>Get a Data Preview as well as generate and view profiling information (lower left main area).</li> </ul> <p>A dataset represents an abstraction over raw data. In order to work with data, you have to make it available in the form of a dataset. A dataset provides a source or destination for the different kinds of tasks. I.e., it may be used to read entities for transformation or linking, and it can be used to write transformed entities and generated links.</p> <p>There is a range of different dataset types for different kinds of source data. Important dataset types include:</p> <ul> <li>Knowledge Graph - Read RDF from or write RDF to a Knowledge Graph embedded in Corporate Memory.</li> <li>CSV - Read from or write to an CSV file.</li> <li>XML - Read from or write to an XML file.</li> <li>JSON - Read from or write to a JSON file.</li> <li>JDBC endpoint - Connect to an existing JDBC endpoint.</li> <li>Variable dataset - Dataset that acts as a placeholder in workflows and is replaced at request time.</li> <li>Excel - Read from or write to an Excel workbook in Open XML format (XLSX).</li> <li>Multi CSV ZIP - Reads from or writes to multiple CSV files from/to a single ZIP file.</li> </ul> <p>Other options include:</p> <ul> <li>Internal dataset - Dataset for storing entities between workflow steps.</li> <li>RDF - Dataset which retrieves and writes all entities from/to an RDF file. The dataset is loaded in-memory and thus the size is restricted by the available memory. Large datasets should be loaded into an external RDF store and retrieved using the SPARQL dataset instead.</li> <li>SPARQL endpoint - Connect to an existing SPARQL endpoint.</li> <li>In-memory dataset - A dataset that holds all data in-memory.</li> <li>Avro - Read from or write to an Apache Avro file.</li> <li>ORC - Read from or write to an Apache ORC file.</li> <li>Parquet - Read from or write to an Apache Parquet file.</li> <li>Hive database - Read from or write to an embedded Apache Hive endpoint.</li> <li>SQL endpoint - Provides a JDBC endpoint that exposes workflow or transformation results as tables, which can be queried using SQL.</li> <li>Neo4j - Connect to an existing Neo4j property graph database system.</li> </ul>"},{"location":"build/introduction-to-the-user-interface/#transform-tasks","title":"Transform tasks","text":"<p>The purpose of transform tasks\u00a0is to transform datasets (e.g., from one format or schema to another). More specifically, Transform Tasks create Knowledge Graphs out of raw data sources.</p> <p>Selecting a transform task in a project or from the search results\u00a0opens the transform task details view.</p> <p></p> <p>Here you have the following options:</p> <ul> <li>Change the metadata of the transformation task (Summary - label and description, top left main area)</li> <li>Change the Configuration parameters of the transformation task (lower right side area),</li> <li>Browse to other Related Items, such as used datasets or workflows using this task (top right side area),</li> <li>Open the Mapping Editor in order to change the transformation rules,</li> <li>Evaluate the Transformation to check for issues with the rules,</li> <li>Execute the Transformation rules in order to create new data,</li> <li>Clone the task, creating a new transform task with the same rules.</li> </ul>"},{"location":"build/introduction-to-the-user-interface/#linking-tasks","title":"Linking tasks","text":"<p>The purpose of linking tasks is to integrate two datasets (source and target) by generating a linkset that contains links from the source to the target dataset.</p> <p>Selecting a linking task in a project or from the search results opens the transform task details view.</p> <p> </p> <p>Here you have the following options:</p> <ul> <li>Change the metadata of the linking task (Summary - label and description, top left main area),</li> <li>Change the Configuration parameters of the linking task (lower right side area),</li> <li>Browse to other Related Items, such as used datasets or workflows using this task (top right side area),</li> <li>Open the Linking Editor in order to change the comparison rules,</li> <li>Evaluate the Linking to check for issues with the rules,</li> <li>Execute the Linking task in order to create a linkset,</li> <li>Clone the task, creating a new linking task with the same comparisons.</li> </ul> <p>As in the case of transform tasks, the output linkset of a linking task can either be written directly to an output dataset, or provide the input to another task if integrated into a workflow.</p>"},{"location":"build/introduction-to-the-user-interface/#workflows","title":"Workflows","text":"<p>For projects that go beyond one or two simple transform or linking tasks, you can create complex workflows. In a workflow, you can orchestrate datasets, transform tasks and linking tasks via connections between their inputs and outputs, and in this way perform complex tasks.</p> <p>Selecting a workflow in a project or from the search results opens the workflow details view.</p> <p></p> <p>Here you have the following options:</p> <ul> <li>Change the metadata of the linking task (Summary - label and description, top left main area)</li> <li>Browse to\u00a0Related Items, such as used datasets and tasks (top right side area),</li> <li>Open the Worflow Editor in order to change the workflow (here you can as well start the workflow),</li> <li>Clone the workflow creating a new workflow with the same task.</li> </ul>"},{"location":"build/kafka-consumer/","title":"Build Knowledge Graphs from Kafka Topics","text":"","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#introduction","title":"Introduction","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. Kafka is widely used in enterprises for data pipelines, streaming analytics, data integration and other applications.</p> <p>By using the cmem-plugin-kafka Python Plugin, you can consume messages from Apache Kafka and use them inside Corporate Memory Workflows.</p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#installation","title":"Installation","text":"<p>In order to use the Kafka Consumer workflow task, you need to extend your Corporate Memory instance with the <code>cmem-plugin-kafka</code> package. This can be done by using cmemc:</p> Installing cmem-plugin-kafka on the instance 'my-cmem'<pre><code>$ cmemc -c my-cmem admin workspace python install cmem-plugin-kafka\nInstall package cmem-plugin-kafka ... done\n</code></pre> <p>You can validate your installation by listing all installed plugins (from all packages):</p> <pre><code>$ cmemc -c my-cmem admin workspace python list-plugins\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ID                                \u2503 Package ID            \u2503 Type           \u2503 Label                             \u2503\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\n\u2503 cmem_plugin_kafka-ReceiveMessages \u2503 cmem-plugin-kafka     \u2503 WorkflowPlugin \u2503 Kafka Consumer (Receive Messages) \u2503\n\u2503 cmem_plugin_kafka-SendMessages    \u2503 cmem-plugin-kafka     \u2503 WorkflowPlugin \u2503 Kafka Producer (Send Messages)    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n</code></pre>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#setup","title":"Setup","text":"<p>Once you installed the package, you can create a Kafka Consumer task. This task connects to a topic on an Apache Kafka broker.</p> <p>Warning</p> <p>The Kafka Consumer task will be executed inside a workflow. This means on each workflow execution this task retrieves new messages from the topic. Hence, the respective workflow needs to be scheduled regularly. It will not run continuously inside the workflow and consume messages. The Kafka Consumer task is finished when there are no more messages in the queue or a configurable message limit has been reached.</p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#create-and-configure-a-kafka-consumer","title":"Create and configure a Kafka Consumer","text":"<p>In Create new item window, select Kafka Consumer (Receive Messages) and click Add.</p> <p></p> <p>Configure the Kafka Consumer according to the topic that shall be consumed:</p> <ul> <li>Bootstrap Server - URL of the Kafka broker including the port number (commonly port \u00b49092)</li> <li>Security Protocol - Security mechanism used for authentication</li> <li>Topic - Name / ID of the topic where messages are published</li> <li>Advanced Section<ul> <li>Messages Dataset - A dataset (XML/JSON) where messages can be written to. Leave this field empty to output the messages as entities (see below).</li> <li>SASL authentication settings as provided by your Kafka broker</li> <li>Auto Offset Reset - Consumption starts either at the earliest offset or the latest offset.</li> <li>Consumer Group Name - Consumer groups can be used to distribute the load of messages (partitions) between multiple consumers of the same group (c.f. Kafka Concepts).</li> <li>Client Id - An optional identifier of the client which is communicated to the server. When this field is empty, the plugin defaults to <code>DNS:PROJECT_ID:TASK_ID</code>.</li> <li>Local Consumer Queue Size - Maximum total message size in kilobytes that the consumer can buffer for a specific partition. The consumer will stop fetching from the partition if it hits this limit. This helps prevent consumers from running out of memory.</li> <li>Message Limit - The maximum number of messages to fetch and process in each run. If <code>0</code> or less, all messages will be fetched.</li> <li>Disable Commit Setting this to <code>true</code> will disable committing messages after retrival. This means you will receive the same messages on the next execution (for testing, development, or debugging).</li> </ul> </li> </ul> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#execute-a-kafka-consumer-within-a-workflow","title":"Execute a Kafka Consumer within a Workflow","text":"<p>There are two main modes how the consumer handles received messages: either the messages are written to a single dataset or the messages are outputted as entities to be processed by subsequent processors in the workflow.</p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#write-messages-to-a-dataset","title":"Write Messages to a Dataset","text":"<p>In order to write the received messages to a dataset, the option Messages Dataset needs to be set. Only JSON and XML message formats are supported in this mode. So depending on the message format a JSON or XML Dataset needs to be created and configured as the Messages Dataset.</p> <p></p> <p>To execute the Kafka Consumer it needs to be placed inside a Workflow. The messages will be written to the dataset, which can then be used as a source for further processing.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#messages-as-entities","title":"Messages as Entities","text":"<p>In the \u201cmessage streaming mode\u201d (Messages Dataset is not set) the received messages will be generated as entities and forwarded to the subsequent operator in the workflow. This mode is not limited to any message format. The generated message entities will have the following flat schema:</p> <ul> <li>key \u2014 the optional key of the message,</li> <li>content \u2014 the message itself as plain text,</li> <li>offset \u2014 the given offset of the message in the topic,</li> <li>ts-production \u2014 the timestamp when the message was written to the topic,</li> <li>ts-consumption \u2014 the timestamp when the message was consumed from the topic.</li> </ul> <p>Connect the output of Kafka Consumer inside a Workflow to a tabular dataset (e.g. a CSV Dataset) or directly to a transformation task.</p> <p></p> <p>The message content is captured as plain text. In order to process complex message content, the <code>content</code> path needs to be parsed with operators such as Parse JSON or Parse XML to process the message content in a transformation.</p> <p></p> <p>Any modifications to the message set, such as filtering, can be done prior to parsing the content. One could for example remove duplicates (according to the message key) from the messages by using the Distinct-by task.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/kafka-consumer/#sample-project","title":"Sample Project","text":"<p>To walk through the examples of this tutorial a sample project is available for download. Note, that the Kafka Broker configured in this project is no longer reachable. You will need to setup your own Kafka Broker.</p>","tags":["AdvancedTutorial","KnowledgeGraph","PythonPlugin"]},{"location":"build/lift-data-from-json-and-xml-sources/","title":"Lift data from JSON and XML source","text":"","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#introduction","title":"Introduction","text":"<p>This tutorial shows how you can build a Knowledge Graph based on input data from hierarchical sources like a JavaScript Object Notation file (.json) or an Extensible Markup Language file (.xml).</p> <p>Abstract</p> <p>The complete tutorial is available as a project file (XML) and a\u00a0project file (JSON). You can import these projects:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li> <p>by using the command line interface</p> <pre><code>cmemc -c my-cmem project import tutorial-xml.project.zip xml-transformation\n</code></pre> <pre><code>cmemc -c my-cmem project import tutorial-json.project.zip json-transformation\n</code></pre> </li> </ul> <p>The documentation consists of the following steps, which are described in detail below.</p> <p>The following material is used in this tutorial:</p> <ul> <li> <p>Sample vocabulary describing the data in the JSON and XML files: products_vocabulary.nt</p> <p></p> </li> <li> <p>Sample JSON file: services.json</p> <pre><code>[\n    {\n        \"Price\": \"748,40 EUR\",\n        \"ProductManager\": \"Lambert.Faust@company.org\",\n        \"Products\": \"O491-3823912, I965-1821441, Z655-3173353, ...\",\n        \"ServiceID\": \"Y704-9764759\",\n        \"ServiceName\": \"Product Analysis\"\n    },\n    {\n        \"Price\": \"1082,00 EUR\",\n        \"ProductManager\": \"Corinna.Ludwig@company.org\",\n        \"Products\": \"Z249-1364492, L557-1467804, C721-7900144, ...\",\n        \"ServiceID\": \"I241-8776317\",\n        \"ServiceName\": \"Component Confabulation\"\n    },\n    ...\n]\n</code></pre> </li> <li> <p>Sample XML file: orgmap.xml</p> <pre><code>&lt;orgmap&gt;\n    &lt;dept id=\"73191\" name=\"Engineering\"&gt;\n        &lt;manager&gt;\n            &lt;email&gt;Thomas.Mueller@company.org&lt;/email&gt;\n            &lt;name&gt;Thomas Mueller&lt;/name&gt;\n            &lt;address&gt;Karl-Liebknecht-Stra\u00dfe 885, 82003 Tettnang&lt;/address&gt;\n            &lt;phone&gt;+49-8200-38218301&lt;/phone&gt;\n        &lt;/manager&gt;\n        &lt;employees&gt;\n            &lt;employee&gt;\n                &lt;email&gt;Corinna.Ludwig@company.org&lt;/email&gt;\n                &lt;name&gt;Corinna Ludwig&lt;/name&gt;\n                &lt;address&gt;Ringstra\u00dfe 276&lt;/address&gt;\n                &lt;phone&gt;+49-1743-24836762&lt;/phone&gt;\n                &lt;productExpert&gt;Memristor, Gauge, Encoder&lt;/productExpert&gt;\n            &lt;/employee&gt;\n            &lt;employee&gt;\n                &lt;email&gt;Karen.Brant@company.org&lt;/email&gt;\n                &lt;name&gt;Karen Brant&lt;/name&gt;\n                &lt;address&gt;Friedrichstra\u00dfe 664, 30805 Willich&lt;/address&gt;\n                &lt;phone&gt;(00530) 5040048&lt;/phone&gt;\n                &lt;productExpert&gt;Inductor&lt;/productExpert&gt;\n            &lt;/employee&gt;\n            ...\n        &lt;/employees&gt;\n        &lt;products&gt;\n            &lt;product id=\"Z249-1364492\" /&gt;\n            &lt;product id=\"O184-6903943\" /&gt;\n            &lt;product id=\"V404-9975399\" /&gt;\n            &lt;product id=\"F344-7012314\" /&gt;\n            &lt;product id=\"N463-8050264\" /&gt;\n            &lt;product id=\"M605-5951566\" /&gt;\n            &lt;product id=\"N733-1946687\" /&gt;\n        &lt;/products&gt;\n        &lt;services&gt;\n            &lt;service id=\"I241-8776317\" /&gt;\n            &lt;service id=\"D215-3449390\" /&gt;\n        &lt;/services&gt;\n    &lt;/dept&gt;\n    &lt;dept id=\"22183\" name=\"Product Management\"&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n        ...\n    &lt;/dept&gt;\n    ...\n&lt;/orgmap&gt;\n</code></pre> </li> </ul>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#1-register-the-vocabulary","title":"1 Register the vocabulary","text":"<p>The vocabulary contains the classes and properties needed to map the source data into entities in the Knowledge Graph.</p> <ol> <li> <p>In Corporate Memory, click Vocabularies in the\u00a0navigation under\u00a0EXPLORE on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Register new vocabulary on the top\u00a0right of the Vocabulary catalog page\u00a0in Corporate Memory.</p> <p></p> </li> <li> <p>Define a Name, a Graph URI and a Description of the vocabulary. In this example we will use:</p> <ul> <li>Name: Product Vocabulary</li> <li>Graph URI: http://ld.company.org/prod-vocab/</li> <li>Description: Example vocabulary modeled to describe relations between products and services.</li> </ul> <p></p> </li> <li> <p>Click\u00a0REGISTER.</p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#2-upload-the-data-file","title":"2 Upload the data file","text":"<p>To add the data files, click Projects under BUILD in the navigation on the left side of the page. Follow the steps below for adding JSON and XML datasets.</p> JSONXML <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>In\u00a0Create new item\u00a0window, select\u00a0JSON\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Define a Label for the dataset and upload the services.json\u00a0file.\u00a0You can leave all the other fields at default values.</p> <p> </p> </li> <li> <p>Click Create.</p> </li> </ol> <ol> <li> <p>Press the Create button and select XML</p> <p></p> </li> <li> <p>Define a Label for the dataset and upload the orgmap.xml example file.\u00a0You can leave all the other fields at default values.</p> <p></p> </li> <li> <p>Click\u00a0Create.</p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#3-create-a-knowledge-graph","title":"3 Create a Knowledge Graph","text":"<ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.\u202f\u00a0</p> </li> <li> <p>In\u00a0Create new item\u00a0window, select\u00a0Knowledge Graph\u00a0and click\u00a0Add. The Create new item of type Knowledge Graph\u00a0window appears.</p> <p></p> </li> <li> <p>Fill in the required details such as Label and Description.</p> JSONXML <p>Define a Label for the Knowlege Graph and provide Graph uri.\u00a0You can leave all the other fields at default values.\u00a0In this example we use:</p> <ul> <li>Name: Service Knowledge Graph</li> <li>Graph: http://ld.company.org/prod-instances/</li> </ul> <p></p> <p>Define a Label for the Knowledge Graph and provide\u00a0Graph uri. You can leave all the other fields at default values.\u00a0In this example we will use:</p> <ul> <li>Name: Organization Knowledge Graph</li> <li>Graph: http://ld.company.org/organization-data/</li> </ul> <p></p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#4-create-a-transformation","title":"4 Create a Transformation","text":"<p>The transformation defines how an input dataset (e.g.: JSON or XML) will be transformed into an output dataset (e.g.: Knowledge Graph).</p> <ol> <li> <p>Click\u00a0Create\u00a0in your project.\u202f\u00a0</p> </li> <li> <p>On the\u00a0Create New Item\u00a0window, select\u00a0Transform\u00a0and click\u00a0Add\u00a0to create a new transformation.</p> <p></p> </li> <li> <p>In the\u00a0Create new item of type Transform\u00a0window, enter the required fields.</p> JSONXML <p>For this example, enter the following:</p> <ul> <li>Name:\u00a0Create Service Triples</li> <li>(optional) Description:\u00a0Lifts the Service file into the Knowledge Graph</li> <li>Select the Source Dataset:\u00a0Services JSON</li> <li>Select the Output Dataset:\u00a0Service_Knowledge_Graph</li> </ul> <p></p> <p>Click Create.</p> <p>For this example, enter the following:</p> <ul> <li>Name:\u00a0Create Organization Triples</li> <li>(optional) Description:\u00a0Lifts the Orgmap XML file into the Knowledge GraphOrgmap XML</li> <li>Select the Source Dataset:\u00a0Orgmap XML</li> <li>Type: dept (define the Source Type, which defines the XML element that should be iterated when creating resources)</li> <li>Select the Output Dataset:\u00a0Organization_Knowledge_Graph</li> </ul> <p></p> <p>Click Create.</p> </li> <li> <p>Expand the \u00a0menu by clicking the arrow on the right side of the page\u00a0to expand the menu.</p> </li> <li> <p>Click Edit\u00a0to create a base mapping.</p> <p></p> </li> <li> <p>Define the Target entity type from the vocabulary, the URI pattern and a Label for the mapping.</p> JSONXML <p>Target Entity Type defines the class that will be instantiated when the mapping rule is applied.</p> <p>The URI pattern that defines the URI that shall be generated for each individual</p> <ul> <li>http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case,</li> <li>service-instances/ complements the instances prefix by adding a common prefix for all service instances</li> <li>and finally {ServiceID} is a placeholder that will resolve to the json-key ServiceID (e.g. \u201cServiceID\u201d: \u201cY704-9764759\u201d)</li> </ul> <p>In this example we will use:</p> <ul> <li>Target Entity Type:\u00a0Service</li> <li>URI Pattern:\u00a0http://ld.company.org/prod-inst/service-instances/{ServiceID}</li> <li>An optional Label: Service</li> </ul> <p>Click SAVE.</p> <p>Example RDF triple in our Knowledge Graph based on the mapping definition:</p> <pre><code>&lt;http://ld.company.org/services-instances/Y704-9764759&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://ld.company.org/prod-vocab/Service&gt;\n</code></pre> <p>Target Entity Type defines the class that will be instantiated when the mapping rule is applied: Department</p> <p>The URI pattern that defines the URI that shall be generated for each individual: http://ld.company.org/department/{@id}</p> <ul> <li>http://ld.company.org/department/ is a common prefix for the department instances in this use case,</li> <li>and finally {@id} is a placeholder that will resolve the XML attribute of the XML tag dept, which was configured as the Source Type of this transformation (see previous steps)</li> </ul> <p>In this example we will use:</p> <ul> <li>Target Entity Type:\u00a0Department</li> <li>URI Pattern: http://ld.company.org/department/{@id}</li> <li>An optional Label: Department</li> </ul> <p>Click Save.</p> <p>Example RDF triple in our Knowledge Graph based on the mapping definition:</p> <pre><code>&lt;http://ld.company.org/department/73191 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://ld.company.org/prod-vocab/Department&gt;\n</code></pre> </li> <li> <p>Evaluate your mapping by pressing on the  button in the Examples of target data property to see at most three generated base URIs.</p> <p></p> <p>We have now created the Service entities in the Knowledge Graph. Next we will now add the name of our entity.</p> </li> <li> <p>Click the circular blue\u00a0+ icon on the lower right and select Add value mapping.</p> <p></p> JSONXML <p>Define the Target property, the Data type, the Value path (column name) and a Label for your value mapping. In this example, enter the following:</p> <ul> <li>Target Property: has product manager</li> <li>Data type: StringValueType</li> <li>Value path: ProductManager/name<ul> <li>which corresponds to the following element in the json-file: [ {\u201cProductManager\u201d: {\u00a0 \u201cname\u201d: \u201cCorinna Ludwig\u201d} \u2026 } \u2026]</li> </ul> </li> <li>An optional Label: has Product Manager</li> </ul> <p></p> <p>Click Save.</p> <p>Define the Target property, the Data type, the Value path (column name) and a Label for your value mapping. In this example we will use:</p> <ul> <li>Target Property: name</li> <li>Data type: StringValueType</li> <li>Value path:\u00a0dept/@name<ul> <li>which corresponds to the <code>department name</code> attribute in the XML file</li> </ul> </li> <li>An optional Label: department name</li> </ul> <p></p> <p>Click Save.</p> </li> </ol> <p>By clicking on the  button in the Examples of target data property, you can get a preview for 3x value mapping to be created.</p> JSONXML <p></p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#5-evaluate-a-transformation","title":"5 Evaluate a Transformation","text":"<p>Click Transform evaluation to evaluate the transformed entities.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#6-build-the-knowledge-graph","title":"6 Build the Knowledge Graph","text":"<ol> <li>Click Transform execution</li> <li>Press the  button and validate the results. In this example, 9x Service entities were created in our Knowledge Graph based on the mapping.</li> <li>You can click Knowledge Graphs\u00a0under\u00a0EXPLORE\u00a0to (re-)view of the created Knowledge Graphs</li> <li> <p>Enter the following URIs in the Enter search term for JSON and XML respectively.</p> <ul> <li>JSON / Service: http://ld.company.org/prod-instances/</li> <li>XML / Department: http://ld.company.org/organization-data/</li> </ul> JSONXML <p></p> <p></p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/","title":"Lift data from tabular data such as CSV, XSLX or database tables","text":"","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#introduction","title":"Introduction","text":"<p>This beginner-level tutorial shows how you can build a Knowledge Graph based on input data from a comma-separated value file (.csv), an excel file (.xlsx) or a database table (jdbc).</p> <p>Abstract</p> <p>The complete tutorial is available as a\u00a0project file. You can import this project</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li>by using the command line interface</li> </ul> <pre><code>$ cmemc -c my-cmem project import tutorial-csv.project.zip tutorial-csv\n</code></pre> <p>This step is optional and makes some of the following steps of the tutorial superfluous.</p> <p>The documentation consists of the following steps, which are described in detail below:</p> <ol> <li>Registration of the target vocabulary</li> <li>Uploading of the data (file)</li> <li>Creating the Transformation</li> <li>Configure Mapping</li> <li>Evaluate a Transformation</li> <li>Build the Knowledge Graph</li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#sample-material","title":"Sample Material","text":"<p>The following material is used in this tutorial, you should download the files and have them at hand throughout the tutorial:</p> <ul> <li> <p>Sample vocabulary which describes the data in the CSV files: products_vocabulary.nt</p> <p></p> </li> <li> <p>Sample CSV file:\u00a0services.csv</p> <p>Info</p> ServiceID ServiceName Products ProductManager Price Y704-9764759 Product Analysis O491-3823912, I965-1821441, Z655-3173353, \u2026 Lambert.Faust@company.org 748,40 EUR I241-8776317 Component Confabulation Z249-1364492, L557-1467804, C721-7900144, \u2026 Corinna.Ludwig@company.org 1082,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 </li> <li> <p>Sample Excel file: products.xlsx</p> <p>Info</p> ProductID ProductName Height Width Depth Weigth ProductManager Price I241-8776317 Strain Compensator 12 68 15 8 Baldwin.Dirksen@company.org 0,50 EUR D215-3449390 Gauge Crystal 77 58 19 15 Wanja.Hoffmann@company.org 2,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 </li> </ul>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#1-register-the-vocabulary","title":"1 Register the vocabulary","text":"<p>The vocabulary contains the classes and properties needed to map the data into the new structure in the Knowledge Graph.</p> Corporate Memorycmemc <ol> <li> <p>In Corporate Memory, click\u00a0Vocabularies\u00a0under\u00a0EXPLORE\u00a0in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Register new vocabulary\u00a0on the top right.</p> <p></p> </li> <li> <p>Define a Name, a Graph URI and a Description of the vocabulary. In this example we will use:</p> <ul> <li>Label: <code>Product Vocabulary</code></li> <li>Graph URI: <code>http://ld.company.org/prod-vocab/</code></li> <li>Description: <code>Example vocabulary modeled to describe relations between products and services.</code></li> <li>Upload File: Browse in your filesystem for the products_vocabulary.nt file and select it to be uploaded.</li> </ul> <p></p> </li> </ol> <pre><code>$ cmemc vocabulary import products_vocabulary.nt\n</code></pre>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#2-uploading-of-the-data-file","title":"2 Uploading of the data (file)","text":"<ol> <li> <p>In Corporate Memory, click\u00a0 Projects\u00a0under\u00a0BUILD\u00a0in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Create \u00a0at the top right of the page.\u202f</p> </li> <li> <p>In\u00a0the Create new item\u00a0window, select\u00a0Project\u00a0and click\u00a0Add. The\u00a0Create new item of type Project\u00a0window appears.\u202f\u00a0</p> </li> <li> <p>Fill in the required details such as Title and Description.\u00a0Alternatively, import the existing project by clicking Import Project File and selecting the file from your system.\u202f\u00a0</p> </li> <li> <p>Click\u00a0Create. Your project is created.</p> </li> </ol> Workflow viewcmemcJDBC <ol> <li> <p>Within your project, click on  Create workflow.</p> <p></p> </li> <li> <p>Fill out a label and click Create.</p> <p></p> </li> <li> <p>Drag and drop the services.csv sample file on the grid.</p> </li> <li> <p>Optionally change the Label, then click on Create.</p> <p></p> </li> <li> <p>Create a second dataset by drag &amp; drop it on the grid using products.xlsx\u00a0file.</p> </li> </ol> <pre><code>$ cmemc project create tutorial-csv\n\n$ cmemc dataset create --project tutorial-csv services.csv\n\n$ cmemc dataset create --project tutorial-csv products.xlsx\n</code></pre> <p>Instead of uploading the services.csv sample file into Corporate Memory, you can also load it into a SQL database and access it from Corporate Memory using the JDBC protocol.</p> <ol> <li> <p>In the project, Click\u00a0Create\u00a0and select the JDBC endpoint type.</p> <p></p> </li> <li> <p>Define a Label for the dataset, specify the JDBC Driver connection URL, the table name and the user and password to connect to the database. In this example we will use:</p> <ul> <li>Name: Services_ServiceDB</li> <li>JDBC Driver Connection URL: jdbc:mysql://mysql:3306/ServicesDB</li> <li>table: Services</li> <li>username: root</li> <li>password: ****</li> </ul> <p></p> <p>The general form of the JDBC connection string is:</p> <pre><code>jdbc:&lt;vendor&gt;://&lt;hostname&gt;:&lt;portNumber&gt;/&lt;databaseName&gt;\n</code></pre> <p>Default JDBC connection strings for popular Relational Database Management Systems:</p> Vendor Default JDBC Connection String Default Port Microsoft SQL Server jdbc:sqlserver::1433/ 1433 PostgreSQL jdbc:postgresql::5432/ 5432 MySQL jdbc:mysql::3306/ 3306 MariaDB jdbc:mariadb::3306/ 3306 IBM DB2* jdbc:db2::50000/ 50000 Oracle* jdbc:oracle:thin::1521/ 1521 <p>Info</p> <p>* IBM DB2 and Oracle JDBC drivers are not by default part of Corporate Memory, but can be added.</p> <p>Info</p> <p>Instead of selecting a table you can also specify a custom SQL query in the source query field.</p>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#3-creating-the-transformation","title":"3 Creating the Transformation","text":"<p>The transformation defines how an input dataset (e.g. CSV) will be transformed into an output dataset (e.g. Knowledge Graph).</p> <ol> <li> <p>Click on the right dot and select Connect to the newly created Transformation.</p> <p></p> </li> <li> <p>Fill out the Label with Lift Service Database.</p> <p></p> </li> <li> <p>Scroll down to Target vocabularies and choose Products vocabulary.</p> <p></p> </li> <li> <p>Click on Create.</p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#4-configure-mapping","title":"4 Configure Mapping","text":"<ol> <li> <p>Click on the 3 Dots from the previous created Transormation an choose Mapping Editor.</p> </li> <li> <p>Click\u00a0Mapping in the main area to expand its menu.</p> </li> <li> <p>Click\u00a0Edit\u00a0to create a base mapping.</p> <p></p> </li> <li> <p>Define the Target entity type from the vocabulary, the URI pattern and a label for the mapping. In this example we will use:</p> <ul> <li>Target entity type: Service</li> <li> <p>URI pattern:</p> <ul> <li>Click\u00a0Create custom pattern</li> <li>Insert\u00a0<code>http://ld.company.org/prod-inst/{ServiceID}</code>, where <code>http://ld.company.org/prod-inst/</code> is a common prefix for the instances in this use case, and <code>{ServiceID}</code> is a placeholder that will resolve to the column of that name.</li> </ul> </li> <li> <p>An optional Label: <code>Service</code></p> </li> </ul> <p></p> </li> <li> <p>Click\u00a0Save</p> </li> </ol> <p>Example RDF triple in our Knowledge Graph based on the mapping definition:</p> <pre><code>&lt;http://ld.company.org/prod-inst/Y704-9764759&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://ld.company.org/prod-vocab/Service&gt;\n</code></pre> <ol> <li> <p>Evaluate your mapping by clicking the Expand  button in the Examples of target data property to see at most three generated base URIs.</p> <p></p> <p>We have now created the Service entities in the Knowledge Graph. As a next step, we will add the name of the Service entity.</p> </li> <li> <p>Press the circular Blue + button on the lower right and select Add value mapping.</p> <p></p> </li> <li> <p>Define the Target property, the Data type, the Value path (column name) and a Label for your value mapping. In this example we will use:</p> <ul> <li>Target Property: name</li> <li>Data type: String</li> <li>Value path: ServiceName (which corresponds to the column of that name)</li> <li>An optional Label: service name</li> </ul> <p></p> </li> <li> <p>Click Save.</p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#5-evaluate-a-transformation","title":"5 Evaluate a Transformation","text":"<p>Go the Transform evaluation tab of your transformation to view a list of generated entities. By clicking one of the generated entities, more details are provided.</p> <p></p>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#6-build-the-knowledge-graph","title":"6 Build the Knowledge Graph","text":"<ol> <li> <p>Switch back to the Workflow view.</p> </li> <li> <p>Select the red dot on the right side and click Connect to the newly created Knowledge graph.</p> <p></p> </li> <li> <p>Define a Label for the Knowledge Graph and provide a graph uri. Leave all the other parameters at the default values. In this example we will use:</p> <ul> <li>Label: <code>Service Knowledge Graph</code></li> <li>Graph: <code>http://ld.company.org/prod-instances/</code></li> </ul> <p></p> </li> <li> <p>Click Create.</p> </li> <li> <p>Press the  button and click on Save and run workflow.</p> </li> <li> <p>Validate the results by selecting Workflow report In this example, 9x Service triples were created in our Knowledge Graph based on the mapping.</p> <p></p> </li> <li> <p>Click Knowledge Graph under Explore in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Optionally, you can click on the Settings Icon and add more columns to the viw.</p> <p></p> </li> <li> <p>Here you can add <code>name</code> for example.</p> <p></p> </li> <li> <p>Finally you can use the Explore Knowledge Graphs module to (re-)view of the created Knowledge Graph: <code>http://ld.company.org/prod-instances/</code></p> <p></p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/loading-jdbc-datasets-incrementally/","title":"Loading JDBC datasets incrementally","text":"","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#introduction","title":"Introduction","text":"<p>This tutorial walks you through the process of loading data incrementally from a JDBC Dataset (relational database table) into a Knowledge Graph.</p> <p>Abstract</p> <p>The complete tutorial is available as a project file. You can import this project:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li>by using the command line interface</li> </ul> <pre><code>cmemc -c my-cmem project import tutorial-webapi.project.zip web-api\n</code></pre> <p>Example SQL query for selecting a predefined range of rows</p> <p>We use the <code>LIMIT</code> and\u00a0 <code>OFFSET</code> clauses in SQL to retrieve only a portion of rows. This prevents loading all data at once from a table,\u00a0SQL Query with OFFSET AND LIMIT:</p> <pre><code>SELECT * FROM services OFFSET 0 LIMIT 5\n</code></pre> <p>This query retrieves the first 5 rows of a table named \u201cservices\u201d. If the OFFSET is changed to 5, it will retrieve the next 5 rows.</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#1-create-the-jdbc-dataset","title":"1 Create the JDBC dataset","text":"<p>To extract data from a relational database, you need to first register a JDBC endpoint in Corporate Memory. This tutorial assumes that you have access to the relational database from the Corporate Memory instance.</p> <ol> <li> <p>In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Click Create at the top of the page.</p> </li> <li>In the Create new item window, select Project and click Add.\u00a0The Create new item of type Project window appears.</li> <li> <p>In the Create new item window,\u00a0select Dataset under Item Type,\u00a0search for JDBC endpoint, and click\u00a0Add.</p> <p></p> </li> <li> <p>Provide the required configuration details for the JDBC endpoint:</p> <ul> <li>Label: Provide a table name.</li> <li>Description: Optionally describe your table.</li> <li>JDBC Driver Connection URL: Provide the JDBC connection. In this tutorial we use a MySQL database. The database server is named mysql and the database is named serviceDB.</li> <li>Table: Provide the name of the table in the database.</li> <li>Source query: Provide a default source query. In this tutorial, the source query will be modified later as the OFFSET changes.</li> <li>Limit: Provide a LIMIT for the SQL query. In this tutorial, we choose 5 for demonstrating the functionality. You may select any value which works for your use case.</li> <li>Query strategy: Select: Execute the given source query. No paging or virtual Query. In this tutorial, this needs to be changed so that when this JDBC endpoint is being used, Corporate Memory will always check for the Source Query that was provided earlier.</li> <li>User: Provide the user name which is allowed to access the database.</li> <li>Password: Provide the user password that is allowed to access the database.</li> </ul> </li> </ol> <p></p> <p></p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#2-create-a-metadata-graph","title":"2 Create a Metadata Graph","text":"<p>To incrementally extract data in Corporate Memory, we need to store the information about the OFFSET that will change with each extraction. To accomplish this, we need to define a new Graph named Services Metadata Graph that will hold this information. To identify the changing OFFSET with the JDBC endpoint we previously created, we will use the Graph IRI that Corporate Memory created for us.</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#to-find-the-jdbc-endpoint-iri","title":"To find the JDBC endpoint IRI","text":"<ol> <li>Visit the Exploration Tab of Corporate Memory</li> <li>Select in Graph (top left) your project, which starts with \u201cCMEM DI Project \u2026 \u201d (if you cannot see it, you might not have the necessary access rights. In this case, please contact your administrator)</li> <li>Select in Navigation (bottom left): functions_Plugins_Jdbc</li> <li>Select the previously created JDBC endpoint (in our example: \u201cServices Table (JDBC)\u201d</li> <li>Press the Turtle tab inside your JDBC endpoint view (right)</li> </ol> <p>In our example, the JDBC Endpoint IRI looks like this: http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC</p> <p>See screenshot below for example:</p> <p></p> <p>Now that we have the JDBC endpoint IRI, we will build the Metadata Graph to store the OFFSET information.</p> <p>The following three RDF triples hold the (minimal) necessary information we need for this tutorial:</p> <ol> <li>The first triple imports the CMEM DI Project graph into our Metadata Graph to enable access to the LIMIT property defined earlier and to additional information we may need in the future.</li> <li>The second triple defines a label for the Graph.</li> <li>The third triple defines the &lt;\u2026lastOffset&gt; property we need for this tutorial. As a default, we set it to 0 to start with the first row in the table.</li> </ol> <p>services_metadata_graph</p> <pre><code>&lt;http://di.eccenca.com/project/services/metadata&gt;\n    &lt;http://www.w3.org/2002/07/owl#imports&gt;\n         &lt;http://di.eccenca.com/project/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload&gt; . # import the original project\n&lt;http://di.eccenca.com/project/services/metadata&gt;\n    &lt;http://www.w3.org/2000/01/rdf-schema#label&gt;\n        \"Services Metadata\"@en . # provide the graph with a label\n&lt;http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC&gt;\n     &lt;https://vocab.eccenca.com/di/functions/param_Jdbc_lastOffset&gt;\n    \"0\" . # set the initial offset to zero to start with the first row in the table\n</code></pre> <p>For your project, please:</p> <ol> <li>adjust the CMEM DI Project IRI and</li> <li>the JDBC endpoint IRI.</li> </ol> <p>Import the Graph in the Exploration tab \u2192 Graph (menu) \u2192 Add new Graph \u2192 Provide Graph IRI + Select file</p> <p>In our example, we used the following Graph IRI for the Metadata Graph: http://di.eccenca.com/project/services/metadata</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#3-create-a-transformation-to-dynamically-compose-a-sql-query","title":"3 Create a Transformation to dynamically compose a SQL Query","text":"<p>To extract rows based on the predefined (changing) OFFSET and LIMIT from a table, we need to create a Transformation to compose the SQL with each execution.</p> <ol> <li> <p>Click\u00a0Create (top right) in the data integration workspace and select the type Transformation.</p> <ol> <li>Provide a Label.</li> <li>Provide\u00a0Description (Optional).</li> <li>Select the Services Metadata Graph we previously created.</li> </ol> <p></p> </li> <li> <p>Create only a value mapping with the property sourceQuery. The sourceQuery will be used as an input for the JDBC endpoint. A root mapping does not need to be defined. In this screenshot everything is already configured while yours will be empty when you create it for the first time.</p> <p></p> </li> <li> <p>Press the circular pen button to jump into the advanced mapping editor. As source paths we select the data from our Metadata Graph: table, lastOffset and limit. Everything else is defined as a constant as it does not change in the query. For our source paths we defined a \u201cdi\u201d prefix. In case this defintion is missing, your source path may look longer (full IRI).</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#4-create-a-transformation-to-update-the-sql-offset","title":"4 Create a Transformation to update the SQL Offset","text":"<p>Each time we execute the transformation, we want to forward the OFFSET in our SQL Query to extract the next rows. As an example, we have a start OFFSET of 0 and LIMIT of 5. After one execution we want to have an OFFSET of 5, after another execution an OFFSET of 10 and so on. In this tutorial, we assume that the table contains an ID column which incrementally increases by 1 in each row.</p> <p>To store the updated OFFSET, we update the triple with a SPARQL Update query:</p> <ol> <li>Press the Create button (top right) in the data integration workspace and select the type Transformation<ol> <li>Provide a Label</li> <li>Provide a Description (optional)</li> <li>Paste the query into the SPARQL update query form.<ol> <li>The following IRIs need to be adapted for your use cases:<ol> <li>Service Metadata Graph</li> <li>JDBC endpoint <code>jdbc_table_data_config</code></li> <li>Knowledge Graph <code>http://ld.company.org/services/</code></li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>This query will look for the last max service ID found in the Knowledge Graph and update the OFFSET information in the Metadata Graph.</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#last-offset","title":"Last Offset","text":"<pre><code>PREFIX service_metadata_graph: &lt;http://di.eccenca.com/project/services/metadata&gt;\nPREFIX jdbc_table_dataset_config: &lt;http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC&gt;\nPREFIX func: &lt;https://vocab.eccenca.com/di/functions/param_Jdbc_&gt;\nPREFIX prod: &lt;http://ld.company.org/prod-vocab/&gt;\n\nWITH service_metadata_graph:\nDELETE { jdbc_table_dataset_config: func:lastOffset ?lastOffset .}\nINSERT { jdbc_table_dataset_config: func:lastOffset ?newOffset . }\nWHERE {\n  {\n    SELECT ?lastOffset\n    WHERE {\n      GRAPH service_metadata_graph: {\n        jdbc_table_dataset_config: func:lastOffset ?lastOffset\n      }\n    }\n  }\n  {\n   SELECT (max(?id) as ?newOffset)\n    WHERE{\n      GRAPH &lt;http://ld.company.org/services/&gt; {\n        ?services a prod:Service .\n        ?services prod:id ?id .\n      }\n    }\n  }\n}\n</code></pre> <p>Finally, we can build a Workflow which demonstrates how each step works.</p> <p>We compose the SQL query based on the OFFSET and LIMIT information in our Metadata Graph. This SQL query will be used to configure the sourceQuery of the JDBC endpoint. Next, we do a \u201cregular\u201d transformation of data from a JDBC endpoint to RDF. As this step was omitted here, please feel free to read how this Transformation can be built here: Lift data from tabular data such as CSV, XSLX or database tables. As a final step, we use our SPARQL update query to select the max service ID in our Knowledge Graph and update the RDF Triples in our Metadata Graph accordingly.</p> <p></p>","tags":["ExpertTutorial"]},{"location":"build/rule-operators/","title":"Rule Operators","text":"","tags":["Reference"]},{"location":"build/rule-operators/#introduction","title":"Introduction","text":"<p>This page outlines the basic operators that can be used to build linkage and transformation rules.</p> <p>Transformation rules are trees that consist of two types of operators:</p> <ul> <li>Path Operator: Retrieves all values of a specific property path of each entity, such as its label property. The purpose of the path operator is to enable access to values from the dataset.</li> <li>Transformation Operator: Transforms the values of path or transformation operators according to a specific data transformation function.</li> </ul> <p>Linkage rules may use two additional operator types in addition:</p> <ul> <li>Comparison Operator: Evaluates the similarity between two entities based on the values that are returned by two path or transformation operators by applying a distance measure and a distance threshold. Examples of distance measures include Levenshtein, Jaccard, or geographic distance.</li> <li>Aggregation Operator: Due to the fact that, in most cases, the similarity of two entities cannot be determined by evaluating a single comparison, an aggregation operator combines the similarity scores from multiple comparison or aggregation operators into a single score according to a specific aggregation function. Examples of common aggregation functions include the weighted average or yielding the minimum score of all operators.</li> </ul>","tags":["Reference"]},{"location":"build/rule-operators/#path-operator","title":"Path Operator","text":"<p>A path operator retrieves all values which are connected to the entities by a specific path. Every path statement consists of a series of path elements. If a path cannot be resolved due to a missing property or a too restrictive filter, an empty result set is returned.</p> <p>The following operators can be used to traverse the dataset:</p> Operator Example Description <code>/&lt;property&gt;</code> <code>/dbpedia:director/rdfs:label</code> Moves forward from a subject resource through an operator property to its value(s). <code>\\&lt;property&gt;</code> <code>\\dbpedia:artist</code> Moves backward from a subject resource through an operator property to its value(s). <code>[&lt;property&gt; &lt;comp_operator&gt; value]</code> <code>/dbpedia:work[rdf:type = dbpedia:Album]</code> Filters the currently selected values using a filter expression. <code>comp_operator</code> may be one of <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code> <code>[@lang = 'lang']</code> <code>/rdfs:label[@lang = 'en']</code> Filter literal values by their language.","tags":["Reference"]},{"location":"build/rule-operators/#transformation-operator","title":"Transformation Operator","text":"<p>As different datasets usually use different data formats, a transformation can be used to normalize the values prior to comparison.</p> <p>Examples of transformation functions include case normalization, tokenization or concatenation of values from multiple operators. Multiple transformation operators can be nested in order to apply a chain of transformations.</p>","tags":["Reference"]},{"location":"build/rule-operators/#comparison-operator","title":"Comparison Operator","text":"<p>A comparison operator evaluates two inputs and computes the similarity based on a user-defined distance measure and a user-defined threshold.</p> <p>The distance measure always outputs 0 for a perfect match, and a higher value for an imperfect match. Only distance values between 0 and the threshold will result in a positive similarity score. Therefore it is important to know how the distance measures work and what the range of their output values is in order to set a threshold value sensibly.</p> <p>The following parameters can be set for each comparison:</p> Parameter Description required If required is true, the parent aggregation only yields a confidence value if the given inputs have values for both instances. weight Weight of this operator in the parent aggregation. The weight is used by some aggregations such as the weighted average aggregation. threshold The maximum distance. For normalized distance measures, the threshold should be between 0.0 and 1.0. <p></p> <p>The threshold is used to convert the computed distance to a confidence between -1.0 and 1.0. Links will be generated for confidences above 0 while higher confidence values imply a higher similarity between the compared entities.</p> <p>If distance measures generate multiple distance scores the lowest is used to generate the confidence.</p>","tags":["Reference"]},{"location":"build/rule-operators/#aggregation-operator","title":"Aggregation Operator","text":"<p>An aggregation combines multiple confidence values into a single value. In order to determine if two entities are duplicates it is usually not sufficient to compare a single property. For instance, when comparing geographic entities, an aggregation may aggregate the similarities between the names of the entities and the similarities based on the distance between the entities.</p> <p>If an aggregation is fed with missing values (e.g., if inputs paths returned no values), the behavior is as follows:</p> <ul> <li>Boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d.</li> <li>Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing.</li> <li>If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.</li> </ul>","tags":["Reference"]},{"location":"build/snowflake-tutorial/","title":"Connect to Snowflake","text":"","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#introduction","title":"Introduction","text":"<p>eccenca Corporate Memory is a semantic data management platform that allows organizations to store, manage, and interconnect vast amounts of structured data.</p> <p>Snowflake is a cloud-based data warehousing solution that provides a scalable and flexible platform for data storage and analysis.</p> <p>The eccenca Corporate Memory platform can connect to Snowflake to take advantage of Snowflake\u2019s scalability and flexibility for cloud data warehousing. This connection allows organizations to store and manage large amounts of data in Snowflake while using eccenca Corporate Memory to link and interconnect with other data sources, such as databases and cloud applications, to form a comprehensive and unified view of all data assets.</p> <p>By integrating Snowflake with eccenca Corporate Memory, organizations can achieve a centralized and unified data management system that allows them to gain a complete and accurate view of all their data assets. This integration enables organizations to make informed decisions, improve their business processes, and drive growth and innovation by leveraging their data assets.</p> <p>This tutorial contains the following step-by-step instructions to connect the Snowflake data-warehouse with eccenca corporate memory:</p> <ul> <li>1. Configure Custom JDBC Driver</li> <li>2. Create a database in Snowflake</li> <li>3. Create a project in eccenca Corporate Memory</li> <li>4. Create a transformation to build mapping rules</li> <li>5. Create a knowledge graph</li> </ul>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#sample-material","title":"Sample material","text":"<p>The following material is used in this tutorial, you should download the files and have them at hand throughout the tutorial:</p> <ul> <li>The product data vocabulary products_vocabulary.nt</li> </ul>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#1-configure-custom-jdbc-driver","title":"1. Configure Custom JDBC Driver","text":"<p>To connect to the Snowflake cloud data warehouse a JDBC driver is required.</p> <p>The SQL-Dataset of eccenca Corporate Memory can access any database that offers a (supported/tested) JDBC driver. This happens partly via Apache Spark SQL but requires no Spark specific configuration for eccenca Corporate Memory. For Snowflake supported (e.g. 3.13.34) JDBC drivers can be found at:</p> <ul> <li>MVN Repository (direct jar download)</li> <li>to verify and build yourself: github.com/snowflakedb/snowflake-jdbc</li> </ul> <p>To use the driver it needs to be part of the classpath of eccenca Build (DataIntegration). That can be achieved in multiple ways but it is recommended to register the driver via the <code>dataintegration.conf</code> configuration file.</p> <p>There are 3 settings to specify:</p> <ol> <li>The most important is to add the driver name to <code>spark.sql.options.jdbc.drivers</code> - a comma separated list of drivers.     The names in this list are the same as the database name in its JDBC-connection string (i.e. <code>snowflake</code> for its connection URL which looks like <code>jdbc:snowflake://&lt;account_identifier&gt;.snowflakecomputing.com/?&lt;connection_params&gt;</code>).</li> <li>Snowflake specific property for the jar file location: <code>spark.sql.options.jdbc.snowflake.jar=\"/location/for/snowflake-*-jdbc.jar\"</code></li> <li>Snowflake specific property for the driver class name: <code>spark.sql.options.jdbc.snowflake.name=\"com.snowflake.client.jdbc.SnowflakeDriver\"</code></li> </ol> <p>Example configuration snippet</p> <pre><code># \u2026\nspark.sql.options {\n    # \u2026\n    # configure Snowflake JDBC driver\n    jdbc.drivers = \"snowflake\"\n    jdbc.snowflake.jar = ${ELDS_HOME}\"/etc/dataintegration/conf/snowflake-jdbc-3.13.30.jar\"\n    jdbc.snowflake.name = \"net.snowflake.client.jdbc.SnowflakeDriver\"\n    # \u2026\n}\n# \u2026\n</code></pre>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#2-create-a-database-in-snowflake","title":"2. Create a database in Snowflake","text":"<ul> <li>Login to Snowflake enter the username and password, then click on Sign in.</li> </ul> <ul> <li>Click on Database on the left side of the page.</li> </ul> <ul> <li>Click on +Database on the right side of the page.</li> </ul> <ul> <li>Type the database name Product, then click on Create.</li> </ul> <ul> <li>Click on database product, then click on +Schema on the right side of the page.</li> </ul> <ul> <li>Type the schema name products_vocabulary and click on Create.</li> </ul> <ul> <li>Click on scheme products_vocabulary on the left side of the page then click on Create on the right side of the page, then click on Table, then select then Standard.</li> </ul> <ul> <li>Click on schema name products_vocabulary on the left side of the page and type the  sql query for creating a table in the center, then click on Run on the right side of the page.</li> </ul> <p>You can create the PRODUCT table with the following SQL query:</p> <pre><code>CREATE TABLE product(\n   product_id      VARCHAR(12) NOT NULL PRIMARY KEY\n  ,product_name    VARCHAR(50) NOT NULL\n  ,height          INTEGER  NOT NULL\n  ,width           INTEGER  NOT NULL\n  ,depth           INTEGER  NOT NULL\n  ,weigth          INTEGER  NOT NULL\n  ,product_manager VARCHAR(50) NOT NULL\n  ,price           VARCHAR(10) NOT NULL\n) ;\n</code></pre> <ul> <li>Type or copy the SQL query for creating a database in the table that is created, then click on Run.</li> </ul> <p></p> <p>In the Worksheets view (you might need to create a new worksheet), select the product database, the products_vocabulary schema and finally the product table. Here you can populate some test data with the following SQL query:</p> INSERT query <pre><code>INSERT INTO product(product_id,product_name,height,width,depth,weigth,product_manager,price) VALUES\n ('I241-8776317','Strain Compensator',12,68,15,8,'Baldwin.Dirksen@company.org','0,50 EUR')\n,('D215-3449390','Gauge Crystal',77,58,19,15,'Wanja.Hoffmann@company.org','2,00 EUR')\n,('P925-8919074','Inductor Switch',72,61,13,8,'Sabrina.Geiger@company.org','1,86 EUR')\n,('P516-8211068','Film Multiplexer Rheostat Warp',80,57,15,6,'Kristen.Bauers@company.org','0,43 EUR')\n,('N558-1730215','Flow Coil Dipole Strain',43,12,14,14,'Kevin.Feigenbaum@company.org','1,62 EUR')\n,('Y274-1029755','Polymer Transistor Transformer',80,80,12,7,'Adolfina.Hoch@company.org','4,49 EUR')\n,('U360-2815908','Potentiometer Rheostat',65,19,14,3,'Manfred.Foth@company.org','3,09 EUR')\n,('O662-4012383','Resistor Crystal Encoder',70,60,18,19,'Karen.Brant@company.org','3,17 EUR')\n,('D642-3058791','Polymer LCD Dipole Switch',33,68,17,6,'Valda.Everhart@company.org','1,01 EUR')\n,('K167-1377420','Coil Potentiometer Transducer',58,16,17,5,'Siglind.Brinkerhoff@company.org','2,66 EUR')\n,('C977-9932879','Memristor Encoder',30,22,19,2,'Siglind.Brinkerhoff@company.org','3,79 EUR')\n,('E585-3605747','Resistor Dipole',30,67,16,6,'Heinrich.Hoch@company.org','1,09 EUR')\n,('Q523-3322183','Resistor Gauge Capacitor',13,46,13,3,'Lambert.Faust@company.org','2,76 EUR')\n,('G934-5417476','Aluminum Multiplexer Memristor Transformer',57,44,16,17,'Erhard.Fried@company.org','1,84 EUR')\n,('N180-3300253','Encoder Transducer',57,46,17,9,'Rebecca.Hall@company.org','1,35 EUR')\n,('X874-7370643','Network Resistor',54,29,20,10,'Franz.Kornhaeusel@company.org','1,94 EUR')\n,('H745-5284103','Gauge Breaker Compensator',76,51,17,5,'Manfred.Foth@company.org','0,23 EUR')\n,('L275-4377274','Inductor Transformer',27,52,19,6,'Lambert.Faust@company.org','1,23 EUR')\n,('R181-9365849','Log-periodic LCD Transformer',64,52,15,4,'Jarvis.Jans@company.org','2,40 EUR')\n,('U341-6920661','Capacitor Transducer',69,63,14,7,'Franziska.Acker@company.org','5,55 EUR')\n,('K764-8378288','Aluminum Resistor Warp',66,19,13,8,'Nadia.Schubert@company.org','5,06 EUR')\n,('I409-8215134','Film LCD Transducer',67,17,20,1,'Baldwin.Dirksen@company.org','2,46 EUR')\n,('X510-5668523','Coil Resistor Encoder',70,14,12,3,'Karch.Moeller@company.org','3,60 EUR')\n,('T792-4232124','Encoder',26,14,19,6,'Ulrik.Denzel@company.org','5,74 EUR')\n,('Y134-8040496','Oscillator Memristor',37,76,20,11,'Kristen.Bauers@company.org','1,85 EUR')\n,('V436-9027098','Polymer Gauge Crystal',37,14,14,5,'Gretel.Roth@company.org','1,97 EUR')\n,('P844-4114854','Field-effect Strain Compensator',42,73,12,19,'Herr.Haan.Bader@company.org','0,96 EUR')\n,('H389-3327633','Flow Driver Strain',36,36,11,14,'Miles.Amsel@company.org','0,16 EUR')\n,('J259-5185660','Memristor Encoder',40,55,12,16,'Heinrich.Hoch@company.org','4,86 EUR')\n,('I590-4406621','Heisenberg Coil Resistor Meter',32,78,17,12,'Sabrina.Bayer@company.org','0,60 EUR')\n,('A816-3021832','Rheostat Breaker',19,28,14,18,'Lili.Geier@company.org','3,86 EUR')\n,('E424-4700158','Rheostat Compensator Meter',72,58,20,1,'Sabrina.Bayer@company.org','1,80 EUR')\n,('K334-8882985','Network Resonator Encoder',21,49,17,10,'Bert.Blumstein@company.org','3,09 EUR')\n,('H690-4065164','Heisenberg Oscillator Memristor Meter',47,55,20,1,'Jarvis.Jans@company.org','1,81 EUR')\n,('J625-3464908','Sensor Crystal Warp',24,57,11,17,'Thomas.Mueller@company.org','0,75 EUR')\n,('V104-2082346','Flow Dipole Transformer',64,73,17,12,'Anamchara.Foerstner@company.org','3,36 EUR')\n,('R298-1578179','Gauge Strain Breaker',30,17,12,14,'Reiner.Widmann@company.org','4,45 EUR')\n,('O761-2575092','Switch Transformer Transducer',51,71,20,13,'Nadia.Schubert@company.org','4,18 EUR')\n,('N560-4369045','Resistor Compensator',21,57,11,4,'Herr.Burgh.Eichel@company.org','2,61 EUR')\n,('D903-5325470','Flow Crystal Rheostat',70,17,13,20,'Franziska.Acker@company.org','0,45 EUR')\n,('Y632-7948469','Multiplexer Coil',14,67,13,5,'Lili.Geier@company.org','2,24 EUR')\n,('O204-4321819','Field-effect Resistor Potentiometer Rheostat',76,14,19,7,'Karch.Moeller@company.org','5,86 EUR')\n,('C527-6179790','LCD Oscillator Sensor',67,61,13,2,'Arnelle.Gerber@company.org','0,16 EUR')\n,('A981-3634031','Driver Memristor',50,59,20,13,'Jarvis.Jans@company.org','5,55 EUR')\n,('R774-2450170','Log-periodic Network Warp',16,19,13,9,'Ida.Halle@company.org','5,23 EUR')\n,('E709-4829800','Transducer Warp',18,14,13,18,'Henny.Foth@company.org','4,51 EUR')\n,('W268-8954866','Gauge Warp',15,24,11,9,'Ida.Halle@company.org','3,80 EUR')\n,('J628-9483622','Field-effect Transistor Crystal',77,76,13,16,'Emil.Gotti@company.org','0,16 EUR')\n,('E661-6880042','Crystal Rheostat Breaker',76,39,20,10,'Adolfina.Hoch@company.org','2,60 EUR')\n,('F553-8518538','Aluminum Sensor Capacitor',52,39,14,3,'Lambert.Faust@company.org','2,99 EUR')\n,('P982-4384687','Oscillator Sensor Resonator',47,62,11,4,'Franz.Kornhaeusel@company.org','4,40 EUR')\n,('R187-6602262','Film Resistor Crystal Encoder',61,67,11,8,'Reiner.Widmann@company.org','1,27 EUR')\n,('N478-8857002','Transformer Capacitor Breaker',19,32,11,12,'Nadia.Schubert@company.org','3,33 EUR')\n,('I884-8763759','Ceramic Inductor Breaker',69,22,19,11,'Bert.Blumstein@company.org','2,64 EUR')\n,('F424-2241578','Strain Meter',66,79,11,9,'Elena.Herzog@company.org','1,80 EUR')\n,('J178-5863159','Crystal Transducer',38,13,20,7,'Waldtraud.Kuttner@company.org','4,82 EUR')\n,('C699-1385746','Resistor Driver Breaker',59,59,17,4,'Herr.Haan.Bader@company.org','5,97 EUR')\n,('L433-1376544','LCD Gauge Warp',25,12,18,7,'Liese.Adam@company.org','0,78 EUR')\n,('O270-3409076','Flow Network Dipole Warp',55,33,19,4,'Corinna.Ludwig@company.org','2,05 EUR')\n,('T592-5377501','Field-effect Strain Capacitor',13,63,13,5,'Berlin.Schulz@company.org','4,28 EUR')\n,('F383-6450755','Strain Encoder',33,53,11,17,'Frauke.Faerber@company.org','0,25 EUR')\n,('K313-8452822','Heisenberg Crystal Transformer',15,71,18,17,'Franz.Kornhaeusel@company.org','2,92 EUR')\n,('C844-8161134','Crystal Compensator',19,39,13,11,'Miles.Amsel@company.org','2,90 EUR')\n,('E296-9034321','Heisenberg Inductor Breaker Encoder',63,11,12,6,'Corinna.Ludwig@company.org','4,09 EUR')\n,('A548-4778785','Coil Potentiometer Dipole',45,36,18,20,'Frauke.Faerber@company.org','0,41 EUR')\n,('E563-8448172','Warp Capacitor Meter',20,57,13,14,'Lili.Geier@company.org','5,78 EUR')\n,('E267-7496794','Crystal Rheostat',34,47,11,6,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','1,11 EUR')\n,('V881-4674578','Planck Dipole Driver Crystal',73,24,16,2,'Dietlinde.Boehme@company.org','4,23 EUR')\n,('R112-2955867','Oscillator Encoder Compensator',53,67,13,14,'Sylvester.Brant@company.org','0,64 EUR')\n,('H577-3512936','Film Coil Crystal Strain',52,46,17,15,'Arnelle.Gerber@company.org','0,10 EUR')\n,('U990-5234138','LCD Inductor',15,78,16,15,'Anamchara.Foerstner@company.org','1,52 EUR')\n,('E358-6492536','Resistor Crystal',40,69,11,15,'Reiner.Widmann@company.org','2,18 EUR')\n,('R287-3749176','Log-periodic Memristor Encoder',64,14,17,12,'Manfred.Foth@company.org','3,78 EUR')\n,('N915-2423517','Aluminum Encoder Transformer Meter',47,75,20,1,'Frauke.Faerber@company.org','4,02 EUR')\n,('C770-9751514','Transistor Resonator Breaker',74,45,17,1,'Xochitl.Aue@company.org','4,97 EUR')\n,('X863-5063447','Coil Potentiometer Transducer',41,53,19,2,'Thomas.Mueller@company.org','5,69 EUR')\n,('Z553-4314789','Dipole Capacitor',76,11,15,1,'Marius.Fux@company.org','3,23 EUR')\n,('M244-7026376','Coil Compensator',34,55,17,11,'Sabrina.Geiger@company.org','3,22 EUR')\n,('J859-3337215','Inductor Memristor',74,50,13,14,'Liese.Adam@company.org','5,59 EUR')\n,('R786-6586508','Coil LCD Rheostat',42,32,19,19,'Thomas.Mueller@company.org','1,79 EUR')\n,('G858-9670227','Network Transducer',57,39,15,5,'Siglind.Brinkerhoff@company.org','0,38 EUR')\n,('J884-3242067','Network Inductor',29,26,18,3,'Sabrina.Geiger@company.org','2,57 EUR')\n,('F812-7003324','Rheostat Compensator',31,56,15,18,'Baldwin.Guenther@company.org','1,73 EUR')\n,('U507-1853778','LCD Dipole Capacitor',45,78,18,13,'Lili.Geier@company.org','2,93 EUR')\n,('M253-4759368','Multiplexer Resonator',71,66,12,1,'Ratt.Hartmann@company.org','4,16 EUR')\n,('Y185-1184618','Gauge Compensator',12,38,14,12,'Waldtraud.Kuttner@company.org','1,64 EUR')\n,('J481-5585150','Multiplexer Potentiometer Strain',77,19,14,9,'Wolfgang.Martin@company.org','5,81 EUR')\n,('Z452-1805723','Capacitor Switch',34,67,17,11,'Yanka.Schreiber@company.org','3,24 EUR')\n,('F525-2265345','Oscillator Driver',12,39,13,19,'Sigmund.Gros@company.org','0,85 EUR')\n,('N279-5579447','Sensor Potentiometer Dipole',30,16,17,12,'Sigmund.Gros@company.org','0,28 EUR')\n,('F326-8777433','Polymer Inductor Transistor Warp',23,22,18,18,'Herr.Haan.Bader@company.org','2,68 EUR')\n,('A998-9941987','Flow Compensator',55,18,11,1,'Herr.Haan.Bader@company.org','5,29 EUR')\n,('F611-6226129','Phase LCD Dipole',59,62,12,3,'Jarvis.Jans@company.org','5,33 EUR')\n,('I314-5607546','Polymer Resistor Dipole Gauge',57,69,16,7,'Yanka.Schreiber@company.org','5,06 EUR')\n,('W988-3702091','Inductor Rheostat Warp',79,35,12,12,'Kevin.Feigenbaum@company.org','5,65 EUR')\n,('X663-2500265','Transformer Transducer',47,27,19,1,'Arnelle.Gerber@company.org','4,78 EUR')\n,('M750-1800450','Crystal Encoder',26,51,14,10,'Wolfgang.Martin@company.org','2,68 EUR')\n,('W156-1748173','Crystal Rheostat Warp',58,11,11,13,'Jarvis.Jans@company.org','2,50 EUR')\n,('Z739-3332146','Polymer LCD Transistor',39,58,17,7,'Erhard.Fried@company.org','3,29 EUR')\n,('J872-7568181','Encoder Breaker',18,37,16,8,'Berlin.Schulz@company.org','1,22 EUR')\n,('L485-8083934','Inductor Warp Switch',68,20,15,7,'Valda.Everhart@company.org','0,78 EUR')\n,('J628-3649699','Breaker Meter',78,34,17,15,'Nadia.Schubert@company.org','4,21 EUR')\n,('T831-2675171','Planck Coil Oscillator',40,24,13,20,'Bert.Blumstein@company.org','2,26 EUR')\n,('B436-6020212','Potentiometer Crystal',59,18,14,19,'Minnie.Kuehn@company.org','3,59 EUR')\n,('L691-1489542','Phase Multiplexer Driver Capacitor',62,55,13,14,'Lukas.Gerver@company.org','3,77 EUR')\n,('T808-9704137','Gauge Encoder Breaker',48,39,18,9,'Karch.Moeller@company.org','3,22 EUR')\n,('P774-3681449','Resonator Switch',52,35,13,19,'Karen.Brant@company.org','1,82 EUR')\n,('I264-7314323','Film Inductor Memristor',66,24,19,11,'Yanka.Schreiber@company.org','3,39 EUR')\n,('R477-9013874','Coil Warp',76,46,17,6,'Berlin.Schulz@company.org','5,88 EUR')\n,('Y467-5818685','Memristor Compensator',22,30,18,1,'Bert.Blumstein@company.org','3,93 EUR')\n,('N673-7692368','Driver Resonator',65,60,15,13,'Nadia.Schubert@company.org','5,57 EUR')\n,('E576-3538706','Encoder Breaker',45,55,14,8,'Lili.Geier@company.org','4,27 EUR')\n,('B507-9536014','LCD Network',48,49,17,11,'Sylvester.Brant@company.org','1,76 EUR')\n,('P253-1288849','Dipole Compensator',55,24,12,15,'Sylvester.Brant@company.org','0,98 EUR')\n,('S418-2584457','Bipolar-junction Dipole',17,47,13,16,'Ratt.Beyer@company.org','4,32 EUR')\n,('V940-2277346','Aluminum Oscillator Potentiometer Encoder',78,52,18,2,'Manfred.Foth@company.org','4,16 EUR')\n,('P323-7286189','Sensor Encoder Warp',16,27,16,14,'Reiner.Widmann@company.org','1,66 EUR')\n,('T341-6404509','LCD Transducer Meter',78,29,20,17,'Marius.Fux@company.org','0,39 EUR')\n,('S388-7116324','Polymer Inductor Transistor Oscillator',44,66,11,11,'Emil.Gotti@company.org','1,70 EUR')\n,('G439-8153345','Memristor Driver Breaker',52,49,13,10,'Henny.Foth@company.org','2,33 EUR')\n,('F496-3982542','Resonator Rheostat Breaker',67,27,11,16,'Herr.Burgh.Eichel@company.org','2,79 EUR')\n,('G494-2537921','Coil Encoder Switch',62,59,13,11,'Kevin.Feigenbaum@company.org','3,16 EUR')\n,('D516-5106885','Driver Rheostat Capacitor',54,42,13,1,'Herr.Burgh.Eichel@company.org','0,96 EUR')\n,('R902-1645052','Film Network Gauge Compensator',72,65,11,8,'Emil.Gotti@company.org','0,65 EUR')\n,('P494-3529490','Ceramic Crystal Switch',76,64,12,9,'Henny.Foth@company.org','1,72 EUR')\n,('K199-8327732','Phase Transistor Crystal',49,79,13,12,'Karen.Brant@company.org','3,60 EUR')\n,('C200-5363446','Network Transducer',27,59,12,3,'Ratt.Beyer@company.org','4,55 EUR')\n,('P785-3702584','Log-periodic Multiplexer Potentiometer Memristor',59,26,18,6,'Sylvester.Brant@company.org','4,57 EUR')\n,('L940-7353182','Resonator Warp',41,28,18,20,'Ida.Halle@company.org','3,21 EUR')\n,('H747-8053953','Resonator Meter',25,37,16,1,'Elisabeth.Harman@company.org','0,52 EUR')\n,('U681-7406159','Film Driver Switch',68,73,14,8,'Sabrina.Bayer@company.org','0,27 EUR')\n,('A360-3041803','Inductor Transformer Warp',33,62,14,16,'Marius.Fux@company.org','4,89 EUR')\n,('F675-6890144','Resistor Inductor Strain',66,31,12,10,'Arnelle.Gerber@company.org','1,06 EUR')\n,('E938-3071637','LCD Dipole',15,24,15,17,'Ida.Halle@company.org','5,44 EUR')\n,('Q694-8417409','Sensor Crystal',58,55,15,9,'Baldwin.Dirksen@company.org','2,44 EUR')\n,('K780-7736227','Driver Rheostat',18,36,14,20,'Ida.Halle@company.org','2,95 EUR')\n,('E365-4375068','Strain Compensator',72,28,17,17,'Bert.Blumstein@company.org','2,09 EUR')\n,('M774-4843227','Breaker Transducer',38,21,18,18,'Dietlinde.Boehme@company.org','1,32 EUR')\n,('A994-9085459','LCD Compensator',41,49,18,5,'Franziska.Acker@company.org','4,08 EUR')\n,('X204-1803083','Crystal Strain Compensator',18,33,14,20,'Karen.Brant@company.org','5,67 EUR')\n,('H197-3970480','Coil Gauge Transformer',56,72,19,6,'Valda.Everhart@company.org','5,55 EUR')\n,('L827-5554014','Flow Oscillator Dipole',80,15,18,8,'Corinna.Ludwig@company.org','5,58 EUR')\n,('L103-3316729','Multiplexer Potentiometer',49,66,12,9,'Siglind.Brinkerhoff@company.org','3,69 EUR')\n,('C171-3616793','Coil Resonator Capacitor',16,70,12,8,'Wanja.Hoffmann@company.org','2,34 EUR')\n,('W780-8057984','Planck Transformer Compensator',52,67,11,20,'Kevin.Feigenbaum@company.org','2,46 EUR')\n,('I395-2294150','Gauge Resonator',80,65,13,10,'Heinrich.Hoch@company.org','0,00 EUR')\n,('M367-8246717','Potentiometer Gauge Warp',56,54,20,7,'Miles.Amsel@company.org','1,07 EUR')\n,('Z165-5413714','Gauge Transducer',13,59,16,10,'Franz.Kornhaeusel@company.org','2,05 EUR')\n,('E246-3200290','Field-effect Network Rheostat',75,28,12,13,'Dietlinde.Boehme@company.org','1,06 EUR')\n,('U733-5722614','Network Breaker Compensator',62,17,13,5,'Ulrik.Denzel@company.org','2,14 EUR')\n,('T735-5591779','Memristor Encoder',48,34,20,12,'Manfred.Foth@company.org','0,22 EUR')\n,('W726-4190110','Ceramic Multiplexer Transistor LCD',43,41,12,10,'Ratt.Hartmann@company.org','4,90 EUR')\n,('F204-7999856','Meter Warp',52,37,14,8,'Elisabeth.Harman@company.org','5,99 EUR')\n,('N773-8807466','Multiplexer Switch',79,18,17,18,'Elisabeth.Harman@company.org','5,77 EUR')\n,('L984-7886943','Resistor Oscillator',23,33,13,8,'Sabrina.Geiger@company.org','0,18 EUR')\n,('F735-3322876','Planck Potentiometer Capacitor',68,15,19,13,'Xochitl.Aue@company.org','0,95 EUR')\n,('N832-1055352','Coil Driver',25,13,12,13,'Elisabeth.Harman@company.org','3,48 EUR')\n,('R410-7957011','Planck Network Dipole',22,79,15,2,'Emil.Gotti@company.org','5,07 EUR')\n,('Z373-9393076','Resonator Transformer',40,71,17,10,'Franziska.Acker@company.org','5,45 EUR')\n,('H956-3958783','Aluminum Resistor Transistor Transformer',18,40,11,14,'Elisabeth.Harman@company.org','4,57 EUR')\n,('H482-4970770','Transistor Rheostat Switch',21,71,11,1,'Marius.Fux@company.org','2,84 EUR')\n,('D483-7794770','Memristor Gauge',20,69,20,1,'Herr.Haan.Bader@company.org','2,12 EUR')\n,('I892-8435352','Heisenberg LCD Driver',55,30,11,3,'Sigmund.Gros@company.org','4,19 EUR')\n,('G547-8961166','Bipolar-junction Oscillator Compensator',51,51,12,15,'Siglind.Brinkerhoff@company.org','0,81 EUR')\n,('B161-8326597','Inductor Potentiometer',65,27,20,14,'Waldtraud.Kuttner@company.org','5,68 EUR')\n,('L760-6079543','Resistor Breaker',24,35,17,4,'Thomas.Mueller@company.org','1,49 EUR')\n,('O256-6180697','Heisenberg Rheostat Switch',31,49,17,2,'Bert.Blumstein@company.org','5,42 EUR')\n,('V285-7238338','LCD Inductor',61,25,12,14,'Bert.Blumstein@company.org','2,55 EUR')\n,('Q992-9818584','Flow Oscillator Inductor Compensator',12,27,18,2,'Nadia.Schubert@company.org','3,20 EUR')\n,('Y695-6135491','Polymer Warp',76,49,20,16,'Arendt.Beitel@company.org','4,59 EUR')\n,('Y966-2972645','Field-effect Driver',30,42,14,4,'Wolfgang.Martin@company.org','0,89 EUR')\n,('W434-6067873','Network Encoder',24,65,11,17,'Gretel.Roth@company.org','1,87 EUR')\n,('U889-6360502','Encoder Capacitor Compensator',40,35,12,19,'Adolfina.Hoch@company.org','5,07 EUR')\n,('Z170-8513315','Sensor Crystal Resonator',23,52,16,20,'Anamchara.Foerstner@company.org','5,57 EUR')\n,('C409-9349178','Multiplexer Rheostat Encoder',35,80,15,6,'Ulrik.Denzel@company.org','4,77 EUR')\n,('B131-7597100','LCD Sensor Transducer',51,44,16,18,'Sabrina.Geiger@company.org','3,70 EUR')\n,('N869-4606944','Film Memristor Compensator',57,63,16,18,'Sigmund.Gros@company.org','3,11 EUR')\n,('H973-6742173','Bipolar-junction Transistor Transformer',78,75,17,15,'Arendt.Beitel@company.org','0,21 EUR')\n,('M210-9470943','Network Transducer',75,27,16,6,'Siglind.Brinkerhoff@company.org','4,94 EUR')\n,('J209-5198739','Phase Memristor Capacitor',15,28,14,4,'Wolfgang.Martin@company.org','3,87 EUR')\n,('L205-6345377','Planck Coil Potentiometer',51,57,15,10,'Kevin.Feigenbaum@company.org','4,65 EUR')\n,('C633-6541408','Field-effect Potentiometer Transformer Switch',59,14,12,5,'Adolfina.Hoch@company.org','5,28 EUR')\n,('G205-5318100','LCD Gauge',41,79,11,15,'Elena.Herzog@company.org','5,90 EUR')\n,('R135-6598379','Capacitor Meter',54,33,16,8,'Reiner.Widmann@company.org','5,52 EUR')\n,('Z222-5977620','Transformer Switch',80,14,14,13,'Minnie.Kuehn@company.org','4,94 EUR')\n,('V787-2710358','Ceramic Sensor Encoder',62,71,18,5,'Jarvis.Jans@company.org','4,18 EUR')\n,('W892-1983772','Log-periodic Coil Sensor Transducer',55,68,12,12,'Yanka.Schreiber@company.org','4,86 EUR')\n,('Y440-4146454','Resistor Memristor',77,74,18,12,'Arendt.Beitel@company.org','1,40 EUR')\n,('E529-9160800','Network Inductor',16,62,12,19,'Arendt.Beitel@company.org','3,83 EUR')\n,('E355-4376121','Film Oscillator Dipole',57,50,17,2,'Karch.Moeller@company.org','5,53 EUR')\n,('W988-9267091','Transistor Strain',66,58,11,3,'Sabrina.Geiger@company.org','1,40 EUR')\n,('X517-5656435','Multiplexer Capacitor',35,32,15,15,'Karen.Brant@company.org','5,45 EUR')\n,('J824-9483042','Dipole Resonator Breaker',38,38,18,17,'Manfred.Foth@company.org','2,58 EUR')\n,('M292-6646786','Phase Sensor Crystal Capacitor',33,64,12,19,'Emil.Gotti@company.org','4,18 EUR')\n,('M703-4614993','Coil Potentiometer Transducer',77,66,13,4,'Miles.Amsel@company.org','5,11 EUR')\n,('P903-7514284','Dipole Capacitor',47,56,20,14,'Reiner.Widmann@company.org','5,97 EUR')\n,('I122-1391097','Inductor Memristor Oscillator',40,32,13,19,'Karch.Moeller@company.org','3,67 EUR')\n,('D817-8084362','Multiplexer Resistor Compensator',74,79,20,19,'Heinrich.Hoch@company.org','2,53 EUR')\n,('W744-1785439','Film Multiplexer Dipole Strain',62,74,19,8,'Ulrik.Denzel@company.org','1,98 EUR')\n,('C402-5072074','Coil Inductor Encoder',27,49,17,12,'Herr.Haan.Bader@company.org','0,76 EUR')\n,('E660-5579477','Film Oscillator Transistor Capacitor',41,38,14,1,'Berlin.Schulz@company.org','0,38 EUR')\n,('W776-4360677','Field-effect Transducer Meter',75,77,14,13,'Franziska.Acker@company.org','3,26 EUR')\n,('F179-4094930','Bipolar-junction Sensor Crystal',27,24,19,12,'Adolfina.Hoch@company.org','2,96 EUR')\n,('Z293-3675192','Ceramic Potentiometer Gauge Encoder',79,56,13,7,'Nadia.Schubert@company.org','2,33 EUR')\n,('B818-8738213','Crystal Capacitor',37,52,11,17,'Berlin.Schulz@company.org','2,45 EUR')\n,('A243-3332548','Bipolar-junction Crystal Meter',43,46,17,10,'Ulrik.Denzel@company.org','3,18 EUR')\n,('G420-6271055','Potentiometer Driver',70,36,17,13,'Kristen.Bauers@company.org','1,82 EUR')\n,('R930-5997475','Coil Driver Capacitor',31,61,15,13,'Minnie.Kuehn@company.org','4,33 EUR')\n,('J129-5121523','Inductor Driver Transducer',20,63,18,11,'Reiner.Widmann@company.org','0,23 EUR')\n,('Y162-1430218','Ceramic Dipole Memristor Breaker',28,63,13,11,'Jarvis.Jans@company.org','4,11 EUR')\n,('K630-6895992','LCD Gauge Switch',34,58,13,17,'Ratt.Hartmann@company.org','1,58 EUR')\n,('Z190-5822042','Polymer Multiplexer Crystal Capacitor',48,60,12,10,'Jarvis.Jans@company.org','0,20 EUR')\n,('J734-1368842','Inductor Gauge Meter',55,70,18,7,'Baldwin.Guenther@company.org','4,99 EUR')\n,('H664-9736043','Log-periodic Potentiometer Resonator',18,40,20,19,'Lambert.Faust@company.org','5,02 EUR')\n,('C614-4108640','Log-periodic LCD Breaker',27,75,18,14,'Bert.Blumstein@company.org','1,79 EUR')\n,('N377-1619045','Capacitor Encoder Compensator',47,70,13,2,'Corinna.Ludwig@company.org','5,70 EUR')\n,('S424-4152456','Potentiometer',22,17,19,12,'Lambert.Faust@company.org','0,45 EUR')\n,('T319-6530857','Sensor Transducer',43,41,20,15,'Sigmund.Gros@company.org','1,37 EUR')\n,('S680-6146547','Rheostat Capacitor',68,57,20,5,'Erhard.Fried@company.org','1,04 EUR')\n,('V437-5667353','Sensor Driver Strain',36,14,20,3,'Emil.Gotti@company.org','0,84 EUR')\n,('P717-5835879','Rheostat Transformer',22,43,13,14,'Ulrik.Denzel@company.org','5,31 EUR')\n,('X234-5007377','Polymer Multiplexer Potentiometer Resistor',21,73,19,7,'Dieterich.Blau@company.org','5,38 EUR')\n,('V673-7881809','Heisenberg Resistor Resonator Compensator',57,56,15,13,'Dieterich.Blau@company.org','0,26 EUR')\n,('E471-9316820','Bipolar-junction Coil Dipole Transformer',65,56,14,14,'Anamchara.Foerstner@company.org','2,66 EUR')\n,('S429-3352092','Dipole Switch',77,41,14,17,'Henny.Foth@company.org','2,16 EUR')\n,('E189-1255687','LCD Potentiometer Breaker',79,13,13,18,'Rebecca.Hall@company.org','2,68 EUR')\n,('Z617-4660142','Ceramic Potentiometer Compensator',15,41,16,12,'Arnelle.Gerber@company.org','3,37 EUR')\n,('W493-4799721','Network Warp',64,50,16,20,'Kristen.Bauers@company.org','5,69 EUR')\n,('H338-7439287','Field-effect Resistor Potentiometer Meter',21,51,15,2,'Franz.Kornhaeusel@company.org','1,69 EUR')\n,('T348-1607769','LCD Resistor Sensor',36,14,16,10,'Kristen.Bauers@company.org','1,27 EUR')\n,('B365-4394675','Flow Gauge Resonator Breaker',27,13,13,12,'Kevin.Feigenbaum@company.org','2,50 EUR')\n,('R231-2442628','Transformer Transducer',58,20,14,9,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','3,29 EUR')\n,('R383-8831143','Flow Potentiometer Resonator',41,47,12,16,'Franziska.Acker@company.org','1,14 EUR')\n,('V509-7873455','LCD Potentiometer Transformer',61,14,17,13,'Yanka.Schreiber@company.org','2,40 EUR')\n,('W903-2104201','Bipolar-junction Encoder Compensator',52,66,15,15,'Baldwin.Guenther@company.org','5,28 EUR')\n,('R675-2629492','Sensor Breaker Warp',56,67,13,7,'Emil.Gotti@company.org','1,81 EUR')\n,('N823-7028680','LCD Potentiometer Switch',58,57,18,9,'Arendt.Beitel@company.org','3,76 EUR')\n,('Y616-2122188','Potentiometer Driver',49,63,11,14,'Henny.Foth@company.org','5,42 EUR')\n,('H426-2605604','Film Network Multiplexer Compensator',15,22,12,2,'Miles.Amsel@company.org','2,57 EUR')\n,('N105-3985051','Multiplexer Resistor',71,49,19,20,'Lukas.Gerver@company.org','1,83 EUR')\n,('S915-2398244','Bipolar-junction Capacitor Crystal Breaker',41,76,19,19,'Nadia.Schubert@company.org','4,52 EUR')\n,('V178-8820348','LCD Inductor',60,16,20,11,'Herr.Burgh.Eichel@company.org','2,30 EUR')\n,('H660-8942410','Sensor Warp',39,53,20,2,'Waldtraud.Kuttner@company.org','4,24 EUR')\n,('W769-8151254','Field-effect LCD Multiplexer Capacitor',66,40,12,12,'Thomas.Mueller@company.org','3,57 EUR')\n,('I571-8597034','Transistor Compensator',51,74,17,7,'Manfred.Foth@company.org','0,49 EUR')\n,('D965-2729258','Film Sensor Switch',48,68,13,3,'Elena.Herzog@company.org','0,03 EUR')\n,('D605-8095260','Network Inductor',38,64,19,18,'Gretel.Roth@company.org','2,31 EUR')\n,('A837-2549775','LCD Gauge',25,46,18,19,'Siglind.Brinkerhoff@company.org','3,23 EUR')\n,('R599-8820686','Film Sensor Dipole',31,67,14,5,'Liese.Adam@company.org','2,58 EUR')\n,('Y728-2083256','Gauge Crystal Encoder',51,31,13,14,'Ratt.Hartmann@company.org','1,74 EUR')\n,('C799-8367143','Coil Strain Meter',32,50,20,4,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','2,28 EUR')\n,('X148-3354774','Multiplexer Compensator',25,44,18,3,'Lili.Geier@company.org','2,04 EUR')\n,('I966-3317124','Dipole Breaker Transducer',14,51,14,7,'Elena.Herzog@company.org','3,64 EUR')\n,('X946-6308579','Field-effect Resistor Encoder',43,48,12,11,'Baldwin.Guenther@company.org','4,00 EUR')\n,('D690-6928884','Flow Network Oscillator Encoder',71,50,17,3,'Sabrina.Bayer@company.org','1,54 EUR')\n,('F565-7076382','Dipole Resonator Transducer',54,21,19,10,'Manfred.Foth@company.org','1,40 EUR')\n,('U714-9883090','Aluminum Multiplexer LCD Capacitor',73,33,13,1,'Wanja.Hoffmann@company.org','1,07 EUR')\n,('P228-7323906','Bipolar-junction Network Inductor Oscillator',36,38,12,3,'Anamchara.Foerstner@company.org','1,39 EUR')\n,('C916-8310851','Bipolar-junction Oscillator Transformer',34,23,18,19,'Sabrina.Geiger@company.org','1,71 EUR')\n,('N269-8309461','LCD Potentiometer Warp',41,17,12,9,'Sabrina.Bayer@company.org','4,99 EUR')\n,('B120-9942467','Coil Driver',45,66,12,3,'Henny.Foth@company.org','1,06 EUR')\n,('U967-6553099','Bipolar-junction Gauge Warp',52,80,14,19,'Sabrina.Geiger@company.org','2,82 EUR')\n,('V759-7427700','LCD Dipole Resonator',53,29,17,19,'Ratt.Beyer@company.org','2,21 EUR')\n,('N583-9253059','Dipole Breaker',68,12,17,7,'Manfred.Foth@company.org','1,06 EUR')\n,('Z775-8853334','Aluminum Multiplexer Dipole',33,63,18,8,'Nadia.Schubert@company.org','3,89 EUR')\n,('E917-4866901','Capacitor Warp',15,52,14,5,'Dietlinde.Boehme@company.org','0,42 EUR')\n,('M175-2087039','Film Oscillator',28,70,17,1,'Franziska.Acker@company.org','2,18 EUR')\n,('W985-1612943','Aluminum Switch',31,38,13,13,'Sylvester.Brant@company.org','1,48 EUR')\n,('C119-5354812','Planck Sensor Strain',19,68,18,11,'Sabrina.Geiger@company.org','0,40 EUR')\n,('L619-5092078','Crystal Resonator Transformer',44,66,16,20,'Wolfgang.Martin@company.org','5,97 EUR')\n,('E652-6887116','Gauge Compensator',26,56,17,3,'Franziska.Acker@company.org','2,29 EUR')\n,('E617-6594892','Field-effect Driver Strain Compensator',37,56,13,8,'Wanja.Hoffmann@company.org','4,38 EUR')\n,('M449-5231838','Transistor Crystal',50,30,17,12,'Miles.Amsel@company.org','4,22 EUR')\n,('J856-1304399','Resistor Transducer',72,54,11,13,'Xochitl.Aue@company.org','2,88 EUR')\n,('W295-9452529','Sensor Encoder',67,72,14,7,'Lukas.Gerver@company.org','5,90 EUR')\n,('X235-5406274','Sensor Transducer',15,34,16,7,'Franz.Kornhaeusel@company.org','3,00 EUR')\n,('F330-3792974','Coil',51,23,12,5,'Sigmund.Gros@company.org','3,13 EUR')\n,('S840-8753783','Polymer Multiplexer Sensor',80,21,20,14,'Yanka.Schreiber@company.org','0,36 EUR')\n,('A607-3251492','Bipolar-junction Transistor Compensator',63,51,11,6,'Bert.Blumstein@company.org','2,79 EUR')\n,('T814-8858070','Multiplexer Transistor',40,60,19,16,'Karch.Moeller@company.org','2,54 EUR')\n,('B541-8783084','Flow Potentiometer Gauge',30,28,12,14,'Minnie.Kuehn@company.org','0,76 EUR')\n,('Q176-6210359','Oscillator Transistor Crystal',74,28,20,10,'Valda.Everhart@company.org','5,56 EUR')\n,('U128-5766392','Multiplexer Resistor',35,73,17,14,'Liese.Adam@company.org','1,48 EUR')\n,('C551-8482722','Planck Transducer',39,40,16,17,'Jarvis.Jans@company.org','2,22 EUR')\n,('L569-8145670','Capacitor Transistor Compensator',35,45,17,1,'Arendt.Beitel@company.org','2,85 EUR')\n,('F251-2666285','Bipolar-junction Strain Transducer',43,60,20,18,'Frauke.Faerber@company.org','0,11 EUR')\n,('J154-2269983','Capacitor Compensator',48,34,17,4,'Elena.Herzog@company.org','4,85 EUR')\n,('S877-1860797','Driver Transistor',53,26,16,11,'Gretel.Roth@company.org','3,95 EUR')\n,('W521-8006606','Flow Memristor Transistor Compensator',65,31,12,6,'Elena.Herzog@company.org','0,02 EUR')\n,('I679-1230971','Memristor Compensator',64,55,18,1,'Berlin.Schulz@company.org','1,74 EUR')\n,('Q881-7871943','Oscillator Resonator Compensator',45,53,17,4,'Wolfgang.Martin@company.org','5,93 EUR')\n,('X342-6389543','Planck Resistor Crystal Breaker',80,71,13,8,'Sabrina.Geiger@company.org','3,81 EUR')\n,('E813-9861759','Ceramic Memristor Warp',24,16,19,17,'Elena.Herzog@company.org','1,62 EUR')\n,('L173-2699667','Coil LCD',51,70,16,17,'Bert.Blumstein@company.org','4,44 EUR')\n,('S859-8143033','Field-effect Breaker Strain Transformer',63,74,13,20,'Xochitl.Aue@company.org','0,08 EUR')\n,('E558-1962104','Planck Breaker Compensator',64,58,13,20,'Miles.Amsel@company.org','5,12 EUR')\n,('H660-6112027','Network Multiplexer Encoder',62,74,16,10,'Manfred.Foth@company.org','3,89 EUR')\n,('N982-3577798','Network Transformer',68,28,11,16,'Manfred.Foth@company.org','3,78 EUR')\n,('X407-9945990','Transistor Breaker',55,61,17,17,'Yanka.Schreiber@company.org','2,36 EUR')\n,('M650-8586992','Memristor Gauge',59,28,14,4,'Lambert.Faust@company.org','4,88 EUR')\n,('Z887-4941382','Encoder Compensator',51,15,13,10,'Kristen.Bauers@company.org','4,57 EUR')\n,('J986-8306638','Phase Multiplexer Crystal Transducer',15,39,20,5,'Wanja.Hoffmann@company.org','0,81 EUR')\n,('C431-5986267','Transformer Meter',16,26,17,5,'Ulrik.Denzel@company.org','2,09 EUR')\n,('I468-8034393','Network Rheostat',23,49,17,5,'Wolfgang.Martin@company.org','3,79 EUR')\n,('L748-3922794','Phase Driver Switch',70,74,14,9,'Lili.Geier@company.org','1,40 EUR')\n,('B934-4668099','Sensor Potentiometer',79,43,15,14,'Karen.Brant@company.org','3,08 EUR')\n,('O944-9764063','Inductor Memristor Resonator',50,59,11,5,'Heinrich.Hoch@company.org','5,38 EUR')\n,('L365-6842646','Driver Transducer',23,22,12,2,'Sigmund.Gros@company.org','1,70 EUR')\n,('E502-4333702','Heisenberg Resistor Gauge Dipole',15,22,16,1,'Minnie.Kuehn@company.org','0,38 EUR')\n,('J234-3498557','Inductor Driver Breaker',50,28,16,7,'Wolfgang.Martin@company.org','5,92 EUR')\n,('Q516-8248086','Transducer Switch',24,30,20,9,'Lili.Geier@company.org','5,41 EUR')\n,('P360-3533771','Aluminum Transducer Capacitor Switch',46,31,13,9,'Waldtraud.Kuttner@company.org','2,45 EUR')\n,('K367-1320550','Strain Encoder',62,27,16,8,'Dietlinde.Boehme@company.org','4,08 EUR')\n,('M812-1436368','Bipolar-junction Crystal Rheostat',25,42,11,8,'Adolfina.Hoch@company.org','2,03 EUR')\n,('A750-6901242','Film Resistor Memristor Meter',37,37,19,4,'Kristen.Bauers@company.org','5,49 EUR')\n,('B150-4370781','Planck Coil Crystal Meter',31,52,14,18,'Ulrik.Denzel@company.org','5,87 EUR')\n,('D160-4387774','Gauge Transformer',56,26,12,17,'Herr.Burgh.Eichel@company.org','5,51 EUR')\n,('S176-7294665','Planck Gauge Compensator',23,64,16,15,'Ulrik.Denzel@company.org','0,76 EUR')\n,('L189-7913415','Oscillator Warp',57,33,14,11,'Baldwin.Dirksen@company.org','5,95 EUR')\n,('C917-9516418','LCD Coil',73,32,18,16,'Miles.Amsel@company.org','3,30 EUR')\n,('R725-9753976','Coil Resistor Encoder',44,66,11,13,'Baldwin.Dirksen@company.org','3,31 EUR')\n,('T721-4459242','Coil Compensator',38,56,14,16,'Marius.Fux@company.org','5,55 EUR')\n,('U229-5087557','Log-periodic Sensor Inductor Transistor',58,19,19,14,'Ida.Halle@company.org','2,12 EUR')\n,('L371-9651048','Inductor Sensor',30,61,15,19,'Nadia.Schubert@company.org','1,17 EUR')\n,('J391-2253339','Heisenberg LCD Resonator',26,28,19,17,'Karen.Brant@company.org','3,73 EUR')\n,('G840-8611458','Aluminum Sensor Memristor',26,11,15,5,'Henny.Foth@company.org','4,07 EUR')\n,('D975-3237312','Planck Coil Memristor',13,31,12,7,'Sigmund.Gros@company.org','2,00 EUR')\n,('P957-8177638','Multiplexer Dipole Transducer',52,45,14,17,'Arnelle.Gerber@company.org','3,31 EUR')\n,('E585-4643170','LCD Memristor Rheostat',36,75,18,7,'Minnie.Kuehn@company.org','0,71 EUR')\n,('N206-9059859','Polymer Memristor Capacitor',43,80,19,1,'Bert.Blumstein@company.org','4,10 EUR')\n,('U623-2779596','Multiplexer LCD Switch',17,66,19,10,'Bert.Blumstein@company.org','5,95 EUR')\n,('P989-7962038','Bipolar-junction LCD Resistor',37,75,19,5,'Bert.Blumstein@company.org','4,60 EUR')\n,('J653-7148856','Coil Transducer Warp',42,65,12,11,'Waldtraud.Kuttner@company.org','2,67 EUR')\n,('P966-3555304','Planck Dipole Rheostat Breaker',67,22,12,4,'Erhard.Fried@company.org','5,44 EUR')\n,('J178-7002767','Flow LCD Transistor Transducer',40,56,13,15,'Liese.Adam@company.org','1,78 EUR')\n,('A932-9527078','Ceramic Multiplexer Potentiometer Gauge',67,49,13,12,'Heinrich.Hoch@company.org','1,48 EUR')\n,('M885-1885470','Field-effect Transistor Crystal Switch',48,23,11,5,'Erhard.Fried@company.org','4,22 EUR')\n,('H747-8390476','Flow Multiplexer Inductor Breaker',25,16,14,18,'Elisabeth.Harman@company.org','5,14 EUR')\n,('J370-9195708','Multiplexer Potentiometer Compensator',61,14,13,3,'Lambert.Faust@company.org','4,54 EUR')\n,('Z288-2942538','Network Dipole Capacitor',28,46,15,18,'Thomas.Mueller@company.org','0,00 EUR')\n,('L592-1084147','Coil Potentiometer Compensator',52,61,12,20,'Ida.Halle@company.org','4,90 EUR')\n,('H491-2171849','Multiplexer Meter',52,74,11,5,'Ratt.Hartmann@company.org','0,29 EUR')\n,('M827-2856044','Driver Warp',28,36,16,3,'Kevin.Feigenbaum@company.org','2,87 EUR')\n,('Y714-9954664','Bipolar-junction Oscillator Gauge',58,29,17,15,'Arendt.Beitel@company.org','2,09 EUR')\n,('X502-7135246','Heisenberg Resonator Rheostat Transducer',55,11,16,5,'Jarvis.Jans@company.org','2,77 EUR')\n,('J498-2858887','Bipolar-junction Oscillator Memristor Capacitor',66,77,12,11,'Kevin.Feigenbaum@company.org','0,25 EUR')\n,('W847-4354260','Aluminum Resistor Gauge',72,39,19,5,'Gretel.Roth@company.org','4,66 EUR')\n,('U333-8518360','Driver Switch Transducer',25,13,19,19,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','0,73 EUR')\n,('A964-7949458','Bipolar-junction Encoder Transducer',72,46,19,10,'Miles.Amsel@company.org','2,91 EUR')\n,('X365-6429727','Coil Inductor Multiplexer',58,60,11,16,'Lukas.Gerver@company.org','4,77 EUR')\n,('N654-6497636','Coil Transformer Compensator',46,43,16,18,'Liese.Adam@company.org','0,80 EUR')\n,('S314-3937138','Heisenberg Memristor Breaker Switch',33,34,15,14,'Sabrina.Bayer@company.org','0,14 EUR')\n,('G261-8681976','Multiplexer Oscillator Inductor',52,21,15,1,'Rebecca.Hall@company.org','4,62 EUR')\n,('N411-3014396','Network Compensator',65,72,13,10,'Baldwin.Dirksen@company.org','1,59 EUR')\n,('K296-6028981','Inductor Strain',76,73,20,20,'Franziska.Acker@company.org','1,75 EUR')\n,('P174-7697886','Dipole Transducer',66,62,15,12,'Elisabeth.Harman@company.org','3,74 EUR')\n,('L612-4428366','Resonator Rheostat',53,15,18,20,'Arendt.Beitel@company.org','2,76 EUR')\n,('W436-5301120','Encoder Transducer',32,63,19,20,'Baldwin.Dirksen@company.org','5,21 EUR')\n,('F969-8946081','Inductor Rheostat',19,69,13,2,'Baldwin.Dirksen@company.org','5,83 EUR')\n,('O531-4282131','Multiplexer',70,28,19,20,'Kristen.Bauers@company.org','4,67 EUR')\n,('N366-8062929','Network Oscillator Strain',15,70,19,17,'Reiner.Widmann@company.org','3,40 EUR')\n,('X989-6385984','Sensor Encoder Transformer',41,40,17,8,'Adolfina.Hoch@company.org','1,00 EUR')\n,('X308-3411861','Aluminum Breaker Capacitor Warp',38,17,12,7,'Wanja.Hoffmann@company.org','3,22 EUR')\n,('Y117-1025328','Planck Switch',46,29,12,5,'Elena.Herzog@company.org','1,67 EUR')\n,('R524-4869682','Planck Multiplexer Inductor',13,67,20,10,'Gretel.Roth@company.org','3,50 EUR')\n,('E812-9665212','Bipolar-junction LCD Potentiometer Encoder',44,40,13,3,'Wanja.Hoffmann@company.org','1,11 EUR')\n,('C970-6304679','Ceramic Coil Rheostat',47,69,20,9,'Henny.Foth@company.org','1,62 EUR')\n,('E103-2799984','Encoder Transducer',26,23,17,11,'Baldwin.Guenther@company.org','4,67 EUR')\n,('X643-1063819','Dipole Compensator',20,55,17,6,'Franziska.Acker@company.org','0,06 EUR')\n,('H598-5149929','Transistor Rheostat',53,48,19,8,'Franziska.Acker@company.org','2,17 EUR')\n,('T769-2350399','Multiplexer LCD',62,11,12,5,'Karch.Moeller@company.org','1,70 EUR')\n,('H631-9376983','Aluminum Network Oscillator Capacitor',19,28,19,10,'Berlin.Schulz@company.org','5,36 EUR')\n,('C213-4995667','Potentiometer Crystal Encoder',14,43,12,1,'Sabrina.Geiger@company.org','0,12 EUR')\n,('V382-3883746','Network Dipole Warp',74,77,13,12,'Kristen.Bauers@company.org','3,68 EUR')\n,('O636-3428529','Resonator Strain Meter',34,33,17,2,'Dietlinde.Boehme@company.org','0,77 EUR')\n,('R774-3816629','Log-periodic Encoder Transducer',19,40,18,3,'Ratt.Hartmann@company.org','4,21 EUR')\n,('P913-2073572','Network Dipole Rheostat',48,16,15,5,'Adolfina.Hoch@company.org','4,07 EUR')\n,('B567-3760235','LCD Resonator',12,20,18,9,'Ida.Halle@company.org','3,57 EUR')\n,('B488-6957957','Strain Capacitor',26,48,15,18,'Minnie.Kuehn@company.org','2,91 EUR')\n,('I696-9739398','Breaker Resonator Encoder',45,60,15,20,'Manfred.Foth@company.org','4,28 EUR')\n,('E890-4143899','Coil Crystal Strain',40,70,14,1,'Thomas.Mueller@company.org','4,62 EUR')\n,('D525-4805979','Compensator Transducer',16,68,14,6,'Herr.Burgh.Eichel@company.org','2,92 EUR')\n,('H962-4715431','Phase Dipole Transformer',12,49,14,20,'Henny.Foth@company.org','4,75 EUR')\n,('P708-1537318','Bipolar-junction Network Strain',62,68,13,10,'Ratt.Hartmann@company.org','5,99 EUR')\n,('E868-5063965','Coil Compensator',64,21,13,1,'Emil.Gotti@company.org','5,26 EUR')\n,('N568-8608034','Coil Oscillator Switch',49,71,16,5,'Herr.Burgh.Eichel@company.org','1,80 EUR')\n,('C402-4354819','Film Potentiometer',65,48,11,8,'Lili.Geier@company.org','4,92 EUR')\n,('Z655-3173353','Multiplexer Inductor Crystal',21,19,14,5,'Baldwin.Guenther@company.org','2,84 EUR')\n,('Q696-9337626','Ceramic Coil Breaker',40,17,20,10,'Ratt.Beyer@company.org','2,72 EUR')\n,('K288-9703549','Sensor Rheostat',23,49,12,16,'Berlin.Schulz@company.org','1,27 EUR')\n,('F332-3707903','Gauge Breaker',24,39,12,16,'Elisabeth.Harman@company.org','2,44 EUR')\n,('N879-4735381','Multiplexer Inductor Crystal',29,15,12,5,'Sigmund.Gros@company.org','0,55 EUR')\n,('X620-7792033','Ceramic Resistor Rheostat',38,36,20,19,'Lukas.Gerver@company.org','3,80 EUR')\n,('P453-8155326','Encoder Breaker',22,71,19,12,'Karen.Brant@company.org','0,39 EUR')\n,('X602-2515162','Dipole Warp',40,32,17,10,'Karch.Moeller@company.org','1,64 EUR')\n,('D548-3561584','Potentiometer Gauge Compensator',72,32,18,14,'Franziska.Acker@company.org','2,09 EUR')\n,('H267-1492366','Sensor Transducer',79,74,19,12,'Elisabeth.Harman@company.org','5,05 EUR')\n,('Z739-8572107','Network Potentiometer Gauge',54,19,12,19,'Gretel.Roth@company.org','2,23 EUR')\n,('C390-4121800','Sensor Strain',74,17,18,6,'Yanka.Schreiber@company.org','5,75 EUR')\n,('T941-8766844','Planck Inductor Gauge Resonator',43,11,17,10,'Lukas.Gerver@company.org','1,63 EUR')\n,('F264-7752472','Resonator Switch',21,44,12,17,'Karch.Moeller@company.org','0,71 EUR')\n,('F266-5876962','Multiplexer Transistor',73,32,13,9,'Ulrik.Denzel@company.org','0,29 EUR')\n,('P602-5728865','Phase Compensator Capacitor Switch',45,30,13,5,'Sigmund.Gros@company.org','1,39 EUR')\n,('F708-2915261','Crystal Resonator',23,79,11,6,'Yanka.Schreiber@company.org','4,23 EUR')\n,('C371-9169438','Polymer Transducer Meter',79,68,19,12,'Henny.Foth@company.org','2,27 EUR')\n,('A769-5120124','Flow Memristor Gauge',62,63,12,6,'Karch.Moeller@company.org','5,23 EUR')\n,('Q248-7597886','Oscillator Strain',17,76,12,15,'Emil.Gotti@company.org','3,36 EUR')\n,('Z848-4991684','Multiplexer Dipole',79,24,15,9,'Frauke.Faerber@company.org','1,92 EUR')\n,('H972-9616381','Film Coil Inductor Transformer',28,34,19,16,'Elena.Herzog@company.org','1,99 EUR')\n,('J505-2473322','Coil Rheostat Transducer',69,23,19,5,'Manfred.Foth@company.org','3,32 EUR')\n,('N153-4376308','Aluminum Inductor Breaker',80,47,16,19,'Elena.Herzog@company.org','0,39 EUR')\n,('S113-2439377','LCD Potentiometer',34,56,19,9,'Herr.Haan.Bader@company.org','2,44 EUR')\n,('W821-9428247','Dipole Memristor Encoder',50,55,20,4,'Liese.Adam@company.org','1,44 EUR')\n,('B693-5414825','Planck Transformer Transistor Capacitor',50,14,15,15,'Gretel.Roth@company.org','5,09 EUR')\n,('X634-4382180','Inductor Driver Potentiometer',14,44,13,3,'Valda.Everhart@company.org','2,16 EUR')\n,('Q189-5857908','Phase Multiplexer Crystal Warp',39,73,19,15,'Nadia.Schubert@company.org','3,90 EUR')\n,('M206-5050706','Resistor Transducer',12,44,12,12,'Minnie.Kuehn@company.org','0,56 EUR')\n,('U614-3483402','Sensor Compensator Transducer',38,50,15,14,'Sabrina.Bayer@company.org','0,04 EUR')\n,('N324-6810821','Aluminum Rheostat Transformer Warp',20,67,15,20,'Kristen.Bauers@company.org','3,71 EUR')\n,('B646-2108570','Dipole Transducer',46,26,16,9,'Kristen.Bauers@company.org','1,14 EUR')\n,('P577-5587693','Network Dipole Memristor',15,51,18,6,'Ida.Halle@company.org','3,86 EUR')\n,('Y505-9919340','Ceramic Capacitor Transformer Warp',59,78,20,13,'Ratt.Hartmann@company.org','1,23 EUR')\n,('U389-9635839','Multiplexer Switch',24,58,12,19,'Jarvis.Jans@company.org','0,51 EUR')\n,('J820-7132026','Polymer Inductor',41,65,15,4,'Arnelle.Gerber@company.org','3,67 EUR')\n,('A225-1988393','Field-effect Dipole Gauge',59,77,18,4,'Bert.Blumstein@company.org','0,06 EUR')\n,('K651-4147885','LCD Crystal',48,50,16,15,'Rebecca.Hall@company.org','2,14 EUR')\n,('K850-3410875','Transducer Gauge Switch',55,11,16,6,'Baldwin.Guenther@company.org','3,82 EUR')\n,('Q187-2944814','Multiplexer Potentiometer LCD',33,12,13,9,'Adolfina.Hoch@company.org','5,64 EUR')\n,('M205-1376206','Dipole Memristor Encoder',17,80,15,19,'Kevin.Feigenbaum@company.org','1,02 EUR')\n,('R302-4754313','Aluminum Inductor Gauge Capacitor',43,66,15,7,'Wolfgang.Martin@company.org','1,49 EUR')\n,('S100-7238368','Sensor Potentiometer',62,20,12,5,'Xochitl.Aue@company.org','0,28 EUR')\n,('L816-8238278','Inductor',48,70,14,7,'Yanka.Schreiber@company.org','3,21 EUR')\n,('M810-8954183','Aluminum Gauge Resonator',13,16,12,4,'Gretel.Roth@company.org','3,09 EUR')\n,('X283-7840276','Planck Sensor Dipole Capacitor',55,20,12,6,'Corinna.Ludwig@company.org','1,87 EUR')\n,('M974-1997588','Potentiometer Dipole',41,19,18,4,'Herr.Haan.Bader@company.org','3,38 EUR')\n,('Y863-3538159','Multiplexer Rheostat',69,15,11,17,'Thomas.Mueller@company.org','4,77 EUR')\n,('H510-3094779','Resonator Transformer',16,65,15,20,'Yanka.Schreiber@company.org','0,81 EUR')\n,('N998-2489600','Bipolar-junction Strain',19,43,13,7,'Liese.Adam@company.org','1,15 EUR')\n,('Z646-5864967','Coil Encoder Switch',77,62,13,20,'Ratt.Hartmann@company.org','0,19 EUR')\n,('H244-4330589','LCD Strain',29,40,20,13,'Franz.Kornhaeusel@company.org','0,63 EUR')\n,('L792-8374906','Sensor Memristor Meter',29,66,15,3,'Erhard.Fried@company.org','1,58 EUR')\n,('X897-7676293','Coil Potentiometer Strain',42,33,18,13,'Manfred.Foth@company.org','1,66 EUR')\n,('W304-6414305','Transducer',42,22,17,16,'Baldwin.Dirksen@company.org','3,37 EUR')\n,('T504-8448784','Flow Potentiometer Gauge Oscillator',75,11,12,1,'Arendt.Beitel@company.org','5,90 EUR')\n,('J731-7469427','Transformer Compensator',75,28,20,2,'Ratt.Beyer@company.org','2,75 EUR')\n,('Y889-4226936','Coil Oscillator',24,34,13,17,'Corinna.Ludwig@company.org','5,64 EUR')\n,('A909-7626614','Transistor Encoder',36,15,19,20,'Valda.Everhart@company.org','5,43 EUR')\n,('R414-3098561','Multiplexer Meter',52,56,16,16,'Berlin.Schulz@company.org','4,35 EUR')\n,('M436-2993715','LCD Resonator Compensator',26,41,17,3,'Heinrich.Hoch@company.org','5,66 EUR')\n,('G826-1197003','Heisenberg LCD Oscillator Breaker',53,28,15,4,'Anamchara.Foerstner@company.org','0,44 EUR')\n,('C375-1115425','Polymer Inductor Gauge Rheostat',74,38,14,17,'Ratt.Hartmann@company.org','1,63 EUR')\n,('V645-2413888','Log-periodic Oscillator Transistor Rheostat',46,62,13,13,'Elisabeth.Harman@company.org','2,24 EUR')\n,('P787-7863897','Switch Transducer',72,20,17,6,'Nadia.Schubert@company.org','0,18 EUR')\n,('T995-5683542','Resonator Rheostat',55,21,14,7,'Valda.Everhart@company.org','4,33 EUR')\n,('U169-4232721','Driver Breaker Warp',21,68,19,15,'Reiner.Widmann@company.org','4,54 EUR')\n,('V876-7098157','Potentiometer Transistor',71,16,15,5,'Gretel.Roth@company.org','1,26 EUR')\n,('W358-5750223','Coil Warp',67,34,17,20,'Sigmund.Gros@company.org','4,01 EUR')\n,('E971-2487589','Network Encoder',21,13,16,8,'Henny.Foth@company.org','1,84 EUR')\n,('P360-3765415','LCD Dipole',71,22,14,1,'Manfred.Foth@company.org','1,00 EUR')\n,('T806-6069877','Planck Crystal Rheostat Switch',27,37,19,18,'Jarvis.Jans@company.org','4,20 EUR')\n,('U619-9969216','Coil Inductor Transistor',72,56,13,4,'Dieterich.Blau@company.org','2,00 EUR')\n,('L262-1109442','Polymer Inductor Rheostat Breaker',58,37,17,2,'Bert.Blumstein@company.org','0,31 EUR')\n,('N881-4812973','Meter',49,22,17,6,'Wolfgang.Martin@company.org','1,60 EUR')\n,('R389-4348258','Network Gauge',69,77,18,2,'Baldwin.Guenther@company.org','3,92 EUR')\n,('M400-3382615','Field-effect Transistor Compensator',29,77,20,14,'Xochitl.Aue@company.org','5,50 EUR')\n,('D243-3238752','Log-periodic Transistor Encoder Compensator',63,47,15,4,'Henny.Foth@company.org','0,98 EUR')\n,('Q861-1618446','Bipolar-junction Driver Crystal',50,24,14,14,'Gretel.Roth@company.org','2,17 EUR')\n,('N709-2262876','Multiplexer Memristor Switch',35,25,12,15,'Karen.Brant@company.org','3,05 EUR')\n,('Q546-7014038','Field-effect Memristor Rheostat',56,62,18,14,'Franziska.Acker@company.org','2,48 EUR')\n,('C622-9716605','Oscillator Transducer',58,20,14,7,'Franziska.Acker@company.org','4,09 EUR')\n,('R247-6538517','Crystal Breaker',47,66,17,13,'Liese.Adam@company.org','1,53 EUR')\n,('J470-3164222','Multiplexer Memristor Rheostat',41,54,19,4,'Franziska.Acker@company.org','2,70 EUR')\n,('V600-6144297','Driver Transducer',70,45,18,19,'Ulrik.Denzel@company.org','5,54 EUR')\n,('E162-5553215','Film Inductor Rheostat',43,14,17,14,'Liese.Adam@company.org','2,27 EUR')\n,('E187-6115721','LCD Inductor Transducer',56,11,13,5,'Sabrina.Bayer@company.org','5,61 EUR')\n,('F559-6763700','Gauge Capacitor',37,79,12,19,'Henny.Foth@company.org','0,55 EUR')\n,('N704-3896920','Strain Encoder',25,48,14,3,'Thomas.Mueller@company.org','0,16 EUR')\n,('U318-1465198','Aluminum Network Dipole Transducer',50,57,19,17,'Ulrik.Denzel@company.org','3,83 EUR')\n,('Y788-9882822','Planck Multiplexer Capacitor',24,31,13,10,'Thomas.Mueller@company.org','5,24 EUR')\n,('Z587-4413312','Aluminum Dipole Sensor Capacitor',17,45,12,5,'Elena.Herzog@company.org','2,04 EUR')\n,('R862-8479315','Aluminum Transducer Rheostat Warp',43,12,15,13,'Minnie.Kuehn@company.org','4,25 EUR')\n,('H569-9184293','Inductor Rheostat Breaker',73,12,19,17,'Franziska.Acker@company.org','3,96 EUR')\n,('H641-1089353','Log-periodic Potentiometer Resonator',76,26,19,5,'Henny.Foth@company.org','2,17 EUR')\n,('K375-1173149','Inductor Breaker',66,53,17,12,'Siglind.Brinkerhoff@company.org','4,25 EUR')\n,('I327-6567979','Gauge Transducer',41,30,16,8,'Nadia.Schubert@company.org','2,13 EUR')\n,('L374-3374767','Resistor Potentiometer Strain',51,53,13,8,'Heinrich.Hoch@company.org','5,91 EUR')\n,('M650-3458375','Heisenberg Network Strain Encoder',45,42,13,8,'Emil.Gotti@company.org','2,53 EUR')\n,('H958-9648652','Potentiometer Crystal Warp',71,49,15,1,'Wanja.Hoffmann@company.org','3,42 EUR')\n,('L781-7008508','Bipolar-junction Multiplexer Inductor Transformer',16,21,11,13,'Kevin.Feigenbaum@company.org','5,13 EUR')\n,('V887-9194738','Gauge Meter Transducer',71,31,18,8,'Waldtraud.Kuttner@company.org','1,22 EUR')\n,('F779-9528637','Phase Sensor Transistor',56,61,18,12,'Reiner.Widmann@company.org','3,29 EUR')\n,('Z254-1002324','Transistor Crystal',22,46,14,16,'Erhard.Fried@company.org','2,34 EUR')\n,('Q594-6895704','LCD Network',42,76,17,20,'Rebecca.Hall@company.org','1,54 EUR')\n,('O300-8464663','Crystal Strain',80,69,18,13,'Dietlinde.Boehme@company.org','3,57 EUR')\n,('R338-2935955','Film Multiplexer Breaker',35,13,20,6,'Nadia.Schubert@company.org','1,42 EUR')\n,('U861-6382993','Compensator Meter',60,65,11,8,'Corinna.Ludwig@company.org','2,82 EUR')\n,('Q353-5311148','Ceramic Compensator Breaker Transducer',48,16,12,6,'Lambert.Faust@company.org','0,25 EUR')\n,('E793-4560979','Capacitor Resonator Compensator',69,68,18,5,'Dieterich.Blau@company.org','4,21 EUR')\n,('H374-8481414','Network Multiplexer Resonator',34,43,18,10,'Ratt.Beyer@company.org','3,92 EUR')\n,('Z889-8463159','Resistor Inductor',41,61,15,2,'Anamchara.Foerstner@company.org','0,57 EUR')\n,('O553-6585255','Resistor Encoder Breaker',71,13,16,7,'Gretel.Roth@company.org','3,07 EUR')\n,('G625-3606813','Phase Multiplexer Inductor Crystal',53,79,14,8,'Adolfina.Hoch@company.org','0,01 EUR')\n,('K353-7420061','Rheostat Compensator',71,64,11,3,'Anamchara.Foerstner@company.org','5,85 EUR')\n,('D603-1225899','Field-effect Memristor Resonator Transducer',64,50,19,5,'Wolfgang.Martin@company.org','0,61 EUR')\n,('J544-6748850','LCD Compensator',80,23,16,18,'Sigmund.Gros@company.org','1,10 EUR')\n,('W846-7438265','Resistor Memristor Capacitor',60,27,11,20,'Erhard.Fried@company.org','5,32 EUR')\n,('E373-8515317','Field-effect Resistor Resonator',64,47,17,16,'Franziska.Acker@company.org','5,56 EUR')\n,('I771-2451857','Potentiometer Breaker Oscillator',59,28,16,6,'Karch.Moeller@company.org','1,13 EUR')\n,('T732-3194846','Film Oscillator Rheostat Transformer',52,25,14,20,'Herr.Burgh.Eichel@company.org','1,41 EUR')\n,('R946-7112528','Sensor Breaker',47,64,14,10,'Baldwin.Guenther@company.org','2,86 EUR')\n,('G812-4734922','Heisenberg Network Potentiometer',64,76,12,12,'Elena.Herzog@company.org','5,23 EUR')\n,('B825-7596233','Resistor Compensator',24,74,19,14,'Arendt.Beitel@company.org','4,50 EUR')\n,('B429-3694560','Network Switch',67,11,19,5,'Wanja.Hoffmann@company.org','0,52 EUR')\n,('W232-7656495','Transistor Compensator Transducer',13,25,12,12,'Henny.Foth@company.org','3,33 EUR')\n,('K542-2074800','Coil Encoder',54,71,20,3,'Sabrina.Geiger@company.org','3,06 EUR')\n,('U489-9025040','Compensator Warp',78,57,15,10,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','1,72 EUR')\n,('Z763-7274829','Potentiometer Rheostat',77,68,16,1,'Wanja.Hoffmann@company.org','5,40 EUR')\n,('X954-5873970','Bipolar-junction Oscillator Memristor Sensor',57,35,20,18,'Yanka.Schreiber@company.org','3,16 EUR')\n,('M805-4248390','Polymer Inductor Crystal Capacitor',49,13,18,14,'Wolfgang.Martin@company.org','2,91 EUR')\n,('A166-3766336','Aluminum LCD Driver Gauge',53,77,14,2,'Xochitl.Aue@company.org','1,27 EUR')\n,('G144-7255154','Dipole Rheostat Capacitor',34,54,13,3,'Arnelle.Gerber@company.org','3,54 EUR')\n,('E469-1218832','Dipole Crystal',76,50,15,8,'Elisabeth.Harman@company.org','5,72 EUR')\n,('R586-3728998','Gauge Breaker',62,72,16,13,'Arnelle.Gerber@company.org','4,78 EUR')\n,('O194-8514321','Aluminum Capacitor Warp',12,78,15,9,'Elisabeth.Harman@company.org','1,67 EUR')\n,('I272-3912947','Ceramic Inductor Resonator',36,67,13,18,'Kevin.Feigenbaum@company.org','1,90 EUR')\n,('J420-5667802','Ceramic Inductor Crystal Transformer',55,13,19,11,'Elisabeth.Harman@company.org','1,27 EUR')\n,('B625-4480024','Sensor Resonator Compensator',23,59,13,13,'Lili.Geier@company.org','0,52 EUR')\n,('Y194-2779293','Field-effect LCD Strain',30,28,16,19,'Herr.Haan.Bader@company.org','4,06 EUR')\n,('P337-4636612','Sensor Driver Crystal',64,80,16,12,'Emil.Gotti@company.org','5,37 EUR')\n,('V283-8186620','Dipole Crystal',48,48,16,1,'Franz.Kornhaeusel@company.org','0,52 EUR')\n,('U379-6681071','Heisenberg Coil Inductor LCD',30,40,19,2,'Waldtraud.Kuttner@company.org','1,66 EUR')\n,('N317-6012752','Oscillator Driver Sensor',42,58,18,7,'Marius.Fux@company.org','2,98 EUR')\n,('Y354-7075426','Aluminum LCD Warp',15,49,19,4,'Emil.Gotti@company.org','4,82 EUR')\n,('B937-9245602','Driver Compensator',47,62,13,6,'Berlin.Schulz@company.org','2,16 EUR')\n,('J769-7213127','Planck Multiplexer Driver Transistor',73,13,19,15,'Emil.Gotti@company.org','5,24 EUR')\n,('V488-1866672','Log-periodic Rheostat Strain Transducer',60,22,13,8,'Jarvis.Jans@company.org','5,55 EUR')\n,('U833-1786284','Oscillator Crystal Resonator',19,64,17,16,'Rebecca.Hall@company.org','5,11 EUR')\n,('R506-1245812','Film Strain Breaker',70,62,17,5,'Jarvis.Jans@company.org','4,47 EUR')\n,('S480-3531134','Oscillator Gauge',14,12,17,5,'Waldtraud.Kuttner@company.org','0,20 EUR')\n,('N607-6979614','Rheostat Strain Meter',13,40,18,19,'Manfred.Foth@company.org','1,26 EUR')\n,('W176-3285571','Log-periodic Oscillator Breaker',13,49,20,20,'Corinna.Ludwig@company.org','2,63 EUR')\n,('E226-4279524','Resistor Compensator',35,12,13,4,'Marius.Fux@company.org','4,05 EUR')\n,('P317-3419187','Network Breaker Transducer',52,25,11,20,'Ida.Halle@company.org','2,09 EUR')\n,('Q514-9410667','Phase Strain Transducer',23,13,13,10,'Lambert.Faust@company.org','3,16 EUR')\n,('N137-2655981','Inductor Compensator',27,45,18,4,'Wolfgang.Martin@company.org','2,16 EUR')\n,('A566-1562523','Planck Dipole Sensor Warp',43,57,13,19,'Adolfina.Hoch@company.org','2,83 EUR')\n,('C245-8365837','Potentiometer Resonator Encoder',16,44,14,20,'Berlin.Schulz@company.org','0,49 EUR')\n,('C182-2689274','Potentiometer Driver',20,54,11,17,'Thomas.Mueller@company.org','5,99 EUR')\n,('P472-9724615','Field-effect Potentiometer Resonator',34,75,11,3,'Elisabeth.Harman@company.org','1,43 EUR')\n,('E835-1316991','Resistor Breaker',37,20,19,20,'Thomas.Mueller@company.org','3,94 EUR')\n,('G403-4566802','LCD Resonator Warp',22,66,11,16,'Lili.Geier@company.org','4,20 EUR')\n,('B286-1369879','Flow Coil Meter',58,51,16,5,'Arendt.Beitel@company.org','0,69 EUR')\n,('E172-8066897','Driver Compensator',15,68,13,19,'Ratt.Hartmann@company.org','4,60 EUR')\n,('G625-4620445','Log-periodic Driver Compensator',19,35,20,16,'Dietlinde.Boehme@company.org','4,32 EUR')\n,('M522-9124638','Flow LCD Resistor Encoder',16,73,18,2,'Corinna.Ludwig@company.org','5,11 EUR')\n,('S839-8840069','Phase Resonator Meter',22,75,11,9,'Baldwin.Guenther@company.org','0,12 EUR')\n,('B918-9468392','Multiplexer Resistor Memristor',29,45,13,18,'Reiner.Widmann@company.org','3,60 EUR')\n,('Y580-9027193','Bipolar-junction LCD Resistor',40,77,20,10,'Dieterich.Blau@company.org','0,47 EUR')\n,('A755-9228475','Resistor Driver',79,64,12,16,'Frauke.Faerber@company.org','4,15 EUR')\n,('W177-1965331','Ceramic Coil Memristor Gauge',29,74,15,3,'Reiner.Widmann@company.org','2,92 EUR')\n,('L932-9433395','Dipole Inductor Crystal',67,76,11,3,'Lukas.Gerver@company.org','5,82 EUR')\n,('V610-4162567','Breaker Encoder',64,13,17,10,'Thomas.Mueller@company.org','3,81 EUR')\n,('H502-2553729','Flow Resistor Inductor Transformer',68,71,18,18,'Ulrik.Denzel@company.org','5,73 EUR')\n,('S450-5654221','Multiplexer Transistor',41,43,11,6,'Frauke.Faerber@company.org','1,96 EUR')\n,('O856-7652159','Coil Gauge Strain',32,30,16,17,'Heinrich.Hoch@company.org','0,74 EUR')\n,('Y884-3616085','Phase Potentiometer Memristor Sensor',26,48,18,6,'Erhard.Fried@company.org','1,36 EUR')\n,('J671-5760255','Driver Crystal',63,40,12,14,'Sabrina.Bayer@company.org','0,59 EUR')\n,('G187-5631756','Polymer Multiplexer Compensator Transducer',16,11,15,6,'Wolfgang.Martin@company.org','3,23 EUR')\n,('N905-1544917','Phase Dipole Transistor',54,50,11,9,'Gretel.Roth@company.org','5,78 EUR')\n,('E405-1045893','Polymer Sensor',21,52,15,1,'Karch.Moeller@company.org','3,58 EUR')\n,('J519-7020928','Driver Transducer',14,44,16,3,'Ulrik.Denzel@company.org','4,09 EUR')\n,('N744-5971232','Coil Strain',57,72,17,8,'Siglind.Brinkerhoff@company.org','4,82 EUR')\n,('S531-7887299','Transistor Switch',61,60,18,12,'Adolfina.Hoch@company.org','5,75 EUR')\n,('T914-4676603','Crystal Rheostat Meter',64,62,13,19,'Berlin.Schulz@company.org','3,51 EUR')\n,('T161-7769514','Network Breaker Capacitor',15,61,14,2,'Nadia.Schubert@company.org','5,56 EUR')\n,('W830-6681347','Aluminum Resonator Capacitor Warp',80,47,20,7,'Rebecca.Hall@company.org','5,62 EUR')\n,('Z615-8618743','Film Resistor Inductor Switch',43,72,17,19,'Baldwin.Guenther@company.org','5,31 EUR')\n,('W529-7595129','Film LCD Strain',61,18,16,1,'Anamchara.Foerstner@company.org','2,98 EUR')\n,('U772-6050161','Resonator Encoder Warp',35,66,17,12,'Bert.Blumstein@company.org','1,41 EUR')\n,('H941-5264433','Transformer Breaker Meter',65,79,15,6,'Marius.Fux@company.org','4,65 EUR')\n,('T124-5609647','Network Compensator',19,51,20,15,'Kevin.Feigenbaum@company.org','3,20 EUR')\n,('N237-3608803','Flow Capacitor Meter',40,11,15,5,'Berlin.Schulz@company.org','0,26 EUR')\n,('I242-5347848','Compensator Warp',24,28,20,5,'Corinna.Ludwig@company.org','1,26 EUR')\n,('M521-8491113','Multiplexer Coil Resonator',61,67,17,19,'Manfred.Foth@company.org','5,07 EUR')\n,('D237-6587630','Heisenberg Transformer Crystal Encoder',35,74,19,18,'Herr.Haan.Bader@company.org','4,86 EUR')\n,('V519-6173906','Inductor Memristor Breaker',70,35,20,6,'Siglind.Brinkerhoff@company.org','4,26 EUR')\n,('T608-9573692','Coil Resonator',71,74,11,1,'Reiner.Widmann@company.org','3,87 EUR')\n,('U743-1581581','Memristor Driver Strain',63,13,11,2,'Anamchara.Foerstner@company.org','2,01 EUR')\n,('H634-7337115','Gauge Crystal',35,24,20,3,'Herr.Burgh.Eichel@company.org','1,36 EUR')\n,('B187-7652875','Driver Memristor Crystal',17,42,14,4,'Miles.Amsel@company.org','3,18 EUR')\n,('J781-8212433','Oscillator Meter',55,25,11,6,'Bert.Blumstein@company.org','0,15 EUR')\n,('E882-2235305','Phase Multiplexer Network Warp',51,62,18,4,'Rebecca.Hall@company.org','2,02 EUR')\n,('U651-8669022','Heisenberg Inductor Rheostat Transformer',65,23,12,1,'Kevin.Feigenbaum@company.org','4,25 EUR')\n,('I893-5009730','Polymer Potentiometer Meter',18,12,16,1,'Liese.Adam@company.org','4,96 EUR')\n,('W686-1387652','Oscillator Breaker',65,65,13,3,'Herr.Haan.Bader@company.org','2,77 EUR')\n,('Z322-3336300','Transducer Encoder Switch',14,74,16,3,'Lambert.Faust@company.org','5,99 EUR')\n,('G346-2864946','LCD Strain Encoder',43,75,18,1,'Sabrina.Bayer@company.org','5,54 EUR')\n,('G881-8555057','Flow Dipole Meter Warp',21,71,13,16,'Ratt.Beyer@company.org','2,21 EUR')\n,('T151-7042410','Memristor Switch',22,52,12,1,'Wolfgang.Martin@company.org','3,20 EUR')\n,('Q672-2667601','Multiplexer Resonator',59,54,12,6,'Thomas.Mueller@company.org','0,22 EUR')\n,('M901-8670057','Field-effect Resonator Switch',72,52,19,9,'Sylvester.Brant@company.org','5,33 EUR')\n,('Z358-5797618','Sensor Encoder',68,38,16,8,'Wanja.Hoffmann@company.org','1,22 EUR')\n,('K995-8098017','Resonator Meter',71,28,17,6,'Ratt.Beyer@company.org','2,13 EUR')\n,('E465-6674831','Network Sensor Transistor',52,61,11,2,'Liese.Adam@company.org','5,38 EUR')\n,('L787-2053792','Potentiometer Rheostat Compensator',20,17,20,15,'Liese.Adam@company.org','1,58 EUR')\n,('V156-6277722','Multiplexer Resistor Transducer',55,23,17,3,'Elisabeth.Harman@company.org','4,17 EUR')\n,('S212-6028302','Rheostat Encoder',71,48,20,20,'Arnelle.Gerber@company.org','0,16 EUR')\n,('I479-8852507','Sensor Memristor Strain',64,31,18,9,'Jarvis.Jans@company.org','1,66 EUR')\n,('O502-4324008','Sensor Transistor Encoder',36,29,12,16,'Liese.Adam@company.org','0,84 EUR')\n,('C744-6535902','Coil Warp',77,43,13,10,'Waldtraud.Kuttner@company.org','0,45 EUR')\n,('I625-9097378','Coil Compensator',29,22,14,3,'Lukas.Gerver@company.org','0,06 EUR')\n,('O489-4154201','Potentiometer Transistor Compensator',72,21,12,8,'Manfred.Foth@company.org','4,37 EUR')\n,('R318-8770198','Flow Transistor Compensator Transducer',66,76,11,14,'Baldwin.Dirksen@company.org','5,68 EUR')\n,('Q493-2919102','Oscillator Capacitor',70,31,17,18,'Franziska.Acker@company.org','2,96 EUR')\n,('O712-3456018','Log-periodic Compensator',71,31,15,14,'Miles.Amsel@company.org','4,45 EUR')\n,('B888-3582334','Aluminum Potentiometer Crystal',42,35,20,20,'Karch.Moeller@company.org','2,50 EUR')\n,('S874-6150679','Multiplexer Transistor',37,28,18,20,'Sylvester.Brant@company.org','2,63 EUR')\n,('D371-6210252','Memristor Capacitor Transformer',48,39,17,12,'Gretel.Roth@company.org','0,77 EUR')\n,('G127-3809321','Heisenberg Memristor Gauge',71,53,16,1,'Baldwin.Guenther@company.org','2,96 EUR')\n,('O787-2734023','Aluminum Memristor Strain',18,22,15,1,'Wanja.Hoffmann@company.org','1,97 EUR')\n,('U955-7894277','Transducer Gauge Meter',12,77,14,7,'Nadia.Schubert@company.org','4,03 EUR')\n,('S174-1960652','Ceramic Resonator Rheostat',35,34,11,7,'Anamchara.Foerstner@company.org','0,48 EUR')\n,('C721-7900144','Planck Network Warp',61,60,16,20,'Herr.Haan.Bader@company.org','4,51 EUR')\n,('M133-5945489','Transducer Warp',72,51,15,8,'Ratt.Beyer@company.org','3,08 EUR')\n,('A599-2465791','Driver Memristor',56,19,14,8,'Baldwin.Dirksen@company.org','1,87 EUR')\n,('X230-3586307','LCD Network',70,25,11,17,'Herr.Haan.Bader@company.org','5,11 EUR')\n,('Q223-1316238','Dipole Inductor Switch',45,32,13,9,'Lili.Geier@company.org','2,11 EUR')\n,('Z288-2722877','Polymer Transistor Breaker Compensator',45,27,19,12,'Baldwin.Guenther@company.org','2,60 EUR')\n,('D679-1940878','LCD Potentiometer Transducer',62,14,12,7,'Dietlinde.Boehme@company.org','2,95 EUR')\n,('D381-6342696','Inductor',67,75,15,6,'Sylvester.Brant@company.org','0,06 EUR')\n,('Y553-3929384','Crystal Capacitor',40,80,20,3,'Wanja.Hoffmann@company.org','5,02 EUR')\n,('X480-1491345','LCD Gauge Crystal',34,32,14,1,'Reiner.Widmann@company.org','3,67 EUR')\n,('C605-1105328','Log-periodic Gauge Compensator Meter',25,68,11,20,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','0,00 EUR')\n,('I409-8883822','Polymer Sensor Dipole Meter',48,21,14,2,'Rebecca.Hall@company.org','5,23 EUR')\n,('W697-5712939','Transformer Capacitor',66,17,20,10,'Valda.Everhart@company.org','0,53 EUR')\n,('M128-9664568','Sensor Rheostat Breaker',15,30,14,9,'Arendt.Beitel@company.org','4,25 EUR')\n,('F442-1761220','Aluminum Oscillator Transistor Strain',43,33,12,17,'Kevin.Feigenbaum@company.org','2,16 EUR')\n,('O475-7125508','Multiplexer Oscillator Coil',68,67,17,10,'Minnie.Kuehn@company.org','2,17 EUR')\n,('T725-1852362','Field-effect Multiplexer Potentiometer',39,64,15,17,'Kevin.Feigenbaum@company.org','2,02 EUR')\n,('B820-6334766','LCD Strain Capacitor',63,56,14,4,'Lili.Geier@company.org','5,85 EUR')\n,('Y191-7589606','Gauge Breaker',34,66,18,18,'Ratt.Hartmann@company.org','4,35 EUR')\n,('Q568-8156489','Gauge Capacitor Meter',17,79,17,4,'Karch.Moeller@company.org','0,37 EUR')\n,('J785-9314350','Inductor Crystal Oscillator',70,16,18,3,'Elisabeth.Harman@company.org','3,26 EUR')\n,('F661-8988230','Warp Transistor Meter',66,58,12,11,'Wolfgang.Martin@company.org','1,88 EUR')\n,('M361-5073440','Film Transformer Transducer',54,16,19,3,'Wolfgang.Martin@company.org','5,05 EUR')\n,('K689-4865625','Log-periodic Multiplexer Dipole Transducer',15,64,18,18,'Ulrik.Denzel@company.org','3,54 EUR')\n,('Q951-7651773','Coil Compensator',56,16,17,1,'Xochitl.Aue@company.org','2,05 EUR')\n,('P395-9316579','Aluminum Multiplexer Oscillator Memristor',25,27,11,12,'Adolfina.Hoch@company.org','3,50 EUR')\n,('W457-2405463','LCD Gauge Warp',18,57,15,10,'Siglind.Brinkerhoff@company.org','4,59 EUR')\n,('U367-8732482','Coil Sensor Potentiometer',14,55,11,8,'Baldwin.Dirksen@company.org','1,24 EUR')\n,('C301-4034359','Oscillator Breaker',13,13,13,19,'Franz.Kornhaeusel@company.org','4,55 EUR')\n,('J998-8511305','Resonator Transformer',12,48,15,2,'Baldwin.Dirksen@company.org','5,54 EUR')\n,('V892-8476786','Encoder Meter',27,41,20,9,'Sylvester.Brant@company.org','1,18 EUR')\n,('Q751-8742744','Inductor Resonator',29,46,11,17,'Franziska.Acker@company.org','3,74 EUR')\n,('C440-1370895','Potentiometer Capacitor',56,79,20,18,'Sabrina.Bayer@company.org','4,88 EUR')\n,('Q891-1871898','Coil LCD',38,22,13,5,'Arendt.Beitel@company.org','4,08 EUR')\n,('M645-5460777','Log-periodic Multiplexer Rheostat',27,18,11,19,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','2,94 EUR')\n,('J518-7067023','Phase Oscillator Strain',49,13,14,1,'Baldwin.Dirksen@company.org','3,85 EUR')\n,('U573-1229465','Potentiometer Transformer Oscillator',71,47,18,12,'Karen.Brant@company.org','0,86 EUR')\n,('B741-3218460','Dipole Oscillator Warp',25,23,13,3,'Minnie.Kuehn@company.org','1,05 EUR')\n,('L741-9253790','LCD Strain',78,56,20,2,'Dieterich.Blau@company.org','1,87 EUR')\n,('O203-5447809','Dipole Encoder Compensator',24,70,17,12,'Kevin.Feigenbaum@company.org','0,48 EUR')\n,('S271-9518696','Crystal Encoder',58,34,13,2,'Franz.Kornhaeusel@company.org','5,14 EUR')\n,('Y979-3662601','Memristor Strain Switch',52,50,12,12,'Ratt.Hartmann@company.org','1,68 EUR')\n,('H487-6374164','Inductor Memristor',29,72,18,12,'Lili.Geier@company.org','3,42 EUR')\n,('D764-9088510','Crystal Strain',43,13,12,9,'Corinna.Ludwig@company.org','2,22 EUR')\n,('G966-6452177','Coil Driver Resonator',71,63,11,17,'Gretel.Roth@company.org','0,11 EUR')\n,('D206-3028092','Resonator Compensator Switch',31,63,13,16,'Marius.Fux@company.org','5,18 EUR')\n,('T973-5442896','Sensor Driver Transducer',60,76,17,3,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','1,84 EUR')\n,('K636-3664460','Memristor Capacitor',14,64,19,13,'Erhard.Fried@company.org','1,44 EUR')\n,('Z367-7507010','Dipole Strain Encoder',69,44,13,8,'Lili.Geier@company.org','3,97 EUR')\n,('B232-9941304','Ceramic Rheostat Compensator',52,14,17,11,'Henny.Foth@company.org','1,90 EUR')\n,('H676-3055632','Film LCD Warp',51,75,17,7,'Corinna.Ludwig@company.org','1,67 EUR')\n,('Z212-3451369','Driver Strain',37,49,15,20,'Sylvester.Brant@company.org','5,92 EUR')\n,('N664-8476091','Rheostat Encoder',31,66,19,9,'Siglind.Brinkerhoff@company.org','0,57 EUR')\n,('B888-7564557','Log-periodic Resistor Inductor Potentiometer',52,72,17,4,'Dieterich.Blau@company.org','5,46 EUR')\n,('O212-8971793','Strain Meter',66,16,12,11,'Frauke.Faerber@company.org','3,93 EUR')\n,('H605-1270029','Transistor Rheostat',61,28,17,19,'Gretel.Roth@company.org','2,30 EUR')\n,('P965-2818538','Warp Compensator Transducer',78,55,11,2,'Waldtraud.Kuttner@company.org','2,55 EUR')\n,('N892-9855685','Crystal Strain Capacitor',77,45,14,11,'Karch.Moeller@company.org','2,98 EUR')\n,('A617-7075018','Crystal Switch',32,30,15,19,'Sabrina.Geiger@company.org','1,40 EUR')\n,('H439-1697643','Log-periodic LCD Gauge',17,21,16,1,'Herr.Haan.Bader@company.org','0,14 EUR')\n,('Z143-3180282','Multiplexer Sensor Encoder',19,37,13,16,'Anamchara.Foerstner@company.org','4,31 EUR')\n,('Y788-8477334','Inductor Encoder Capacitor',67,24,17,1,'Anamchara.Foerstner@company.org','1,44 EUR')\n,('T230-5902294','LCD Resistor',22,69,20,6,'Jarvis.Jans@company.org','3,05 EUR')\n,('H944-2667418','Coil Resistor',78,54,12,9,'Kevin.Feigenbaum@company.org','4,51 EUR')\n,('Z397-1183067','Bipolar-junction Memristor Transistor Breaker',46,58,16,11,'Heinrich.Hoch@company.org','0,69 EUR')\n,('Z556-4824310','Memristor Transistor Capacitor',71,50,18,15,'Ratt.Beyer@company.org','2,85 EUR')\n,('M350-8985312','Oscillator Memristor Transistor',20,80,17,15,'Lambert.Faust@company.org','2,04 EUR')\n,('U827-2294099','Potentiometer Inductor Warp',65,23,19,13,'Yanka.Schreiber@company.org','3,21 EUR')\n,('S321-8453459','Resonator Transformer',68,53,13,2,'Gretel.Roth@company.org','1,35 EUR')\n,('B387-9270009','Film Inductor Warp',46,66,18,16,'Arnelle.Gerber@company.org','4,67 EUR')\n,('E495-3503010','Dipole Resonator Transformer',38,64,17,12,'Kevin.Feigenbaum@company.org','5,96 EUR')\n,('S841-8644004','Flow Sensor Transistor Encoder',22,47,13,1,'Corinna.Ludwig@company.org','2,92 EUR')\n,('V485-9644250','Polymer Network Oscillator Breaker',27,45,11,13,'Adolfina.Hoch@company.org','5,26 EUR')\n,('K479-8347265','Phase Resistor Memristor Meter',67,37,11,12,'Jarvis.Jans@company.org','0,60 EUR')\n,('I904-2574215','Coil Encoder',15,46,16,7,'Thomas.Mueller@company.org','3,24 EUR')\n,('Y676-8284278','Encoder Warp',41,43,11,18,'Herr.Haan.Bader@company.org','2,59 EUR')\n,('X408-2200602','Inductor Capacitor Meter',18,26,12,20,'Lukas.Gerver@company.org','4,67 EUR')\n,('K473-9950981','Coil Compensator',48,33,20,20,'Henny.Foth@company.org','2,56 EUR')\n,('V571-2893837','Network Driver',27,30,19,13,'Elena.Herzog@company.org','2,65 EUR')\n,('W220-5347961','Ceramic Coil Oscillator Breaker',30,21,17,13,'Herr.Burgh.Eichel@company.org','0,19 EUR')\n,('D627-4615108','Coil Dipole Breaker',63,31,15,9,'Erhard.Fried@company.org','4,43 EUR')\n,('N694-7625752','Field-effect Resistor Sensor',27,61,16,8,'Emil.Gotti@company.org','2,65 EUR')\n,('X504-5674380','Ceramic Memristor Resonator Compensator',51,52,13,18,'Ratt.Hartmann@company.org','3,88 EUR')\n,('N589-9719896','Sensor Transformer',41,30,14,2,'Corinna.Ludwig@company.org','2,60 EUR')\n,('B688-8088841','Log-periodic Crystal Rheostat Encoder',67,23,20,9,'Herr.Haan.Bader@company.org','3,47 EUR')\n,('H642-6966395','Polymer Strain Rheostat',37,69,15,8,'Herr.Haan.Bader@company.org','3,62 EUR')\n,('I503-2217600','Aluminum Dipole Breaker',21,44,20,3,'Lambert.Faust@company.org','4,51 EUR')\n,('Y973-8480546','Resistor Transistor Switch',80,29,17,2,'Siglind.Brinkerhoff@company.org','2,19 EUR')\n,('N866-6363347','Polymer Memristor Resonator Capacitor',12,68,17,5,'Miles.Amsel@company.org','4,23 EUR')\n,('S871-7304748','Bipolar-junction Resonator Switch',57,70,11,13,'Marius.Fux@company.org','0,53 EUR')\n,('Y557-7149751','Coil Rheostat',35,14,14,3,'Bert.Blumstein@company.org','0,87 EUR')\n,('X215-2461803','Crystal Capacitor',65,38,20,6,'Elena.Herzog@company.org','5,10 EUR')\n,('G223-2092566','Dipole Transistor',55,23,11,20,'Berlin.Schulz@company.org','2,27 EUR')\n,('K832-2729798','Resistor Potentiometer Oscillator',61,65,13,16,'Xochitl.Aue@company.org','2,73 EUR')\n,('P393-8310950','Multiplexer Crystal Switch',66,14,15,14,'Karen.Brant@company.org','2,89 EUR')\n,('Q476-3668478','LCD Crystal Resonator',77,63,11,8,'Franz.Kornhaeusel@company.org','2,91 EUR')\n,('R228-5965688','Field-effect LCD Gauge Transformer',15,58,16,7,'Sylvester.Brant@company.org','1,77 EUR')\n,('V303-8717097','Network',51,67,14,12,'Kristen.Bauers@company.org','2,77 EUR')\n,('J725-8697253','Resistor Inductor Switch',68,70,11,15,'Xochitl.Aue@company.org','4,50 EUR')\n,('N324-9642439','Driver Warp',72,24,12,1,'Wolfgang.Martin@company.org','2,56 EUR')\n,('I334-4449270','Resistor Dipole Gauge',38,28,16,6,'Baldwin.Dirksen@company.org','5,34 EUR')\n,('U501-5365139','Sensor Transformer Compensator',48,53,18,12,'Liese.Adam@company.org','0,63 EUR')\n,('F800-9858235','Field-effect Gauge Transistor',15,20,19,20,'Franziska.Acker@company.org','1,86 EUR')\n,('Q661-7217088','Crystal Rheostat',80,33,18,12,'Sabrina.Geiger@company.org','3,76 EUR')\n,('H380-8298145','Flow Coil Switch',25,26,19,6,'Rebecca.Hall@company.org','4,20 EUR')\n,('C898-2055295','Sensor Potentiometer',42,74,18,13,'Yanka.Schreiber@company.org','3,70 EUR')\n,('S649-2935217','LCD Inductor Memristor',26,22,11,19,'Thomas.Mueller@company.org','5,22 EUR')\n,('T294-9722443','Heisenberg LCD Crystal Resonator',59,25,16,16,'Emil.Gotti@company.org','3,78 EUR')\n,('U281-1671869','Dipole Transformer Meter',27,62,11,15,'Kristen.Bauers@company.org','1,45 EUR')\n,('Q263-2186291','Sensor Crystal',35,58,18,6,'Thomas.Mueller@company.org','3,02 EUR')\n,('A828-3713433','Multiplexer Breaker',69,40,15,8,'Thomas.Mueller@company.org','4,14 EUR')\n,('R645-9208510','Memristor Breaker',75,71,16,15,'Heinrich.Hoch@company.org','0,45 EUR')\n,('B308-8130581','Memristor Breaker',37,29,17,15,'Franziska.Acker@company.org','4,02 EUR')\n,('Q210-8168184','Network Multiplexer',18,67,12,20,'Dietlinde.Boehme@company.org','4,49 EUR')\n,('A403-4549719','Flow Gauge Transformer',39,65,20,8,'Gretel.Roth@company.org','5,33 EUR')\n,('K242-3459162','Resistor Warp',54,18,15,14,'Kristen.Bauers@company.org','0,41 EUR')\n,('M672-3016632','Multiplexer Crystal Compensator',53,54,15,19,'Corinna.Ludwig@company.org','5,99 EUR')\n,('K143-3113342','Inductor Driver Breaker',30,48,15,18,'Sabrina.Bayer@company.org','4,01 EUR')\n,('O906-8511345','Ceramic Inductor Transistor',67,24,16,15,'Lukas.Gerver@company.org','3,08 EUR')\n,('A529-2906246','Sensor Crystal',38,22,13,9,'Elisabeth.Harman@company.org','2,47 EUR')\n,('S590-1665348','Heisenberg LCD Resonator Transformer',48,78,13,10,'Reiner.Widmann@company.org','5,77 EUR')\n,('T958-2055544','Bipolar-junction Network Transformer',52,26,19,11,'Berlin.Schulz@company.org','5,76 EUR')\n,('O952-1686669','LCD Resonator Breaker',28,77,11,10,'Berlin.Schulz@company.org','3,32 EUR')\n,('B926-8983325','Gauge Encoder Capacitor',73,76,11,5,'Wanja.Hoffmann@company.org','4,53 EUR')\n,('T274-5886301','Coil Gauge Transformer',58,46,17,3,'Adolfina.Hoch@company.org','2,07 EUR')\n,('E354-7057568','Resonator',27,17,11,19,'Elena.Herzog@company.org','1,58 EUR')\n,('W872-7508740','Gauge Rheostat',53,76,20,16,'Xochitl.Aue@company.org','1,45 EUR')\n,('U723-8910149','Memristor Gauge Transformer',61,18,18,20,'Wolfgang.Martin@company.org','1,16 EUR')\n,('W915-7858301','Capacitor Warp',53,50,11,14,'Sabrina.Bayer@company.org','3,42 EUR')\n,('H156-2424615','Polymer Driver Transistor Breaker',28,50,16,1,'Wolfgang.Martin@company.org','3,76 EUR')\n,('W615-3080767','Resistor Resonator Switch',17,44,13,19,'Henny.Foth@company.org','0,17 EUR')\n,('L822-2103281','Polymer Encoder Switch',15,63,12,5,'Dieterich.Blau@company.org','3,25 EUR')\n,('H754-2495350','Crystal Breaker',68,43,15,19,'Lambert.Faust@company.org','2,77 EUR')\n,('D599-6113892','Aluminum Multiplexer Coil Strain',80,17,18,14,'Adolfina.Hoch@company.org','5,85 EUR')\n,('Z927-4746244','Phase Resistor Transistor Meter',42,69,18,15,'Henny.Foth@company.org','3,50 EUR')\n,('V284-9786067','Field-effect Transformer Warp',34,21,13,15,'Wolfgang.Martin@company.org','3,05 EUR')\n,('D915-4717890','Oscillator Switch',23,14,13,1,'Liese.Adam@company.org','4,28 EUR')\n,('I251-4065887','Aluminum Dipole Crystal',35,13,11,17,'Berlin.Schulz@company.org','5,04 EUR')\n,('O494-6195301','Capacitor Strain Breaker',36,49,12,7,'Ida.Halle@company.org','3,82 EUR')\n,('O626-4153303','Driver Memristor',19,38,19,11,'Thomas.Mueller@company.org','2,54 EUR')\n,('N171-1815828','LCD Sensor',72,72,12,20,'Minnie.Kuehn@company.org','5,38 EUR')\n,('Q316-9683240','Multiplexer Transistor',65,26,16,5,'Berlin.Schulz@company.org','4,21 EUR')\n,('C836-5221890','Driver Rheostat Transformer',28,62,17,17,'Ratt.Beyer@company.org','5,63 EUR')\n,('R454-6248815','LCD Sensor Network',16,37,17,3,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','3,04 EUR')\n,('U217-4742599','Transistor Transducer',45,39,19,15,'Sabrina.Bayer@company.org','1,76 EUR')\n,('F574-4728030','Inductor Memristor Resonator',16,57,20,17,'Berlin.Schulz@company.org','5,83 EUR')\n,('A391-9508821','Oscillator Compensator',15,42,11,20,'Baldwin.Dirksen@company.org','1,77 EUR')\n,('Y372-1509836','Oscillator Breaker',60,43,13,18,'Franz.Kornhaeusel@company.org','0,72 EUR')\n,('Y327-7144928','Planck Resonator LCD Strain',14,21,18,10,'Baldwin.Dirksen@company.org','4,44 EUR')\n,('G950-4257402','Transformer Transducer',32,68,15,1,'Minnie.Kuehn@company.org','1,71 EUR')\n,('L855-9772914','Phase Breaker Encoder',39,26,14,9,'Anamchara.Foerstner@company.org','5,19 EUR')\n,('T119-8723477','Driver Crystal',53,67,13,17,'Anamchara.Foerstner@company.org','2,61 EUR')\n,('F126-7002599','Log-periodic Oscillator Transistor',75,48,18,7,'Jarvis.Jans@company.org','1,47 EUR')\n,('C717-1997689','Coil Oscillator',17,15,17,6,'Dietlinde.Boehme@company.org','1,42 EUR')\n,('O875-5580798','Heisenberg LCD Network Memristor',54,24,11,19,'Heinrich.Hoch@company.org','2,94 EUR')\n,('C625-4647902','Heisenberg LCD Dipole Crystal',41,61,11,18,'Nadia.Schubert@company.org','3,36 EUR')\n,('K926-9334427','LCD Encoder Compensator',39,29,19,9,'Karen.Brant@company.org','3,13 EUR')\n,('M627-4661911','Phase Memristor Driver',58,59,19,7,'Franziska.Acker@company.org','1,44 EUR')\n,('N462-6714196','Gauge Encoder',55,43,12,20,'Erhard.Fried@company.org','3,41 EUR')\n,('Q245-9575444','Inductor Driver Encoder',67,59,13,19,'Wolfgang.Martin@company.org','5,73 EUR')\n,('J437-7431991','Potentiometer Crystal',43,27,18,10,'Emil.Gotti@company.org','2,76 EUR')\n,('B143-5457756','Flow Coil Resistor Capacitor',55,62,17,6,'Yanka.Schreiber@company.org','5,96 EUR')\n,('G226-1299624','Multiplexer Crystal Transformer',44,19,13,2,'Arendt.Beitel@company.org','3,64 EUR')\n,('Z604-4291151','Log-periodic Oscillator Capacitor',80,13,15,18,'Sylvester.Brant@company.org','2,22 EUR')\n,('T872-6914723','Resistor Oscillator',23,68,19,14,'Xochitl.Aue@company.org','2,54 EUR')\n,('L805-3283253','Coil Compensator Warp',20,40,11,19,'Elena.Herzog@company.org','2,80 EUR')\n,('E172-7848498','LCD Transducer',50,56,15,10,'Dieterich.Blau@company.org','5,34 EUR')\n,('O571-8892490','Network Rheostat Transformer',26,76,18,2,'Sabrina.Geiger@company.org','5,29 EUR')\n,('R490-4226805','Coil Resonator LCD',49,32,13,9,'Bert.Blumstein@company.org','4,78 EUR')\n,('K968-2682119','Field-effect Inductor Oscillator Compensator',52,72,16,2,'Lili.Geier@company.org','4,37 EUR')\n,('Z518-1747933','Resonator Encoder Warp',24,58,19,16,'Manfred.Foth@company.org','0,98 EUR')\n,('T381-5661009','Sensor Compensator',14,23,14,3,'Manfred.Foth@company.org','2,59 EUR')\n,('Z872-5435339','Polymer Gauge Capacitor',18,37,19,1,'Ratt.Beyer@company.org','5,70 EUR')\n,('J878-7667870','LCD Crystal',65,12,20,2,'Anamchara.Foerstner@company.org','5,51 EUR')\n,('M721-8978045','LCD Memristor Warp',55,76,11,18,'Corinna.Ludwig@company.org','1,30 EUR')\n,('A560-7347187','Oscillator Driver',52,39,16,9,'Kevin.Feigenbaum@company.org','0,52 EUR')\n,('M770-3602005','Sensor Capacitor Breaker',20,63,19,1,'Thomas.Mueller@company.org','4,60 EUR')\n,('Q890-8785073','Film Dipole Rheostat',56,66,13,12,'Berlin.Schulz@company.org','2,50 EUR')\n,('G451-8179125','Phase Transistor Warp',50,47,12,6,'Heinrich.Hoch@company.org','2,89 EUR')\n,('J385-6425761','Dipole Driver Meter',78,17,16,11,'Bert.Blumstein@company.org','4,17 EUR')\n,('G378-7132339','Resistor Dipole',19,30,19,5,'Rebecca.Hall@company.org','0,06 EUR')\n,('E815-1132509','Resistor Sensor Resonator',36,77,17,19,'Lukas.Gerver@company.org','1,18 EUR')\n,('F414-1779577','Multiplexer Potentiometer Breaker',75,67,16,16,'Elisabeth.Harman@company.org','5,33 EUR')\n,('D194-9594371','Sensor Gauge Transistor',43,23,19,2,'Nadia.Schubert@company.org','1,11 EUR')\n,('X223-5282026','LCD Capacitor',57,79,14,18,'Franziska.Acker@company.org','1,26 EUR')\n,('X842-8356738','Aluminum Resistor Transformer',17,24,20,7,'Bert.Blumstein@company.org','2,91 EUR')\n,('D729-5737042','Coil Resistor Warp',36,13,16,3,'Adolfina.Hoch@company.org','5,52 EUR')\n,('T802-5361374','Gauge Breaker Switch',17,30,18,8,'Lukas.Gerver@company.org','2,10 EUR')\n,('F388-7030185','Oscillator Transistor Transducer',42,42,12,13,'Elisabeth.Harman@company.org','0,10 EUR')\n,('C794-6433363','Sensor Strain',54,57,20,18,'Reiner.Widmann@company.org','2,84 EUR')\n,('I199-7642085','Coil Strain',48,21,16,9,'Waldtraud.Kuttner@company.org','1,38 EUR')\n,('M323-1526287','Encoder Meter',67,45,17,7,'Ida.Halle@company.org','5,04 EUR')\n,('A509-5571891','Multiplexer Inductor',64,34,11,12,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','0,49 EUR')\n,('K267-2045349','Inductor Crystal Oscillator',27,52,13,14,'Erhard.Fried@company.org','4,05 EUR')\n,('I653-5994473','Bipolar-junction Potentiometer Capacitor',60,76,16,4,'Elisabeth.Harman@company.org','3,72 EUR')\n,('E958-8187096','Multiplexer Resistor Driver',45,41,19,13,'Arendt.Beitel@company.org','2,83 EUR')\n,('X324-6249454','Breaker Meter',71,74,17,16,'Karen.Brant@company.org','5,76 EUR')\n,('N687-2334901','Potentiometer Transducer',50,56,11,12,'Sigmund.Gros@company.org','0,98 EUR')\n,('K845-4116844','Breaker Encoder Transducer',36,54,15,4,'Thomas.Mueller@company.org','3,60 EUR')\n,('H752-8461936','Network LCD',35,41,16,4,'Wanja.Hoffmann@company.org','3,62 EUR')\n,('V421-9533599','Network Switch',18,44,16,13,'Emil.Gotti@company.org','2,13 EUR')\n,('P385-1200138','Potentiometer Resonator',34,45,15,18,'Elisabeth.Harman@company.org','0,45 EUR')\n,('R591-4930195','Multiplexer Resonator Transformer',48,49,11,8,'Minnie.Kuehn@company.org','3,81 EUR')\n,('J575-3390923','Phase Coil Capacitor Transducer',79,57,16,11,'Wanja.Hoffmann@company.org','1,51 EUR')\n,('W579-1877166','Polymer Oscillator Potentiometer Meter',40,57,12,14,'Herr.Burgh.Eichel@company.org','2,63 EUR')\n,('M313-3585673','Heisenberg Memristor Rheostat Transducer',77,19,14,13,'Xochitl.Aue@company.org','0,10 EUR')\n,('N573-1498086','Dipole Transformer',63,19,12,17,'Ratt.Hartmann@company.org','5,53 EUR')\n,('G444-8036184','Network Memristor Breaker',47,45,15,15,'Manfred.Foth@company.org','2,73 EUR')\n,('K662-1238230','Memristor Transistor Compensator',67,23,11,3,'Wolfgang.Martin@company.org','4,58 EUR')\n,('U958-4696127','Memristor Compensator Meter',28,52,12,6,'Liese.Adam@company.org','2,39 EUR')\n,('K559-3177627','Rheostat Transformer Warp',61,11,13,20,'Frauke.Faerber@company.org','4,83 EUR')\n,('S439-3814007','LCD Coil Strain',75,49,18,16,'Xochitl.Aue@company.org','4,86 EUR')\n,('N451-6994769','Multiplexer Memristor Crystal',25,14,12,14,'Corinna.Ludwig@company.org','5,58 EUR')\n,('A181-1118563','Compensator Switch',32,22,14,5,'Adolfina.Hoch@company.org','3,70 EUR')\n,('J571-5923698','Multiplexer Resistor',26,31,19,16,'Emil.Gotti@company.org','4,27 EUR')\n,('G694-2879694','Planck Transistor Switch Transducer',76,67,20,19,'Thomas.Mueller@company.org','0,13 EUR')\n,('N243-4639047','Aluminum Resistor Meter',17,22,14,16,'Herr.Haan.Bader@company.org','2,76 EUR')\n,('N778-8753186','Planck Multiplexer Transformer Warp',75,20,16,6,'Ratt.Beyer@company.org','3,88 EUR')\n,('K739-4867689','Multiplexer Resonator Compensator',77,25,19,16,'Elisabeth.Harman@company.org','1,64 EUR')\n,('E829-2591611','Memristor Compensator',29,76,15,9,'Sigmund.Gros@company.org','2,35 EUR')\n,('Q980-7885274','Heisenberg Strain Capacitor',74,35,14,4,'Herr.Haan.Bader@company.org','2,67 EUR')\n,('P983-2994865','Log-periodic Sensor Switch Transducer',34,11,15,14,'Wanja.Hoffmann@company.org','1,19 EUR')\n,('D518-3930277','Sensor Resonator',71,11,17,11,'Xochitl.Aue@company.org','3,18 EUR')\n,('G272-3422671','Resistor Sensor',18,52,15,2,'Arnelle.Gerber@company.org','0,61 EUR')\n,('Y728-5119478','Sensor Crystal Transducer',36,21,17,11,'Liese.Adam@company.org','4,05 EUR')\n,('C710-5880579','Inductor Rheostat Capacitor',18,24,14,6,'Minnie.Kuehn@company.org','2,73 EUR')\n,('A145-1240844','Bipolar-junction Coil Compensator Transducer',28,17,20,14,'Ratt.Hartmann@company.org','3,72 EUR')\n,('F818-8141054','LCD Gauge',51,61,20,3,'Sabrina.Geiger@company.org','1,25 EUR')\n,('L747-7633290','Resistor Sensor Encoder',76,18,14,16,'Franz.Kornhaeusel@company.org','4,81 EUR')\n,('Y167-8599364','Oscillator Meter',37,39,18,7,'Erhard.Fried@company.org','2,19 EUR')\n,('Z545-6121719','LCD Sensor Breaker',23,58,14,10,'Arendt.Beitel@company.org','4,29 EUR')\n,('W658-9979899','Multiplexer Switch',46,38,11,10,'Kevin.Feigenbaum@company.org','4,90 EUR')\n,('P528-2149873','Encoder Compensator',40,79,14,4,'Franziska.Acker@company.org','0,72 EUR')\n,('N480-3487616','Resonator Transformer',17,19,12,14,'Lukas.Gerver@company.org','0,79 EUR')\n,('E737-8373948','Oscillator Inductor',45,79,12,9,'Franz.Kornhaeusel@company.org','2,32 EUR')\n,('H915-3627727','Flow Coil Gauge',62,34,18,13,'Frauke.Faerber@company.org','2,32 EUR')\n,('K411-1729714','Driver Gauge',45,69,20,16,'Minnie.Kuehn@company.org','4,02 EUR')\n,('I311-9589498','Coil LCD Capacitor',44,31,17,6,'Ratt.Beyer@company.org','5,80 EUR')\n,('C800-3270129','Network Potentiometer Encoder',46,48,19,19,'Gretel.Roth@company.org','1,10 EUR')\n,('K481-3281345','Network Dipole Driver',63,35,19,11,'Marius.Fux@company.org','3,31 EUR')\n,('L586-5133830','Multiplexer Dipole',52,34,14,7,'Henny.Foth@company.org','3,08 EUR')\n,('T805-4210259','Heisenberg Network Oscillator Strain',48,36,19,17,'Emil.Gotti@company.org','1,03 EUR')\n,('Z980-8040792','Inductor Transistor Oscillator',41,42,17,16,'Jarvis.Jans@company.org','5,54 EUR')\n,('H402-6061531','Switch',57,69,15,11,'Sabrina.Geiger@company.org','0,44 EUR')\n,('X375-4984404','Multiplexer Transistor',37,45,20,6,'Rebecca.Hall@company.org','4,56 EUR')\n,('Z358-9013730','Flow Potentiometer Transistor',66,76,15,12,'Lukas.Gerver@company.org','3,00 EUR')\n,('A739-4780210','Memristor Driver Strain',57,26,14,2,'Jarvis.Jans@company.org','2,54 EUR')\n,('P870-6495639','Inductor Transistor Warp',61,73,11,18,'Dieterich.Blau@company.org','5,71 EUR')\n,('W917-2544526','Resonator Encoder Compensator',77,68,19,3,'Heinrich.Hoch@company.org','0,89 EUR')\n,('J826-7793079','Coil Potentiometer Memristor',64,31,20,9,'Wolfgang.Martin@company.org','4,86 EUR')\n,('J555-1586043','Transformer Inductor Encoder',59,21,17,6,'Dietlinde.Boehme@company.org','4,40 EUR')\n,('H355-1126195','Polymer Memristor Meter',24,45,18,1,'Bert.Blumstein@company.org','5,11 EUR')\n,('G556-4971578','Flow Coil Memristor Breaker',34,15,20,3,'Sigmund.Gros@company.org','5,84 EUR')\n,('T147-7011803','Oscillator Transistor Capacitor',20,74,16,13,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','5,37 EUR')\n,('O727-5999075','Field-effect Transistor Rheostat Breaker',49,11,19,6,'Wanja.Hoffmann@company.org','1,59 EUR')\n,('Z994-6661823','Potentiometer Transducer Meter',30,57,15,7,'Waldtraud.Kuttner@company.org','0,38 EUR')\n,('E952-1325145','Driver Gauge',70,18,17,19,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','1,07 EUR')\n,('G249-4926490','Film Network Crystal',62,61,20,3,'Franziska.Acker@company.org','2,44 EUR')\n,('Q627-3868402','Network Memristor',16,52,13,19,'Berlin.Schulz@company.org','0,89 EUR')\n,('A628-8869626','Inductor Gauge Oscillator',76,47,11,19,'Nadia.Schubert@company.org','1,28 EUR')\n,('J225-9158499','Resistor Rheostat Capacitor',42,19,20,15,'Arendt.Beitel@company.org','2,00 EUR')\n,('R272-9406400','Potentiometer Driver',38,76,18,1,'Herr.Haan.Bader@company.org','0,38 EUR')\n,('X704-7151241','Film Dipole Crystal',36,24,12,2,'Baldwin.Guenther@company.org','2,16 EUR')\n,('M986-2342719','Resistor Memristor',15,33,18,14,'Manfred.Foth@company.org','5,60 EUR')\n,('F773-4598178','Transistor Breaker Switch',54,32,14,3,'Reiner.Widmann@company.org','1,79 EUR')\n,('F797-8658626','Multiplexer Gauge Crystal',60,80,11,19,'Bert.Blumstein@company.org','5,56 EUR')\n,('O491-3823912','LCD Sensor',20,26,12,1,'Elisabeth.Harman@company.org','5,82 EUR')\n,('D544-9061559','Rheostat Capacitor Meter',57,48,20,4,'Baldwin.Guenther@company.org','3,90 EUR')\n,('O748-4307356','LCD Inductor Oscillator',59,33,18,5,'Thomas.Mueller@company.org','0,37 EUR')\n,('O537-7333259','Oscillator Transformer',21,79,17,20,'Reiner.Widmann@company.org','4,00 EUR')\n,('G535-8172375','Film Breaker Transducer',74,51,16,11,'Arendt.Beitel@company.org','3,84 EUR')\n,('C845-4085909','Flow Network Strain',38,39,16,12,'Erhard.Fried@company.org','1,54 EUR')\n,('P163-9337479','Transistor Encoder Compensator',55,78,19,1,'Corinna.Ludwig@company.org','1,08 EUR')\n,('M225-8144152','Phase Resistor Encoder Warp',29,26,18,2,'Berlin.Schulz@company.org','5,89 EUR')\n,('D146-5615241','LCD Gauge Transformer',67,73,12,5,'Sabrina.Bayer@company.org','5,12 EUR')\n,('A315-1730287','Network Dipole',28,74,12,11,'Miles.Amsel@company.org','3,03 EUR')\n,('Z319-4514647','Phase Resistor Breaker Oscillator',14,75,11,1,'Corinna.Ludwig@company.org','1,25 EUR')\n,('N451-2350273','Network Strain Meter',42,25,18,17,'Heinrich.Hoch@company.org','4,50 EUR')\n,('B519-3674576','LCD Crystal Encoder',20,75,12,10,'Xochitl.Aue@company.org','5,15 EUR')\n,('R771-3893828','Multiplexer LCD Transducer',71,45,19,6,'Valda.Everhart@company.org','4,46 EUR')\n,('C247-3833661','Encoder Transformer',58,25,12,12,'Ratt.Beyer@company.org','3,44 EUR')\n,('T586-1678071','Resistor Transistor Capacitor',72,70,18,16,'Gretel.Roth@company.org','2,40 EUR')\n,('R658-8902629','Strain Meter',62,23,20,20,'Arnelle.Gerber@company.org','0,89 EUR')\n,('A595-2446575','Sensor Driver Capacitor',27,77,11,7,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','3,67 EUR')\n,('S702-5472237','Field-effect Driver Switch',16,64,19,3,'Gretel.Roth@company.org','4,98 EUR')\n,('P162-1457428','Polymer Coil Capacitor Warp',62,71,13,5,'Karen.Brant@company.org','4,24 EUR')\n,('N733-1946687','LCD Encoder',20,60,14,5,'Ida.Halle@company.org','1,80 EUR')\n,('N819-5417282','Inductor Dipole',19,52,12,15,'Heinrich.Hoch@company.org','0,41 EUR')\n,('D762-3837218','Inductor Capacitor Switch',18,49,11,4,'Sylvester.Brant@company.org','2,43 EUR')\n,('H538-7285000','Breaker Meter',74,51,12,2,'Gretel.Roth@company.org','0,87 EUR')\n,('G179-6566342','Resistor Rheostat Meter',67,36,18,3,'Baldwin.Guenther@company.org','3,11 EUR')\n,('D519-3521758','Sensor Driver Resonator',67,13,16,3,'Elena.Herzog@company.org','2,91 EUR')\n,('L557-1467804','Phase Resistor Transistor Strain',58,22,15,2,'Lukas.Gerver@company.org','0,29 EUR')\n,('R481-9898984','LCD Sensor',73,13,15,15,'Lambert.Faust@company.org','5,64 EUR')\n,('W981-1196694','Multiplexer Memristor Breaker',75,51,12,13,'Heinrich.Hoch@company.org','5,99 EUR')\n,('T812-6060686','Coil Gauge Strain',14,47,16,16,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','5,21 EUR')\n,('R944-4832283','Phase Memristor Strain',42,17,16,3,'Lili.Geier@company.org','1,43 EUR')\n,('M662-6209836','Resistor Potentiometer Meter',61,41,14,2,'Xochitl.Aue@company.org','3,29 EUR')\n,('W661-3032609','LCD Inductor Oscillator',54,65,17,3,'Adolfina.Hoch@company.org','0,11 EUR')\n,('D973-4134519','Aluminum Network Transducer',67,29,17,8,'Lili.Geier@company.org','5,24 EUR')\n,('E395-9906117','Flow Inductor Crystal Encoder',24,62,18,16,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','0,80 EUR')\n,('K671-7410535','Driver Rheostat',65,67,17,15,'Erhard.Fried@company.org','0,64 EUR')\n,('F146-1429118','Flow Gauge Rheostat Encoder',72,41,20,1,'Miles.Amsel@company.org','5,15 EUR')\n,('P729-6290809','Heisenberg Driver Resonator Transducer',39,50,13,5,'Ratt.Beyer@company.org','4,38 EUR')\n,('O311-4466005','Coil Inductor',64,64,14,18,'Miles.Amsel@company.org','3,10 EUR')\n,('T782-3734983','Dipole Transformer',35,34,20,14,'Erhard.Fried@company.org','3,04 EUR')\n,('M914-2979544','Log-periodic Oscillator Sensor',39,62,16,18,'Arnelle.Gerber@company.org','4,34 EUR')\n,('H274-5987347','Driver Resonator',73,49,20,4,'Arendt.Beitel@company.org','2,59 EUR')\n,('Z768-8346288','Coil Sensor Network',64,21,17,6,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','5,91 EUR')\n,('H288-7210201','Log-periodic Memristor Meter',39,54,14,18,'Corinna.Ludwig@company.org','3,53 EUR')\n,('H388-1193255','Resonator Rheostat',64,14,19,1,'Wolfgang.Martin@company.org','3,86 EUR')\n,('D400-2425852','Sensor Crystal',25,45,17,15,'Sabrina.Bayer@company.org','4,58 EUR')\n,('R860-6284767','Oscillator Compensator Warp',75,27,20,7,'Lili.Geier@company.org','1,02 EUR')\n,('J720-4179367','Phase Sensor Crystal',45,37,15,3,'Minnie.Kuehn@company.org','2,93 EUR')\n,('I319-7938682','Phase Network Transistor',13,33,14,7,'Lukas.Gerver@company.org','0,19 EUR')\n,('C858-3557118','Sensor Resonator',26,17,18,9,'Gretel.Roth@company.org','5,23 EUR')\n,('G333-6105148','Planck Oscillator Encoder Transducer',26,15,19,3,'Sigmund.Gros@company.org','2,33 EUR')\n,('W501-5990901','Film Memristor Resonator Compensator',18,20,19,13,'Xochitl.Aue@company.org','0,30 EUR')\n,('W986-7950553','Field-effect Memristor Resonator Compensator',13,45,20,8,'Siglind.Brinkerhoff@company.org','5,27 EUR')\n,('Z439-5790785','Log-periodic Network Meter Switch',55,32,19,13,'Sabrina.Bayer@company.org','2,90 EUR')\n,('I482-3778442','Dipole Rheostat Compensator',69,41,18,3,'Karen.Brant@company.org','3,76 EUR')\n,('A688-6056899','Multiplexer Inductor',63,33,11,14,'Jarvis.Jans@company.org','0,98 EUR')\n,('B633-4277974','Multiplexer Strain',71,56,11,15,'Bert.Blumstein@company.org','1,99 EUR')\n,('C574-6212593','Planck Network Memristor',13,17,11,19,'Bert.Blumstein@company.org','1,74 EUR')\n,('M558-2275045','Sensor Switch',40,12,16,5,'Anamchara.Foerstner@company.org','3,41 EUR')\n,('H609-2196524','Planck Potentiometer Transformer',56,77,15,11,'Frau.Irmalinda\u201a\u00c4\u00f2.Becker@company.org','4,50 EUR')\n,('V654-5789502','Polymer Resistor Memristor Strain',33,67,15,2,'Kristen.Bauers@company.org','5,81 EUR')\n,('N463-8050264','Multiplexer Network',75,15,11,1,'Herr.Haan.Bader@company.org','1,56 EUR')\n,('W344-5163065','Heisenberg Capacitor Transducer',16,50,18,8,'Yanka.Schreiber@company.org','2,97 EUR')\n,('H173-1200706','Memristor Rheostat Transformer',14,27,18,2,'Elisabeth.Harman@company.org','2,77 EUR')\n,('A464-7310986','Network Driver',16,40,16,10,'Berlin.Schulz@company.org','2,99 EUR')\n,('L128-5499058','Field-effect Multiplexer Potentiometer',56,46,14,19,'Adolfina.Hoch@company.org','0,51 EUR')\n,('Q774-7287508','Flow Potentiometer Crystal Rheostat',44,45,15,17,'Frauke.Faerber@company.org','3,94 EUR')\n,('R726-8194447','Oscillator Crystal Switch',44,26,14,17,'Marius.Fux@company.org','5,03 EUR')\n,('D334-6681399','Resistor Inductor Gauge',55,71,16,11,'Lambert.Faust@company.org','3,98 EUR')\n,('U836-7709298','Driver Switch Transducer',38,33,19,15,'Berlin.Schulz@company.org','0,47 EUR')\n,('E416-7318916','Dipole Crystal',13,48,13,11,'Wolfgang.Martin@company.org','4,47 EUR')\n,('Y920-5864778','LCD Sensor Potentiometer',44,32,14,9,'Emil.Gotti@company.org','3,73 EUR')\n,('P901-7842562','Dipole Transistor Strain',66,66,20,12,'Elena.Herzog@company.org','1,30 EUR')\n,('Q852-7359409','Ceramic Network Rheostat',49,14,20,3,'Baldwin.Dirksen@company.org','1,19 EUR')\n,('I532-1549244','Resonator Encoder',34,33,12,7,'Yanka.Schreiber@company.org','1,87 EUR')\n,('Y968-9133870','Oscillator Rheostat',26,76,13,19,'Herr.Burgh.Eichel@company.org','3,14 EUR')\n,('O857-5463957','Transistor Compensator',37,35,18,2,'Franziska.Acker@company.org','3,62 EUR')\n,('C697-6765940','Network Driver',13,28,16,19,'Arnelle.Gerber@company.org','5,22 EUR')\n,('H236-9180061','Oscillator Dipole Capacitor',30,61,13,6,'Manfred.Foth@company.org','3,61 EUR')\n,('Z249-1364492','Meter Transducer',25,45,14,12,'Corinna.Ludwig@company.org','4,12 EUR')\n,('Y914-2603866','Aluminum Strain Transformer',68,67,19,16,'Karen.Brant@company.org','3,30 EUR')\n,('S218-3305033','Phase Network Sensor Compensator',20,65,13,9,'Heinrich.Hoch@company.org','4,16 EUR')\n,('Q442-2123335','Aluminum Potentiometer Capacitor',67,25,19,19,'Liese.Adam@company.org','1,65 EUR')\n,('U286-4664935','Multiplexer Memristor',14,59,14,17,'Heinrich.Hoch@company.org','0,38 EUR')\n,('J164-5917711','Film Multiplexer Crystal Rheostat',78,29,11,18,'Manfred.Foth@company.org','4,33 EUR')\n,('V999-1676345','Potentiometer Switch Warp',47,36,15,19,'Reiner.Widmann@company.org','0,85 EUR')\n,('F198-1414890','Crystal Transformer',78,46,12,11,'Gretel.Roth@company.org','2,89 EUR')\n,('V404-9975399','Resonator Transformer',78,70,11,16,'Manfred.Foth@company.org','3,20 EUR')\n,('O184-6903943','Oscillator Crystal',44,38,17,19,'Karen.Brant@company.org','1,15 EUR')\n,('L316-4863597','Potentiometer Inductor',36,54,15,4,'Dieterich.Blau@company.org','1,23 EUR')\n,('D331-6211280','LCD Coil Transistor',75,20,12,5,'Ulrik.Denzel@company.org','2,19 EUR')\n,('B523-5464510','Bipolar-junction Switch Transducer',29,80,14,5,'Lukas.Gerver@company.org','0,54 EUR')\n,('O125-6715778','Ceramic Network Driver',31,30,16,10,'Siglind.Brinkerhoff@company.org','2,51 EUR')\n,('G251-8414984','Coil Rheostat',13,53,12,7,'Yanka.Schreiber@company.org','3,66 EUR')\n,('Q986-9996088','Planck Transistor Transformer',24,61,14,7,'Gretel.Roth@company.org','1,12 EUR')\n,('Z272-2955088','Coil Sensor Resonator',43,29,15,10,'Wolfgang.Martin@company.org','0,82 EUR')\n,('V450-8692412','Film Network Gauge Meter',28,22,18,17,'Minnie.Kuehn@company.org','2,61 EUR')\n,('H252-2977732','Field-effect Oscillator Switch Transducer',13,51,12,15,'Kevin.Feigenbaum@company.org','0,38 EUR')\n,('I980-1040313','Film Resonator Capacitor Strain',28,16,20,3,'Sigmund.Gros@company.org','3,38 EUR')\n,('T291-4144066','LCD Gauge Warp',49,60,20,20,'Emil.Gotti@company.org','4,32 EUR')\n,('F344-7012314','Switch Encoder Transducer',59,37,20,7,'Lukas.Gerver@company.org','3,25 EUR')\n,('G144-5498082','Multiplexer Transistor Transformer',69,17,19,15,'Wanja.Hoffmann@company.org','0,86 EUR')\n,('J824-5227925','Strain Switch',73,34,11,20,'Kevin.Feigenbaum@company.org','0,94 EUR')\n,('M605-5951566','Rheostat Transformer',54,65,16,15,'Karch.Moeller@company.org','3,94 EUR')\n,('D228-6993564','Log-periodic LCD Gauge Warp',50,15,11,2,'Nadia.Schubert@company.org','4,36 EUR')\n,('I965-1821441','Multiplexer Resistor',48,26,15,1,'Kevin.Feigenbaum@company.org','2,73 EUR')\n,('L536-5185541','Coil Gauge',39,25,14,6,'Ratt.Beyer@company.org','4,22 EUR')\n,('C831-2580759','Warp',32,57,19,12,'Baldwin.Dirksen@company.org','4,28 EUR')\n,('D844-3535311','Resistor Gauge Encoder',12,36,17,11,'Bert.Blumstein@company.org','0,90 EUR')\n,('K898-8238720','Multiplexer Transformer',33,76,18,10,'Thomas.Mueller@company.org','5,10 EUR')\n,('Y299-9772513','Log-periodic Oscillator Transformer',62,40,17,3,'Kevin.Feigenbaum@company.org','0,38 EUR')\n,('X716-6172862','Coil Resonator',74,36,16,8,'Baldwin.Guenther@company.org','3,20 EUR');\n</code></pre> <p></p> <p>Step Result</p> <p>The database has been populated as shown below.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#3-create-a-project-in-eccenca-corporate-memory","title":"3. Create a project in eccenca Corporate Memory","text":"<ul> <li>Click on Create on the right side of the page.</li> </ul> <ul> <li>Click on Project, then click on Add.</li> </ul> <ul> <li>Type the project name product in the title field, then click on Create.</li> </ul> <ul> <li>Click on Create on the right side of the page.</li> </ul> <ul> <li>Click on JDBC endpoint, then click on Add.</li> </ul> <ul> <li>Type the name product table (JDBC) in the label field.</li> </ul> <ul> <li>Type the JDBC URL path in the JDBC Driver connection URL field.</li> </ul> <p>Note</p> <p>This is a JDBC connection string for connecting to Snowflake data warehouse in eccenca corporate memory.</p> <p>Example</p> <p><code>jdbc:snowflake://kiaouyb-fe21477.snowflakecomputing.com/?db=product&amp;schema=products_vocabulary</code></p> <p>Here is a breakdown of the elements of this example connection string.</p> <ul> <li> <p><code>jdbc:snowflake://</code> is the prefix for the snowflake JDBC driver.</p> </li> <li> <p><code>kiaouyb-fe21477.snowflakecomputing.com</code> is the URL for the snowflake account you want to connect to.     The number <code>WTXSZXM-FS77078</code> is the organization number you will get from Snowflake as shown below.</p> </li> </ul> <p></p> <ul> <li> <p><code>?db=product</code> specifies the name of the Snowflake database you want to connect to.In this case, the database is named product.</p> </li> <li> <p><code>&amp;schema=products_vocabulary</code> specifies the name of the Snowflake schema that you want to use within the specified database.     In this case, the schema name is products_vocabulary.</p> </li> </ul> <p></p> <ul> <li> <p>Type Source query as</p> <pre><code>SELECT * from product\n</code></pre> </li> </ul> <p></p> <ul> <li>Select the Query strategy as Execute the given source query.No paging or virtual query.</li> </ul> <p></p> <ul> <li>Select the Write strategy as An exception will be thrown, if the table already exists.</li> </ul> <p></p> <ul> <li>Click on the ADVANCED OPTIONS.</li> </ul> <p></p> <ul> <li>Type Username and Password in the dialog window, then click on Create.</li> </ul> <p></p> <p>Step Result</p> <p>JDBC endpoint is created and data is transferred from Snowflake to eccenca Corporate Memory.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#4-create-a-transformation-to-build-mapping-rules","title":"4. Create a transformation to build mapping rules","text":"<ul> <li>Click on Create on the right side of the page.</li> </ul> <ul> <li>Click on Transform on the left side of the page, then on Transform in the centre of the page, then click on Add.</li> </ul> <ul> <li>Type the name product in the Label field, in the INPUT TASK Dataset select Product Table (JDBC) and in the Type field select table.</li> </ul> <ul> <li>In the Output dataset field select product graph, then click on Create .</li> </ul> <ul> <li>Click on Mapping, then click on Edit.</li> </ul> <ul> <li>For  the target entity select  Product (pv:product).</li> </ul> <ul> <li>Click on create custom pattern.</li> </ul> <ul> <li>Type the URI pattern as <code>&lt;http://id.company.org/product/jdbc&gt;</code>.     You can use either company.org or company.com as per your requirement.     Then type the label name as product and then click on save.</li> </ul> <ul> <li>Click on +Icon, then select the Add value mapping.</li> </ul> <ul> <li>Select the target property according to transformation requirements, for example name, id, etc., then select the value path according to the target property as the product name, product id etc.     This step will help in mapping the data from the source to the target property.</li> </ul> <ul> <li>Type the label name product name, then click on Save.</li> </ul> <p>Step Result</p> <p>Mapping rule is created successfully.</p> <p></p> <p>Note</p> <p>We have the suggestion option as well; click on the +Icon and select the Suggestion mapping.</p> <p></p> <p>Step Result</p> <p>Suggestion appears as below can select as per the requirement.</p> <p></p> <p>Note</p> <p>Suggestions generated are based on vocabulary which describes the data in the CSV files: products_vocabulary.nt</p> <ul> <li>Tick the box to select the suggestions to be added, then click on Add.</li> </ul> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/snowflake-tutorial/#5-create-a-knowledge-graph","title":"5. Create a knowledge graph","text":"<ul> <li>Click on Create on the right side of the page.</li> </ul> <ul> <li>Select Knowledge Graph, then click on Add.</li> </ul> <ul> <li>Select the target project from the drop down menu as product.</li> </ul> <ul> <li>Type  product graph in the label field, then enter the graph URI in the Graph field, then click on Create.</li> </ul> <p>Step Result</p> <p>Graph is created successfully.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/tutorial-how-to-link-ids-to-osint/","title":"How to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)","text":"","tags":["ExpertTutorial"]},{"location":"build/tutorial-how-to-link-ids-to-osint/#plan-of-tutorial","title":"Plan of tutorial","text":"<ol> <li>Introduction, level, material</li> <li>Define the need, the expected result and the use case</li> <li>Specify the dashboards before the RDF models</li> <li>Build Knowledge Graphs from:<ol> <li>STIX 2.1 data such as the MITRE ATT&amp;CK\u00ae datasets</li> <li>Indicators of compromise rules, like Hayabusa and Sigma rules</li> </ol> </li> <li>Link IDS event to a knowledge graph in dashboards via:<ol> <li>Queries</li> <li>Inferences (for the advanced users of Corporate Memory)</li> </ol> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/tutorial-how-to-link-ids-to-osint/#introduction","title":"Introduction","text":"<p>\u201cEverything as code\u201d has become the status quo among leading organizations adopting DevSecOps and SRE practices, and yet, monitoring and observability have lagged behind the advancements made in application and infrastructure delivery for the Cyber Investigation Analysts. \u201cInvestigating as code\u201d is not just automated installation and configuration of agents, plugins, and exporters to collect the alerts of Indicator of Compromise (IoC) \u2014 it encompasses the conception of a custom dashboard to navigate in gigabytes of event data linked to open data of Open-Source INTelligence (OSINT). However, traces and knowledge about attacks are heterogeneous due to fragmented cyber communities. This fact prevents rapid development and makes the \u201cinvestigating as code\u201d a difficult and tedious task, including cybersecurity event browsing.</p> <p>This tutorial is going to demonstrate solutions that exist to browse the knowledge of an attack like on the Web with Linked Data technology. It also includes the development of new custom links between events and knowledge, ie. inferences.</p> <p>This self-service monitoring/alerting and now inferencing allows analysts breaking data silos which enables by continuous improvement the linking of Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT).</p>","tags":["ExpertTutorial"]},{"location":"build/tutorial-how-to-link-ids-to-osint/#level","title":"Level","text":"<p>This tutorial is also suitable for beginners. Simple examples will allow you to discover Linked Data technologies.</p>","tags":["ExpertTutorial"]},{"location":"build/tutorial-how-to-link-ids-to-osint/#material","title":"Material","text":"<p>eccenca will offer an online sandbox running an instance of Corporate Memory. You can also install Corporate Memory Control (cmemc) on your computer to test the example of bash scripts in this tutorial. For the part \u201cLink IDS event to a knowledge graph in dashboards\u201d, you need to have a Splunk instance where you can install the Splunk apps of this tutorial.</p> <p>Next chapter: Define the need, the expected result and the use cases</p>","tags":["ExpertTutorial"]},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-interfaces/","title":"Specify the dashboards before the RDF models","text":"<p>Another typical problem curing the creation of a knowledge graph is the time waste and performance loss due to the extraction of unnecessary information. Thus, we put emphasis on the specification of the data model and as such limiting the data scope and size instead of conceptualizing useless ontology terms and bloating our knowledge graph with meaningless triples.</p> <p>In this tutorial, we will show how to (1) define the information available in an alert of a IDS source and how to link this with OSINT, (2) create the interfaces for the final user to show these information, (3) specify the RDF model of each concept to extract the minimal vocabulary of your minimal knowledge graph.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-interfaces/#the-interfaces-of-use-cases","title":"The interfaces of use cases","text":"<p>In the previous tutorial, we have defined the use cases in contact with the humans, ie. analysts (see figure 1)</p> <p> </p> <p>Figure 1. We need to imagine an interface where analysts can list the IoCs during the incident and read all their documentations.</p> <p>The classic Splunk interface is a set of panels, like \u201cstatic table\u201d panel. This table panel can show a table of cells and also one cell with a text via in input a Splunk Search Processing Language (SPL). With the plugin Linked Data App (tutorial page), we can insert a SPARQL query and select the part of your knowledge graph to print (figure 2).</p> <p></p> <p>Figure 2. An analyst can insert a SPARQL query with Splunk token in input of one \u201cstatic table\u201d panel of his dashboard with the plugin \u201cLinked Data App\u201d</p> <p>The first dashboard to do for our use cases is the list of IoCs with classic SPL queries of analysts via a static table and allow the analyst to select one IoC via a click in the table. The dashboard with this selected row can save the ID of IoC in a global variable for the other panels in the same dashboard (a Splunk token). When this variable (Splunk token) is changed by the user, Splunk is able to recalculate automatically the queries with this variable in the other static tables. So with this mechanism, we can print the details in the knowledge graph (with SPARQL queries) and the IoC statistics in the Splunk indexes (with SPL queries) around of one selected IoC. With these knowledge about Splunk dashboard, we proposed to analysts a first naive interface in the figure 3.</p> <p> </p> <p>Figure 3. Imagine the expected Splunk dashboard with its interaction</p> <p>Here, the figure 3 is nice but before this first schema during the project, there are a lot of shemas and all were minimalist and ugly often only on a whiteboard. This  type schema before the technical feasibility is only to validate the objective with the analysts before starting the development. During the technical feasibility, we can decrease/increase step-by-step your objectives to show finally a first result in figure 4 in a real dashboard.</p> <p></p> <p>Figure 4. First interface with only SPARQL queries in SPLUNK static tables.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-interfaces/#technical-feasibility-with-the-available-information","title":"Technical feasibility with the available information","text":"<p>It\u2019s not really technical to check if the data is available or not, but without technical knowledge, it\u2019s hard to evaluate the difficulty to link each id to their instance in your RDF knowledge graph. In this tutorial, we learn to use Corporate Memory of Eccenca to transform these IDs to IRI to import properly these ID with the other data necessary to build these interface.</p> <p>After research and one meeting with analysts, we have chosen the datasets of Mitre Attack, the datasets of IoC rules (Sigma and Hayabusa) in Github and of course, the IoCs data already in the Splunk indexes.</p> <p></p> <p>Figure 5. Define the information available in alerts of IDS and in OSINT to link these information.</p> <p>The Splunk indexes of IoCs are selected by the analysts in the dashboard via the component multiselect input in the form part of dashboard (the form part inits other Splunk tokens). We have choosen the IDs to link these data and the figure 5 resumes how we are going to link these data via Corporate Memory of Eccenca.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-interfaces/#understand-the-base-of-splunk-dashboards","title":"Understand the base of Splunk dashboards","text":"<p>A Splunk dashboard is coded in XML. The Splunk user can modify a dashboard via a no-code interface or directly in the code.</p> <p>A user can clone any dashboard before modifying it.</p> <p>For example, in our dashboard, you can find:</p> <ul> <li>the root element <code>form</code>,</li> <li>the definition of input component to select the Splunk indexes by the user and</li> <li>the table panel to execute a SPL query and show the result in a table</li> </ul> <pre><code>&lt;form&gt;\n  &lt;label&gt;SIGMA&lt;/label&gt;\n...\n    &lt;input type=\"multiselect\" token=\"selected_index\" searchWhenChanged=\"true\"&gt;\n      &lt;label&gt;index&lt;/label&gt;\n      &lt;valuePrefix&gt;index=\"&lt;/valuePrefix&gt;\n      &lt;valueSuffix&gt;\"&lt;/valueSuffix&gt;\n      &lt;delimiter&gt; OR &lt;/delimiter&gt;\n      &lt;fieldForLabel&gt;index&lt;/fieldForLabel&gt;\n      &lt;fieldForValue&gt;index&lt;/fieldForValue&gt;\n      &lt;search&gt;\n        &lt;query&gt;| eventcount summarize=false index=*\n| search NOT index IN (\"history\", \"cim_modactions\", \"summary\")\n| dedup index\n| fields index&lt;/query&gt;\n        &lt;earliest&gt;0&lt;/earliest&gt;\n        &lt;latest&gt;&lt;/latest&gt;\n      &lt;/search&gt;\n      &lt;choice value=\"*\"&gt;all&lt;/choice&gt;\n      &lt;default&gt;*&lt;/default&gt;\n    &lt;/input&gt;\n...\n  &lt;row&gt;\n    &lt;panel&gt;\n      &lt;table&gt;\n        &lt;search&gt;\n          &lt;query&gt;| tstats count where $selected_index$ ld.ld_source_type=hayabusa Level!=info $level$ by RuleTitle\n| rename RuleTitle as \"Rule name\"\n| sort - count&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"drilldown\"&gt;cell&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;drilldown&gt;\n          &lt;set token=\"selected_rule\"&gt;$click.value$&lt;/set&gt;\n        &lt;/drilldown&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n    ...\n  &lt;/row&gt;\n...\n&lt;/form&gt;\n</code></pre> <p>To read this code, you need to know the Splunk concept \u201ctoken\u201d. Quickly, a Splunk token is a global variable in the dashboard between all the components of dashboard.</p> <p>In this example, the token \u201cselected_index\u201d is defined by the input component and reuse it in the SPL query of the table panel. When the user click on a label in this panel, this selected <code>RuleTitle</code> is saved in the token <code>selected_rule</code> and the panel which uses this token, are refreshed. So, we use tokens in the SPARQL queries to refresh automatically the SPARQL results inside dashboards. For example, this SPARQL query prints three columns \u201cSource\u201d,\u201dDescription\u201d and \u201cMitreID\u201d, only if the user initializes before the token <code>selected_rule</code> in another panel of dashboard:</p> <pre><code>  &lt;row&gt;\n    &lt;panel&gt;\n      &lt;title&gt;Rule's sources&lt;/title&gt;\n      &lt;table&gt;\n        &lt;search&gt;\n          &lt;query&gt;| sparql\nquery=\"prefix ctis: &amp;lt;https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#&amp;gt;\nprefix rdf:  &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt;\nprefix rdfs: &amp;lt;http://www.w3.org/2000/01/rdf-schema#&amp;gt;\nprefix xsd:  &amp;lt;http://www.w3.org/2001/XMLSchema#&amp;gt;\n\nSELECT DISTINCT (STRBEFORE(STRAFTER(STR(?link),\\\"https://github.com/\\\"),\\\"/\\\") as ?Source) (?comment as ?Description) ?link  (?mitreID as ?MitreID)\nFROM &amp;lt;http://example.com/rule&amp;gt;\nWHERE {\n  VALUES ?title { \\\"$selected_rule$\\\" }\n\n        ?ruleHayabusa a ctis:Rule ;\n            rdfs:label ?title ;\n            rdfs:comment ?comment ;\n            rdfs:seeAlso ?referenceLink;\n            rdfs:isDefinedBy ?link ;\n            ctis:filename ?filename .\n      OPTIONAL {\n       ?ruleHayabusa ctis:mitreAttackTechniqueId ?mitreID .\n      }\n}\"&lt;/query&gt;\n...\n        &lt;/search&gt;\n        ...\n        &lt;fields&gt;[\"Source\",\"Description\",\"MitreID\"]&lt;/fields&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n  &lt;/row&gt;\n</code></pre> <p>Tip</p> <p>The editor of Splunk is limited for SPARQL. You can develop your SPARQL query in Corporate Memory Sparql editor. After, like you can see in this example, you have to insert (automatically) <code>\\</code> before all <code>\"</code> in your query before to copy this SPARQL in a Splunk query. If your SPARQL query works in the dashboard, you can insert the tokens.</p> <p>Warning</p> <p>You can follow the tutorial about Mitre Attack and Rules wihout using Splunk. If you want execute the dashboards with your knowledge graph, you need to modify the SPL queries of dashboards according to your data in Splunk. We do not share the indexes of Splunk in this tutorial.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-interfaces/#technical-feasibility-with-the-splunk-dashboard","title":"Technical feasibility with the Splunk dashboard","text":"<p>During our project, we have implemented the SPARQL command necessary to execute a SPARQL query in a SPL query but also several scripts to extend the panels of dashboard. For example, these are problems to print a HTML text and open a external Web page in a dashboard. Before starting a knowledge graph, we need to know if we have to work with a specific syntax in output for Splunk. So, we have searched the simplest solution to print the HTML literal in our knowledge graph with their links. We found it and implemented simple Javascript scripts to resolve these problems. These scripts are imported via the header of dashboard XML and called in the XML of static table panel. You can see the final dashboard with the Mitre description in HTML (the Mitre in these datasets uses Markdown but we show how convert Markdown link to HTML). We give you these scripts in your Linked Data App (tutorial page).</p> <p></p> <p>Figure 6. With an extern Javascript script, static tables support HTML and the user can open.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-interfaces/#conclusion-starting-to-specify-the-necessary-rdf-models-for-these-interfaces","title":"Conclusion: starting to specify the necessary RDF models for these interfaces","text":"<p>With the interfaces, the available data and their links in head, the analyst can now imagine the necessary RDF models of concepts (for example, figure 7 and 8) in his future knowledge graph to generate expected dashboards. These RDF models evolve at the same time as the interfaces (forever) and according to future RDF standards of Cyber world. With Corporate Memory, after each evolution of your models, you can rebuild your knowledge graph \u201cfrom scratch\u201d when you want. Several RDF models of different versions can exist in your knowledge graphs, so you can update progressively your dashboards without service interruption of old dashboards.</p> <p> Figure 7. RDF model of Mitre concept \u201ccourse of action\u201d in our future knowledge graph.</p> <p> Figure 8. RDF model of concept \u201cIoC Rule\u201d in our future knowledge graph.</p> <p>Tutorial: how to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)</p> <p>Next chapter: Build a Knowledge Graph from MITRE ATT&amp;CK\u00ae datasets</p> <p>Previous chapter: Define the need, the expected result and the use cases</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/","title":"Define the need, the expected result and the use cases","text":"<p>A typical mistake when staring to work on a complex ontology is the absence of a real use case and a lack of understanding of the cyber analysts requirements.</p> <p>In this part, we explains how to:</p> <ol> <li>Clarify a need</li> <li>Propose an expected result</li> <li>Define an use case to implement</li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#define-the-need-of-precise-users","title":"Define the need of precise users","text":"<p>There are three things to know to describe a need:</p> <ol> <li>For who</li> <li>What</li> <li>The context</li> </ol> <p>With our customers, we claim a knowledge graph can reduce the work of analysts in the Cybersecurity. So, one of our customer wanted to help their IN Analysts with the software Corporate Memory of eccenca. After several meeting, we defined the need.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#for-who","title":"For who","text":"<p>For who</p> <p>The future use case is to help the IN Analysts (Incident Responders) but also SOC Analysts (Security Operation Center).</p> <p>Each field of activity is vast and brings together very different professions. Cybersecurity is no exception. It is necessary to organize work meetings with end users even if they have no knowledge of knowledge graphs for the moment. So you can ask them about their trades and identify what they want. However, keep in mind the most famous quote attributed to Ford: \u201cIf I had asked people what they wanted, they would have said faster horses.\u201d During the meetings, the question of \u201cWhat\u201d is not about the final result or the method to do it. The question is \u201cwhat do you think to need?\u201d. After only, we can start to imagine how a knowledge graph can help or not.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#what","title":"What","text":"<p>The conclusion of first meetings was:</p> <p>What</p> <p>The analysts want a solution to link the heterogenous data of different indexes in Splunk to improve their capacity to analyze an incident via their Splunk dashboards.</p> <p>Of course, during the meetings, a lot of needs are explained by the analysts but you need to choose the need to resolve according to the context.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#the-context","title":"The context","text":"<p>During the meetings, we try to understand the \u201cwhy\u201d of their needs and their feeling about the knowledge graph technology.</p> <p>Context of the analyst job</p> <p>Today, a IN Analysts (Incident Responders) switches between the timeline of alerts\u2019 messages and an autonomous research in his event store (like Splunk). During this research of evidences for the last incident, there are often new unknown alerts\u2019 messages of recent indicators of compromise rules. Each analyst for each unknown message will try to understand the reason of this message before investigating in the metadata behind this new alert. A professional analyst takes between 30s and 2 minutes to open all references in his browser about a new alert.</p> <p>Feeling of the analyst about the knowledge graph technology:</p> <p>Feeling of the analyst about the knowledge graph technology</p> <p>Today, typically, the IN Analysts have the skills necessary to write NoSQL requests to query their event store. As eccenca is going to create RDF graphs of OSINT, IN analysts will need to write SPARQL queries to jointly query IDS and knowledge graphs. SPARQL is an untypical skill among IN Analysis, thus, they need to make the investment to gain this knowledge. Like every investment a payoff is expected, in this case we see a significant reduction of the time spent by the analyst. Only after this demonstration, the IN Analysts will try to learn maybe to write SPARQL queries in their dashboards or they will create a new job in their team to do these new dashboards.</p> <p>Very quickly (in a lot of customer meetings), the skills are the first problem to work with a knowledge graph and the no interoperability of a lot of knowledge graph on the market does not help to resolve this problem. To onboard the future final users with the technologies of Linked Data, a minimal and simple need is often the best way to start to think \u201cgraph\u201d. Only after, we can propose more complex graphs to resolve other needs and only after, we can do to understand the objectives to build/use in their domain a global ontology.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#propose-an-expected-result","title":"Propose an expected result","text":"<p>Here, the context shows this project is an exploration with like deliverable probably a demonstrator. So, your project started like a exploration with a lot of meetings to understand how these analysts work concretely. We choose a public dataset of one previous incident with the analysts. After, they create the Splunk indexes with these datasets. With these Splunk indexes near of the reality, we can learn alone to use Splunk in order to understand the desire to use this tool to build their dashboards with a knowledge graph.</p> <p>During this step of R&amp;D, we are developing the tools to request a SPARQL service via an authentification OAuth directly by Splunk dashboard. You will install these Splunk apps step-by-step during this tutorial or you can download their \u201ctar.gz\u201d in SPlunk now:</p> <ul> <li>\u201cLinked Data App\u201d to install the SPARQL command. You need to configure it with the details in his README file or in its tutorial page.</li> <li>\u201cInvestigate lateral movements with a knowledge graph\u201d to install our demonstrator to connect SPLUNK to CMEM. You need to read his README or its tutorial page to understand this example.</li> </ul> <p>After several propositions, analysts oriented the implementation of our first dashboards and finally, we showed clearly a benefice to use a knowledge graph via these results.</p> <p>In this tutorial, we study only this first result:</p> <p>Expected result</p> <pre><code>A knowledge graph will reduce the time required to research details on the Web of each new alerts in the IDS of IN Analysts.\nTo achieve such savings we aggregate all links of sources and references about alerts in the Security information and event management (SIEM) in a knowledge graph.\nAnalyst are able to read the Mitre information directly in his timeline (e.g. in SPLUNK) and to access all references about an alert from this central place.\n\n![](slide_result_expected.png)\n\n*Figure 1. Example of expected results for analysts during the task to understand the meaning and relevance of new alerts in their IDS.*\n</code></pre> <p>When we know the waited results, we can imagine the necessary use cases.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#first-use-cases","title":"First use cases","text":"<p>We know the final need, the expected results and the limits of knowledge graph with the Linked Data technologies in Splunk. So, we can define the probable use cases to implement and all the actors who will interact with these use cases.</p> <p></p> <p>Figure 2. UML use cases to resolve this basic need and several use cases with Wikidata to show the interoperability of knowledge graphs with the Linked data technologies. Each bubble in this type of schema is a use case.</p> <p>With a simple UML schema of use cases, you can delimited each use case, their priorities and their tasks for the next step, ie. specify the essential interfaces to limit the complexity of future RDF graph.</p> <p>In this tutorial, after to test this first result, we claim a knowledge graph can reduce the time required to research details on the Web of each new alerts in the IDS of IN Analysts by 50 to 95%, see figure 1. The next step is to specify the interfaces before starting to think about the graphs.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/define-the-need/#advanced-use-cases","title":"Advanced use cases","text":"<p>Another result of this project was to resolve this other need:</p> <p>For who</p> <pre><code>IN analysts\n</code></pre> <p>What</p> <pre><code>Calculate and manage their investigations' knowledge graphs of high-level and low-level directly in Splunk\n</code></pre> <p>Context</p> <pre><code>Linking IDS events to a knowledge graph can be complex.\nThis is for several reasons like labels/IDs/structures of the same resources can be different.\nCorporate Memory provides advanced capabilities to perform this in an automatic way.\nTo use these tools, we need to export the data of SPLUNK to Corporate Memory.\nAnalysts need to export data from SPLUNK to Corporate Memory on the fly and execute Corporate Memory workflows with reconciling complex data automatically and SPARQL update queries directly triggered via their SPLUNK dashboards.\n</code></pre> <p>For the moment, we are searching the best way to resolve this need but a demonstrator to manage several investigations in the same knowledge graphs is available with several examples of dasboards in the Splunk app \u201cInvestigate lateral movements with a knowledge graph\u201d (tutorial page). This need is for advanced users of Corporate Memory and it may be proposed in a future tutorial.</p> <p></p> <p>Figure 3. UML use cases to resolve this avanced need.</p> <p>Tutorial: how to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)</p> <p>Next chapter: Specify the dashboards of use cases before the RDF models</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/","title":"Build a Knowledge Graph from STIX 2.1 data such as the MITRE ATT&amp;CK\u00ae datasets","text":""},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#introduction","title":"Introduction","text":"<p>MITRE ATT&amp;CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations. The ATT&amp;CK knowledge base is used as a foundation for the development of specific threat models and methodologies in the private sector, in government, and in the cybersecurity product and service community.</p> <p>The MITRE ATT&amp;CK datasets in STIX 2.1 JSON collections are here:</p> <ul> <li>enterprise-attack.json</li> <li>mobile-attack.json</li> <li>ics-attack.json</li> </ul> <p>Structured Threat Information Expression (STIX\u2122) is a language and serialization format used to exchange cyber threat intelligence (CTI).</p> <p>The \u201contology\u201d of MITRE ATT&amp;CK with STIX is here: https://github.com/mitre/cti/blob/master/USAGE.md</p> <p>The objective of this tutorial is not focus on the ontologies. In our use case, we just need to extract several metadata. If the community of STIX wants to resolve their problems, it will be the moment to define a solid ontology. If you are a newbie with the Linked Data technologies, you have to learn to generate in first a functional knowledge graph for your needs before building a perfect ontology for everybody. When you masterize the ontologies, we will modify this first ontology and you could refresh your knowledge graph when you want with Corporate Memory.</p> <p>This tutorial is written in order to gradually acquire all the skills necessary to build from scratch a knowledge graph with Corporate Memory and update it automatically via Corporate Memory Console. This tutorial must be completed in order.</p> <p>Labs:</p> <ol> <li>Create a new project for your knowledge graph in your Sandbox</li> <li>Import the datasets to convert in RDF</li> <li>Create named graphs of your knowledge graph</li> <li>Create a RDF transformer for STIX 2.1</li> <li>Create the workflow to transform all STIX datasets to RDF</li> <li>Create the global named graph of your knowledge graph</li> <li>Test the SPARQL query to obtain the name, the description and the references of a Mitre tag</li> <li>(optional) Create the Void description of knowledge graph</li> <li>(optional) Refresh your knowledge graph automatically</li> </ol> <p>You can improve this first knowledge graph with these exercises:</p> <ol> <li>Create an inference in your knowledge graph via a SPARQL Update query</li> <li>Create an other knowledge graph for CAPEC linked to MITRE ATT&amp;CK</li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#labs","title":"Labs","text":""},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-your-sandbox","title":"Create your sandbox","text":"<p>You need to create your sandbox of Corporate Memory to create your tempory knowledge graph for this tutorial.</p> <p>Follow the instructions here: https://eccenca.my</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-a-project","title":"Create a project","text":"<p>For each type of dataset, you can create an new project with all the tools necessary to convert this dataset in a knowledge graph.</p> <p>Create a new project, reproduce the demonstration in the following video:</p> <ul> <li> <p>Title: MITRE ATT&amp;CK\u00ae</p> </li> <li> <p>Description: MITRE ATT&amp;CK\u00ae is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.</p> </li> </ul> <p></p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#import-datasets","title":"Import datasets","text":"<p>MITRE ATT&amp;CK\u00ae has 3 domains: Entreprise, Mobile and ICS.</p> <p>Each domain dataset is saved in GitHub:</p> <ul> <li>enterprise-attack.json</li> <li>mobile-attack.json</li> <li>ics-attack.json</li> </ul> <ol> <li>Download these 3 files</li> <li>Create for each JSON file, a JSON dataset: </li> </ol> <p>Tip</p> <p>Give a short name at each dataset/transformer/etc in Corporate Memory to recognize it easily in the workflow view. For example, we will use \u201cMA Entreprise (JSON)\u201d like label and \u201cMITRE ATT&amp;CK\u00ae Entreprise dataset STIX 2.1\u201d like description for the Entreprise dataset and so \u201cMA Mobile (JSON)\u201d for Mobile, \u201cMA ICS (JSON)\u201d for ICS, etc.</p> <p>Success</p> <p>Now, you can see these JSON datasets in Corporate Memory: </p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-named-graphs","title":"Create named graphs","text":"<p>Info</p> <p>A knowledge graph is an abstract concept. Concretly in a triplestore or a RDF graph database via Corporate Memory, the database saves each RDF triple of graph in a named graph or RDF dataset in Corporate Memory. A graph named is a set of triples. So, a knowledge graph can be composed by one or several named graphs.</p> <p>Tip</p> <p>A named graph can be modify without affecting the other named graphs. Each dataset of Mitre can be updated at any moment, so we are going to create a specific named graph for each Mitre dataset to simplify the update of each dataset in your knowledge graph.</p> <p>A good practice is to name the named graph by the URI of its real source on the Web, so the labels and graph names of your RDF datasets can be:</p> <ul> <li> <p>Entreprise domain</p> <ul> <li>Label: MA Entreprise (knowledge graph)</li> <li>Graph name: https://github.com/mitre-attack/attack-stix-data/raw/master/enterprise-attack/enterprise-attack.json</li> </ul> </li> <li> <p>Mobile domain</p> <ul> <li>Label: MA Mobile (knowledge graph)</li> <li>Graph name: https://github.com/mitre-attack/attack-stix-data/raw/master/mobile-attack/mobile-attack.json</li> </ul> </li> <li> <p>ICS domain</p> <ul> <li>Label: MA ICS (knowledge graph)</li> <li>Graph name: https://github.com/mitre-attack/attack-stix-data/raw/master/ics-attack/ics-attack.json</li> </ul> </li> </ul> <p>Create one RDF dataset for each Mitre dataset:</p> <ol> <li>Add component \u201cKnowledge Graph\u201d</li> <li>Put a label</li> <li>Put a URI of named graph</li> <li>Enable \u201cClear graph before workflow execution\u201d</li> </ol> <p></p> <p>Success</p> <p>Now, you can see these RDF datasets in Corporate Memory: </p> <p>Tip</p> <p>The consequence of the option \u201cClear graph before workflow execution\u201d is the named graph will be deleted (with all its triples) before receiving new triples  when you use this named graph like an output in a workflow and also in the transformer task (in the next step).</p> <p>This option is to use only for the graphs which will generate automatically by Corporate Memory.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-a-transformer","title":"Create a transformer","text":"<p>Tip</p> <p>There are not bad manners to build a knowledge graph but there are knowledge graphs useless or very hard to use by the analysts or developers in their missions.</p> <p>Without having all queries necessary in their missions, your knowledge will continue to evolve to sastify all needs of your users.</p> <p>With Corporate Memory, you can develop progressively your vocabularies RDFS or your ontologies OWL to describes your knowledge graph.</p> <p>If it\u2019s your first knowledge graph, the best manner to start is with RDFS vocabularies because you can develop it like you develop classes and their instances in an object oriented language. It\u2019s exactly the same manner to describe the world. Of course, there are differences but you can start a first functional knowledge graph without being an expert.</p> <p>Here, you will create all classes and attributes necessary in your use case case. Not more, not less. So, we are adding each STIX object in your knowledge base with its STIX type, its label, its description and its references. Each reference can have an url, a label, a description and an external ID, like Mitre ID or CAPEC ID.</p> <p>In UML, you can represent your targeted model like that: here a RDF model to describe an instance of type \u201ccourse-of-action\u201d in MITRE ATT&amp;CK. (you can download the File drawio of schemas)</p> <p></p> <p>The SPARQL query for this model can be specify in UML with a RDF pattern: here a RDF pattern to select the \u201ccourse-of-action\u201d objects with a known Mitre ID</p> <p></p> <p>Without an official vocabulary and its official prefix, we are using the documentation on the Web of its datasets: https://github.com/mitre/cti/blob/master/USAGE.md</p> <p>So, to make a prefix, we choosed a short name, for example \u201cctia\u201d, and the IRI will build with the Web address of its documentation with a # at the end (to link to anchors of attributes in the Web page, if they exist):</p> <pre><code>prefix ctia: &lt;https://github.com/mitre/cti/blob/master/USAGE.md#&gt;\n</code></pre> <ol> <li> <p>Create the prefix of your vocabulary:</p> <pre><code>prefix ctia: &lt;https://github.com/mitre/cti/blob/master/USAGE.md#&gt;\n</code></pre> </li> </ol> <p></p> <ol> <li>Create the (Mitre) STIX 2.1 transformer</li> </ol> <p>This transformer will be a component of your worflow. You could reuse it in several workflows in other projects. To create a new transformer, you need to give a:</p> <ul> <li>Label: STIX 2.1 transformer</li> <li>Input: MA Entreprise (JSON)</li> <li>Output: MA Entreprise (knowledge graph)</li> </ul> <p></p> <p>Tip</p> <p>In your use case, there is only this transformer to build this named graph, so there is no consequence on the final knowledge graph when we test this transformer on this graph (automatically cleared after each execution of transformer). However, a good practice is to create a tempory graph in ouput for each transformer, so your final knowledge graph is not affected during the modification of your transformer before executing the workflows with this transformer. In this case, you need to hide this tempory graph of your users.</p> <p>You can create a transformer for several syntaxes in input: JSON, XML, CSV, etc. If your format does not exist in Corporate Memory, you can convert your data in JSON before importing this data in Corporate Memory.</p> <p>Info</p> <p>STIX gives the possibility to extend its syntaxes. Mitre uses this possibility. So, in theory, if we need to import all the data, we can extend this transformer at all STIX attributes and add the Mitre attributes described in its documentation.</p> <ol> <li>Study the tree of STIX data</li> </ol> <pre><code>{\n    \"type\": \"bundle\",\n    \"id\": \"bundle--19413d5e-67e5-4a48-a4c8-afb06b7954de\",\n    \"spec_version\": \"2.1\",\n    \"objects\": [\n        {\n            \"type\": \"x-mitre-collection\",\n            \"id\": \"x-mitre-collection--1f5f1533-f617-4ca8-9ab4-6a02367fa019\",\n            \"name\": \"Enterprise ATT&amp;CK\",\n            \"description\": \"ATT&amp;CK for Enterprise provides a knowledge base of real-world adversary behavior targeting traditional enterprise networks. ATT&amp;CK for Enterprise covers the following platforms: Windows, macOS, Linux, PRE, Office 365, Google Workspace, IaaS, Network, and Containers.\",\n            ...\n        },\n        {\n            \"id\": \"attack-pattern--0042a9f5-f053-4769-b3ef-9ad018dfa298\",\n            \"type\": \"attack-pattern\",\n            \"name\": \"Extra Window Memory Injection\",\n            \"description\": \"Adversaries may inject malicious code...\" ,\n            \"external_references\": [\n                {\n                    \"source_name\": \"mitre-attack\",\n                    \"external_id\": \"T1055.011\",\n                    \"url\": \"https://attack.mitre.org/techniques/T1055/011\"\n                },\n                {\n                    \"url\": \"https://msdn.microsoft.com/library/windows/desktop/ms633574.aspx\",\n                    \"description\": \"Microsoft. (n.d.). About Window Classes. Retrieved December 16, 2017.\",\n                    \"source_name\": \"Microsoft Window Classes\"\n                },...\n</code></pre> <p>To extract STIX objects with its type, its label, its description and its references, we need to navigate via a root object of type \u201cbundle\u201d before touching the STIX objects. Each object has an ID, we suppose unique in all Mitre datasets to generate IRI of all objects. We use your prefix ctia to build the class name and the properties of your RDFS vocabulary. Here, we build the vocabulary of manner agile for your use case because Mitre had not proposed a RDFS vocabulary for its datasets.</p> <ol> <li>Create the root object and give it an unique IRI:</li> </ol> <ul> <li>RDF type: ctia:Object</li> <li>IRI pattern: https://github.com/mitre-attack/attack-stix-data#{id}</li> </ul> <p></p> <p>Tip</p> <p>You can develop an IRI from scratch in the IRI formula editor, like here or directly in the form and improve it after, if necessary (see an example in the next step).</p> <p>The important is to test the result in the evaluation view.</p> <p>Success</p> <p>During the development of a transformer, you can test your transformation and check all the steps.</p> <p></p> <ol> <li>Link the sub-objects to their root:</li> </ol> <ul> <li>Value path: objects</li> </ul> <p>with their IRI and the property ctia:object:</p> <ul> <li>RDF property: ctia:object</li> <li>RDF type: ctia:Object</li> <li>IRI pattern: https://github.com/mitre-attack/attack-stix-data#{id}</li> </ul> <p></p> <p>Tip</p> <p>The RDFS classes start by an uppercase and the property by a lowercase and apply the camel case notation, if possible. The objective is to create cool IRI, ie. lisible IDs for humans and unique on the Web.</p> <p>There are exceptions, like Wikidata which prefers to use a number for their IRI but with a explicit label in all languages.</p> <p>Moreover, if there is no clear ontology in your domain, the best is to take the name of parameters of the source (here json). So, we will use the property, like <code>ctia:external_id</code> with underscore because it\u2019s the convention of Mitre in its datasets. If Mitre defines a best RDF ontology, we will modify simply your transformer to respect their new ontology.</p> <p>Tip</p> <p>We could limit the number of objects to import, if you add conditions in the formula editor with the field \u201ctype\u201d of objects, for example.</p> <ol> <li>Extract now their type, label and description with these properties for example:</li> </ol> <ul> <li>ctia:type<ul> <li>RDF type: URI</li> <li>Via the \u201cvalue forma editor\u201d create the IRI: <code>https://github.com/mitre/cti/blob/master/USAGE.md#{type}</code></li> </ul> </li> <li>rdfs:label<ul> <li>value path: name</li> <li>RDF type: String</li> </ul> </li> <li>ctia:description<ul> <li>value path: description</li> <li>RDF type: String</li> </ul> </li> </ul> <p></p> <p>Tip</p> <p>STIX type doesn\u2019t apply the camel case and doesn\u2019t start by an uppercase. We prefers to create a specific property ctia:type for this reason.</p> <p>You can reuse a vocabulary already in Corporate Memory (like rdfs) but you are also free to develop a new vocabulary on the fly with your prefixes.</p> <p>Success</p> <p>When you test your transformer, you can see the future instances in your knowledge graph: </p> <ol> <li>At the end of the last step, we saw the dataset uses the syntax of Markdown to define a Web link. In the interface of SPLUNK, we need to use the HTML syntax. Modify the formula for the description with the operator \u201cregex replace\u201d.</li> </ol> <ul> <li>Regex:  <code>\\[([^\\[\\]]*)\\]\\(([^\\(\\)]*)\\)</code></li> <li>Replace: <code>&lt;a href='$2' target='blank'&gt;$1&lt;/a&gt;</code></li> </ul> <p></p> <p>Success</p> <pre><code>In the \"value formula editor\", you can immediatly check the result of your formula.\n![](23-1-regex-replace.png)\n</code></pre> <p>Tip</p> <p>At any moment, you will modify your vocabulary according to your needs that you will find during your development. You need to modify this transformer and relaunch all your workflows which use this transformer.</p> <p>Tip</p> <p>The regular expression are often necessary in the components of \u201cvalue formula editor\u201d. The website regex101 will help you to develop and debug the regular expressions.</p> <ol> <li>Via the same method, we are linking the references objects to their STIX objects:</li> </ol> <ul> <li>via the property: <code>ctia:external_references</code></li> <li>Type: ctia:Reference</li> <li>value path: external_references</li> <li>IRI of each object: its own URL ()</li> </ul> <p>ctia:Reference object has these properties:</p> <ul> <li>ctia:source_name</li> <li>ctia:description</li> <li>ctia:url</li> <li>ctia:external_id</li> </ul> <p></p> <p>Tip</p> <p>Sometimes, several urls are not correct. You can use the component \u201cFix URI\u201d to fix the classic problems.</p> <p></p> <p>Warning</p> <p>When you make a transformer on a dataset, you see quickly the limit of data. For example with Mitre, several references are a set of citations without URL.</p> <p></p> <p>For example references with this description: <code>(Citation: Palo Alto menuPass Feb 2017)(Citation: DOJ APT10 Dec 2018)(Citation: District Court of NY APT10 Indictment December 2018)</code></p> <p>The URL for the majority of citations can be found in the dataset but we need to do a first pass before to link correctly the citations at their URL.</p> <p>Moreover, we can find also citation directly in the description of several objects but without URL and without their references in their JSON tree.</p> <p>Here, it\u2019s a simple tutorial. So, we do not try to fix this problem of citations for the moment, but if you want a tutorial to fix it, let me a comment in this page.</p> <p>Success</p> <p>To test your transformer, you need to develop one or several SPARQL queries with the RDF pattern which will use in your use case. You are developing this query in the SPARQL editor:</p> <pre><code>#Test 1 transformer STIX 2.1\n\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX ctia: &lt;https://github.com/mitre/cti/blob/master/USAGE.md#&gt;\n\nSELECT\n?title ?description\n(GROUP_CONCAT(?link; separator=\"&lt;br/&gt;\") as ?references)\nFROM &lt;https://github.com/mitre-attack/attack-stix-data/raw/master/enterprise-attack/enterprise-attack.json&gt;\nWHERE {\n{\n    ?resource ctia:type ctia:course-of-action .\n} union {\n    ?resource ctia:type ctia:attack-pattern .\n}\n\n?resource rdfs:label ?title ;\n            ctia:description ?description ;\n            ctia:external_references ?mitre_url .\n\n?mitre_url ctia:external_id  \"T1490\" ;\n            ctia:source_name  \"mitre-attack\" .\n\nOPTIONAL {\n    ?resource ctia:external_references [\n            ctia:url ?reference_url ;\n            ctia:source_name ?reference_label ;\n            ctia:description ?reference_description\n            ] .\n    BIND( CONCAT(\"&lt;a ref=\",STR(?reference_url),\"\\\"&gt;\",?reference_label,\": \",?reference_description ,\"&lt;/a&gt;\") as ?link)\n}\n}\nGROUP BY ?title ?description\n</code></pre> <p></p> <ol> <li>During the building of interfaces, we saw the same MITRE ID of IoC rules is used by the concepts of tactic, mitigation, technique,\u2026 In the final interface, we will print properly the label of each concept for the same Mitre ID, like \u201cTechnique TXX\u201d or \u201cMitigation TXX\u201d.</li> </ol> <p>Tip</p> <p>Moreover, Corporate Memory indexes some specific properties automatically, like rdfs:label. Without this property, it\u2019s not easy to find the objects by a search by text. To facilite the research of references, like the mitre id, you are adding the property rdfs:label to reference objects.</p> <p>So, we add a new property <code>rdfs:label</code> to object <code>ctia:Reference</code>. If the reference is not a Mitre ID, we will copy the source_name else we will extract the type of concept in the URL and concat his Mitre ID:</p> <ul> <li>In the transformer STIX, add the property rdfs:label (type string) to object <code>ctia:Reference</code>.</li> </ul> <p></p> <ul> <li>Customize the value of label, like in this RDF model: (try to do this rule alone before to look at this possible response)</li> </ul> <p></p> <p>Success</p> <p>You can test the result when you search the Mitre ID via the explorer of knowledge graph \u201cMA Entreprise\u201d:</p> <p></p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-a-workflow","title":"Create a workflow","text":"<p>You have now a STIX transformer. We are building here a workflow to apply this transformer for all datasets in same time.</p> <ol> <li>Create a workflow with a name, for example \u201cMITRE ATT&amp;CK\u00ae workflow\u201d</li> <li>Insert the input JSON dataset</li> <li>Insert the output RDF dataset</li> <li>Insert the transformer</li> <li>Link the three components</li> <li>Execute the workflow to test it</li> <li> <p>Save it</p> <p></p> </li> <li> <p>Do the same operations for the two other datasets.</p> </li> </ol> <p>Success</p> <p>At the end, the workflow looks like that:</p> <p></p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-a-global-named-graph","title":"Create a global named graph","text":"<p>To simplify the requests by a SPARQL query on your knowledge graph, we are offering the possibility to request all data of these 3 datasets in same time.</p> <p>We are showing the \u201cSPARQL tasks\u201d, another important feature available in Corporate Memory. More precisely, we will work with the SPARQL Update task with Jinja template.</p> <p>Note</p> <p>Jinja is a text-based template language and thus can be used to generate any markup as well as source code, like SPARQL. Corporate Memory gives the possibility to insert the name of named graph in a SPARQL query according to its position in the worflow to execute.</p> <p>For example, <code>$outputProperties.uri(\"graph\")</code> inserts the name of graph connected to the output of the task in the workflow and <code>$inputProperties.uri(\"graph\")</code> inserts the name of graph connected to the input. It\u2019s very practice to do repetive tasks, like to calculate the VoiD description at each update of graph.</p> <ol> <li> <p>Create a \u201cKnowledge Graph\u201d dataset (ie, a RDF dataset)</p> <ul> <li>Label: MITRE ATT&amp;CK\u00ae  (knowledge graph)</li> <li>URI (name of graph): https://attack.mitre.org</li> <li>Enable \u201cClear graph before workflow execution\u201d</li> </ul> </li> <li> <p>Create a \u201cSPARQL Update query\u201d task without missing to enable the Jinja Template</p> <ul> <li>Label: Import graph</li> </ul> </li> </ol> <pre><code>PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\nINSERT DATA {\n  GRAPH $outputProperties.uri(\"graph\") {\n   $outputProperties.uri(\"graph\")\n        owl:imports $inputProperties.uri(\"graph\") .\n  }\n}\n</code></pre> <p>Note</p> <p>In this query, Jinja replace $outputProperties.uri(\u201cgraph\u201d) and $inputProperties.uri(\u201cgraph\u201d) according to our workflow so the final code executed of this query is, for example:</p> <pre><code>PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n\nINSERT DATA {\n    GRAPH &lt;https://attack.mitre.org&gt;  {\n        &lt;https://attack.mitre.org&gt;\n            owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/tree/master/enterprise-attack/enterprise-attack.json&gt; .\n    }\n}\n</code></pre> <p>Success</p> <p>In the Turtle view of RDF dataset \u201cMITRE ATT&amp;CK\u00ae\u201d, you can see the triples inserted by your SPARQL query.</p> <pre><code>&lt;https://attack.mitre.org&gt;\n    owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/tree/master/enterprise-attack/enterprise-attack.json&gt;;\n    owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/blob/master/mobile-attack/mobile-attack.json&gt;;\n    owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/blob/master/ics-attack/ics-attack.json&gt;\n    .\n</code></pre> <ol> <li> <p>In the same workflow add one SPARQL task for each RDF datasets and in output add the RDF dataset \u201cMITRE ATT&amp;CK\u00ae\u201d. Execute it and save it.</p> <p></p> </li> </ol> <p>Success</p> <p></p> <p>In the Turtle view of RDF dataset \u201cMITRE ATT&amp;CK\u00ae\u201d, you can see the triples inserted by your SPARQL query.</p> <pre><code>&lt;http://attack.mitre.org&gt;\n    owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/tree/master/enterprise-attack/enterprise-attack.json&gt; ;\n    owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/blob/master/mobile-attack/mobile-attack.json&gt; ;\n    owl:imports &lt;https://github.com/mitre-attack/attack-stix-data/blob/master/ics-attack/ics-attack.json&gt;\n    .\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#test-your-final-sparql-query","title":"Test your final SPARQL query","text":"<p>Now, you can request all the datasets in same time through the named graph <code>https://attack.mitre.org</code> to respond at the final query of our use case:</p> <pre><code>#Test 2 final query\n\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX ctia: &lt;https://github.com/mitre/cti/blob/master/USAGE.md#&gt;\n\nSELECT\n?title ?description\n(GROUP_CONCAT( distinct ?link; separator=\"&lt;br/&gt;\") as ?references)\nFROM &lt;https://attack.mitre.org&gt;\nWHERE {\n    {\n        ?resource ctia:type ctia:course-of-action .\n    } union {\n        ?resource ctia:type ctia:attack-pattern .\n    }\n\n    ?resource rdfs:label ?title ;\n        ctia:description ?description ;\n        ctia:external_references ?mitre_url .\n\n    ?mitre_url ctia:external_id  \"T1490\" ;\n        ctia:source_name  \"mitre-attack\" .\n\n    OPTIONAL {\n        ?resource ctia:external_references [\n            ctia:url ?reference_url ;\n            ctia:source_name ?reference_label ;\n            ctia:description ?reference_description\n        ] .\n        BIND( CONCAT(\"&lt;a ref=\",STR(?reference_url),\"\\\"&gt;\",?reference_label,\": \",?reference_description ,\"&lt;/a&gt;\") as ?link)\n    }\n}\nGROUP BY ?title ?description\n</code></pre> <p>Success</p> <p></p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-the-void-description","title":"Create the Void description","text":"<p>In theory, RDF datasets in the Linked Open Data have to have a VoID description with their statistics. The objective is to catalog automatically these datasets.</p> <p>Info</p> <p>VoID is an RDF Schema vocabulary for expressing metadata about RDF datasets. It is intended as a bridge between the publishers and users of RDF data.</p> <p>Here, we are creating a new SPARQL Update task to calculate and insert automatically the statistics of our global graph and add a VoID description.</p> <ol> <li>In the same workflow, insert a new SPARQL Update task with this query to calculate the statistics:</li> </ol> <ul> <li>label: Calculate VoID</li> </ul> <pre><code>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\nprefix void: &lt;http://rdfs.org/ns/void#&gt;\n\nINSERT\n{\n    GRAPH $outputProperties.uri(\"graph\") {\n        $outputProperties.uri(\"graph\") a void:Dataset;\n            rdfs:label  \"MITRE ATT&amp;CK\u00ae\";\n            rdfs:comment  \"MITRE ATT&amp;CK\u00ae is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.\";\n            void:triples ?triples ;\n            void:entities ?entities .\n    }\n}\nUSING $outputProperties.uri(\"graph\")\nWHERE {\n    {\n        SELECT (COUNT(DISTINCT ?resource) as ?entities)\n        WHERE {\n            ?resource a ?class .\n        }\n    }\n    {\n        SELECT (COUNT(?s) as ?triples)\n        WHERE {\n            ?s ?p ?o .\n        }\n    }\n}\n</code></pre> <p>Tip</p> <p>This query uses the variable <code>$outputProperties.uri(\"graph\")</code> (Jinja template). If the name of graph changes, the code of the query stays stable in your workflow.</p> <p></p> <p>Success</p> <p>The final triples in the graph <code>https://attack.mitre.org</code>after this worflow.</p> <pre><code>prefix owl:  &lt;http://www.w3.org/2002/07/owl#&gt;\nprefix rdf:  &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nprefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix xsd:  &lt;http://www.w3.org/2001/XMLSchema#&gt;\n\n&lt;https://attack.mitre.org&gt;\n    rdf:type      &lt;http://rdfs.org/ns/void#Dataset&gt; ;\n    rdfs:comment  \"MITRE ATT&amp;CK\u00ae is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.\" ;\n    rdfs:label    \"MITRE ATT&amp;CK\u00ae\" ;\n    &lt;http://rdfs.org/ns/void#entities&gt; 28081 ;\n    &lt;http://rdfs.org/ns/void#triples&gt; 150120 ;\n    owl:imports\n        &lt;https://github.com/mitre-attack/attack-stix-data/raw/master/ics-attack/ics-attack.json&gt; ,\n        &lt;https://github.com/mitre-attack/attack-stix-data/raw/master/mobile-attack/mobile-attack.json&gt; ,\n        &lt;https://github.com/mitre-attack/attack-stix-data/raw/master/enterprise-attack/enterprise-attack.json&gt; .\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#refresh-all-automatically","title":"Refresh all automatically","text":"<p>The datasets of Mitre are updated regularly. You may want to update them automatically via a command line in a bash file. In this script, we use CMEM.</p> <ol> <li> <p>Install CMEMC - a Command Line Interface of CMEM</p> </li> <li> <p>Open your config file:</p> </li> </ol> <pre><code>cmemc config edit\n</code></pre> <ol> <li>Insert your sandbox in your CMEMC config, example with a password grant type:</li> </ol> <pre><code>[johndo.eccenca.my]\nCMEM_BASE_URI=https://johndo.eccenca.my/\nOAUTH_GRANT_TYPE=password\nOAUTH_CLIENT_ID=cmemc\nOAUTH_USER=johndo@example.com\nOAUTH_PASSWORD=XXXXXXXXX\n</code></pre> <p>You need to replace \u201cjohndo\u201d by other thing, \u201cjohndo@example.com\u201d by your login (email) in the sandbox and XXXXXXXXX by your password. Save the file (with VI, :wq).</p> <p>Tip</p> <p>Immediatly, in the file ~/.bashrc, you can specify your sandbox like your instance by default for CMEMC with this line:</p> <pre><code>export CMEMC_CONNECTION=johndo.eccenca.my\n</code></pre> <p>Test:</p> <pre><code>cmemc graph list\n# or cmemc -c johndo.eccenca.my graph list\n</code></pre> <p>If you can connect it, you can see your knowledge graph \u201chttps://attack.mitre.org\u201d in the list.</p> <ol> <li>You need to know the IDs of your JSON datasets IDs and your workflow ID to implement the command lines with the tool Corporate Memory Console (</li> </ol> <p></p> <p>For example in my demo the JSON datasets and the workflow have these IDs:</p> <pre><code>MITREATTCK_3dc114458dfd4c57:MAEntrepriseJSON_14f0f94ed5de5daa\nMITREATTCK_3dc114458dfd4c57:MAICSJSON_e024c6433ed523e1\nMITREATTCK_3dc114458dfd4c57:MAMobileJSON_3f890442dad17750\n\nMITREATTCK_3dc114458dfd4c57:MITREATTCKworkflow_0b8fa5454ef21a00\n</code></pre> <ol> <li>You can now import the file directly of Mitre repository on GitHub and import the files in the sandbox and execute your workflow.</li> </ol> <pre><code>wget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/enterprise-attack/enterprise-attack.json\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/mobile-attack/mobile-attack.json\nwget https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/ics-attack/ics-attack.json\n\ncmemc dataset download --replace MITREATTCK_3dc114458dfd4c57:MAEntrepriseJSON_14f0f94ed5de5daa enterprise-attack.json\ncmemc dataset download --replace MITREATTCK_3dc114458dfd4c57:MAMobileJSON_3f890442dad17750 mobile-attack.json\ncmemc dataset download --replace MITREATTCK_3dc114458dfd4c57:MAICSJSON_e024c6433ed523e1 ics-attack.json\ncmemc workflow execute --wait MITREATTCK_3dc114458dfd4c57:MITREATTCKworkflow_0b8fa5454ef21a00\n</code></pre> <p>Success</p> <p>You can see the result in the shell but also via the \u201cActivities Board\u201d. It\u2019s useful to follow the errors of your workflows, if you execute a script via a Linux Cron, for example.</p> <p></p> <p>Tip</p> <p>With these command lines, you can now start a cron every day to check the Mitre updates and start refreshing your datasets.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#exercices","title":"Exercices","text":""},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#create-inferences","title":"Create inferences","text":"<p>After this tutorial, you want probably to navigate in your new knowledge graph between the relationships of Objects STIX. Before, you need to create inferences of these STIX \u201crelationships\u201d in your knowledge graph via a SPARQL Update query.</p> <ol> <li> <p>In the STIX transformer, import also the fields: <code>ctia:source_ref</code>, <code>ctia:target_ref</code> and <code>ctia:relationship_type</code>.</p> </li> <li> <p>Create a new SPARQL Update task \u201cconvert STIX relationships to rdf statements\u201d with this code:</p> </li> </ol> <pre><code>PREFIX ctia: &lt;https://github.com/mitre/cti/blob/master/USAGE.md#&gt;\n\nINSERT {\n    GRAPH  $outputProperties.uri(\"graph\") {\n        ?sourceIRI  ?propertyIRI ?targetIRI .\n    }\n}\nWHERE {\n    GRAPH  $inputProperties.uri(\"graph\") {\n        ?relationship\n            ctia:type ctia:relationship ;\n            ctia:source_ref ?source ;\n            ctia:target_ref ?target ;\n            ctia:relationship_type ?property .\n    }\n\n    BIND (IRI(CONCAT(\"https://github.com/mitre-attack/attack-stix-data#\",STR(?source))) as ?sourceIRI)\n    BIND (IRI(CONCAT(\"https://github.com/mitre/cti/blob/master/USAGE.md#\",STR(?property))) as ?propertyIRI)\n    BIND (IRI(CONCAT(\"https://github.com/mitre-attack/attack-stix-data#\",STR(?target))) as ?targetIRI)\n}\n</code></pre> <p>This SPARQL query create explicitly the STIX links in the knowledge graph. Here, we create a new inference via a simple query.</p> <ol> <li>Create a new Knowledge graph dataset \u201cSTIX inferences\u201d with this IRI: https://attack.mitre.org/inferences</li> </ol> <p>Tip</p> <p>Separate always the facts extracted of raw data and the inferences calculate with other graphs. So, you can recalculate your inferences without rebuild all knowledge graph.</p> <ol> <li>Split the workflow in two workflows:</li> </ol> <ul> <li> <p>\u201cTransform all STIX data to RDF\u201d to calculate the inferences after RDF triples         </p> </li> <li> <p>\u201cAssemble the global knowledge graph\u201d, it will import all the graphs of projects         </p> </li> </ul> <ol> <li> <p>Create a new workflow \u201cMITRE ATT&amp;CK\u00ae workflow\u201d where you will insert the other workflows, like that:</p> <p></p> </li> </ol> <p>Success</p> <p>You can now navigate in your first knowledge graph: </p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#reconcile-automatically-the-stix-concepts-via-the-linking-tasks","title":"Reconcile automatically the STIX concepts via the Linking tasks","text":"<p>The \u201cLinking task\u201d is very useful to reconcile the instance of concepts in your graphs when their labels are inserted manually with some light differences. For example, you can reconcile the tool, malware, etc of different STIX documents.</p> <ol> <li> <p>Read the documentation of \u201cLinking task\u201d</p> </li> <li> <p>Use the json of STIX report of Mandiant\u2019s APT1 Report to reconcile the STIX tools in this report and the tools in the Mitre knowledge graph with your transformer STIX and a Linking task.</p> </li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#add-the-capec-dataset","title":"Add the CAPEC dataset","text":"<p>The Common Attack Pattern Enumeration and Classification (CAPEC\u2122) effort provides a publicly available catalog of common attack patterns that helps users understand how adversaries exploit weaknesses in applications and other cyber-enabled capabilities.</p> <ul> <li>Dataset: https://github.com/mitre/cti/blob/master/capec/2.1/stix-capec.json</li> <li>The CAPEC \u201contology\u201d: https://github.com/mitre/cti/blob/master/USAGE-CAPEC.md</li> </ul> <ol> <li>Import the CAPEC dataset in Corporate Memory</li> <li>Create the named graph of CAPEC</li> <li>In the workflows of MITRE ATT&amp;CK, generate also the CAPEC dataset</li> <li>Modify the transformer to support the references to CAPEC dataset from MITRE datasets.</li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#conclusion","title":"Conclusion","text":"<p>STIX uses JSON syntax and can therefore be converted to RDF via Corporate Memory. Here, we have only extracted a few useful fields for our use case but if you want to import all the data, you will need to import the other properties from STIX 2.1, the extended properties in your Mitre datasets and convert the other STIX relationships to RDF statements (like in the exercice \u201cCreate inferences\u201d).</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-STIX-2.1-data-of-mitre-attack/#ressources","title":"Ressources","text":"<ul> <li>RDF schemas (Model, pattern, etc)</li> <li>Archive of CMEM project</li> </ul> <p>Tutorial: how to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)</p> <p>Next chapter: Build a Knowledge Graph of compromise rules, like Hayabusa and Sigma rules</p> <p>Previous chapter: Specify the dashboards before the RDF models</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/","title":"Build the Knowledge Graph from indicators of compromise rules, like Hayabusa and Sigma rules","text":""},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/#introduction","title":"Introduction","text":"<p>There are a lot of sources to download the indicators of compromise rules to detect a possible future incident.</p> <p>There are rules for Host-based intrusion detection systems (HIDS) with Hayabusa/Sigma, for example, and Network intrusion detection systems (NIDS) with Suricata/Zeek for example.</p> <p>Here, we are working with the Hayabusa/Sigma rules available via GitHub:</p> <ul> <li>https://github.com/Yamato-Security/hayabusa-rules</li> <li>https://github.com/SigmaHQ/sigma</li> </ul> <p>The problem of interoperability, here, is the YAML format of files, their random position in their folders in their Github projets. Moreover, the same rule can exist in different projects but in this tutorial, we will not fix this problem and we consider the  IRI rule is their Web address. In Corporate Memory, we would fix that with the Linked Tool, we will study this tool in a next part of this tutorial.</p> <p>To build this knowledge graph of rules, we need to:</p> <ol> <li>Create a JSON dataset with all rules</li> <li>Build the transformer of a JSON rule to RDF</li> <li>Build a workflow to insert each rule in an unique knowledge graph</li> <li>Use this workflow in CMEMC to import automatically all rules</li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/#import-the-json-datasets","title":"Import the JSON datasets","text":"<p>The YAML syntax is used to define each rule and there is one file by rule.</p> <p>Corporate Memory doesn\u2019t support YAML (for the moment) but you can convert the files in JSON with this bash where you need to install git and yq.</p> <p>Moreover, we use yq to add the field <code>rulePath</code> in each file with their paths in their repositories to have the possibility to rebuild their positions on the Web and so allowing the analyst to click directly on this link to read the details and may be, modify this rule.</p> <p>At the end of this bash, you will have a tree of JSON files and we will apply a workflow on each file.</p> <p>Tip</p> <p>Don\u2019t forget to replace the final folder before using this bash.</p> <pre><code>#!/bin/bash -x\n\noutput_dir_rules=/home/karima/datasets/rules\n\nmkdir -p  ${output_dir_rules}\ncd ${output_dir_rules}\ngit clone --depth 1 https://github.com/Yamato-Security/hayabusa-rules\ngit clone --depth 1 https://github.com/SigmaHQ/sigma\n\nfor file in $(find . -name '*.yml'); do\n    [ -f \"$file\" ] || break\n    yq \".rulePath = \\\"${file}\\\"\" -o=json  $file &gt; ${file}.json\n\ndone\n</code></pre> <p>We can test this script:</p> <pre><code>cd ~/git/tutorial-how-to-link-ids-to-osint/docs/build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma\nchmod +x importRules.sh\n./importRules.sh\n</code></pre> <p>For example, the file proc_creation_win_bcdedit_boot_conf_tamper.yml will become this JSON file:</p> <pre><code>{\n  \"title\": \"Boot Configuration Tampering Via Bcdedit.EXE\",\n  \"id\": \"1444443e-6757-43e4-9ea4-c8fc705f79a2\",\n  \"status\": \"stable\",\n  \"description\": \"Detects the use of the bcdedit command to tamper with the boot configuration data. This technique is often times used by malware or attackers as a destructive way before launching ransomware.\",\n  \"references\": [\n    \"https://github.com/redcanaryco/atomic-red-team/blob/f339e7da7d05f6057fdfcdd3742bfcf365fee2a9/atomics/T1490/T1490.md\",\n    \"https://eqllib.readthedocs.io/en/latest/analytics/c4732632-9c1d-4980-9fa8-1d98c93f918e.html\"\n  ],\n  ...\n  \"tags\": [\n    \"attack.impact\",\n    \"attack.t1490\"\n  ],\n  ...\n  \"level\": \"high\",\n  \"rulePath\": \"./sigma/rules/windows/process_creation/proc_creation_win_bcdedit_boot_conf_tamper.yml\"\n}\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/#create-the-knowledge-graph","title":"Create the knowledge graph","text":"<p>The collected rules are from Sigma and Hayabusa repositories. Hayabusa \u201care trying to make this rules as close to sigma rules as possible\u201d. In your use case, we need properties defined by Sigma and which also exist in Hayabusa rules. The day where there will be a official RDF vocabulary to define a rule, we will use it. Waiting, your minimal vocabulary is \u201cdefined\u201d here: https://github.com/SigmaHQ/sigma-specification/blob/main/specification/sigma-rules-specification.md. We use this address for the prefix of your RDF vocabulary for your use case.</p> <p>The filename of the same rule between repositories does not change. So, we are making the IRI of rules with their filename and a arbitrary IRI, like \u201chttp://example.com/rule/\u201d. However, we want to give the possibility to open the original YAML rule directly via SPLUNK, so we add the property <code>rdfs:isDefinedBy</code> to associate the rule Web URLs to a rule. We will not use the guid id or Web address of the rule in its IRI because rules are often duplicate between the repositories and the filename and the title seem to be the used IDs of rules in Splunk and not the guid id.</p> <p>This new transformer are building the following RDF model for your use case:</p> <p></p> <ol> <li> <p>Create a new project to build the knowledge graph of \u201cRules Hayabusa Sigma\u201d</p> </li> <li> <p>In this project, create a RDF dataset \u201cRules Hayabusa Sigma\u201d in Corporate Memory for all rules with the named graph: <code>http://example.com/rule</code></p> </li> <li> <p>Create a JSON dataset \u201cRule example (JSON)\u201d in Corporate Memory with one example of rule:</p> <p></p> </li> <li> <p>Create the prefix of your vocabulary:</p> <pre><code>prefix ctis: &lt;https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#&gt;\n</code></pre> <p></p> </li> <li> <p>Create the transformer for \u201cSIGMA Hayabusa rule\u201d to build this RDF model.</p> </li> </ol> <p>Rule object:</p> <ul> <li> <p>type: <code>ctis:Rule</code></p> </li> <li> <p>IRI: concatenation of \u201chttp://example.com/rule/\u201d with the result of this regular expression <code>^.*?([^\\/]*)$</code> on the rule path</p> </li> </ul> <p></p> <ul> <li>property <code>ctis:filename</code> with the result of this regular expression <code>^.*?([^\\/]*)$</code> on the value path <code>rulePath</code></li> <li>property <code>rdfs:label</code> with the value path <code>title</code></li> <li>property <code>rdfs:comment</code> with the value path <code>description</code></li> <li>property <code>rdfs:seeAlso</code> with the value path <code>references</code></li> <li>property <code>ctis:mitreAttackTechniqueId</code> is building with this formula with the value path <code>tags</code><ul> <li>Filter by regex: <code>^attack\\.t\\d+$</code></li> <li>Regex replace <code>attack\\.t</code> by <code>T</code></li> </ul> </li> </ul> <p></p> <ul> <li>property <code>rdfs:isDefinedBy</code> on the value path <code>rulePath</code> is building with this formula to link the rules to their Web addresses.<ul> <li>Add two \u201cRegex replace\u201d<ul> <li>replace <code>\\./hayabusa-rules/</code> by <code>https://github.com/Yamato-Security/hayabusa-rules/blob/main/</code></li> <li>replace <code>\\./sigma/</code> by <code>https://github.com/SigmaHQ/sigma/blob/master/</code></li> </ul> </li> </ul> </li> </ul> <p></p> <p>So the rulepath <code>./sigma/rules/windows/process_creation/proc_creation_win_bcdedit_boot_conf_tamper.yml</code> becomes the link <code>https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_bcdedit_boot_conf_tamper.yml</code> and <code>./hayabusa-rules/hayabusa/sysmon/Sysmon_15_Info_ADS-Created.yml</code>becomes <code>https://github.com/Yamato-Security/hayabusa-rules/blob/main/hayabusa/sysmon/Sysmon_11_Med_FileCreated_RuleAlert.yml</code></p> <p>Tips</p> <p>To test your transformer, you can use the tab \u201cTransform execution\u201d. Here, the knowledge graph will not be cleared after each workflow or execution to test your transformer because the option \u201cclear graph before workflow\u201d is disabled. However during the steps to build this transformer, you can enable tempory this option to see and test the final transformer. You need only to disable this option when your transformer is finished.</p> <p>Success</p> <p>Your example of rule exists now in your knowledge graph:  </p> <ol> <li> <p>Make the workflow \u201cImport rules\u201d with one input</p> <p></p> </li> </ol> <p>And don\u2019t forget to allow the replacement of JSON dataset because it allows to replace this specific JSON by all other rules during the execution of this worflow.</p> <p></p> <p></p> <ol> <li> <p>Copy the workflow ID</p> <p></p> </li> </ol> <p>Success</p> <p>In this example the ID of workflow is <code>RulesHayabusaSigma_671e1f43d94bbc36:Importrules_6ccbc14b656c75c9</code></p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/#apply-the-worflow-to-all-files","title":"Apply the worflow to all files","text":"<p>We modify the first bash where we add the line to clear the knowledge graph before importing all rules via our worflow where we subtitute the JSON dataset in input by the rules\u2019 files.</p> <p>Tip</p> <p>Don\u2019t forget to replace the worflow ID and the final folder before using this bash.</p> <p>Tip</p> <p>CMEMC config file need to be correctly configurated before to execute this bash, like in the previous tutorial.</p> <p>For example:</p> <p><pre><code>[johndo.eccenca.my]\nCMEM_BASE_URI=https://johndo.eccenca.my/\nOAUTH_GRANT_TYPE=password\nOAUTH_CLIENT_ID=cmemc\nOAUTH_USER=johndo@example.com\nOAUTH_PASSWORD=XXXXXXXXX\n</code></pre> You need to replace \u201cjohndo\u201d by other thing, \u201cjohndo@example.com\u201d by your login (email) in the sandbox and XXXXXXXXX by your password. Save the file (with VI, :wq).</p> <p>Don\u2019t forget to specify the config by default to use by CMEMC.</p> <pre><code>export CMEMC_CONNECTION=johndo.eccenca.my\n</code></pre> <pre><code>#!/bin/bash -x\n\noutput_dir_rules=/home/karima/datasets/rules\n\nmkdir -p  ${output_dir_rules}\ncd ${output_dir_rules}\ngit clone --depth 1 https://github.com/Yamato-Security/hayabusa-rules\ngit clone --depth 1 https://github.com/SigmaHQ/sigma\n\nfor file in $(find . -name '*.yml'); do\n    [ -f \"$file\" ] || break\n yq \".rulePath = \\\"${file}\\\"\" -o=json  $file &gt; ${file}.json\ndone\n\ncmemc graph delete http://example.com/rule\n\nfor file in $(find . -name '*.json'); do\n    [ -f \"$file\" ] || break\n    cmemc workflow io  RulesHayabusaSigma_671e1f43d94bbc36:Importrules_6ccbc14b656c75c9 -i ${file}\ndone\n</code></pre> <p>We can test this script:</p> <pre><code>./importRules2.sh\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/#conclusion","title":"Conclusion","text":"<p>Here, we learnt how to generate a knowledge graph with files in input with Corporate Memory to prepare the worflow and cmemc to execute this worklow on all files.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/lift-data-from-YAML-data-of-hayabusa-sigma/#ressources","title":"Ressources","text":"<ul> <li>RDF schemas (Model, pattern, etc)</li> <li>script 1</li> <li>script 2</li> </ul> <p>Tutorial: how to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)</p> <p>Next chapter: Link IDS event to a knowledge graph in dashboards via queries</p> <p>Previous chapter: Build a Knowledge Graph of MITRE ATT&amp;CK\u00ae datasets</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/","title":"Link IDS event to a knowledge graph in dashboards via SPARQL queries","text":""},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/#introduction","title":"Introduction","text":"<p>In this tutorial, we are using the Linked Data App for Splunk. This app contains the SPARQL command necessary to dasboards to help the analysts to understand the Hayabusa and Sigma alerts before searching manually in the data via a SPL (Search Processing Language) query in Splunk.</p> <p>In the demo of this app in the video 1, the user selects the indexes of his investigation and select an alert message to open its sources on the Web before searching manually via the Splunk interfaces. Splunk, automatically, refreshes the SPARQL queries in the dashboard after each interaction of user.</p> <p></p> <p>Video 1: Splunk dashboards of the Linked Data App</p> <p>In this tutorial, we learn to:</p> <ol> <li>Install the \u201cLinked Data App\u201d for this tutorial</li> <li>Configure the SPARQL endpoint of your sandbox</li> <li>Add other SPARQL endpoints</li> <li>An example of dashboard with your private knowledge graphs</li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/#install-the-linked-data-app-in-splunk-for-this-tutorial","title":"Install the \u201cLinked Data App\u201d in Splunk for this tutorial","text":"<p>The \u201cLinked Data App\u201d extends Splunk Search Processing Language (SPL) to support the SPARQL protocol.</p> <ol> <li> <p>Download the tar.gz: Linked Data App</p> </li> <li> <p>Open the App window in Splunk via the icon \u201ctools\u201d (see figure 1)</p> </li> </ol> <p></p> <p>Figure 1: In the top of the list of installed Splunk apps, you need to click on the icon \u201ctools\u201d to open the window to manage your apps</p> <ol> <li>Upload the app in Splunk (see video 2)</li> </ol> <p></p> <p>Video 2: When the tar.gz of the \u201cLinked Data App\u201d, you can upload it manually directly in Splunk.</p> <p>Tip</p> <p>The dependencies of this app are already in its tar.gz but you can update the dependencies yourself, via the lines:</p> <pre><code>cd eccenca_commands\npip install sparqlwrapper -t bin --upgrade\npip install splunk-sdk  -t bin --upgrade\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/#configure-the-sparql-endpoint-of-your-sandbox","title":"Configure the SPARQL endpoint of your sandbox","text":"<p>When you made your knowledge graphs in the previous pages, you have used the eccenca sandbox. To use these structured data in Splunk dashboards, you need to connect the SPARQL endpoint of your sandbox to Splunk via the \u201cLinked Data App\u201d.</p> <p>After the installation, this app is in the folder <code>etc/apps/eccenca_commands</code> of Splunk directory.</p> <ol> <li>Create the file <code>settings.conf</code>:</li> </ol> <pre><code>cd etc/apps/eccenca_commands\ncp default/settings_template_sandbox.conf default/settings.conf\nvi default/settings.conf\n</code></pre> <p>You have an example of configuration for the eccenca sandbox SPARQL endpoint in the file <code>default/settings_template_sandbox.conf</code> (and another example via Oauth2 secret ID in the file <code>default/settings_template_oauth_secret_id.conf</code>).</p> <ol> <li>Insert your credentials in the the file <code>settings.conf</code>, ie. replace <code>johndo</code> by the name of your sandbox (endpointRead, token_endpoint), <code>johndo@example.com</code> by your email and <code>XXXXXXXXX</code> by your password. Don\u2019t change the parameters OAUTH_CLIENT_ID and OAUTH_GRANT_TYPE.</li> </ol> <pre><code>[config:default]\n# replace johndo.eccenca.my by your sandbox\nendpointRead=https://johndo.eccenca.my/dataplatform/proxy/default/sparql\naccessMethod=oauth2\n# replace johndo.eccenca.my by your sandbox\ntoken_endpoint=https://johndo.eccenca.my/auth/realms/cmem/protocol/openid-connect/token\nOAUTH_CLIENT_ID=cmemc\nOAUTH_GRANT_TYPE=password\n# replace johndo@example.com  by your email\nOAUTH_USER=johndo@example.com\n# insert your password\nOAUTH_PASSWORD=XXXXXXXXX\n</code></pre> <ol> <li> <p>Restart after your Splunk instance (via the administration windows)</p> </li> <li> <p>Test your sandbox endpoint in Splunk with this SPL query:</p> </li> </ol> <pre><code>| sparql\n    query=\"\n        select *\n        where {\n            ?s ?p ?v\n        }\n        LIMIT 10\n    \"\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/#add-other-sparql-endpoints","title":"Add other SPARQL endpoints","text":"<p>To add a new SPARQL endpoint, add these two lines in your file <code>settings.conf</code> where your need to replace here <code>wikidata</code> by the name of new public endpoint and <code>https://query.wikidata.org/sparql</code> by the url of endpoint.</p> <pre><code>[config:wikidata]\nendpointRead=https://query.wikidata.org/sparql\n</code></pre> <p>Restart after your Splunk instance and request in Splunk your endpoint with the parameter config (here wikidata) to select the config to use in the file <code>settings.conf</code>:</p> <pre><code>| sparql\n    config=\"wikidata\"\n    query=\"\n        select *\n        where {\n            ?s ?p ?v\n        }\n        LIMIT 10\n    \"\n</code></pre> <p>Tip</p> <p>You can clone the dashboards of this app to see and modify the SPARQL examples with Wikidata.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/#an-example-of-dashboard-with-your-private-knowledge-graphs","title":"An example of dashboard with your private knowledge graphs","text":"<p>To work, our example of dashboard need to have Splunk indexes of IoCs. We cannot share our indexes but you can modify our example with your own SPL queries according to your Splunk indexes.</p> <p></p> <p>Figure 2: Dashboard with SPARQL commands and the script <code>table_html.js</code> to print the HTML and to open Web pages of alerts\u2019 references</p> <p>Tip</p> <p>The SPARQL command respects the logic of SPLUNK to see all metadata of the SPARQL response (types of literal, etc). However, in a dashboard via a static table panel, you want probably to see only the columns in the header of your SPARQL query. In the XML element <code>table</code>, you can select the columns in output via the XML element <code>fields</code>:</p> <pre><code>    &lt;fields&gt;[\"Source\",\"Description\",\"MitreID\"]&lt;/fields&gt;\n</code></pre> <p>Of course, you can do it also via the SPL query.</p> <p>You can see the XML of dashboard in the figure 2:</p> <pre><code>&lt;form version=\"1.1\" script=\"eccenca_commands:table_html.js\"&gt;\n&lt;label&gt;SIGMA/Hayabusa with knowledge graph&lt;/label&gt;\n  &lt;init&gt;\n    &lt;unset token=\"help\"&gt;&lt;/unset&gt;\n  &lt;/init&gt;\n  &lt;fieldset submitButton=\"false\" autoRun=\"true\"&gt;\n    &lt;input type=\"multiselect\" token=\"selected_index\" searchWhenChanged=\"true\"&gt;\n      &lt;label&gt;index&lt;/label&gt;\n      &lt;valuePrefix&gt;index=\"&lt;/valuePrefix&gt;\n      &lt;valueSuffix&gt;\"&lt;/valueSuffix&gt;\n      &lt;delimiter&gt; OR &lt;/delimiter&gt;\n      &lt;fieldForLabel&gt;index&lt;/fieldForLabel&gt;\n      &lt;fieldForValue&gt;index&lt;/fieldForValue&gt;\n      &lt;search&gt;\n        &lt;query&gt;| eventcount summarize=false index=*\n| search NOT index IN (\"history\", \"cim_modactions\", \"summary\")\n| dedup index\n| fields index&lt;/query&gt;\n        &lt;earliest&gt;0&lt;/earliest&gt;\n        &lt;latest&gt;&lt;/latest&gt;\n      &lt;/search&gt;\n      &lt;choice value=\"*\"&gt;all&lt;/choice&gt;\n      &lt;default&gt;*&lt;/default&gt;\n    &lt;/input&gt;\n    &lt;input type=\"multiselect\" token=\"level\"&gt;\n      &lt;label&gt;Level&lt;/label&gt;\n      &lt;choice value=\"critical\"&gt;critical&lt;/choice&gt;\n      &lt;choice value=\"high\"&gt;high&lt;/choice&gt;\n      &lt;choice value=\"medium\"&gt;medium&lt;/choice&gt;\n      &lt;choice value=\"low\"&gt;low&lt;/choice&gt;\n      &lt;default&gt;critical,medium,high,low&lt;/default&gt;\n      &lt;valuePrefix&gt;Level=\"&lt;/valuePrefix&gt;\n      &lt;valueSuffix&gt;\"&lt;/valueSuffix&gt;\n      &lt;delimiter&gt; OR &lt;/delimiter&gt;\n    &lt;/input&gt;\n  &lt;/fieldset&gt;\n  &lt;row&gt;\n    &lt;panel&gt;\n      &lt;single&gt;\n        &lt;search&gt;\n          &lt;query&gt;| tstats count where $selected_index$ ld_metadata.ld_source_type=hayabusa Level=low&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"colorMode\"&gt;block&lt;/option&gt;\n        &lt;option name=\"rangeColors\"&gt;[\"0x53a051\",\"0xdc4e41\"]&lt;/option&gt;\n        &lt;option name=\"rangeValues\"&gt;[1]&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;option name=\"underLabel\"&gt;LOW&lt;/option&gt;\n        &lt;option name=\"useColors\"&gt;1&lt;/option&gt;\n      &lt;/single&gt;\n    &lt;/panel&gt;\n    &lt;panel&gt;\n      &lt;single&gt;\n        &lt;search&gt;\n          &lt;query&gt;| tstats count where $selected_index$ ld_metadata.ld_source_type=hayabusa Level=medium&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"colorMode\"&gt;block&lt;/option&gt;\n        &lt;option name=\"rangeColors\"&gt;[\"0x53a051\",\"0xdc4e41\"]&lt;/option&gt;\n        &lt;option name=\"rangeValues\"&gt;[1]&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;option name=\"underLabel\"&gt;MEDIUM&lt;/option&gt;\n        &lt;option name=\"useColors\"&gt;1&lt;/option&gt;\n      &lt;/single&gt;\n    &lt;/panel&gt;\n    &lt;panel&gt;\n      &lt;single&gt;\n        &lt;search&gt;\n          &lt;query&gt;| tstats count where $selected_index$ ld_metadata.ld_source_type=hayabusa Level=high&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"colorMode\"&gt;block&lt;/option&gt;\n        &lt;option name=\"rangeColors\"&gt;[\"0x53a051\",\"0xdc4e41\"]&lt;/option&gt;\n        &lt;option name=\"rangeValues\"&gt;[1]&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;option name=\"underLabel\"&gt;HIGH&lt;/option&gt;\n        &lt;option name=\"useColors\"&gt;1&lt;/option&gt;\n      &lt;/single&gt;\n    &lt;/panel&gt;\n    &lt;panel&gt;\n      &lt;single&gt;\n        &lt;search&gt;\n          &lt;query&gt;| tstats count where $selected_index$ ld_metadata.ld_source_type=hayabusa Level=critical&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"colorMode\"&gt;block&lt;/option&gt;\n        &lt;option name=\"rangeColors\"&gt;[\"0x53a051\",\"0xdc4e41\"]&lt;/option&gt;\n        &lt;option name=\"rangeValues\"&gt;[1]&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;option name=\"underLabel\"&gt;CRITICAL&lt;/option&gt;\n        &lt;option name=\"useColors\"&gt;1&lt;/option&gt;\n      &lt;/single&gt;\n    &lt;/panel&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;panel&gt;\n      &lt;table&gt;\n        &lt;search&gt;\n          &lt;query&gt;| tstats count where $selected_index$ ld_metadata.ld_source_type=hayabusa Level!=info $level$ by RuleTitle\n| rename RuleTitle as \"Rule name\"\n| sort - count&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"drilldown\"&gt;cell&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;drilldown&gt;\n          &lt;set token=\"selected_rule\"&gt;$click.value$&lt;/set&gt;\n        &lt;/drilldown&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;panel&gt;\n      &lt;title&gt;Rule's sources&lt;/title&gt;\n      &lt;table&gt;\n        &lt;search&gt;\n          &lt;query&gt;| sparql\nquery=\"prefix ctis: &amp;lt;https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#&amp;gt;\nprefix rdf:  &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt;\nprefix rdfs: &amp;lt;http://www.w3.org/2000/01/rdf-schema#&amp;gt;\nprefix xsd:  &amp;lt;http://www.w3.org/2001/XMLSchema#&amp;gt;\n\nSELECT DISTINCT (STRBEFORE(STRAFTER(STR(?link),\\\"https://github.com/\\\"),\\\"/\\\") as ?Source) (?comment as ?Description) ?link  (?mitreID as ?MitreID)\nFROM &amp;lt;http://example.com/rule&amp;gt;\nWHERE {\n  VALUES ?title { \\\"$selected_rule$\\\" }\n\n        ?ruleHayabusa a ctis:Rule ;\n            rdfs:label ?title ;\n            rdfs:comment ?comment ;\n            rdfs:seeAlso ?referenceLink;\n            rdfs:isDefinedBy ?link ;\n            ctis:filename ?filename .\n      OPTIONAL {\n       ?ruleHayabusa ctis:mitreAttackTechniqueId ?mitreID .\n      }\n}\"&lt;/query&gt;\n          &lt;earliest&gt;-24h@h&lt;/earliest&gt;\n          &lt;latest&gt;now&lt;/latest&gt;\n          &lt;done&gt;\n            &lt;set token=\"MitreID\"&gt;$result.MitreID$&lt;/set&gt;\n          &lt;/done&gt;\n        &lt;/search&gt;\n        &lt;option name=\"drilldown\"&gt;row&lt;/option&gt;\n        &lt;drilldown&gt;\n          &lt;link target=\"_blank\"&gt;$row.link|n$&lt;/link&gt;\n        &lt;/drilldown&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;option name=\"wrap\"&gt;true&lt;/option&gt;\n        &lt;fields&gt;[\"Source\",\"Description\",\"MitreID\"]&lt;/fields&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n    &lt;panel&gt;\n      &lt;title&gt;Rule's references&lt;/title&gt;\n      &lt;table&gt;\n        &lt;search&gt;\n          &lt;query&gt;| sparql\nquery=\"prefix ctis: &amp;lt;https://github.com/SigmaHQ/sigma-specification/blob/main/Sigma_specification.md#&amp;gt;\nprefix rdf:  &amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&amp;gt;\nprefix rdfs: &amp;lt;http://www.w3.org/2000/01/rdf-schema#&amp;gt;\nprefix xsd:  &amp;lt;http://www.w3.org/2001/XMLSchema#&amp;gt;\n\nSELECT DISTINCT (GROUP_CONCAT(STRBEFORE(STRAFTER(STR(?link),\\\"https://github.com/\\\"),\\\"/\\\"); separator=', ') as ?Source)  (?referenceLink as ?Reference)\nFROM &amp;lt;http://example.com/rule&amp;gt;\nWHERE {\n  VALUES ?title { \\\"$selected_rule$\\\" }\n\n        ?ruleHayabusa a ctis:Rule ;\n            rdfs:label ?title ;\n            rdfs:comment ?comment ;\n            rdfs:seeAlso ?referenceLink;\n            rdfs:isDefinedBy ?link ;\n            ctis:filename ?filename .\n}\nGROUP BY ?referenceLink\"&lt;/query&gt;\n          &lt;earliest&gt;-24h@h&lt;/earliest&gt;\n          &lt;latest&gt;now&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"drilldown\"&gt;row&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;option name=\"wrap\"&gt;true&lt;/option&gt;\n        &lt;fields&gt;[\"Source\",\"Reference\"]&lt;/fields&gt;\n        &lt;drilldown&gt;\n          &lt;link target=\"_blank\"&gt;$row.Reference|n$&lt;/link&gt;\n        &lt;/drilldown&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;panel&gt;\n      &lt;table id=\"column_html__1\"&gt;\n        &lt;title&gt;Mitre description in relation with this rule&lt;/title&gt;\n        &lt;search&gt;\n          &lt;query&gt;| sparql\nquery=\"\nPREFIX rdfs: &amp;lt;http://www.w3.org/2000/01/rdf-schema#&amp;gt;\nPREFIX ctia: &amp;lt;https://github.com/mitre/cti/blob/master/USAGE.md#&amp;gt;\n\nSELECT\n(CONCAT (\\\"&amp;lt;b&amp;gt;\\\",?title,\\\"&amp;lt;/b&amp;gt;\\\",\\\"&amp;lt;br/&amp;gt;\\\",?description,\\\"&amp;lt;br/&amp;gt;&amp;lt;br/&amp;gt;\\\",GROUP_CONCAT( distinct ?link; separator=\\\"&amp;lt;br/&amp;gt;\\\")) as ?html)\nFROM &amp;lt;https://github.com/mitre-attack/attack-stix-data/raw/master/enterprise-attack/enterprise-attack.json&amp;gt;\nWHERE  {\n\n  {\n    ?resource ctia:type ctia:course-of-action .\n  } union {\n    ?resource ctia:type ctia:attack-pattern .\n  }\n\n  ?resource rdfs:label ?title ;\n            ctia:description ?description ;\n            ctia:external_references ?mitre_url .\n\n  ?mitre_url ctia:external_id  \\\"$MitreID$\\\" ;\n             ctia:source_name  \\\"mitre-attack\\\" .\n\n  OPTIONAL {\n    ?resource ctia:external_references [\n        ctia:url ?reference_url ;\n        ctia:source_name ?reference_label ;\n        ctia:description ?reference_description\n    ] .\n    BIND( CONCAT(\\\"&amp;lt;a href='\\\",STR(?reference_url),\\\"'&amp;gt;\\\",?reference_label,\\\": \\\",?reference_description ,\\\"&amp;lt;/a&amp;gt;\\\") as ?link)\n  }\n\n}\nGROUP BY ?title ?description\"&lt;/query&gt;\n          &lt;earliest&gt;-24h@h&lt;/earliest&gt;\n          &lt;latest&gt;now&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"drilldown\"&gt;none&lt;/option&gt;\n        &lt;option name=\"refresh.display\"&gt;progressbar&lt;/option&gt;\n        &lt;fields&gt;[\"html\"]&lt;/fields&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;panel depends=\"$selected_rule$\"&gt;\n      &lt;table&gt;\n        &lt;search&gt;\n          &lt;query&gt;$selected_index$ ld_metadata.ld_source_type=hayabusa RuleTitle=\"$selected_rule$\"\n| strcat Channel \" type \" EventID   event_source\n| table RecordID, Timestamp, event_source, Computer, Details&lt;/query&gt;\n          &lt;earliest&gt;0&lt;/earliest&gt;\n          &lt;latest&gt;&lt;/latest&gt;\n        &lt;/search&gt;\n        &lt;option name=\"drilldown\"&gt;cell&lt;/option&gt;\n        &lt;drilldown&gt;\n          &lt;link target=\"_blank\"&gt;search?q=$selected_index$%20ld_metadata.ld_source_type%3Devtx%20EventRecordID%3D$click.value$&amp;amp;earliest=0&amp;amp;latest=&lt;/link&gt;\n        &lt;/drilldown&gt;\n      &lt;/table&gt;\n    &lt;/panel&gt;\n  &lt;/row&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG/#conclusion","title":"Conclusion","text":"<p>In the \u201cLinked Data App\u201d, we implemented a simple SPARQL command to request the Linked Open Data and also your private knowledge graphs.</p> <p>The Linked Data technologies give the opportunity to push the Open-Source INTelligence (OSINT) in the Linked Open Data and it will simplify the work of analysts via their SIEM, like Splunk or other.</p> <p>In the previous pages of this tutorial, you are able to create new classes and new properties in your knowledge graphs for Mitre Attack or IoC rules when you feel the need to do so. This natural behavior in the concept \u201cEverything as code\u201d creates a natural entropy of the global ontology of cyber domain. The first victim of this entropy is all the analysts. This problem can be resolve, if the analysts work together to build their cyber ontologies with the Linked Data technology, like Wikipedia contributors made Wikidata. It is only a matter of willpower and skill that you have now obtained through this tutorial.</p> <p>In the next page, we are using advanced tools in Coporate Memory to \u201cAccelerate Cyber Threat Hunting\u201d.</p> <p>Tutorial: how to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)</p> <p>Next chapter: Link IDS event to a knowledge graph in dashboards via inferences (for the advanced users of Corporate Memory)</p> <p>Previous chapter: Build a Knowledge Graph from indicators of compromise rules, like Hayabusa and Sigma rules</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/","title":"Link IDS event to a knowledge graph via advanced tools","text":""},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#introduction","title":"Introduction","text":"<p>In this tutorial, we are using the Splunk app \u201cInvestigate lateral movements with a knowledge graph\u201d, like example. This app contains several dasboards to help the analysts to navigate in the Hayabusa or Sigma alerts before searching in the Suricata and Sysmon alerts. We hope this method can \u201cAccelerate Cyber Threat Hunting\u201d.</p> <p>In the demo of this Splunk app via the video 1, the user selects the data about one investigation via Splunk and generate a bash script to export these data via the Splunk API in tempory graphs in Corporate Memory for each investigation.</p> <p></p> <p>Video 1: Splunk dashboards of the Splunk app \u201cInvestigate lateral movements with a knowledge graph\u201d</p> <p>In this page, we are showing how, we :</p> <ol> <li>Install the Splunk app \u201cInvestigate lateral movements with a knowledge graph\u201d</li> <li>\u201cAccelerate Cyber Threat Hunting\u201d with dashboards</li> <li>Manage the graphs of your application</li> <li>Export data in Splunk to Corporate Memory</li> <li>Reasoning with data in Corporate Memory via Splunk</li> <li>Reconcile automatically the complex data via Linking tasks</li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#install-the-splunk-app-investigate-lateral-movements-with-a-knowledge-graph","title":"Install the Splunk app \u201cInvestigate lateral movements with a knowledge graph\u201d","text":"<p>This tutorial describes the method in the app \u201cInvestigate lateral movements with a knowledge graph\u201d. You can install it and modify the source code. The queries and workflows used in its dashboards are not shared because they are built according to Splunk indexes which are not shared also. But, you can modify the dashboards and build your queries and workflows in your sandbox.</p> <p>This app is not directly connected to your Corporate Memory instance. The custom REST endpoint in this app via the file \u201cInvestigation.py\u201d generates bash scripts for CMEMC to create or delete the tempory graphs of each investigation (and create/delete tempory folders). When the script is generated for one investigation, the script \u201ccreateInvestigation.sh\u201d or \u201cdeleteInvestigation.sh\u201d  via a cron service on your Splunk server calls it and CMEMC will use this configuration by default on this server to connect to your Corporate Memory instance.</p> <p>Position of these scripts in the folders of this app:</p> <pre><code>+---bin\n|       Investigation.py\n+---cmem\n|       createInvestigation.sh\n|       deleteInvestigation.sh\n</code></pre> <p>Moreover, a settings file is necessary to insert the credentials of Splunk (like token, IP, port) and to specify the folders to manage the files of investigations on the server, ie. the generated scripts and the exported raw data of Splunk. Read the file \u201cREADME.md\u201d for more information.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#accelerate-cyber-threat-hunting-with-dashboards","title":"\u201cAccelerate Cyber Threat Hunting\u201d with dashboards","text":"<p>In this Proof of Concept, we have implemented two types of investigation:</p> <ul> <li>high-level with the data of alerts of Zeek and Hayabusa/Sigma</li> <li>low-level with the data of Suricata and Sysmon</li> </ul> <p>For each investigation, an analyst selects and navigates in the data with two dashboards for each type of investigation:</p> <ul> <li>one dashboard (see figure 1) to select the data to transfer to knowledge graph: typeA_request.xml</li> <li>one dashboard (see figure 2,3,4) to navigate in the knowledge graph: typeA_dashboard.xml</li> </ul> <p></p> <p>Figure 1: Splunk dashboard to select the data before executing a high-level investigation</p> <p></p> <p>Figure 2: High-level investigation dashboard with the list of computers implicated in the incident</p> <p></p> <p>Figure 3: High-level investigation dashboard with the panel to select a specific period during an incident according to IoCs details</p> <p></p> <p>Figure 4: low-level investigation dashboard contains the command lines of Windows processus rised Suricata alerts during the period selected by the analyst in a high-level investigation dashboard.</p> <p>The idea is the analyst can do an investigation high-level without using a lot of ressources (a little graph) but when he want to see the suspicious processus on one computer in a specific period, he can ask an investigation low-level with a maximum of details.</p> <p>To follow the calculation of investigations and free memory when one investigation is closed, we developed another dashboard \u201cinvestigation_list.xml\u201d (see figure 5). This dashboard prints the status of investigations actually in the knowledge graph. Here, each investigation is saved in tempory graphs and the analyst can create and delete them directly in Splunk.</p> <p></p> <p>Figure 5: The dashboard \u201cinvestigation list\u201d shows all the tempory graph actually in the knowledge graph. The analyst can open an investigation, see the SPL query generated when he has created an investigation and delete it when he want.</p> <p>With these interfaces to manage and calculate different investigations with different levels of details, we imagined a first method to \u201cfollow lateral movements\u201d (see figure 6) in order to understand the objectives of the incident. We hope this PoC will \u201cAccelerate Cyber Threat Hunting\u201d.</p> <p></p> <p>Figure 6: Analyst can select a computer and a period to analyze the suspicious processus implicated in Suricata alerts. So, an analyst can follow the \u201clateral movements\u201d and see the command lines executed by these suspicious processus.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#manage-the-graphs-of-your-application","title":"Manage the graphs of your application","text":"<p>A knowledge graph in Corporate Memory is a set of named RDF graphs. Each named graph can be managed and each named graph can have missions very different. In this PoC, when a new investigation is created, the app creates in first the named graph \u201cinvestigation\u201d to link all the other tempory named graphs for this investigation with the property \u201cowl:imports\u201d. So, when an analyst want to delete an investigation, the app can find all the tempory graphs of this investigation and can delete all its tempory graphs simply. Moreover, the analyst can request with SPARQL all these tempory graphs via one named graph \u201cinvestigation\u201d (thanks to \u201cowl:imports\u201d), so the complexity of tempory graphs is invisible for the final user.</p> <p>An error of novices is to save all the triples of data in input and their inferences in the same graph. For each upload of facts, each calculated inference, etc, you need to save their triples in its own named graph because during the life of data (and the development), you want to restore easily one named graph without re-building all the other graphs.</p> <p>This manner to manage the graphs has been applied in this app, so an analyst is able to write in his dashboard code the data to upload from Splunk to knowledge graph with the tokens \u201csource_x\u201d, for example:</p> <pre><code>  &lt;set token=\"source_1\"&gt;{\n  \"index\": $selected_index|s$,\n  \"search\": \"sauron_metadata.sauron_source_type=hayabusa Level!=info | table RuleTitle RuleFile | dedup RuleTitle RuleFile\",\n  \"workflowID\": \"b5deffdd-f4b9-4d1a-8ea0-9b3410d915e7_PoC21:investigation-hayabusa\"\n  }&lt;/set&gt;\n</code></pre> <p>This token contains a json object where:</p> <ul> <li>\u201cindex\u201d the list of splunk indexes of the SPL query</li> <li>\u201csearch\u201d the second part of the SPL query</li> <li>\u201cworkflowID\u201d the ID of workflow in Corporate Memory to convert the raw data of Splunk in RDF in a tempory graph</li> </ul> <p>An analyst can import as many sources as needed with several tokens, ie. \u201csource_1\u201d, \u201csource_2\u201d, etc.</p> <p>There are not consensus about the manner to calculate inferences on the RDF data in a knowledge graph. To simplify, one inference is a set of triples in a graph according to calculations with other triples in input. These calculations are possible only after all the sources are imported in the knowlege graph, ie. here, when all temporary graphs are created, the analyst can apply multiple calculations on these temporary graphs. Each calculation will create new triples in new tempory graphs always associated at the same investigation. We have created the token \u201cinferences\u201d to insert in the dashboard the json array of inferences to calculate when all data are imported from Splunk. For example:</p> <pre><code>&lt;set token=\"inferences\"&gt;\n  [\n    {\n        \"comment\": \"POC 2.1: inference 1: Resolve hostname in Zeek Notice with Zeek DNS\",\n        \"command\": \"query\",\n        \"queryIRI\": \"https://ns.eccenca.com/data/queries/d063d87e-9122-41a3-84e9-4a05c2d0766e\",\n        \"inferenceID\": \"1\"\n    },\n    {\n        \"comment\": \"Linking hostname to prepare to calculate computers\",\n        \"command\": \"workflow\",\n        \"workflowID\": \"b5deffdd-f4b9-4d1a-8ea0-9b3410d915e7_PoC21:WORFLOW_CONSOLIDATION_HOSTNAME_bebe8b4a7f975e90\"\n    },\n    {\n        \"comment\": \"POC 2.1: inference 2: Calculate computers\",\n        \"command\": \"query\",\n        \"queryIRI\": \"https://ns.eccenca.com/data/queries/10cd6a60-c5d4-444c-8a09-6dc63f51576f\",\n        \"inferenceID\": \"2\"\n    },\n    ...\n  ]\n&lt;/set&gt;\n</code></pre> <p>We use two manners to calculate new inferences:</p> <ul> <li>when the inference is simple to calculate with SPARQL, we use a SPARQL update query with parameters in Corporate Memory (like \u201cinferenceID\u201d to build the name of destination tempory graph). \u201cqueryIRI\u201d is the IRI of the query in the catalog of Corporate Memory.</li> <li>when the inference is complex to calculate, we use a workflow of Corporate Memory.</li> </ul> <p>With these tokens \u201csource_1\u201d, \u201csource_2\u201d, etc and \u201cinferences\u201d in the dashboard, the app can generate a bash script for CMEMC.</p> <p>We explain now how to insert these tokens in a dashboard.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#export-data-in-splunk-to-corporate-memory","title":"Export data in Splunk to Corporate Memory","text":"<p>In the previous chapter, you explain the role of token \u201csource_x\u201d to create a new tempory graph with Splunk data. This token contains SPL queries with other external tokens. To calculate correctly these SPL queries, you need to \u201cset\u201d the token \u201csource_x\u201d in the xml element <code>change</code> of last input component of these external tokens, like in this example with the external token \u201cselected_index\u201d:</p> <pre><code>    &lt;input type=\"multiselect\" token=\"selected_index\" searchWhenChanged=\"true\"&gt;\n      &lt;label&gt;index&lt;/label&gt;\n      &lt;valuePrefix&gt;index=\"&lt;/valuePrefix&gt;\n      &lt;valueSuffix&gt;\"&lt;/valueSuffix&gt;\n      &lt;delimiter&gt; OR &lt;/delimiter&gt;\n      &lt;fieldForLabel&gt;index&lt;/fieldForLabel&gt;\n      &lt;fieldForValue&gt;index&lt;/fieldForValue&gt;\n      &lt;search&gt;\n        &lt;query&gt;| eventcount summarize=false index=*\n| search NOT index IN (\"history\", \"cim_modactions\", \"summary\")\n| dedup index\n| fields index&lt;/query&gt;\n        &lt;earliest&gt;0&lt;/earliest&gt;\n        &lt;latest&gt;&lt;/latest&gt;\n      &lt;/search&gt;\n      &lt;choice value=\"*\"&gt;all&lt;/choice&gt;\n      &lt;default&gt;*&lt;/default&gt;\n      &lt;change&gt;\n          &lt;set token=\"source_1\"&gt;{\n          \"index\": $selected_index|s$,\n          \"search\": \"sauron_metadata.sauron_source_type=hayabusa Level!=info | table RuleTitle RuleFile | dedup RuleTitle RuleFile\",\n          \"workflowID\": \"b5deffdd-f4b9-4d1a-8ea0-9b3410d915e7_PoC21:investigation-hayabusa\"\n          }&lt;/set&gt;\n      &lt;/change&gt;\n    &lt;/input&gt;\n</code></pre> <p>Here the example is simple with one external token, but when you have several external tokens in the token \u201csource_x\u201d, there are often problems to generate a complet SPL query (to check the SPL query, you need to use the Javascript console via the Javascript \u201cinvestigation.js\u201d to read the final SPL queries in the tokens \u201csource_x\u201d).</p> <p>You can add in your dashboard several tokens \u201csource_1\u201d, \u201csource_2\u201d, \u201csource_3\u201d, etc. Each source generates a part of final bash script where a curl command requests the Splunk API and a CMEMC command executes the specified worflow via its ID in the token. With this method, the analyst is free to import any Splunk data and choose any CMEM workflow according to these data. The SPL queries are executed by the Splunk API (via curl), so the analyst is free to use all commands supported in Splunk to select the data. For example, with low-level investigation, these SPL queries contains also the selected period by the analyst to limit the quantity of data to import.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#reasoning-with-data-in-corporate-memory-via-splunk","title":"Reasoning with data in Corporate Memory via Splunk","text":"<p>When all the sources are imported in the knowlege graph, the app will use the token \u201cinferences\u201d to execute the last calculations to do on the knowledge graphs. Here, there is not SPL query to calculate so we \u201cset\u201d simply the token \u201cinferences\u201d in the xml element <code>init</code>:</p> <pre><code>&lt;init&gt;\n    &lt;set token=\"inferences\"&gt;\n      [\n...\n        {\n          \"comment\": \"Linking hostname to prepare to calculate computers\",\n         \"command\": \"workflow\",\n         \"workflowID\": \"b5deffdd-f4b9-4d1a-8ea0-9b3410d915e7_PoC21:WORFLOW_CONSOLIDATION_HOSTNAME_bebe8b4a7f975e90\"\n        },\n        {\n          \"comment\": \"POC 2.1: inference 2: Calculate computers\",\n         \"command\": \"query\",\n         \"queryIRI\": \"https://ns.eccenca.com/data/queries/10cd6a60-c5d4-444c-8a09-6dc63f51576f\",\n         \"inferenceID\": \"2\"\n        },\n...\n      ]\n    &lt;/set&gt;\n&lt;/init&gt;\n</code></pre> <p>If you want to reuse your own algorithm to calculate new inferences, you can develop your own plugin in Corporate Memory and call it in a workflow that you can call in a Splunk dashboard.</p>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#reconcile-automatically-the-complex-data-via-a-linking-task","title":"Reconcile automatically the complex data via a Linking task","text":"<p>In the previous chapter, you saw it is possible to call a workflow in a Splunk dashboard. A workflow is a set of components available in Corporate Memory, like transformer, tasks (SPARQL update query), etc.</p> <p>One component is very useful to reconcile the instance of concepts in your graphs like the concept of tools of different STIX documents or again, the computer according to their hostnames in the logs of systems and networks, etc. This component is the Linking task.</p> <p>To use it:</p> <ol> <li> <p>Read the documentation of \u201cLinking task\u201d to learn to use it</p> </li> <li> <p>Do the exercice \u201cReconcile automatically the STIX concepts via the Linking tasks\u201d in the page of tutorial about Mitre\u2019s datasets</p> </li> </ol>"},{"location":"build/tutorial-how-to-link-ids-to-osint/link-IDS-event-to-KG-via-cmem/#conclusion","title":"Conclusion","text":"<p>RDF is often considerated like too simple to manage the complexe knowledge but the reality is this simplicity is the core to manage all type of complexity. This abstraction is rarely simple for the novices but when you know use RDF properly with professional tools like Corporate Memory, you will make all type of applications, like applications in the cybersecurity.</p> <p>Often with this tutorial, the analysts said there is already in SIEM (like Splunk) a lot of tools to hunt via the data. It\u2019s truth. There are a lot of tools in the SIEM to help the analysts. The difference with Corporate Memory and the Linked Data technologies, YOU ARE FREE because you can calculate your own inferences on your data, you can connect your SIEM with your knowledge graphs and ofcourse, your knowledge graphs is INTEROPERABLE, ie. you can connect your structured data with other tools in your information systems. Moreover, Corporate Memory can support several databases, so you are able to choose the best RDF database according the size of your investigations and the type of calculations to do.</p> <p>Now, you know what is it possible to do simply an app in the CyberSecurity with the Linked Data technologies and Corporate Memory.</p> <p>Tutorial: how to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)</p> <p>Previous chapter: Link IDS event to a knowledge graph in dashboards via queries</p>"},{"location":"build/variables/","title":"Project and Global Build Variables","text":"","tags":["KnowledgeGraph","Variables"]},{"location":"build/variables/#introduction","title":"Introduction","text":"<p>Build variables are used to configure and customize build workflows and processes. These variables define various aspects of the integration tasks, such as the source target data formats, transformation rules, mapping definitions etc. The variables are not technically typed. They can be used in most Build configuration and input fields that take inputs of the following data types:</p> <ul> <li>simple text/string parameters (any string),</li> <li>integer parameters (any integer),</li> <li>and boolean values (<code>true</code>/<code>false</code>).</li> </ul> <p>Two kinds of variables can be defined:</p> Global variables <p>It is defined by the administrator in the configuration file at deployment time and cannot be set by a normal user.</p> Project variables (User-defined) <p>It is defined by the user in the UI. Project variables can only be used in the same project. If a project is exported those will be exported as well.</p> <p>Build variables can be particularly useful in scenarios where multiple tasks or components within a system need access to the same data or configuration values. Instead of repeating the same information in multiple places, project variables provide a centralized and reusable way to store and retrieve these values.</p>","tags":["KnowledgeGraph","Variables"]},{"location":"build/variables/#benefits-of-using-variables","title":"Benefits of using variables","text":"<ol> <li> <p>When sending an email to all employees, instead of manually typing or copy pasting each email address, you can conveniently store all the email addresses once and utilize them with a single word.     This saves time and ensures that no email addresses are missed or incorrectly entered.</p> </li> <li> <p>Another scenario where variables can be beneficial is when dealing with lengthy or hard-to-remember values.     For instance, consider the value \u201cxmhnjnnjkmnlbbhbvfhnbjkm\u201d.     By assigning it to a variable, you can store it once and easily recall it whenever needed.     This avoids the need to repeatedly type or remember complex values, enhancing efficiency and accuracy in documentation and other tasks.</p> </li> <li> <p>In software development, when working with URLs or file paths that are long or subject to change, you can store them in variables.     This allows for easy modification and reuse throughout the codebase, reducing the chances of errors and making maintenance more efficient.     For example, you can assign a URL like \u201chttps://example.com/api/data\u201d to a variable named <code>apiURL</code> for consistent referencing.</p> </li> <li> <p>When creating templates or form letters, variables can be used to personalize the content.     For instance, you can include variables such as {firstName}, {lastName}, and {companyName} to dynamically populate the recipient\u2019s name and company information.     This way, you can generate customized communications quickly without manually editing each instance.</p> </li> <li> <p>You can save the message, port, host or IP address, tokens, passwords, properties etc.</p> </li> </ol> <p>The use of variables provides flexibility, efficiency, and consistency, making it easier to manage complex or frequently used values across various tasks and documents.</p>","tags":["KnowledgeGraph","Variables"]},{"location":"build/variables/#global-variables","title":"Global Variables","text":"<p>Global variables are statically defined in the <code>dataintegration.conf</code> configuration file.</p> <p>Global variables can be marked as sensitive (<code>isSensitive = true</code>) for storing passwords. Sensitive variables can only be used in password fields. Using sensitive variables in other fields or in variable templates fails and does not expose the value.</p> <p>Global variables can have an optional description (<code>description = \"the description of my global variable\"</code>).</p> <p>Global variables are set in a section like in the example below:</p> <pre><code>###############################################\n### Global Variables\n###############################################\n\nconfig.variables = {\n  # Template engine\n  # Can be disabled by setting: engine = \"disabled\"\n  engine = \"jinja\"\n\n  global = {\n    # Insert global variables here\n\n    # simple notation: key = \"value\"\n    jdbcHost = \"my-jdbc-host\"\n\n    # more variables \u2026\n\n    # advanced notation: to add `isSensitive` and/or `description`\n    jdbcPassword = {\n        value = \"my-secret-password\"\n        description = \"the JDBC password for my-jdbc-host\"\n        isSensitive = true\n    }\n\n    # more variables \u2026\n  }\n}\n</code></pre> <p>Global variables are used in the same way as project variables. Which is described in the following sections.</p>","tags":["KnowledgeGraph","Variables"]},{"location":"build/variables/#project-variables","title":"Project Variables","text":"<p>In order to add project variables, login to eccenca Corporate Memory, select the build module and click on the project to open.</p> <p></p> <p>On the right side of the page you can see the variable section Click on  to add the variables.</p> <p></p> <p>The variable definition dialog opens. Type the name, value and description of the variable.</p> <p>Note</p> <p>Variable names may only consist of uppercase and lowercase letters (<code>A-Z</code> , <code>a-z</code>), digits (<code>0-9</code>), and the underscore character (<code>_</code>). Space in variable names is not supported hence Instead of space we can use underscore <code>_</code> to name the variable. In addition, the first character of a variable name cannot be a digit.</p> <p>Example</p> <p>We have to send an email to all the team members. Instead of typing or copy pasting all the mail id we can add all the email ids in variable and use whenever we require.</p> <p>Type name as <code>email_ids</code>, in values we have updated all the email id\u2019s of the team member\u2019s and in description we have updated as email address then click on Add.</p> <p></p> <p>Step Result</p> <p>The email_ids variable is added as shown below.</p> <p></p>","tags":["KnowledgeGraph","Variables"]},{"location":"build/variables/#using-variables","title":"Using Variables","text":"<p>Let\u2019s see how these variables are useful.</p> <p>Click on the symbol {#} it turns blue in colour it means the variables feature is active then click in the To option and type <code>{{email_ids}}</code>.</p> <p>Note</p> <p>Always use double curly brackets\u00a0<code>{{</code> to use the variables. When you start typing the variable name it shows you the variable name that you can use as shown below.</p> <p></p> <p>Select the <code>project.email_ids</code> and include the double curly brackets at last.</p> <p></p> <p>Step Result</p> <p>The email address of all the team members has been updated in the email field.\u00a0</p> <p>As shown in the below image you can see the Evaluated template value as all the email id addresses we have added in the variable.</p> <p></p> <p>Example</p> <p>The password is hard to remember and we used this password frequently then\u00a0 we can add this password in variable for reusable.</p> <p>Same as above, add the variable click on  and type name as\u00a0<code>password</code>, value as <code>xmhnjnnjkmnlbbhbvfhnbjkm</code> and in the description <code>Gmail account password</code> as shown below then click on Add.</p> <p></p> <p>Step Result</p> <p>The password variable is added.</p> <p>Let\u2019s see how this variable can be used.</p> <p>Click on the symbol {#} it turns blue in color. It means the variable\u2019s feature is active then click in the Password option and type <code>{{password}}</code> select the option as shown below and click on Update.</p> <p></p> <p>Step result</p> <p>The complex password is added in the password field easily.</p> <p>Note</p> <p>Parameters that are typed as password will not show the evaluated template for security reasons and should only show\u00a0after you saved the operator.</p>","tags":["KnowledgeGraph","Variables"]},{"location":"build/variables/#accessing-variables-with-cmemc","title":"Accessing Variables with cmemc","text":"<p>In order to allow the automation of activities with build variables from external processes, the Corporate Memory command line interfaces cmemc has a dedicated <code>project variable</code> command group for this.</p> <p>Please have a look at command group documentation to learn how to use these commands.</p>","tags":["KnowledgeGraph","Variables"]},{"location":"build/workflow-reconfiguration/","title":"Workflow Reconfiguration","text":"","tags":["Workflow"]},{"location":"build/workflow-reconfiguration/#introduction","title":"Introduction","text":"<p>The operators of a workflow can be reconfigured completely in the context of a workflow. During its execution, new parameters are loaded from any possible source and translated by a transformation task to allow an injection into the dataset configuration that overwrites originally set parameters. To reconfigure a workflow operator, the transformation task has to be connected to the red dot at the top of this operator as shown in the following image:</p> <p></p> <p>Although this feature has been developed to support the ingestion of database deltas, the possible applications are various since any parameter can be overwritten to make workflow operators even more dynamic and reusable in various contexts. The incremental ingestion of database content that was implemented as a first use-case can be found the application section of this page. However, we intend to add other use-cases that have been implemented. The following parameters seem to be good starting points for possible applications:</p> <ul> <li>Transformation Task:<ul> <li>Source Type</li> <li>Source Restriction</li> </ul> </li> <li>JDBC endpoint (remote)<ul> <li>Source Query</li> <li>Write Strategy</li> <li>Restriction</li> </ul> </li> <li>Knowledge Graph (embedded)<ul> <li>Clear Graph before workflow execution</li> </ul> </li> <li>Scheduler<ul> <li>Interval</li> <li>Enabled</li> </ul> </li> <li>\u2026</li> </ul>","tags":["Workflow"]},{"location":"build/workflow-reconfiguration/#implementation","title":"Implementation","text":"<p>To reconfigure a workflow operator, you need to create a transformation task, the data source of which is the intended source of the dynamic parameters of the workflow operator. Once you have created this task, you need to create a data value mapping for each parameter you want to overwrite.</p> <p>Info</p> <p>Only one transformation task can be used to reconfigure the workflow operator and one source can be used for a transformation task\u2019s source. Thus, it is necessary to pre-process all parameters that need to be rewritten into one single dataset, e.g. a CSV file or a in-memory dataset. Then, you can use this dataset to inject all parameters with one transformation task.</p> <p>Once you are sure, that your mapping rule entails the correct value, you can set the workflow operator parameter as the target property of the mapping rule. After this is done, you can reconfigure any workflow operator that uses this parameter as part of its configuration.</p> <p>Info</p> <p>The transformation task needs a suffix of the workflow parameter\u2019s URI in the workflow operator\u2019s serialization as its target property. This differs from the documentation that just refers to the parameter\u2019s <code>_name_</code>. If you want to overwrite the source query of a JDBC endpoint, you need to define <code>sourceQuery</code> as the target property, which is the suffix of <code>&lt;https://vocab.eccenca.com/di/functions/param_Jdbc_sourceQuery&gt;</code>.</p>","tags":["Workflow"]},{"location":"build/workflow-reconfiguration/#applications","title":"Applications","text":"<p>Tutorials that showcase this function in an application context:</p> <ul> <li>Loading JDBC datasets incrementally</li> </ul>","tags":["Workflow"]},{"location":"consume/","title":"Consume","text":""},{"location":"consume/#consume","title":"Consume","text":"<p>This section outlines how to consume data from Corporate Memory Knowledge Graphs. While there are several options to retrieve information from the Knowledge Graph, the most direct way is to issue SPARQL queries. SPARQL queries can be managed and executed in the Query Module UI. External applications may access the query catalog and execute queries through the REST API directly or more conveniently by using the cmemc - Command Line Interface. Since not all applications allow the direct use of SPARQL, this section includes tutorials to access Knowledge Graphs using BI tools (such as Power BI) as well as relational databases.</p> <p> Intended audience: Linked Data Experts</p> <ul> <li> <p> Power BI</p> <p>Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector.</p> </li> <li> <p> Redash</p> <p>Create Dashboards based on your Knowledge Graphs with the open-source application Redash.</p> </li> <li> <p> SQL Databases</p> <p>If direct access to the knowledge graph is not sufficient, fragments of the Knowledge Graph may also be pushed into external SQL databases.</p> </li> <li> <p> Custom APIs</p> <p>Learn how to provide data via a customized Corporate Memory API in a text format of your choice and how to consume it in your applications.</p> </li> <li> <p> Neo4j</p> <p>Learn how to populate graphs to Neo4j.</p> </li> <li> <p> Apache Kafka</p> <p>Use a Apache Kafka Producer in order to export parts of your Knowledge Graph as a message stream.</p> </li> </ul>"},{"location":"consume/consume-graphs-in-apache-kafka/","title":"Populate Graphs to Apache Kafka","text":"","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consume-graphs-in-apache-kafka/#introduction","title":"Introduction","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. Kafka is widely used in enterprises for data pipelines, streaming analytics, data integration and other applications.</p> <p>By using the cmem-plugin-kafka Python Plugin, you can produce and send messages to Apache Kafka from inside of our Corporate Memory Workflows.</p>","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consume-graphs-in-apache-kafka/#installation","title":"Installation","text":"<p>In order to use the Kafka Producer workflow task, you need to extend your Corporate Memory instance with the <code>cmem-plugin-kafka</code> package. This can be done by using cmemc:</p> Installing cmem-plugin-kafka on the instance 'my-cmem'<pre><code>$ cmemc -c my-cmem admin workspace python install cmem-plugin-kafka\nInstall package cmem-plugin-kafka ... done\n</code></pre> <p>You can validate your installation by listing all installed plugins (from all packages):</p> <pre><code>$ cmemc -c my-cmem admin workspace python list-plugins\nID                                 Package ID         Type            Label\n---------------------------------  -----------------  --------------  ---------------------------------\ncmem_plugin_kafka-ReceiveMessages  cmem-plugin-kafka  WorkflowPlugin  Kafka Consumer (Receive Messages)\ncmem_plugin_kafka-SendMessages     cmem-plugin-kafka  WorkflowPlugin  Kafka Producer (Send Messages)\n</code></pre>","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consume-graphs-in-apache-kafka/#usage","title":"Usage","text":"<p>Once you installed the package, you can use the Kafka Producer by simply creating a new task, e.g. search for <code>kafka</code> in the Create new item screen:</p> <p></p> <p>Follow the in-app documentation on how to configure the task (e.g. for providing credentials or preparing data to be sent in messages).</p>","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consuming-graphs-in-power-bi/","title":"Consuming Graphs in Power BI","text":"","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#introduction","title":"Introduction","text":"<p>Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector.</p> <p>This manual and tutorial describes how you can consume data from your knowledge graph in Microsoft Power BI through our Corporate Memory Power-BI-Connector.</p> <p>Power BI is a business analytics service by Microsoft. It aims to provide interactive visualizations and business intelligence capabilities with an interface simple enough for end users to create their own reports and dashboards. Power BI can be obtained from the official Microsoft page  and/or in the Windows Software Store.</p> <p>The latest (unsigned) version of our Power-BI-Connector is available from its source repository a version signed by eccenca is available with each Corporate Memory release.</p> <ul> <li>eccenca github.com repository (unsigned .mez file) </li> <li>eccenca Corporate Memory Releases (signed .pqx file)</li> <li>Thumbprint of the signature: FB6C562BD0B08107AAA420EDDE94507420C7FE1A</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#installation","title":"Installation","text":"<ul> <li>Download the <code>.pqx</code> or <code>.mez</code> file from the locations linked above.</li> <li>Move the file into the folder <code>Documents\\Power BI Desktop\\Custom Connectors</code> .</li> <li>Create the folder if it does not exist.</li> <li>In case you are running Windows on Parallels Desktop: Do not use the Local <code>Disk\\Users\\UserName\\Documents</code> folder but your shared folder with macOS.</li> <li>Register the Thumbprint (for .pqx) or setup PowerBI Desktop to allow any 3rd party connector (for .pqx or .mez) (we recommend to register the Thumbprint)</li> </ul> Setup Register Thumbprint (.pqx)Allow 3rd Party Connectors (.mez and .pqx) <ul> <li>In order to allow the eccenca Corporate Memory Power-BI-Connector in your Power BI Desktop installation you need to register the Thumbprint of the file signature in the windows registry.</li> <li>Cf. official Microsoft documentation<ul> <li>The registry path is <code>HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Power BI Desktop</code> . Make sure the path exists, or create it.</li> <li>Add a new value under the path specified above. The type should be \u201cMulti-String Value\u201d ( <code>REG_MULTI_SZ</code> ), and it should be called <code>TrustedCertificateThumbprints</code></li> <li>Add the thumbprints of the certificates you want to trust. You can add multiple certificates by using \u201c\\0\u201d as a delimiter, or in the registry editor, right click \u2192 modify and put each thumbprint on a new line. </li> <li>(Re-)Start Power BI Desktop</li> </ul> </li> </ul> <p>If you wish to automate this setup you can use the reg windows command line tool to make this entry like:</p> <pre><code>REM list existing entries in Power BI Desktop &gt; TrustedCertificateThumbprints\nreg query \"\"HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Power BI Desktop\" /v TrustedCertificateThumbprints\n\nREM add eccenca Corporate Memory Power-BI-Connector Thumbprint\nreg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Power BI Desktop\" /v TrustedCertificateThumbprints /t REG_MULTI_SZ /d FB6C562BD0B08107AAA420EDDE94507420C7FE1A\n</code></pre> <ul> <li>In case you are using the .mez (works for .pqx file too) file or simply want to trust any third party connector extension</li> <li>(Re-)Start Power BI Desktop, go to <code>File \u2192 Options and settings \u2192 Options \u2192 Security</code></li> <li>Under Data Extensions, select(Not Recommended). Allow any extension to load without validation or warning.</li> <li>Select OK, and then restart Power BI Desktop. </li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#add-a-data-source","title":"Add a Data Source","text":"<p>Use the Power-BI-Connector to login with your Corporate Memory Instance:</p> <ul> <li>Open Power BI Desktop</li> <li>Click <code>Edit Queries \u2192 New Source</code> (or directly Get <code>Data</code>) </li> <li>In the dialog search for eccenca <code>Corporate Memory</code>, which is listed in the <code>Database</code> category </li> <li>Select the connector and click <code>Connect</code></li> <li>Read and accept the 3rd party connector notification </li> <li>In the following dialog you need to specify the connection and information and access credentials, ask your Corporate Memory administrator for assistance if you miss any of the requested details. You have the option to use username + password or a client secret for login. In case of a custom setup is used advanced configuration can be provided:</li> </ul> Access-configuration Username + PasswordClientAdvanced Configuration <p>In order to use username +  password based login you need to fill the details shown below:</p> <ul> <li>First Step<ul> <li>Corporate Memory Base URI</li> <li>Grant type = password</li> <li>Client ID   </li> </ul> </li> <li>Second Step<ul> <li>Password / Client Secret   </li> </ul> </li> </ul> <p>In order to use Client Secret based login you need to fill the details shown below:</p> <ul> <li>First Step<ul> <li>Corporate Memory Base URI</li> <li>Grant type = client_credentials</li> <li>Client ID   </li> </ul> </li> <li>Second Step<ul> <li>Password / Client Secret</li> </ul> </li> </ul> <p>In case you installation uses a custom service endpoint layout the individual URIs for Explore backend (DataPlatform) and Keycloak can be configured individually. The configuration keys are the same as for cmemc.</p> <ul> <li>The following configuration parameter can be provided:<ul> <li><code>DP_API_ENDPOINT</code> - specifies the Explore backend (DataPlatform) URI</li> <li><code>OAUTH_TOKEN_URI</code> - specifies the keycloak token URI</li> <li><code>SSL_VERIFY</code> - can be used to set certificate verification to <code>False</code> </li> </ul> </li> <li>In case a <code>Corporate Memory Base URI</code> is configured too, the values from the <code>Config ini</code> section take precedence</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#get-data","title":"Get Data","text":"<p>With the eccenca Corporate Memory Power-BI-Connector you can load data from SELECT queries stored in the query catalog of Corporate Memory. You can use queries without or with placeholders. The steps are different depending if your query uses placeholder:</p> Queries Without placeholdersWith placeholders <ul> <li><code>SELECT</code> queries that use no placeholders are shown with a table icon (e.g. )</li> <li>When selected a preview will be loaded.</li> <li>Check the one(s) you want to load and click <code>OK</code>.</li> <li>The tables will be added to your list of <code>queries</code> and to the <code>fields</code> inventory in Power BI</li> <li>Start using your data in transformations, dashboards and analytics </li> </ul> <ul> <li>SELECT queries that take placeholder arguments are shown with a function icon. (e.g. ) </li> <li>You need to be in <code>Edit Queries</code> mode in Power BI so you can enter the required query parameter.</li> <li>Check the one(s) to be added.</li> <li>Power BI will add the selected query as a <code>query</code> entry.</li> <li>Click \u201cTransform Data\u201d in order to fill in the parameter. </li> <li>This adds a new entry to the list of Power BI queries, which contains the actual data you requested. The new entry will be named \u201cInvoked Function\u201d.</li> <li>It is recommended to rename this automatic generated name to a more speaking one. Right click on \u201cInvoked Function\u201d or select \u201cInvoked Function\u201d and press F2.</li> <li>Rename the table (e.g. to \u201c_search via regex match\u201d). Click \u201cClose &amp; Apply\u201c to save changes. </li> </ul> Hint <p>You can call the function multi times with different parameter values to get different result tables into Power BI.</p> <ul> <li>Start using your data in transformations, dashboards and analytics</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/","title":"Consuming Graphs in Redash","text":"","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/#introduction","title":"Introduction","text":"<p>Redash is an open-source tool designed to help data scientists and analysts visualize and build interactive dashboards of their data. Beside creating Dashboards, users can configure alerts in order to get mails on specific data events. In 03/2021 Redash has added an eccenca Corporate Memory query runner to its core repository, which enables Redash users to query Corporate Memory instances and build visualisations and dashboards based on Knowledge Graphs.</p>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/#adding-an-eccenca-corporate-memory-data-source","title":"Adding an eccenca Corporate Memory Data Source","text":"<p>The key to query your Knowledge Graphs with Redash is to add a new eccenca Corporate Memory data source. To do so, open the Settings &gt; Data Sources Tab, and search for the right type:</p> <p></p> <p>Click on it and you will come into another screen where you have to enter location and access data:</p> <p></p> <p>This configuration screen basically clones the basic configuration of cmemc:</p> <ul> <li>Name is a human friendly identifier for the source,</li> <li>Base URL refers to <code>CMEM_BASE_URI</code>,</li> <li>Client ID refers to <code>OAUTH_CLIENT_ID</code>,</li> <li>Client Secret refers to <code>OAUTH_CLIENT_SECRET</code>.</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/#query-your-knowledge-graph-and-create-dashboards","title":"Query your Knowledge Graph and Create Dashboards","text":"<p>Once you added a eccenca Corporate Memory data source to Redash, you can create queries, configure visualisation widgets based on the query results, and combine these widgets as dashboards.</p> <p>To get familiar with Redash, please have a look at the Redash user guide, especially the Getting Started page.</p> <p>Info</p> <p>In order to query eccenca Corporate Memory data sources in Redash, you have to formulate your query with SPARQL: </p>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-sql-databases/","title":"Consuming Graphs with SQL Databases","text":"","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#introduction","title":"Introduction","text":"<p>If direct access to the Knowledge Graph is not sufficient, fragments of the Knowledge Graph may also be pushed into external SQL databases. While in general all supported databases can be written into, optimized writing support is available and packaged for MySQL and MariaDB.</p> <p>See the documentation of the JDBC dataset for more details.</p>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#writing-a-single-table-into-a-sql-database","title":"Writing a single table into a SQL database","text":"<p>Three buildings blocks are required in eccenca Build (DataIntegration) to write into a remote SQL database:</p> <ul> <li>A dataset that allows access to the Knowledge Graph.</li> <li>A transformation that builds tables from specified resources in the Knowledge Graph.</li> <li>A dataset that configures access to the SQL database using JDBC.</li> </ul> <p>A simple workflow to write the contents of a Knowledge Graph into an SQL database looks like this:</p> <p></p> <p>In the following, we have a more detailed look at each of the three operators.</p>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#create-knowledge-graph-dataset","title":"Create Knowledge Graph dataset","text":"<p>Create a dataset of the type Knowledge Graph (embedded) and set the graph parameter to the URI of the graph that contains the resources to be exported:</p> <p></p>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#create-transformation","title":"Create Transformation","text":"<p>Create a transformation that covers the type of the RDF resources to be exported into a table:</p> <p></p> <p>For each column of the target table add a value mapping to the transformation:</p> <p> </p> Basic Mapping <p>The shown transformation will create a table with two columns:</p> <ul> <li>A column name that contains values of the property foaf:name.</li> <li>A column runtime that contains values of the property dbpediaow:runtime.</li> </ul>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#create-sql-dataset","title":"Create SQL dataset","text":"<p>Create a dataset of type JDBC endpoint (remote):</p> <p></p> <p>The most relevant parameters are:</p> <ul> <li>The JDBC Driver Connection URL should contain the database-specific JDBC URL.</li> <li>The table parameter defines the name of the table to be written.</li> <li>The write strategy specifies the behavior for the case that the configured table already exists in the target SQL database.</li> </ul>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#writing-multiple-tables-into-a-sql-database","title":"Writing multiple tables into a SQL database","text":"<p>If multiple tables should be written from several types of resources, there are two options:</p> <ol> <li>If the types are connected by properties, a single transformation with multiple object mappings can be used. The root mapping will write the table specified by the SQL dataset. Each object mapping writes an additional table. The name of the table is generated based on the target type which is defined in the object mapping.</li> <li>If the types are not directly connected by properties, multiple transformations can be created. Note that for each transformation a separate target SQL dataset needs to be created since the table name is specified in it.</li> </ol>","tags":["ExpertTutorial"]},{"location":"consume/populate-data-to-neo4j/","title":"Populate Data to Neo4j","text":"","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#introduction","title":"Introduction","text":"<p>This tutorial walks you through the process of\u00a0using\u00a0the\u00a0Neo4j dataset. You will learn how to transform data from a source dataset into a graph structure\u00a0into Neo4j.</p> <p>Abstract</p> <p>This tutorial uses a specific dataset called Northwind for your understanding. However, the principles can be applied and reused with any dataset. All the source data for this project is stored in a\u00a0Multi CSV Zip file:\u00a0northwind.zip.</p> <p>The vocabulary used in the tutorial can be found here: schema.ttl.</p> <p>Download these files to have them available before you start the tutorial.</p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#1-load-the-source-dataset","title":"1 Load the source dataset","text":"<p>eccenca\u00a0Corporate Memory provides the framework to create Resource Description Framework (RDF) tuples from non-Graph data sources such as tabular (CSV, Excel, SQL) or hierarchical (XML, JSON) sources. Additionally, Corporate Memory supports Neo4j as a source of or target for data. The Northwind dataset is available on github. This tutorial uses the Multi CSV Zip format to efficiently handle the following CSV files:</p> <ul> <li>employees.csv</li> <li>orders.csv</li> <li>products.csv</li> <li>categories.csv</li> <li>Suppliers.csv</li> </ul> <p>To upload multiple files together as an input:</p> <ol> <li>In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page.     </li> <li>Click Create at the top of the page.\u202f</li> <li>In Create new item window, select Project and click Add.\u00a0The Create new item of type Project window appears.</li> <li>Fill in the required details such as Title and Description. \u00a0Alternatively, import the existing project by clicking Import Project File and selecting the file from your system.\u202f</li> <li>Click Create. Your project (Northwind) is created.     </li> <li>In your project, click Create Item.</li> <li>In the Create new item window, select Multi CSV ZIP and click Add.     </li> <li>Specify a Label of the dataset in the Create new item of type Multi CSV ZIP window.</li> <li>Select the Upload new file option as you have the files. The Multi CSV ZIP file containing the above-listed files is available here. If it is an existing project, you can select the files from the project. For the remaining parameters, the default settings are used.</li> <li>Click Create. You can see the message northwind.zip was successfully uploaded in Green.</li> <li>You can see the Multi CSV ZIP file is uploaded with the datasets, and the item has been created.     </li> <li>Click the Play button and review the dataset in the Data preview section. You can see the contents of the loaded zip file consisting of the CSV files introduced above.</li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#2-create-a-neo4j-dataset","title":"2 Create a Neo4j dataset","text":"<p>A Neo4j dataset holding a Labeled Property Graph (LPG) representation is one of the outputs of the process. Perform the following steps to create a Neo4j dataset:</p> <ol> <li>In your existing project, click Create to create a new item.\u00a0</li> <li>In the item category Dataset select Neo4j.     </li> <li>Click Add.</li> <li>Enter the following details:<ul> <li>Label: Name of the item.\u202f\u00a0</li> <li>Uri: URL of the Neo4j instance.\u202f\u00a0</li> <li>User: Username of the instance.</li> <li>Password: Password of the instance.</li> </ul> </li> <li>Click Create.</li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#3-register-the-vocabulary","title":"3 Register the Vocabulary","text":"<p>The vocabulary contains the classes and properties in order to map the source data into a new (domain specific) structure. In this case we will create an RDF and LPG Knowledge Graph. A vocabulary for this tutorial is available as schema.ttl. Follow\u00a0Lift data from tabular data such as CSV, XSLX or database tables to learn how to register your vocabulary in Corporate Memory.</p> <p>The provided vocabulary is inspired by this structure from the original Neo4j tutorial on the Northwind data:</p> <p></p> <p>The vocabulary for the Northwind project can be visualized like this:</p> <p></p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#4-create-transformations","title":"4 Create Transformations","text":"<p>A transformation defines how the input datasets are transformed into output datasets. In this example, each of the CSV files\u00a0is\u00a0represented\u00a0as nodes with the same labels in the application. The following relationships must be established between the nodes:</p> <p>To register the nodes and relationships, perform the following:</p> <ol> <li>Create nodes\u202f\u00a0</li> <li>Establish the relationships through\u00a0transformations</li> </ol> EmployeesOrdersProductsCategorySuppliers","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#employee","title":"Employee","text":"<ol> <li>Click\u00a0Create\u00a0in your project.\u202f\u00a0</li> <li>On the\u00a0Create New Item\u00a0window, select\u00a0Transform\u00a0and click\u00a0Add\u00a0to create a new transformation.</li> <li>In the\u00a0Create new item of type Transform\u00a0window, select your project from the dropdown and click\u00a0Create.</li> <li>For this tutorial, enter the following:<ul> <li>Label: Northwind Employees</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select employees.csv</li> </ul> </li> <li>Click\u00a0Create. A Transformation Northwind Employees is created.</li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-value-mapping","title":"Add Value Mapping","text":"<ol> <li>On the Mapping Editor, expand the\u00a0Root Mapping\u00a0section and click\u00a0Edit.</li> <li> <p>If the vocabulary is pre-defined, specify the\u202fTarget entity type\u202ffrom the vocabulary. Else, specify the Target entity type and click\u00a0Create\u00a0option. In this example, specify the following:</p> <ul> <li>Target entity type:\u202f<code>nw:Employee</code></li> <li> <p>URI pattern:\u00a0<code>urn:empl-{EmployeeID}</code></p> <p>{EmployeeID}\u202fis a placeholder that points to the column name\u00a0of the\u00a0specific dataset.</p> </li> </ul> </li> <li> <p>Click\u202f&gt;\u00a0button\u202fto evaluate the mapping. The examples of the three\u00a0generated base URIs are as follows:\u00a0</p> </li> <li>Click\u00a0Save. The\u00a0Employee node is created.\u00a0\u00a0Now, you have to add the value mapping to indicate the basis on which the current CSV file must be transformed.\u202f\u00a0</li> <li>Click the\u00a0circular blue button on the lower right and\u00a0select\u202fAdd value mapping.</li> <li>Define the\u202fTarget property,\u00a0Data type,\u00a0Value path\u202f(column name), and a\u00a0Label\u202ffor value mapping. In this example, specify the following:<ul> <li>Target Property:\u00a0<code>rdfs:label</code></li> <li>Datatype: \u202fStringValueType</li> <li>Use a complex transformation to concatenate Last name and First name\u00a0fields to ensure that the rows are mapped individually</li> </ul> </li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#orders","title":"Orders","text":"<p>Create another transformation using the node Orders.\u202f\u00a0</p> <ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Order Transformation.</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, specify the following:<ul> <li>Label: Northwind Orders</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select Orders.csv</li> </ul> </li> </ol> <p>Click\u00a0Create. A transformation for Northwind Orders is created.</p> <p>To add a value mapping, repeat the steps followed while adding the mapping to\u00a0the\u00a0Employees node. In this example, for the orders node specify the following:</p> <ul> <li>Target entity type:\u202f<code>nw:Order</code> </li> <li> <p>URI pattern:\u00a0<code>urn:order-{OrderID}</code></p> <p>{OrderID} is a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-object-mapping","title":"Add Object Mapping","text":"<p>In this example, the Employee and Orders nodes have a relationship SOLD. To create the relationship, create an\u00a0Object mapping.</p> <ol> <li>Click the\u00a0circular blue button on the lower right and\u00a0select\u202fAdd object mapping<ul> <li>Target property:\u202f<code>nw:sold</code></li> <li>Target entity type:\u00a0<code>nw:Employee</code></li> <li>URI pattern:\u00a0<code>urn:empl-{EmployeeID}</code> </li> </ul> </li> <li>Select\u00a0Connect from Orders\u00a0as this relationship is\u00a0from Orders to the Employee.</li> </ol> <p>Info</p> <p>The URI (pattern) must be the same URI specified in the Employee transform, <code>urn:empl-{EmployeeID}</code> in this example.</p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#products","title":"Products","text":"<p>Create another transformation using the node Orders.</p> <ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Order Transformation.\u202f\u00a0</li> <li>In the\u00a0Create new item of type Transform\u00a0window, specify the following for this tutorial:<ul> <li>Label: Northwind Products</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select products.csv</li> </ul> </li> </ol> <p>Click\u00a0Create. A transformation for Northwind Products is created.</p> <ol> <li>To add value mapping define, repeat the steps followed while adding the mapping to\u00a0the\u00a0Employees node. In this example, for\u00a0the\u00a0Products node, specify the following:<ul> <li>Target entity type:\u202f<code>nw:Product</code></li> <li> <p>URI pattern:\u00a0<code>urn:prod-{ProductID}</code></p> <p>{ProductID}\u202fis a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul> </li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#category","title":"Category","text":"<ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Category Transformation.\u202f\u00a0</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, specify the following:<ul> <li>Label: Northwind Category</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select categories.csv</li> </ul> </li> </ol> <p>Click\u00a0Create. A Transformation for Northwind Category is created.</p> <p>To add value mapping, repeat the steps followed while adding the mapping to the Employees node. In this example, specify the following for the category node:</p> <ul> <li>Target entity type:\u202f <code>nw:Category</code></li> <li> <p>URI pattern:\u00a0<code>urn:cat-{CategoryID}</code></p> <p>{CategoryID}\u202fis a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-object-mapping_1","title":"Add Object Mapping","text":"<p>In this example, the Product node is connected to the Category node\u00a0through the relationship,\u00a0partOf. To create this, click add object mapping.</p> <p>For this example, specify the following:</p> <ul> <li>Target property: <code>nw:partOf</code></li> <li>Target entity type: <code>nw:Category</code></li> <li>URI pattern: <code>urn:cat-{CategoryID}</code></li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#suppliers","title":"Suppliers","text":"<ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Suppliers Transformation.\u202f</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, specify the following:<ul> <li>Label: Northwind Suppliers</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select suppliers.csv</li> </ul> </li> <li>Click\u00a0Create. A Transformation for Northwind Suppliers is created.</li> </ol> <p>To add value mapping, repeat the steps followed while adding the mapping to\u00a0the\u00a0Employees node. In this example, for\u00a0suppliers\u00a0node, specify the following:</p> <ul> <li>Target entity type: <code>nw:Supplier</code></li> <li> <p>URI pattern: <code>urn:suppl-{SupplierID}</code></p> <p>{SupplierID} is a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-object-mapping_2","title":"Add Object Mapping","text":"<p>In this example, the Product node is connected to the Supplier node through the relationship, SUPPLIES. As the supplier supplies to the Product, navigate to Product transform and then click Add Object\u00a0Mapping. To create this, click\u00a0Add object mapping.\u00a0For this example, specify the following:</p> <ul> <li>Target property: <code>nw:supplies</code></li> <li>Select Connect to Product</li> <li>Target entity type:\u00a0<code>nw:Suppliers</code></li> <li>URI pattern: <code>urn:suppl-{SupplierID}</code></li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#5-create-a-workflow","title":"5 Create a workflow","text":"<p>To integrate all the transformations, perform the following steps:</p> <ol> <li>Navigate to the project.</li> <li>Click\u00a0Create</li> <li>In the\u00a0Create new item\u00a0window, select\u00a0Workflow\u00a0and click\u00a0Add.\u00a0</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, enter the following:<ul> <li>Label: Northwind Workflow</li> <li>Click\u00a0Create</li> </ul> </li> </ol> <p>In this example, the dataset must be connected to the transformations as an input, and the output should be the Neo4j output.</p> <p>Drag and drop the options available in the Workflow editor pane and link them accordingly.</p> <p></p> <p>Click the Play icon to validate the results. The nodes and edges created are stored in the Neo4j dataset.</p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#6-results-in-neo4j","title":"6 Results in Neo4j","text":"","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#7-results-in-knowledge-graph","title":"7 Results in Knowledge Graph","text":"<p>Optionally, you can use the same transformation and workflow to render the resulting graph data into a Knowledge Graph.</p> <ol> <li>Add a Knowledge Graph dataset and use this as\u00a0an\u00a0additional target in your workflow:     </li> <li>The results can then be reviewed in the Knowledge Graph module, e.g., explored visually:     </li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/","title":"Provide Data in any Format via a Custom API","text":"","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#introduction","title":"Introduction","text":"<p>Learn how to provide data via a customized Corporate Memory API in a text format of your choice and how to consume it in your applications. This tutorial describes how you can provide data in a text format of your choice via your own custom Corporate Memory API, and how you request those APIs.</p> <p>In this tutorial, we describe how you can set up an endpoint which provides iCalendar data. If you want to rebuild the example, you can download this iCalendar RDF data and import it into your Corporate Memory instance: ical_data.ttl</p> <pre><code>BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//hacksw/handcal//NONSGML v1.0//EN\nBEGIN:VEVENT\nUID:20020630T230353Z-3895-69-1-0@jammer\nDTSTAMP:20020630T230353Z\nDTSTART:20020630T090000Z\nDTEND:20020630T103000Z\nSUMMARY:Church\nEND:VEVENT\nEND:VCALENDAR\n</code></pre>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#define-a-sparql-query","title":"Define a SPARQL query","text":"<p>This query selects the event data in our graph which will be provided via the customized API. To rebuild the iCalendar format, we need at least the unique identifier (<code>uid</code>), the datetime start (<code>dtstart</code>), the datetime end (<code>dtend</code>), and the summary of the event. The query filters (<code>REPLACE</code>) the special characters <code>:</code> and <code>-</code> at the end as they are not needed in the iCal DateTime format.</p> <pre><code>PREFIX ical: &lt;http://www.w3.org/2002/12/cal/icaltzd#&gt;\n\nSELECT DISTINCT ?vevent ?uid ?dtstamp ?dtstart ?dtend ?summary\n\nWHERE {\n?vevent a ical:Vevent .\n    ?vevent ical:uid ?uid .\n    ?vevent ical:dtstamp ?dtstamp_raw .\n    ?vevent ical:dtstart ?dtstart_raw .\n    ?vevent ical:dtend ?dtend_raw .\n    ?vevent ical:summary ?summary  .\n\n    BIND(REPLACE(STR(?dtstamp_raw),\"[: -]\",\"\") AS ?dtstamp) .\n    BIND(REPLACE(STR(?dtstart_raw),\"[: -]\",\"\") AS ?dtstart) .\n    BIND(REPLACE(STR(?dtend_raw),\"[: -]\",\"\") AS ?dtend) .\n}\n</code></pre>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#define-a-template-for-the-ical-format","title":"Define a Template for the iCal format","text":"<p>As a next step, we will define a template that generates iCalendar data from our previously defined SPARQL query.</p> <p>Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Result Template and click <code>Create a new Select Result Template</code> to create a new template.</p> <p></p> <p></p> <p>Define a Name, a Description and the Body format. You may also define a header and/or a footer, however, this is not necessary for this example.</p> <p>The template engine we are using Jinja. In Jinja, dynamic data within a template needs to be referenced via double curly brackets <code>{{...}}</code>. So the line <code>{{result.uid}}</code> inserts at execution time the <code>?uid</code> value from our previously defined SPARQL query into this template. Everything outside curly brackets it static. As static data in our example, we define the full iCalendar format (<code>..BEGIN:EVENT..</code>). As we receive multiple results (iCalendar Events) from the SPARQL query, we have to iterate through each of them. To define this iteration in the template, the following line needs to be added:</p> <pre><code>{% for result in results %}\n</code></pre> <p>and for the conclusion of the iteration, this line needs to be added at the end:</p> <pre><code>{% endfor %}\n</code></pre> <p></p> Jira Template for our iCalendar format<pre><code>BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//hacksw/handcal//NONSGML v1.0//EN\n{% for result in results %}\nBEGIN:VEVENT\nUID:{{result.uid}}\nDTSTAMP:{{result.dtstamp}}\nDTSTART:{{result.dtstart}}\nDTEND:{{result.dtend}}\nSUMMARY:{{result.summary}}\nEND:VEVENT\n{% endfor %}\n\nEND:VCALENDAR\n</code></pre>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#create-an-api-based-on-your-template","title":"Create an API based on your template","text":"<p>As a next step, we will set up the API which serves the data in the format we defined in the previous template.</p> <p>Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Query Endpoint and click \u201cCreate a new Select Query Endpoint\u201d to create a new endpoint.</p> <p></p> <p>Define a Name, a human-readable keyword (i.e. the URL Slug) for the API path, specify if it is a Streaming endpoint (false in our example), enter a Description, and select the defined SPARQL Query from our first step and the Template we created in the second step. Once you press save, your endpoint it set up!</p> <p></p>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#consume-data-via-the-endpoint","title":"Consume data via the endpoint","text":"<p>Now that the endpoint is defined, it is possible to make a request to receive the iCal data. The endpoint URL consist of the path <code>/dataplatform/api/custom</code> and the previously defined URL Slug (<code>/ical</code>). cmemc can be used to get the API base URL as well as a valid token:</p> curl request<pre><code>$ curl \"https://$(cmemc -c my config get DP_API_ENDPOINT)/api/custom/ical\" \\\n    -H \"Authorization: Bearer $(cmemc -c my admin token)\"\n</code></pre> API Response<pre><code>BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//hacksw/handcal//NONSGML v1.0//EN\n\nBEGIN:VEVENT\nUID:20020630T230353Z-3895-69-1-0@jammer\nDTSTAMP:20020630T230353Z\nDTSTART:20020630T090000Z\nDTEND:20020630T103000Z\nSUMMARY:Church\nEND:VEVENT\n\nBEGIN:VEVENT\nUID:20020630T230445Z-3895-69-1-7@jammer\nDTSTAMP:20020630T230445Z\nDTSTART:20020703\nDTEND:20020706\nSUMMARY:Scooby Conference\nEND:VEVENT\n\nBEGIN:VEVENT\nUID:20020630T230600Z-3895-69-1-16@jammer\nDTSTAMP:20020630T230600Z\nDTSTART:20020718T090000\nDTEND:20020718T093000\nSUMMARY:Federal Reserve Board Meeting\nEND:VEVENT\n\nEND:VCALENDAR\n</code></pre> <p>This result is represented in valid ICalendar (<code>ics</code>) format and can be imported into your calendar client (event_data.ics).</p>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#configuration-remarks","title":"Configuration remarks","text":"","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#streaming","title":"Streaming","text":"<p>If Is Streaming is set to false for the endpoint (as in the given example), the respective Jinja Template needs to resolve a results variable, which is a list of all query results that needs to be iterated over using Jinja constructs:</p> <pre><code>{% for result in results %}\n</code></pre> <p>A non-streaming result set (the SPARQL query) is limited to 1000 elements. If more results are expected Is Streaming should be set to true.</p> <p>If Is Streaming is set to <code>true</code> the Jinja Template has to resolve a <code>result</code> variable (without the \u2018<code>s</code>\u2019), which is a single query result. The template engine iterates over the results, i.e. the Body template is repeated for each query result.</p>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"deploy-and-configure/","title":"Deploy and Configure","text":"<p>Deploy and configure eccenca Corporate Memory in your own environment.</p> <p> Intended audience: Deployment Engineers and System Administrators.</p> <ul> <li> <p> System Architecture</p> <p>This page describes the overall system architecture of eccenca Corporate Memory and its components.</p> </li> <li> <p> Requirements</p> <p>This page lists software and hardware requirements for eccenca Corporate Memory deployments.</p> </li> <li> <p> Installation</p> <p>These pages describe proven deployment scenarios for eccenca Corporate Memory.</p> </li> <li> <p> Configuration</p> <p>These pages describe specific topics on how to configure eccenca Corporate Memory.</p> </li> </ul>"},{"location":"deploy-and-configure/configuration/","title":"Configuration","text":"<p>This page describes specific topics on how to configure eccenca Corporate Memory.</p> <ul> <li> Access Conditions\u00a0\u2014 Access conditions specify access rights for users and groups to graphs and actions.</li> <li> Build (DataIntegration)\u00a0\u2014 This section is intended to be a reference for all available eccenca Build (DataIntegration) configuration options.</li> <li> Explore\u00a0\u2014 This page describes how to configure the eccenca Explore component.</li> </ul>"},{"location":"deploy-and-configure/configuration/access-conditions/","title":"Access Conditions","text":"","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#introduction","title":"Introduction","text":"<p>Access Conditions specify access rights for users and groups to graphs and actions (1).</p> <ol> <li>Graphs identify specific Knowledge Graphs.     Actions identify specific parts or components of the platform, such as the query catalog or the data integration system (Build).</li> </ol> <p>Access Conditions are managed in a special system graph, so write access to this graph needs to be handled carefully. The management of access conditions can be done either by using the browser based user interface or the command line based user interface (cmemc).</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#attributes-of-access-conditions","title":"Attributes of Access Conditions","text":"<p>In order to understand the different user interfaces to manage access conditions, it is crucial to understand what details can be described with a single access condition. The following list describes the different attributes, a single access condition can have. They are all optional except that a single access condition needs to provide at least one grant or has a dynamic access condition query.</p> <p>The listed IRIs in this section use the following prefix declarations:</p> <pre><code>PREFIX eccauth: &lt;https://vocab.eccenca.com/auth/&gt;\nPREFIX :        &lt;https://vocab.eccenca.com/auth/Action/&gt;\n</code></pre>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#metadata","title":"Metadata","text":"<ul> <li> <p>Name is a short and human readable text you can give to your access condition in order to identify them.</p> </li> <li> <p>Description is an optional and longer text you can add, to provide more context.</p> </li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#define-who-gets-access","title":"Define who gets access","text":"<ul> <li>Use Requires account to specify the user account, which is required by the access condition.   If the account matches the account of a given request, this access condition is used to identify the grants for this request.   Instead of an actual account, the following meta account can be used.</li> </ul> Resource Explanation <code>eccauth:AnonymousUser</code> Represents the anonymous user account. You can use it in the Requires account field. <ul> <li>Use Requires group to specify the group, the account must be member of in order to match the access condition.   If the account of a given request is member of this group, this access condition is used to identify the grants for this request.   Instead of an actual group, the following meta group can be used.</li> </ul> Resource Explanation <code>eccauth:PublicGroup</code> Represents the group which every user is member of (incl. anonymous users). You can use it in the Requires group field. <p>Users and groups cannot have the same name</p> <p>Since both user and group resource are represented in the same namespace in the internal graph representation, users and groups cannot have the same identifier.</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#define-what-grants-are-given","title":"Define what grants are given","text":"<ul> <li>Allow reading graph is a list of graph IRI to allow to read these graphs.   Instead of an actual graph, the following meta graph can be used.</li> </ul> Resource Explanation <code>eccauth:AllGraphs</code> Represents all RDF named graphs. You can use it in the Allow reading graph or Allow writing graph field. <ul> <li>Allow writing graph is a list of graph IRIs to allow to write these graphs.   The grant to write to a graph implicitly grants to read the graph.   Instead of an actual graph, the following meta graph can be used.</li> </ul> Resource Explanation <code>eccauth:AllGraphs</code> Represents all RDF named graphs. You can use it in the Allow reading graph or Allow writing graph field. <ul> <li>Allowed action is a list of action IRI to allow to use the components or capabilities which are identified with this action.   You can use the following actions identifier with this attribute.</li> </ul> Resource Explanation <code>:AllActions</code> Represents all actions. You can use it to grant execution rights to all actions <code>:Build</code> Represents the action needed to use eccenca Build (DataIntegration) component of eccenca Corporate Memory. <code>:Build-AdminPython</code> Represents the action needed to use eccenca Build (DataIntegration)\u2019s Python plugin management component of eccenca Corporate Memory. <code>:Build-AdminWorkspace</code> Represents the action needed to use eccenca Build (DataIntegration)\u2019s workspace administration component of eccenca Corporate Memory. <code>:ChangeAccessConditions</code> Represents the action needed to use the Authorization management API (see Developer Manual). You can use it as object of the <code>eccauth:allowedAction</code> property to grant access to the Authorization management API if the user fulfills the access condition. <code>:Explore-BKE-Manage</code> Represents the action needed to view, create, edit and delete visualisations in the BKE-Module (needs access to config graph as well). <code>:Explore-BKE-Read</code> Allows to use the BKE-Module interface in read-only mode (needs access to config graph as well). <code>:Explore-KnowledgeGraphs</code> Represents the action needed to use the Explore Tab (needs access to at least one graph as well) <code>:Explore-ListSystemGraphs</code> Represents the action needed to list Corporate Memory system graphs (tagged with shui:isSystemResource) in the Knowledge Graph list. <code>:Explore-QueryCatalog</code> Represents the action needed to use the Query Catalog (needs access to catalog graph as well if changes should be allowed) <code>:Explore-ThesaurusCatalog</code> Represents the action needed to use the Thesaurus Catalog as well as Thesaurus Project editing interface (needs access to specific thesaurus graphs as well) <code>:Explore-VocabularyCatalog</code> Represents the action needed to use the Vocabulary Catalog (needs access to specific vocabulary graphs as well) <p>In addition to these attributes, you can use the following special attributes to grant partial access to the access conditions itself:</p> <ul> <li> <p>Graph pattern for granting read access is a pattern to allow users to manage access conditions which grant read access to graphs identified by IRI matching the pattern.</p> </li> <li> <p>Graph pattern for granting write access is a pattern to allow users to manage access conditions which grant write access to graphs identified by IRI matching the pattern.</p> </li> <li> <p>Pattern for granting actions is a pattern to allow users to manage access conditions which grant action usage to action identified by IRI matching the pattern.</p> </li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#dynamic-conditions","title":"Dynamic Conditions","text":"<p>Use this attribute to dynamically compute who get access on which graphs, based on background information from your Knowledge Graphs:</p> <ul> <li>Dynamic access condition is an attribute which requires a SPARQL Select query which returns the following projection variables: <code>user</code>, <code>group</code>, <code>readGraph</code>, <code>writeGraph</code>.</li> </ul> <p>The following example query grants write access to all users which are described as creators (using Dublin Core) in the graph itself.</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX dct: &lt;http://purl.org/dc/terms/&gt;\nPREFIX void: &lt;http://rdfs.org/ns/void#&gt;\n\nSELECT  ?user ?group ?readGraph ?writeGraph\nWHERE\n{\n  GRAPH ?writeGraph {\n    ?writeGraph rdf:type void:Dataset .\n    ?writeGraph dct:creator ?user .\n  }\n}\n</code></pre> <p>Given the following Knowledge Graph <code>https://example.org/my-data/</code>, the account <code>tester</code> will get access to it, because the IRI of the account is related to the graph.</p> <pre><code>PREFIX dct: &lt;http://purl.org/dc/terms/&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX void: &lt;http://rdfs.org/ns/void#&gt;\n\n&lt;https://example.org/my-data/&gt;\n  rdf:type void:Dataset ;\n  rdfs:label \"My Data\"@en;\n  dct:creator &lt;http://eccenca.com/tester&gt; .\n</code></pre> <p>User and group namespace</p> <p>The IRI identifier for users and groups need have the namespace <code>http://eccenca.com/</code>. If your data does not match this requirement, you can manipulate the IRIs with SPARQL functions on-the-fly.</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#managing-access-conditions","title":"Managing Access Conditions","text":"","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#web-interface","title":"Web interface","text":"<p>The access control module can be selected in the Admin section of the left main menu of the Explore component. After clicking it, you will see a screen similar to this:</p> <p></p> Access Control: List Access Conditions <p>You have two major application areas (tabs) here:</p> <ul> <li>In the Manage tab, you can view, edit, add and delete all access conditions, which are manageable by your account.</li> <li>In the Review tab, you can see the effective rights based on a selection of an account and groups.</li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#view-edit-and-update","title":"View, edit and update","text":"<p>Use the following icon buttons for a specific action with a certain access condition:</p> <ul> <li>Use  to view an access condition.</li> <li>Use  to edit an access condition.</li> <li>Use  to delete an access condition.</li> </ul> <p>Use the Create access condition button to create a new access condition.</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#review","title":"Review","text":"<p>In the Review tab, you can see the effective rights based on a selection of an account and groups. This allows for debugging your access condition system as a whole. In order to see the rights select a user and / or group combination from the drop-down list on top (principal). Then you will see a screen similar to this:</p> <p></p> Access Control: Review Access Conditions <p>This screen is split into two main areas:</p> <ul> <li> <p>First, the effective rights are listed, which summarize the resulting access rights.</p> <ul> <li>The Root access field shows if the principal has root access.</li> <li>The Read all and Write all fields show if the principal has read or write access to all graphs.</li> <li>The All actions are allowed field shows if the principal has permission to execute all actions.</li> <li>The Allowed actions field lists the actions the principal is allowed to execute.</li> <li>The Readable graphs and Writable graphs fields list the graphs the principal is allowed to read or write.</li> </ul> </li> <li> <p>Second, the list of all access conditions which contributed to the effective access rights. This section allows to see which access conditions matched the principal and which access rights they grant.</p> </li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#command-line-interface","title":"Command line interface","text":"<p>With cmemc you can use an additional command line based interface to manage access conditions. This interface is primarily used for the automation of provisioning tasks. The important command groups for managing principals and access conditions are:</p> <ul> <li><code>admin acl</code> - List, create, delete, modify and review access conditions.</li> <li><code>admin user</code> - List, create, delete and modify user accounts.</li> <li><code>admin client</code> - List client accounts, get or generate client account secrets.</li> </ul> <p>The following session demonstrates how to create a new user, set a password and grant access to certain areas.</p> <pre><code>$ cmemc admin acl list\nNo access conditions found. Use the `admin acl create` command to create a new access condition.\n\n$ cmemc admin user create tester\nCreating user tester ... done\n\n$ cmemc admin user update tester --assign-group local-users\nUpdating user tester ... done\n\n$ cmemc admin user password tester\nChanging password for account tester ...\nNew password:\nRetype new password:\ndone\n\n$ cmemc admin acl create --id local-users-access --group local-users \\\n    --write-graph https://example.org/ \\\n    --write-graph https://ns.eccenca.com/data/queries/ \\\n    --read-graph https://vocab.eccenca.com/shacl/ \\\n    --action :Explore-QueryCatalog \\\n    --action :Explore-KnowledgeGraphs \\\n    --description \"Access to query catalog and basic exploration of example.org\"\nCreating access condition 'Condition for group local-users' ... done\n\n$ cmemc admin acl list\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 URI                 \u2503 Name                            \u2503\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\n\u2503 :local-users-access \u2503 Condition for group local-users \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n</code></pre>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#typical-use-cases","title":"Typical Use Cases","text":"","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#regular-administration-user","title":"Regular administration user","text":"<p>In order to create a regular administration user account, you need to grant the following rights:</p> <ul> <li>Allowed Actions: All Actions (<code>https://vocab.eccenca.com/auth/Action/AllActions</code>)</li> <li>Allow writing graphs: All Graphs (<code>https://vocab.eccenca.com/auth/AllGraphs</code>)</li> </ul> <p>In the web interface, this will look like:</p> <p></p> Access Control: Create a regular Administration user <p>With cmemc, you can achieve this with the following command:</p> <pre><code>$ cmemc admin acl create --id my-admin-account-acl \\\n    --user my-admin --action :AllActions \\\n    --write-graph https://vocab.eccenca.com/auth/AllGraphs\nCreating access condition 'Condition for user: my-admin' ... done\n\n\u2234 cmemc admin acl list\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 URI                   \u2503 Name                            \u2503\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\n\u2503 :local-users-access   \u2503 Condition for group local-users \u2503\n\u2503 :my-admin-account-acl \u2503 Condition for user: my-admin    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n</code></pre> <p>In case you need to create the user account, you can do this as well:</p> <pre><code>$ cmemc admin user create my-admin\nCreating user my-admin ... done\n\n$ cmemc admin user password my-admin\nChanging password for account my-admin ...\nNew password:\nRetype new password:\ndone\n</code></pre>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#user-with-limited-access","title":"User with limited access","text":"<p>In order to limit access to specific parts of the application, you need to know which actions and graphs should be combined to achieve a certain goal.</p> <p>The following list, provides grants which work together:</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#access-to-the-explore-user-interface","title":"Access to the Explore User Interface","text":"<ul> <li>Allowed Actions: Explore - Knowledge Graphs Exploration (<code>:Explore-KnowledgeGraphs</code>)</li> <li>Allow read graphs:<ul> <li>CMEM Shape Catalog (<code>https://vocab.eccenca.com/shacl/</code>)</li> <li>Any other graph the user should be able to explore</li> </ul> </li> <li>Allow write graphs:<ul> <li>(optional) Any graph the user should be able to change</li> </ul> </li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#access-to-the-query-catalog","title":"Access to the Query Catalog","text":"<ul> <li>Allowed Actions: Explore - Query Catalog (<code>:Explore-QueryCatalog</code>)</li> <li>Allow read graphs:<ul> <li>(optional) Any graph the user should be able to query</li> </ul> </li> <li>Allow write graphs:<ul> <li>CMEM Query Catalog (<code>https://ns.eccenca.com/data/queries/</code>)</li> <li>(optional) Any graph the user should be able to change</li> </ul> </li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#access-to-dataintegration","title":"Access to DataIntegration","text":"<ul> <li>Allowed Actions: Build - Workspace (<code>:Build</code>)</li> <li>Allow write graphs:<ul> <li>All Graphs (<code>https://vocab.eccenca.com/auth/AllGraphs</code>)</li> </ul> </li> </ul>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/caveats/","title":"Caveats","text":"","tags":["Configuration","Docker","Volume","Filesystem","Load Balancer"]},{"location":"deploy-and-configure/configuration/caveats/#filesystem-in-volumes","title":"Filesystem in Volumes","text":"<p>Info</p> <p>Please avoid to deploy on NFS (AWS EFS) volumes, use block storage instead (e.g. AWS EBS).</p> <p>In Kubernetes deployments in AWS we saw issues with volumes based on NFS, such as Amazon EFS (Elastic File System).</p> <p>For Build (DataIntegration) Python Plugins we noticed issues: see Sharing the <code>PYTHONPATH</code> via NFS</p> <p>For GraphDB we saw issues when trying to recover from a previous backups. This is because NFS behaves different why trying to delete files. In Fact GraphDB requires to store the data on EBS (Elastic Block Storage) volumes. See GraphDB technical-requirements.</p>","tags":["Configuration","Docker","Volume","Filesystem","Load Balancer"]},{"location":"deploy-and-configure/configuration/caveats/#load-balancer","title":"Load Balancer","text":"<p>Depending on the deployment, load balancers or proxies in between Corporate Memory components (such as Build (DataIntegration) &lt;-&gt; Explore backend, Explore backend &lt;-&gt; Triplestore) tend to drop long running TCP connections when they are idle. As e.g. SPARQL Update requests may be idle for a long time (while the update is performed) the client will not receive the response. Hence, DI workflow execution will not proceed even though the update went through.</p> <p>Similar problems can occur between the connection between Explore backend (DataPlatform) and GraphDB.</p> <p>In addition to the hints in the next sections you can always change the TCP keep-alive of the system hosting the containers. However in cloud environments this often isn\u2019t practical. In Debian based Linux distribution you have to edit <code>/etc/sysctl.conf</code>. But be advised, only do this, if you are aware of the risks.</p> <pre><code>net.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 60\nnet.ipv4.tcp_keepalive_probes = 20\n</code></pre>","tags":["Configuration","Docker","Volume","Filesystem","Load Balancer"]},{"location":"deploy-and-configure/configuration/caveats/#kubernetes-deployments","title":"Kubernetes Deployments","text":"<p>In Kubernetes deployments services for each deployment usually use load balancers to connect other services. However we never saw problems as long as inter-component-communication stays at service level.</p> <p>In detail, Build (DataIntegration) can be set like this in the cmem-helm value file: <code>.Values.dataintegration.config.dataplatformUrl</code> or let it defaulting to: <code>http://dataplatform.namespace:8080/dataplatform</code>.</p> <p>To connect to Ontotext GraphDB we emphasize to NOT use external load balancers trough an ingress or similar. Instead use the internal connection like <code>graphdb-node.graphdb.cluster.local</code> at the value <code>.Values.dataplatform.store.graphdb.host</code>.</p>","tags":["Configuration","Docker","Volume","Filesystem","Load Balancer"]},{"location":"deploy-and-configure/configuration/caveats/#container-deployments-or-distributing-over-multiple-vms","title":"Container Deployments or distributing over multiple VMs","text":"<p>When deploying Corporate Memory in cloud environments such as EC2 where each component runs on a separate VM, or using Container Services such as ECS (Amazon Elastic Container Service) or ACI (Azure Container Instances) the same rules apply as in Kubernetes deployments. For networking make sure to create a Private Network (VPC) instead of bridges to bypass load balancers. VPN</p> <p>DataIntegration is configured with the <code>ENV</code> variable <code>DATAPLATFORM_URL</code> to set up the connection to Explore backend (DataPlatform) or in its config file <code>dataintegration.conf</code>.</p> <p>For Explore backend (DataPlatform) you have to set this in <code>application.yaml</code> or through <code>ENV</code> variables, such as: <code>STORE_GRAPHDB_HOST</code>, <code>STORE_GRAPHDB_PORT</code> and <code>STORE_GRAPHDB_SSL_ENABLED</code>.</p>","tags":["Configuration","Docker","Volume","Filesystem","Load Balancer"]},{"location":"deploy-and-configure/configuration/caveats/#useful-documentation","title":"Useful Documentation","text":"<ul> <li>AWS VPN Documentation</li> <li>AWS Network load Balancers</li> <li>GraphDB with load balancers</li> </ul>","tags":["Configuration","Docker","Volume","Filesystem","Load Balancer"]},{"location":"deploy-and-configure/configuration/dataintegration/","title":"Build (DataIntegration)","text":"<p>This section is intended to be a reference for all available eccenca Build (DataIntegration) configuration options. The configuration format is based on\u00a0HOCON.</p> <p>The following sections introduce the most important configuration parameters. The entire list of available configuration parameters can be found in the\u00a0dataintegration.conf file found in the release package.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#oauth","title":"OAuth","text":"<p>Authorization in eccenca Build (DataIntegration) is based on OAuth. Typically, the eccenca Explore backend (DataPlatform) is used as an OAuth endpoint, but external endpoints can be used as well. The Authorization Code Grant workflow is used for retrieving the OAuth token. The default configuration is the following:</p> <pre><code># The URL of the eccenca DataPlatform.\neccencaDataPlatform.url = \"http://localhost:9090\"\n\n# Use OAuth for authentification against DataPlatform\neccencaDataPlatform.oauth = false\n\n# Enable if user information should be fetched via DP and OAuth. Only uncomment if OAuth is enabled and DP is configured.\nuser.manager.web.plugin = oauthUserManager\n\n# The DataPlatform endpoint that is used for authentification\neccencaDataPlatform.endpointId = \"default\"\n\n# Define the protocol used for accessing the workbench (http or https), defaults to http\nworkbench.protocol = \"http\"\n\n# Optional parameter for specifying the host.\nworkbench.host = \"localhost:9090\"\n\n# The URL to redirect to after logout.\n# If not set, the user will be redirected to the internal logout page.\noauth.logoutRedirectUrl = \"http://localhost:9090/loggedOut\"\n\n# The OAuth client that will be used to load the workspace initially and run the schedulers.\nworkbench.superuser.client = \"elds\"\nworkbench.superuser.clientSecret = \"elds\"\n\n# Optional parameter for specifying an alternative OAuth authorization endpoint.\n# If not specified, the default OAuth authorization endpoint of the specified eccenca Platform URL is used.\n# Note that if the eccenca Platform URL is internal and not accessible for the user, a public authorization URL must be set here.\noauth.authorizationUrl = \"http://localhost:9090/oauth/authorize\"\n\n# Optional parameter for specifying an alternative OAuth authorization endpoint.\noauth.tokenUrl = \"http://localhost:9090/oauth/token\"\n\n# Optional parameter for specifying an alternative OAuth client ID.\noauth.clientId = \"eldsClient\"\n\n# Optional parameter for specifying an alternative OAuth client secret.\noauth.clientSecret = \"secret\"\n\n# Additional request parameters to append to all OAuth authentication requests.\n# oauth.requestParameters = \"&amp;resource=value\"\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#explore-configuration","title":"Explore configuration","text":"<p>eccenca Build (DataIntegration) can only be run by OAuth users that are granted the\u00a0<code>https://vocab.eccenca.com/auth/Action/Build</code>\u00a0action by the Explore component (aka DataPlatform/DataManager). An example condition can be seen at Access Conditions &gt; Access to Build (DataIntegration)</p> <p>In the shown example, the users also get access to all graphs in the RDF store. This is not a requirement for working with Build (DataIntegration). Access may also be restricted to graphs that the data integration users are allowed to work with.</p> <p>In order to activate OAuth using the eccenca Explore (DataPlatform), the following minimal configuration is required:</p> <pre><code>eccencaDataPlatform.url = \"http://localhost:9090\"\neccencaDataPlatform.oauth = true\noauth.clientId = \"eldsClient\"\noauth.clientSecret = \"secret\"\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#super-user","title":"Super User","text":"<p>By default, the workspace is loaded the first time a user opens eccenca Build (DataIntegration) in the browser using their credentials. Any scheduler that is part of a project must be started manually.</p> <p>By configuring a super user, the workspace will be loaded at startup. After loading, all schedulers will be started using the credentials of the super user.</p> <p>In addition to the configuration of eccenca Explore according to the previous section, a super user is configured by specifying the following two parameters:</p> <pre><code>workbench.superuser.client = \"superUserClient\"\nworkbench.superuser.clientSecret = \"superUserClientSecret\"\n</code></pre> Note <p>The client credentials grant type is used to retrieve a token for the super user. Note that the schedulers are only started automatically when running in production mode.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#workspace-providers","title":"Workspace Providers","text":"<p>The backend that holds the workspace can be configured using the\u00a0<code>workspace.provider.plugin</code>\u00a0parameter in\u00a0<code>dataintegration.conf</code></p> <pre><code>workspace.provider.plugin = &lt;workspace-provider-plugin-name&gt;\n</code></pre> <p>The following sections describe the available workspace provider plugins and how they are configured.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#rdf-store-workspace-backend","title":"RDF-store Workspace - backend","text":"<p>When running in Corporate Memory, by default the workspace is held in the RDF store configured in the eccenca Explore.</p> <p>The workspace is held using the eccenca Explore backend (DataPlatform), i.e., it requires the\u00a0<code>eccencaDataPlatform.url</code>\u00a0parameter to be configured.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ cacheDir String Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start. <code>&lt;empty&gt;</code> <p>By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>eccencaDataPlatform.url = &lt;DATAPLATFORM_URL&gt;\n...\nworkspace.provider.plugin = backend\n...\nworkspace.provider.backend = {\n  # Load prefixes defined by all known vocabularies.\n  loadAllVocabularyPrefixes = false\n  # Load prefixes defined by vocabularies that are actually loaded in the RDF store.\n  loadInstalledVocabularyPrefixes = true\n  # The graph that contains the vocabulary meta data.\n  vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\"\n  # Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start.\n  cacheDir = \"\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#file-based-workspace-file","title":"File-based Workspace - file","text":"<p>The workspace can also be held on the filesystem.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the workspace is persisted. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.provider.plugin = file\n...\nworkspace.provider.file = {\n  # The directory to which the workspace is persisted.\n  dir = ${user.home}\"/myWorkspace\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#hybrid-workspace-fileanddataplatform","title":"Hybrid workspace - fileAndDataPlatform","text":"<p>The so called hybrid workspace holds the workspace in the filesystem and in eccenca Explore backend (DataPlatform) simultaneously. Each time a task is updated, it is written to both the filesystem and to the project RDF graph. In addition, on each (re)load of the Workspace, the contents of the file based workspace are pushed to the RDF store, to make sure that both workspace backends stay synchronized. Contents of the file system may be changed manually (reload the workspace afterwards). Contents of the RDF store are supposed to be read only and will be overwritten on reload.</p> <p>The workspace is held using the eccenca Explore backend (DataPlatform), i.e., it requires the\u00a0<code>eccencaDataPlatform.url</code>\u00a0parameter to be configured.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the workspace is persisted. no default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ failOnDataPlatformError boolean If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the Explore backend (DataPlatform), the entire request fails. If false, an update error in the Explore backend (DataPlatform) will only log a warning. false <p>By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>eccencaDataPlatform.url = &lt;DATAPLATFORM_URL&gt;\n...\nworkspace.provider.plugin = fileAndDataPlatform\n...\nworkspace.provider.fileAndDataPlatform = {\n  # The directory to which the workspace is persisted.\n  dir = ${user.home}\"/myWorkspace\"\n  # Load prefixes defined by all known vocabularies.\n  loadAllVocabularyPrefixes = false\n  # Load prefixes defined by vocabularies that are actually loaded in the RDF store.\n  loadInstalledVocabularyPrefixes = true\n  # The graph that contains the vocabulary meta data.\n  vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\"\n  # If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning.\n  failOnDataPlatformError = false\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-workspace-inmemory","title":"In-memory Workspace - inMemory","text":"<p>A workspace provider that holds all projects in memory. All contents will be gone on restart.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.provider.plugin = inMemory\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-rdf-workspace-inmemoryrdfworkspace","title":"In-memory RDF Workspace - inMemoryRdfWorkspace","text":"<p>A workspace that is held in a in-memory RDF store and loses all its content on restart (mainly used for testing). Needed if operators are used that require an RDF store backend.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ <p>By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.provider.plugin = inMemoryRdfWorkspace\n...\nworkspace.provider.inMemoryRdfWorkspace = {\n  # Load prefixes defined by all known vocabularies.\n  loadAllVocabularyPrefixes = false\n  # Load prefixes defined by vocabularies that are actually loaded in the RDF store.\n  loadInstalledVocabularyPrefixes = true\n  # The graph that contains the vocabulary meta data.\n  vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#resource-repositories","title":"Resource Repositories","text":"<p>Project resources are held by a resource repository which is configured\u00a0using the\u00a0<code>workspace.repository.plugin</code>\u00a0 parameter in\u00a0<code>dataintegration.conf</code></p> <pre><code>workspace.repository.plugin = &lt;resource-repository-plugin-name&gt;\n</code></pre> <p>The following sections describe the available resource repository plugins and how they are configured.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#project-specific-directories-projectfile","title":"Project Specific Directories - projectFile","text":"<p>By default, resources are held in project specific directories.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the resources are persisted. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = projectFile\n...\nworkspace.repository.projectFile = {\n  dir = ${elds.home}\"/var/dataintegration/workspace/\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#shared-directory-file","title":"Shared Directory - file","text":"<p>Alternatively, all resources across all Build (DataIntegration) projects can be held in a single directory on the file system.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the resources are persisted. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = file\n...\nworkspace.repository.file = {\n  dir = ${elds.home}\"/var/dataintegration/resources/\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#hdfs-resources-hdfs","title":"HDFS resources - hdfs","text":"<p>Holds all resources on the HDFS file system.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default path String The directory to which the resources are persisted. no default user String The hadoop user. hadoopuser <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = hdfs\n...\nworkspace.repository.hdfs = {\n  # The directory to which the resources are persisted.\n  dir = \"/data/hdfs-datalake/\"\n  # The hadoop user.\n  user = \"hadoopuser\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#s3-bucket-project-specific-projects3","title":"S3 Bucket, Project Specific -\u00a0projectS3","text":"<p>In addition to storing files in the local filesystem an AWS S3 bucket can be used as the resource repository backend.</p> <p>To use resources stored on S3 the AWS\u00a0<code>keyID</code>\u00a0,\u00a0<code>secretKey</code>\u00a0need to be configured in the\u00a0<code>dataintegration.conf</code>\u00a0\u00a0file. This and the region are used to connect to S3. Further, one bucket name has to be given. This is analog to the root folder of filesystem based resource repositories.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default bucket String The S3 bucket name. no default accessKeyId String The S3 access key ID. no default secretKey String The S3 secret key. no default region String The AWS region the S3 bucket is located in. no default path String OPTIONAL. Path (absolute to the bucket (root)) that defines the folder that will be used to hold the workspace. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <p><pre><code>workspace.repository.plugin = projectS3 # project individual resources\n...\nworkspace.repository.projectS3 = {\n  # The S3 bucket name.\n  bucket = \"your-bucket-name\"\n  # The S3 access key ID.\n  accessKeyId = \"BUCKET-ACCESS-KEY\"\n  # The S3 secret key.\n  secretKey = \"BUCKET-SECRET-KEY\"\n  # The AWS region the S3 bucket is located in.\n  region = \"eu-central-1\"\n  # OPTIONAL path in the bucket used to hold the DataIntegration workspace\n  # /path/to/my-workspace/\n}\n</code></pre> For this S3 plugin make sure the account has at least these permissions attached:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::&lt;YOUR_BUCKET_NAME&gt;\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListAllMyBuckets\",\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"VisualEditor2\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::&lt;YOUR_BUCKET_NAME&gt;/*\"\n        }\n    ]\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#s3-bucket-shared-directory-s3","title":"S3 Bucket, Shared Directory - s3","text":"<p>Holds all resources shared across all your Build (DataIntegration) projects in a single S3 bucket.</p> <p>The available configuration options are the same as for\u00a0<code>projectS3</code>\u00a0.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = s3 # resources shared across projects\n...\nworkspace.repository.s3 = {\n  ... # same configuration as for projectS3\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-inmemory","title":"In-Memory -\u00a0inMemory","text":"<p>Resources can also be held in-memory.\u00a0The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = inMemory\n</code></pre> <p>Note</p> <p>In-memory repositories will be emptied on restart.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#no-resources-empty","title":"No resources -\u00a0empty","text":"<p>In case you do not want to allow to store file resources you can define an\u00a0empty\u00a0repository. The\u00a0<code>empty</code>\u00a0resource repository that does not allow storing any resources.</p> <p>The resource repository can also be specified as empty.\u00a0The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = empty\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-report-manager","title":"Execution Report Manager","text":"<p>The execution report manager is used to persist execution reports. It allows to retrieve previous reports. you can use it with file and in-memory models. In addition you can specify a retention time. Reports older than this time will be deleted, if a new report is added. The retention time is expressed as a Java\u00a0<code>Duration</code>\u00a0string, see\u00a0https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-\u00a0for details.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#disabled-none","title":"Disabled - None","text":"<p>Discards execution reports and does not persist them.</p> <pre><code>workspace.reportManager.plugin = none # Discards execution reports and does not persist them.\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-inmemory_1","title":"In-Memory - inMemory","text":"<p>Holds the reports in memory.</p> <pre><code>workspace.reportManager.plugin = inMemory # Holds the reports in memory.\n\nworkspace.reportManager.inMemory = {\n  retentionTime = \"P30D\" # duration how long to keep - default is 30 Days\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#file-based-file","title":"File based - file","text":"<p>Holds the reports in a specified directory on the filesystem.</p> <pre><code>workspace.reportManager.plugin = file # Holds the reports in a specified directory on the filesystem.\n\nworkspace.reportManager.file = {\n  dir = \"/data/reports\" # directory where the reports will be stored\n  retentionTime = \"P30D\" # duration how long to keep - default is 30 Days\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#internal-datasets","title":"Internal Datasets","text":"<p>Internal datasets hold intermediate results during the execution of a Build (DataIntegration) Workflow. The type of internal dataset that is used can be configured. By default, an in memory dataset is used for storing data between tasks:</p> <pre><code>dataset.internal.plugin = inMemory\n</code></pre> <p>Alternatively, the internal data can also be held in the eccenca Explore backend (DataPlatform):</p> <pre><code>dataset.internal.plugin = eccencaDataPlatform\ndataset.internal.eccencaDataPlatform = {\n  graph = \"https://ns.eccenca.com/dataintegration/internal\"\n}\n</code></pre> <p>If the eccenca Explore backend (DataPlatform) is not available, an external store may also be specified:</p> <pre><code>dataset.internal.plugin = sparqlEndpoint\ndataset.internal.sparqlEndpoint = {\n  endpointURI = \"http://localhost:8890/sparql\"\n  graph = \"https://ns.eccenca.com/dataintegration/internal\"\n}\n</code></pre> <p>Warning</p> <p>If an RDF store based internal dataset is used, all internal data is stored in the same graph. For this reason, multiple different internal datasets cannot be used safely in that case.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#timeouts","title":"Timeouts","text":"<p>Build (DataIntegration) has a number of timeouts to maintain operation while connection issues or problems in other applications occur. The following sections list the most important global timeouts.</p> Note <p>In addition to these global timeouts, many datasets, such as the Knowledge Graph dataset, do provide additional timeout parameters. Refer to the documentation of datasets for individual timeout mechanisms of different datasets.</p> <p>Warning</p> <p>All timeouts set need to be lower than the gateway timeout in the network infrastructure.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#request-timeout","title":"Request timeout","text":"<p>The request timeout specifies how long a HTTP(S) request may take until it times out and is closed:</p> <pre><code>play.server.akka.requestTimeout = 10m\n</code></pre> <p>This timeout mechanism can be disabled, by setting the timeout to\u00a0<code>\"infinite\"</code>.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#idle-request-timeout","title":"Idle request timeout","text":"<p>The idle timeout specifies the maximum inactivity time of a HTTP(S) connection:</p> <pre><code>play.server.http.idleTimeout = 10m\n</code></pre> <p>The connection will be closed after it has been open for the configured idle timeout, without any request or response being written. This timeout mechanism can be disabled, by setting the timeout to\u00a0<code>\"infinite\"</code>.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#explore-backend-dataplatform-timeouts","title":"Explore backend (DataPlatform) timeouts","text":"<p>As Build (DataIntegration) depends on the Explore backend (DataPlatform) for managing the Knowledge Graph, it will query the health of Explore backend (DataPlatform) as part of its own health check. The timeout to wait for Explore backend (DataPlatform)\u2019s response to a health request can be configured:</p> <pre><code>healthCheck.dataplatform.timeout = 10000 # milliseconds\n</code></pre> <p>In addition, there is a timeout when requesting authorization information from the eccenca Explore backend (DataPlatform), which is fixed to 61 seconds.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#rdf-store-timeouts","title":"RDF store timeouts","text":"<p>When reading and writing RDF there are a number of timeouts applied, depending on whether the Graph Store protocol or the SPARQL endpoint are used.</p> <p>For the Graph Store protocol, the following timeouts can be configured:</p> <pre><code>graphstore.default = {\n  # Timeout in which a connection must be established\n  connection.timeout.ms = 15000 # 15s\n  # Timeout in which a response must be read\n  read.timeout.ms = 150000 # 150s\n  # Max request size of a single GraphStore request, larger data is split into multiple requests\n  max.request.size = 300000000 # 300MB\n  # Timeout in which a file upload of size max.request.size must be uploaded\n  fileUpload.timeout.ms = 1800000 # half hour\n}\n</code></pre> <p>For the SPARQL endpoint, the following parameters are applicable:</p> <pre><code>silk.remoteSparqlEndpoint.defaults = {\n  connection.timeout.ms = 15000 # 15s\n  read.timeout.ms = 180000 # 180s\n}\n</code></pre> <p>Note</p> <p>The Knowledge Graph dataset provides additional timeout parameters for more fine-grained control.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#linking-execution-timeout","title":"Linking execution timeout","text":"<p>When executing linking rules there are a number of timeouts to prevent possibly erroneous linkage rules from generating too many links or staling the execution:</p> <pre><code>linking.execution = {\n  # The maximum amount of links that are generated in the linking execution/evaluation\n  linkLimit = {\n    # The default value a link spec is initialized with, this can be changed for each link spec.\n    default = 1000000 # 1 million\n    # The absolute maximum of links that can be generated. This is necessary since the links are kept in-memory.\n    max = 10000000 # 10 million\n  }\n  # The maximum time the matching task is allowed to run, this does not limit the loading time.\n  matching.timeout.seconds = 3600 # 1 hour\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#maximum-upload-size","title":"Maximum Upload Size","text":"<p>By default, the size of uploaded resources is limited to 10 MB. The upload limit can be increased:</p> <pre><code>play.http.parser.maxDiskBuffer = 100MB\n</code></pre> <p>While uploading, resources are cached on the disk, i.e., the limit may exceed the size of the available memory.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#provenance","title":"Provenance","text":"<p>By default, no provenance data is written to the RDF store. To enable writing provenance data to a named graph, a provenance plugin needs to be configured.</p> <p>Provenance output can be configured using the following parameter:</p> Parameter Type Description Default provenance.graph String Set the graph where generated provenance will be written to in the RDF workspace provider. https://ns.eccenca.com/example/data/dataset/ provenance.persistWorkflowProvenancePlugin.plugin String Provenance plugin to set where provenance data should be written to. Possible options: - rdfWorkflowProvenance - writes provenance data to RDF backend - nopWorkflowProvenance - do NOT write provenance data (disable it) nopWorkflowProvenance <p>To enable provenance output, the following lines can be added to\u00a0<code>dataintegration.conf</code>:</p> <pre><code>provenance.graph = https://ns.eccenca.com/example/data/dataset/\nprovenance.persistWorkflowProvenancePlugin.plugin = rdfWorkflowProvenance\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#logging","title":"Logging","text":"<p>Logging for eccenca Build (DataIntegration) is based on the\u00a0Logback\u00a0logging framework. There are two ways to change the logging behavior from the default, the first is to provide a logback.xml file, the second is to set various logging properties in the <code>dataintegration.conf</code> file.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#logback-configuration-file","title":"Logback Configuration File","text":"<p>The\u00a0<code>logback.xml</code>\u00a0file can be added to the\u00a0<code>${ELDS_HOME}/etc/dataintegration/conf/</code>\u00a0folder, from where it is read on application start-up and replaces the default logging config.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-example","title":"Configuration Example","text":"<p>The following example\u00a0<code>logback.xml</code>\u00a0file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n  &lt;appender name=\"TIME_BASED_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n    &lt;file&gt;/opt/elds/var/log/dataintegration.log&lt;/file&gt;\n    &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt;\n      &lt;!-- daily rollover, history for 1 week --&gt;\n      &lt;fileNamePattern&gt;/opt/elds/var/log/dataintegration.%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;\n      &lt;maxHistory&gt;7&lt;/maxHistory&gt;\n    &lt;/rollingPolicy&gt;\n    &lt;encoder&gt;\n      &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;\n    &lt;/encoder&gt;\n  &lt;/appender&gt;\n  &lt;logger name=\"com.eccenca\" level=\"INFO\"&gt;\n    &lt;appender-ref ref=\"TIME_BASED_FILE\" /&gt;\n  &lt;/logger&gt;\n&lt;/configuration&gt;\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#logging-properties","title":"Logging Properties","text":"<p>For debugging purposes and smaller adaptions it is possible to change log levels for any logger in the Build (DataIntegration) config file. There are following possibilities:</p> <pre><code># The following log level properties will overwrite the config from the logback.xml file\n\n# Set the root logger level, valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL.\n# This affects any logger that is not explicitly defined in the logback.xml config file\nlogging.root.level = DEBUG\n\n# Set the DI root log level. This affects all logger in DI packages that are not explicitly specified in the logback.xml\nlogging.di.level = TRACE\n\n# Set the Silk root log level. This affects all logger in Silk packages that are not explicitly specified in the logback.xml\nlogging.silk.level = WARN\n\n# Generic log level config: inside logging.level enter package path as key and log level as value.\n# Valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL\n# This can be used to override any log level, also these defined in the logback.xml file.\nlogging.level {\n  # Set log level of oauth package to TRACE, this overrides the config in the default logback config\n  oauth=TRACE\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#plugin-configuration","title":"Plugin Configuration","text":"<p>The plugin architecture of eccenca Build (DataIntegration) allows to configure certain characteristics of the application, e.g. the persistence backend for the workspace.\\ A full list over all plugins are given in the eccenca Build (DataIntegration) user manual in the sections\u00a0Plugin Reference\u00a0as well as\u00a0Activity Reference.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#blacklisting-plugins","title":"Blacklisting Plugins","text":"<p>In some cases the usage of specific plugins might pose a risk. In order to avoid to load a specific plugin, it can be blacklisted in the configuration file by setting the\u00a0<code>pluginRegistry.plugins.{pluginID}.enabled</code>\u00a0 config parameter to false. The parameter takes a comma-separated list of plugin IDs. The corresponding plugins will not be loaded into the plugin registry and can thus not be selected or executed anymore. The plugin ID for each plugin can be found in the\u00a0Plugin Reference\u00a0and\u00a0Activity Reference\u00a0section of the eccenca Build (DataIntegration) user manual.</p> <p>Example config:</p> <pre><code>pluginRegistry {\n  plugins {\n    pluginToBeBlacklisted.enabled = false\n  }\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#spark-configuration","title":"Spark configuration","text":"<p>The following chapters describe configuration options relevant for the execution of eccenca Build (DataIntegration) workflows on Spark. All options regarding the execution of Build (DataIntegration) on Spark are set in the\u00a0<code>dataintegration.conf</code>\u00a0file. The option to define SparkExecutor as the execution engine is\u00a0<code>execution.manager.plugin</code>\u00a0and needs to be changed from the default value:</p> <pre><code>execution.manager.plugin = LocalExecutionManager\n</code></pre> <p>To the value that specifies the use of the SparkExecutor:</p> <pre><code>execution.manager.plugin = SparkExecutionManager\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-spark-client-mode","title":"Execution in Spark client Mode","text":"<p>Spark provides a simple standalone cluster manager. You can launch a\u00a0standalone\u00a0cluster either manually, by starting a master and workers by hand, or by using the provided launch scripts. Spark can still run alongside Hive, Hadoop and other services in this mode. But in general this mode is preferred if only Spark applications are running in a cluster. When multiple cluster applications are running in parallel (e.g. different databases, interpreters or any software running on top of Yarn) or more advanced monitoring is needed the execution with Yarn is often recommended.</p> <p>For running Build (DataIntegration) in client mode the following configuration can be used</p> <pre><code>spark.interpreter.options = {\n  # Specifies local or client-mode, required: local, cluster or client\n  deploymentMode = \"client\"\n  # URL of the Spark cluster master node\n  sparkMaster = \"spark://spark.master:7077\"\n  # The IP of the driver/client program machine in client-mode, required for client mode\n  sparkLocalIP = \"IP or hostname where DataIntegration runs\"\n  # Jars containing the dependencies, required only for client and cluster modes\n  # In client mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included\n  sparkJars = \"eccenca-DataIntegration-assembly.jar\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-cluster-mode","title":"Execution in cluster mode","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#build-dataintegration-application-configuration","title":"Build (DataIntegration) application configuration","text":"<p>Cluster mode is supported with Apache Yarn only at the moment. To run Build (DataIntegration) in cluster mode the following configuration can be used:</p> <pre><code>spark.interpreter.options = {\n  # Specifies local or client-mode, required: local, cluster or client\n  deploymentMode = \"cluster\"\n  # URL of the Spark cluster master node\n  sparkMaster = \"yarn-master-hostname\"\n  # The IP of the driver/client program machine in client-mode, required for client mode\n  sparkLocalIP = \"IP or hostname where DataIntegration runs\"\n  # Jars containing the dependencies, required only for client and cluster modes\n  # In cluster mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included\n  sparkJars = \"eccenca-DataIntegration-assembly.jar\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#build-dataintegration-cluster-deployment-configuration","title":"Build (DataIntegration) cluster deployment configuration","text":"<p>In cluster mode one should keep in mind that, normally, Build (DataIntegration) will generate a jar and a project export. These artifacts can be copied or send to a cluster and will be executed there via the\u00a0<code>spark-submit</code>\u00a0command. That means the data processing is running in its own remote process separate from the Build (DataIntegration) application.</p> <p>An assembly jar and a workflow can be exported by an activity that belongs to each defined workflow. The activity can be configured in the\u00a0<code>dataintegration.conf</code>\u00a0. There are 3 phases of a deployment: staging, transform, loading and 3 types of artifact compositions as well as some other options deciding the target of the export. First the specified resource are copied to the configured resource folder of a project or a temp folder (staging) and then an action decides how the files are deployed.</p> <p>Artifacts (spark.deployment.options.artifact):</p> <ul> <li>\u2018jar\u2019: The assembly jar is deployed</li> <li>\u2018project\u2019: The exported project zip file is deployed (e.g. if the assembly was already globally deployed)</li> <li>\u2018project-jar\u2019: The jar and the project zip are deployed</li> </ul> <p>The artifacts are copied to the configured resource folder off the project the activity belongs to.</p> <p>Types (<code>spark.deployment.options.[phase].type, e.g. spark.deployment.options.staging.type=\"env-script\"</code>\u00a0):</p> <ul> <li>\u2018script\u2019 A shell script is called to copy the files to the cluster (can be user supplied, contain auth, prepare a DataFactory activity etc.)</li> <li>\u2018copy\u2019 The resource are copied to a specified folder</li> <li>\u2018hdfs\u2019 The resource is imported to HDFS</li> <li>\u2018env-script\u2019 A shell script is loaded from a environment variable</li> <li>\u2018var-script\u2019 A shell script is loaded from a configuration variable</li> </ul> <p>Other options:</p> <ul> <li><code>spark.deployment.options.[phase].[typeName]</code>\u00a0Depending on the selected deployment type this contains one or more (separated by a comma) targeted local file system or HDFS paths or location of the scripts to run\\     e.g.\u00a0<code>spark.deployment.options.staging.type =\"script\"</code>\u00a0and\u00a0<code>spark.deployment.options.staging.script=\"/scripts/script.sh\"</code></li> <li><code>spark.deployment.options.overwriteExecution</code>\u00a0Boolean value that decides if the ExecuteSparkWorkflow action is overwritten by the deployment action and will run this instead.</li> </ul> <p>Example:</p> <pre><code>spark.deployment.options = {\n  # Specifies artifacts: Stage the project export and the executable assembly jar\n  artifact = \"project-jar\"\n  # Type of the deployment: Copy project and jar to /data folder, then run a script to start processing the data\n  staging.type = \"copy\"\n  staging.copy = \"/data\"\n  transform.type = \"script\"\n  transform.script = \"conf/runWorkflow.sh\"\n  # Bind the 2 actions to the \"run workflow\" button\n  overwriteExecution = true\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#activity-parameters-to-skip-deployment-phases","title":"Activity parameters to skip deployment phases","text":"<p>In some scenarios (especially deployments where a jar has to be copied to a remote location) it is required that a deployment phase can be skipped. E.g. the jar upload only has to be done once, the upload is defined in the \u201cstaging\u201d phase and the spark-submit call in the \u201ctransform\u201d phase. The parameter \u201cexecuteTransform\u201d (reachable via the activity tab) can\\ be set to false on the second run to avoid re-uploading artifacts.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-of-the-assembly-jar","title":"Configuration of the assembly jar","text":"<p>In cluster mode, usually, we run a Spark Job by submitting an assembly jar to the cluster. This can be seen a command line version of Build (DataIntegration) and can also be used manually with \u2018spark-submit\u2019. In this case the configuration in the environment the jar runs in should look like this (options are set by the spark-submit configuration and parameters):</p> <pre><code>spark.interpreter.options = {\n  # Specifies deployment mode, requires: local, cluster, client or submit\n  deploymentMode = \"submit\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-local-mode","title":"Execution in local mode","text":"<p>Local mode is mainly for testing, but can be used for deployment on a single server. HDFS and Hive are not required. The following configuration parameters have to be set:</p> <pre><code>spark.interpreter.options = {\n  # Specifies deployment mode, requires: local, cluster, client or submit\n  deploymentMode = \"local\"\n  # URL of the Spark cluster master node, the [*] denotes the number of executors\n  sparkMaster = \"local[4]\"\n}\n</code></pre> <p>In this mode the parameters and Spark settings appended to the \u2018spark-submit\u2019 command will always be used and overwrite configuration settings in other sources.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-and-usage-of-the-sqlendpoint-dataset","title":"Configuration and usage of the SqlEndpoint dataset","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#server-side-configuration","title":"Server Side Configuration","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#general-settings","title":"General Settings","text":"<p>The SqlEndpoint dataset is a table or view behaving analog to a table in a relational database like MySQL. When data is written to an SqlEndpoint dataset, a JDBC server is started and can be queried with any JDBC client. In Build (DataIntegration) the SqlEndpoint dataset behaves like any other dataset. It can be used as a target for workflows, be profiled, used as a source for an operation or workflow etc. There are a two of configuration options that are relevant for the JDBC endpoints:</p> <pre><code>spark.sql.options = {\n  # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. SqlEndpoint\n  # datasets can still be started but can only be accessed internally if set to false.\n  startThriftServer = true\n  # Enable Hive integration\n  # Sets Spark to use an infrastructure for meta data that is compatible with the hive metastore\n  enableHiveSupport = true\n  ...\n}\n</code></pre> <p>The port on which the JDBC connections will be available is\u00a0<code>10005</code>\u00a0by default and can be changed in the\u00a0<code>hive-site.xml</code>\u00a0and\u00a0<code>spark-defaults.conf</code>\u00a0configuration files.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#security-settings","title":"Security Settings","text":"<p>A secure connection can be configured with the authentification settings in the\u00a0<code>hive-site.xml</code>\u00a0,\u00a0<code>spark-defaults.conf</code>\u00a0and\u00a0<code>dataintegration.conf</code>\u00a0files.</p> <p>If Hive support is disabled (\u00a0<code>enableHiveSupport = false</code>\u00a0) or if the property\u00a0<code>hive.server2.authentication</code>\u00a0has the value\u00a0<code>None</code>\u00a0security can be disabled.</p> <p>There exist a number of option for secure JDBC connections via Thrift and Hive:</p> <ul> <li>Kerberos</li> <li>LDAP</li> <li>Custom authentication classes</li> <li>User impersonation</li> <li>Server and Client Certificates</li> </ul> <p>Eccenca provides a custom Authentification provider which allows to set 1 user/password combination for JDBC connections via:</p> <pre><code>spark.sql.options = {\n  endpointUser = \"user\"\n  endpointPassword = \"password\"\n}\n</code></pre> <p>The authentication provider class name is\u00a0<code>com.eccenca.di.sql.endpoint.security.SqlEndpointAuth</code>\u00a0. To use it the following configuration is needed:</p> <pre><code>  &lt;configuration&gt;\n    &lt;property&gt;\n      &lt;name&gt;hive.server2.authentication&lt;/name&gt;\n      &lt;value&gt;CUSTOM&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n      &lt;name&gt;hive.server2.custom.authentication.class&lt;/name&gt;\n      &lt;value&gt;com.eccenca.di.sql.endpoint.security.SqlEndpointAuth&lt;/value&gt;\n      &lt;description&gt;\n        Custom authentication class. Used when property\n        'hive.server2.authentication' is set to 'CUSTOM'. Provided class\n        must be a proper implementation of the interface\n        org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2\n        will call its Authenticate(user, password) method to authenticate requests.\n        The implementation may optionally implement Hadoop's\n        org.apache.hadoop.conf.Configurable class to grab Hive's Configuration object.\n      &lt;/description&gt;\n    &lt;/property&gt;\n    ...\n  &lt;/configuration&gt;\n</code></pre> <p>Check the Hive documentation for details:\u00a0Hive admin manual\u00a0or the documentation of a Hadoop Distribution (MapR, Hortenworks or AWS and Azure in he cloud etc.). Hadoop distributions usually provides instructions for configuring secure endpoints.</p> <p>Integration with various authentication providers can be configured and is mostly set up in\u00a0<code>hive-site.xml</code>\u00a0.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#sqlendpoint-dataset-parameters","title":"SqlEndpoint Dataset Parameters","text":"<p>The dataset only requires that the\u00a0<code>tableNamePrefix</code>\u00a0parameters is given. This will be used as the prefix for the names of the generated tables. When a set of Entities is\u00a0written\u00a0to the endpoint\u00a0a view is generated for each entity type\u00a0(defined by an\u00a0<code>rdf_type</code>\u00a0attribute). That means that the mapping or data source that are used as input for the SqlEndpoint need to have a type or require a user defined type mapping.</p> <p>The operator has a\u00a0compatibility mode. Using it will avoid complex types such as Arrays. When arrays exit in the input they are converted to a String using the given\u00a0<code>arraySeperator</code>.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#sqlendpoint-activity","title":"SqlEndpoint Activity","text":"<p>The activity will\u00a0start\u00a0automatically, when the SqlEndpoint is used as a data sink and Build (DataIntegration) is configured to make the SqlEndpoint accessible remotely.</p> <p>When the activity is started and\u00a0running\u00a0it returns the server status and JDBC Url as its value.</p> <p>Stopping\u00a0the activity will drop all views generated by the activity. It can be\u00a0restarted\u00a0by rerunning the workflow containing it as a sink.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#remote-client-configuration-via-jdbc-and-odbc","title":"Remote Client Configuration (via JDBC and ODBC)","text":"<p>Within Build (DataIntegration) the SqlEndpoint can be used as a source or sink like any other dataset. If the\u00a0startThriftServer\u00a0option is set to\u00a0<code>true</code>\u00a0access via JDBC or ODBC is possible.</p> <p>ODBC\u00a0and\u00a0JDBC\u00a0drivers can be used to connect to relational databases. These drivers are used by clients like, Excel, PowerBI or other BI tools and transform standard SQL-queries to Hive-QL queries and handle the respective query results. Hive-QL support a subset of the SQL-92 standard. Depending on the complexity of the driver it \u2013 in case of a simple driver \u2013 supports the same subset or more modern standards. JDBC drivers are similar to ODBC ones, but serve as connectors for Java applications. When selecting a version of a driver the client operating system and its type (32bit/64 bit) are the most important factors. The version of the client drivers sometimes is the same as the servers. When no version of a driver is given the newest driver of the vendor should work, as it\u00a0should\u00a0be backwards compatible.</p> <p>Any JDBC or ODBC client can connect to a JDBC endpoint provided by an SqlEndpoint dataset. SqlEndpoint uses the same query processing as Hive, therefore the requirements for the client are:</p> <ul> <li>A JDBC driver compatible with\u00a0Hive 1.2.1\u00a0(platform independent driver org.apache.hive.jdbc.HiveDriver is needed) or</li> <li>Hive 1.2.1 is\u00a0ODPi\u00a0runtime compliant</li> <li>A JDBC driver compatible with\u00a0Spark 2.3.3</li> <li>A Hive ODBC driver (ODBC driver for the client architecture and operating system needed)</li> </ul> <p>A detailed instruction to connect to a Hive or SqlEndpoint endpoint with various tools (e.g. SQuirreL, beeline, SQL Developer, \u2026) can be found at\u00a0Apache HiveServer2 Clients.\\ The multi platform database client\u00a0DBeaver\u00a0can connect to the SQLEndpoint out of the box.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#partitioning-and-merging-of-data-sets","title":"Partitioning and merging of data sets","text":"<p>The execution on Spark is possible independent of the used file system as long as it can be referenced/is accessible for all cluster nodes. HDFS is recommended and the default settings are recommended for the best performance on a small cluster. Especially when working in local mode on the local file systems some problems can occur with the parallelism settings of Spark and the resulting partitioned output resources.</p> <p>Problems can be avoided by changing the following are the default settings:</p> <pre><code>spark.interpreter.options = {\n  # If true, data will be repartitioned before execution,\n  # otherwise the existing partitioning or no partitioning will be used\n  partitionOnImport = false\n  # Number of partitions for repartitioning on import, default = 16\n  partitionNumber = 16\n  # Specifies if data is combined before output is written to disk\n  combineOutput = false\n}\n</code></pre> <p>When running only locally the configuration should be like the following example (especially\u00a0<code>combineOutput</code>\u00a0has to be true):</p> <pre><code>spark.interpreter.options = {\n  # If true, data will be repartitioned before execution,\n  # otherwise the existing partitioning or no partitioning will be used\n  partitionOnImport = false\n  # Number of partitions for repartitioning on import, default = 16\n  partitionNumber = 4\n  # Specifies if data is combined before output is written to disk\n  combineOutput = true\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#other-options-specific-to-spark","title":"Other options specific to Spark","text":"<pre><code>#################################################\n# Spark                                        #\n#################################################\n\nspark.interpreter.options = {\n  # the default name used when creating a SparkContext (will override spark.app.name in spark-defaults.conf)\n  sparkAppName = \"eccenca DataIntegration Spark exe\n\n  # Enables more detailed logging, counting of transformed records, etc.\n  debugMode = false\n\n  # Enable or disable a spark executor event log for debugging purposes\n  eventLog = true\n\n  # Folder for logs that people don't want in the log even though the information is necessary for debugging\n  logFolder = ${elds.home}\"/var/dataintegration/logs/\"\n\n  # Enable or disable an execution time log for benchmarking purposes\n  timeLog = true\n\n  # Enable or disable Sparks built-in log for debugging purposes (will override spark.eventLog.enabled in spark-defaults.conf)\n  sparkLog = true\n\n  # If true, data will be repartitioned before execution, otherwise the existing partitioning or no partitioning will be used\n  partitionOnImport = false\n\n  # Number of partitions for repartitioning on Import\n  partitionNumber = 16\n\n  # Specifies number of Spark SQL shuffle partitions (will override spark.sql.shuffle.partitions in spark-defaults.conf)\n  shufflePartitions = 32\n\n  # Minimum partition number for Spark execution\n  defaultMinPartitions = 4\n\n  # Default parallelism partition number for Spark execution (will override spark.sql.shuffle.partitions in spark-defaults.conf)\n  defaultParallelism = 4\n\n  # Specifies if data is combined before output is written to disk. If true the final output will be in a single file on a single partition\n  combineOutput = true\n\n  # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. Views/virtual datasets can still be started but can only be accessed internally if set to false.\n  startThriftServer = false\n\n  # Internal data model used in Spark Data Frames: 'sequence' or 'simple'. Sequence behaves like the entities used by the local executor of DataIntegration and is\n  # sometimes be needed to work with non relational data (i.e. triple stores, dataplatform). The default value is 'simple' and casts most data objects to Strings\n  # which is fast and works in most situations may lead to less clean data.\n  columnType = sequence\n\n  # General additional Java options that will be passed to the executors (worker nodes) in the cluster, default is \"\".\n  sparkExecutorJavaOptions = \"\"\n\n  # General additional Java options that will be passed to the driver application (DataIntegration), default is \"\".\n  sparkDriverJavaOptions = \"\"\n\n  # Enable or disable the Spark UI. This UI provides an overview of the Spark cluster and running jobs. It will start on port 4040\n  # and increase the port number by 1 if the port is already in use. The final port will be shown in the logs. False by default.\n  enableSparkUI = true\n\n  # This property decides if Hive integration is enabled orr not.\n  # To use hive an external DB (such as MYSQL), the meta store, is needed. Please specify the necessary properties in the hive-site.xml. Note that Hive's default meta store (derby) should not be used in production and may lead to issues.\n  enableHiveSupport = false\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/","title":"DataIntegration: Activity Reference","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#activity-reference","title":"Activity Reference","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#project-activities","title":"Project Activities","text":"<p>The following activities are available for each project.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset-matcher","title":"Dataset matcher","text":"<p>Generates matches between schema paths and datasets based on the schema discovery and profiling information          of the datasets.</p> Parameter Type Description Example datasetUri String If set, run dataset matching only for this particular dataset. <p>The identifier for this plugin is <code>DatasetMatcher</code>.</p> <p>It can be found in the package <code>com.eccenca.di.datamatching</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#task-activities","title":"Task Activities","text":"<p>The following activities are available for different types of tasks.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#custom","title":"Custom","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-rest-task","title":"Execute REST Task","text":"<p>Executes the REST task.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteRestTask</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.rest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset","title":"Dataset","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset-profiler","title":"Dataset profiler","text":"<p>Generates profiling data of a dataset, e.g. data types, statistics etc.</p> Parameter Type Description Example datasetUri String Optional URI of the dataset resource that should be profiled. If not specified an URI will be generated. uriPrefix String Optional URI prefix that is prepended to every generated URI, e.g. property URIs for every schema path.  If not specified an URI prefix will be generated. entitySampleLimit String How many entities should be sampled for the profiling. If set to zero or a negative value, all entities will be considered. If left blank the configured default value is used. timeLimit String The time in milliseconds that each of the schema extraction step and profiling step should spend on. Leave blank for unlimited time. classProfilingLimit int The maximum number of classes that are profiled from the extracted schema. schemaEntityLimit int The maximum number of overall schema entities (types, properties/attributes) that will be extracted. executionType String The execution type to be used. At the moment, only \u2018LEGACY\u2019 is supported. <p>The identifier for this plugin is <code>DatasetProfiler</code>.</p> <p>It can be found in the package <code>com.eccenca.di.profiling</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#sql-endpoint-status","title":"SQL endpoint status","text":"<p>Shows the SQL endpoint status.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>SqlEndpointStatus</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.spark.endpoint.activity</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#types-cache","title":"Types cache","text":"<p>Holds the most frequent types in a dataset.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>TypesCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#linkspecification","title":"LinkSpecification","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#active-learning","title":"Active learning","text":"<p>Executes an active learning iteration.</p> Parameter Type Description Example fixedRandomSeed boolean No description <p>The identifier for this plugin is <code>ActiveLearning</code>.</p> <p>It can be found in the package <code>org.silkframework.learning.active</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#active-learning-find-comparison-pairs","title":"Active learning (find comparison pairs)","text":"<p>Suggest comparison pairs for the current linking task.</p> Parameter Type Description Example fixedRandomSeed boolean No description <p>The identifier for this plugin is <code>ActiveLearning-ComparisonPairs</code>.</p> <p>It can be found in the package <code>org.silkframework.learning.active.comparisons</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#evaluate-linking","title":"Evaluate linking","text":"<p>Evaluates the linking task by generating links.</p> Parameter Type Description Example includeReferenceLinks boolean Do not generate a link for which there is a negative reference link while always generating positive reference links. useFileCache boolean Use a file cache. This avoids memory overflows for big files. partitionSize int The number of entities in a single partition in the cache. generateLinksWithEntities boolean Generate detailed information about the matched entities. If set to false, the generated links won\u2019t be shown in the Workbench. writeOutputs boolean Write the generated links to the configured output of this task. linkLimit int If defined, the execution will stop after the configured number of links is reached.\\This is just a hint and the execution may produce slightly fewer or more links. timeout int Timeout in seconds after that the matching task of an evaluation should be aborted. Set to 0 or negative to disable the timeout. <p>The identifier for this plugin is <code>EvaluateLinking</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-linking","title":"Execute linking","text":"<p>Executes the linking task using the configured execution.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteLinking</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#linking-paths-cache","title":"Linking paths cache","text":"<p>Holds the most frequent paths for the selected entities.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>LinkingPathsCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#reference-entities-cache","title":"Reference entities cache","text":"<p>For each reference link, the reference entities cache holds all values of the linked entities.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ReferenceEntitiesCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#scheduler","title":"Scheduler","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#activate","title":"Activate","text":"<p>Executes the scheduler</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteScheduler</code>.</p> <p>It can be found in the package <code>com.eccenca.di.scheduler</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#scripttask","title":"ScriptTask","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-script","title":"Execute Script","text":"<p>Executes the script.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteScript</code>.</p> <p>It can be found in the package <code>com.eccenca.di.scripting.scala</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#transformspecification","title":"TransformSpecification","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-transform","title":"Execute transform","text":"<p>Executes the transformation.</p> Parameter Type Description Example limit IntOptionParameter Limits the maximum number of entities that are transformed. <p>The identifier for this plugin is <code>ExecuteTransform</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.transform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#transform-paths-cache","title":"Transform paths cache","text":"<p>Holds the most frequent paths for the selected entities.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>TransformPathsCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.transform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#target-vocabulary-cache","title":"Target vocabulary cache","text":"<p>Holds the target vocabularies</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>VocabularyCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.transform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#workflow","title":"Workflow","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-locally","title":"Execute locally","text":"<p>Executes the workflow locally.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteLocalWorkflow</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.workflow</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#workflowexecution","title":"WorkflowExecution","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#generate-spark-assembly","title":"Generate Spark assembly","text":"<p>Generate project and Spark assembly artifacts and deploy them using the specified configuration settings: type, artifact and options like destination in case of a simple copy</p> Parameter Type Description Example executeStaging boolean Execute loading phase executeTransform boolean Execute transform phase executeLoading boolean Execute staging phase <p>The identifier for this plugin is <code>DeploySparkWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#default-execution","title":"Default execution","text":"<p>Executes a workflow with the executor defined in the configuration</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteDefaultWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-operator","title":"Execute operator","text":"<p>Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster.</p> Parameter Type Description Example operator TaskReference The workflow to execute. <p>The identifier for this plugin is <code>ExecuteSparkOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-on-spark","title":"Execute on Spark","text":"<p>Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteSparkWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-with-payload","title":"Execute with payload","text":"<p>Executes a workflow with custom payload.</p> Parameter Type Description Example configuration MultilineStringParameter No description configurationType String No description optionalPrimaryResourceManager PluginObjectParameter <p>The identifier for this plugin is <code>ExecuteWorkflowWithPayload</code>.</p> <p>It can be found in the package <code>org.silkframework.workbench.workflow</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#generate-view","title":"Generate view","text":"<p>Generate and share a view on a workflow executed by the Spark executor. Executes a workflow on Spark and generates a SparkSQL temporary table instead of serializing the result. The table can be accessed via JDBC</p> Parameter Type Description Example caching boolean Optional parameter that enables caching (default=false). userDefinedName String Optional View name that is used when a view on a non virtual is generated (default = [TASK-ID]_generated_view). <p>The identifier for this plugin is <code>GenerateSparkView</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.spark.virtual</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/","title":"DataIntegration: Plugin Reference","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#plugin-reference","title":"Plugin Reference","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#plugin-tasks","title":"Plugin Tasks","text":"<p>The following plugin tasks are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cancel-workflow","title":"Cancel Workflow","text":"<p>Cancels a workflow if a specified condition is fulfilled.</p> Parameter Type Description Default typeUri Uri The entity type to check the condition on. condition Enum The cancellation condition empty invertCondition boolean If true, the specified condition will be inverted, i.e., the workflow execution will be cancelled if the condition is not fulfilled. false failWorkflow boolean If true, the workflow execution will fail if the condition is met. If false, the workflow execution would be stopped, but shown as successfull. false <p>The identifier for this plugin is <code>CancelWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.cancel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#concatenate-to-file","title":"Concatenate to file","text":"<p>Concatenates values into a file.</p> Parameter Type Description Default path String Values from this path will be concatenated. no default mimeType String MIME type of the output file. empty string prefix MultilineStringParameter Prefix to be written before the first value. glue MultilineStringParameter Separator to be inserted between concatenated values. suffix MultilineStringParameter Suffix to be written after the last value. charset String The file encoding. UTF-8 fileExtension String File extension of the output file. .tmp <p>The identifier for this plugin is <code>ConcatenateToFile</code>.</p> <p>It can be found in the package <code>com.eccenca.di.concatenate</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sql-query","title":"SQL query","text":"<p>Executes a custom SQL query on the first input dataset and returns the result as its output.</p> Parameter Type Description Default command SqlCodeParameter SQL command. The name of the table in the statement must be \u2018dataset\u2019, regardless the input. <p>The identifier for this plugin is <code>CustomSQLExecution</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.operator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#distinct-by","title":"Distinct by","text":"<p>Removes duplicated entities based on a user-defined path. Note that this operator does not retain the order of the entities.</p> Parameter Type Description Default distinctPath String Entities that share this path will be deduplicated. no default resolveDuplicates Enum Strategy to resolve duplicates. keepLast <p>The identifier for this plugin is <code>DistinctBy</code>.</p> <p>It can be found in the package <code>com.eccenca.di.distinct</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-json","title":"Parse JSON","text":"<p>Parses an incoming entity as a JSON dataset. Typically, it is used before a transformation task. Takes exactly one input of which only the first entity is processed.</p> Parameter Type Description Default inputPath String The Silk path expression of the input entity that contains the JSON document. If not set, the value of the first defined property will be taken. empty string basePath String The path to the elements to be read, starting from the root element, e.g., \u2018/Persons/Person\u2019. If left empty, all direct children of the root element will be read. empty string uriSuffixPattern String A URI pattern that is relative to the base URI of the input entity, e.g., /{ID}, where {path} may contain relative paths to elements. This relative part is appended to the input entity URI to construct the full URI pattern. empty string navigateIntoArrays boolean Navigate into arrays automatically. If set to false, the <code>#array</code> path operator must be used to navigate into arrays. true <p>The identifier for this plugin is <code>JsonParserOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.json</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#join-tables","title":"Join tables","text":"<p>Joins a set of inputs into a single table. Expects a list of entity tables and links.  All entity tables are joined into the first entity table using the provided links.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>Merge</code>.</p> <p>It can be found in the package <code>com.eccenca.di.merge</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#merge-tables","title":"Merge tables","text":"<p>Stores sets of instance and mapping inputs as relational tables with the mapping as an n:m relation. Expects a list of entity tables and links.  All entity tables have a relation to the first entity table using the provided links.</p> Parameter Type Description Default multiTableOutput boolean test true pivotTableName String Name of the pivot table. empty string mappingNames String Name of the mapping tables. Comma separated list. empty string instanceSetNames String Name of the tables joined to the pivot. Comma separated list. empty string <p>The identifier for this plugin is <code>MultiTableMerge</code>.</p> <p>It can be found in the package <code>com.eccenca.di.merge</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pivot","title":"Pivot","text":"<p>The pivot operator takes data in separate rows, aggregates it and converts it into columns.</p> <p>The operator works on a flat input schema only and creates a flat output schema.</p> <p>A pivot table is a data summarization that is used to automatically sort, count, total, or average data in a dataset. It allows you to view the data from a different perspective.</p> <p>The following aggregation (summary) functions are available:</p> <ul> <li>first -  Shows the first value (works with numbers and strings)</li> <li>min - Shows the lowest value (works with numbers and strings)</li> <li>max - Shows the highest value (works with numbers and strings)</li> <li>sum - Adds up the values (works with numbers only)</li> <li>average - Finds the average of the values (works with numbers only)</li> </ul> Parameter Type Description Default pivotProperty String The pivot column refers to the column in the input data that is used to organize the data along the horizontal axis of the pivot table. no default firstGroupProperty String The name of the first group column in the range. All columns starting with this will be grouped. no default lastGroupProperty String The name of the last group column in the range. If left empty, only the first column is grouped. empty string valueProperty String The property that contains the grouped values that will be aggregated. no default aggregationFunction Enum The aggregation function used to aggregate values. sum uriPrefix String Prefix to prepend to all generated pivot columns. empty string <p>The identifier for this plugin is <code>Pivot</code>.</p> <p>It can be found in the package <code>com.eccenca.di.pivot</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#execute-rest-request-deprecated","title":"Execute REST request (deprecated)","text":"<p>Executes a REST request based on fixed configuration and/or input parameters and returns the result as entity.</p> Parameter Type Description Default url String The URL to execute this request against. This can be overwritten at execution time via input. empty string method String The HTTP method. One of GET, PUT, POST, PATCH or DELETE GET accept String The accept header String. empty string requestTimeout int Request timeout in ms. The overall maximum time the request should take. 10000 connectionTimeout int Connection timeout in ms. The time until which a connection with the remote end must be established. 5000 readTimeout int Read timeout in ms. The max. time a request stays idle, i.e. no data is send or received. 10000 contentType String The content-type header String. This can be set in case of PUT, PATCH or POST. If another content type comes back, the task will fail. empty string content String The content that is send with a POST, PATCH or PUT request. For handling this payload dynamically this parameter must be overwritten via the task input. empty string httpHeaders MultilineStringParameter Configure additional HTTP headers. One header per line. Each header entry follows the curl syntax. readParametersFromInput boolean If this is set to true, specific parameters can be overwritten at execution time. Else inputs are ignored. Parameters that can currently be overwritten: url, content false multipartFileParameter String If set to a non-empty String then instead of a normal POST a multipart/form-data file upload request is executed. This value is used as the form parameter name. empty string authorizationHeader String The authorization header. This is usually either \u2018Authorization\u2019 or \u2018Proxy-Authorization\u2019If left empty, no authorization header is sent. empty string authorizationHeaderValue PasswordParameter The authorization header value. Usually this has the form \u2018type secret\u2019, e.g. for OAuth \u2018bearer .\u2019This config parameter will be encrypted in the backend. acceptAnySslCertificate boolean If enabled this will accept any SSL certificate, i.e. make SSL connections unsecure. Only enable if you know what you are doing! false <p>The identifier for this plugin is <code>RestOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.rest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#scheduler","title":"Scheduler","text":"<p>Executes a workflow at specified intervals.</p> Parameter Type Description Default task TaskReference The name of the workflow to be executed no default interval Duration The interval at which the scheduler should run the referenced task. Must be in ISO-8601 duration format PnDTnHnMn.nS PT15M startTime String The time when the scheduled task is run for the first time, e.g., 2017-12-03T10:15:30. If no start time is set, midnight on the day the scheduler is started is assumed. empty string enabled boolean Enables or disables the scheduler. true stopOnError boolean If true, this will stop the scheduler, so the failed task is not scheduled again for execution. false <p>The identifier for this plugin is <code>Scheduler</code>.</p> <p>It can be found in the package <code>com.eccenca.di.scheduler</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#search-addresses","title":"Search addresses","text":"<p>Looks up locations from textual descriptions using the configured geocoding API. Outputs results as RDF.</p> Parameter Type Description Default searchAttributes StringIterableParameter List of attributes that contain search terms. Multiple attributes (comma-separated) will be concatenated into a single search. no default limit IntOptionParameter Optionally limits the number of results for each search. jsonLdContext ResourceOption Optional JSON-LD context to be used for converting the returned JSON to RDF. If not provided, a default context will be used. additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>SearchAddresses</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#send-email","title":"Send eMail","text":"<p>Sends an eMail using an SMTP server. If connected to a dataset that is based on a file in a workflow, it will send that file whenever the workflow is executed It can be used to send the result of a workflow via Mail.</p> Parameter Type Description Default host String The SMTP host, e.g, mail.myProvider.com no default port int The SMTP port 587 user String Username empty string password PasswordParameter Password from String The sender eMail address empty string receiver String The email addresses of the receivers. Email addresses are comma separated. Names must be quoted when containing commas.Example: john.smith@example.com, \u201cDoe, John\u201d john.doe@example.com, needs no quoting needs.no.quoting@example.com empty string cc String The CC-receiver eMail address. Email addresses are comma separated. Names must be quoted when containing commas.Example: john.smith@example.com, \u201cDoe, John\u201d john.doe@example.com, needs no quoting needs.no.quoting@example.com empty string bcc String The BCC-receiver eMail address. Email addresses are comma separated. Names must be quoted when containing commas.Example: john.smith@example.com, \u201cDoe, John\u201d john.doe@example.com, needs no quoting needs.no.quoting@example.com empty string subject String The eMail subject Dataset message MultilineStringParameter The eMail text message withAttachment boolean If enabled a file from the input is attached to the email. A single input to this operator is expected that provides a file, e.g. a file based dataset (XML, JSON etc.). true sslConnection boolean When enabled a SSL/TLS connection will be forced from the start without negotiation with the server. Not to be confused with STARTTLS which upgrades an insecure connection to a SSL/TLS connection, which is done by default. false timeout int Timeout in milliseconds to establish a connection or wait for a server response. Setting it to 0 or negative number will disable the timeout. 10000 readParametersFromInput boolean When enabled this allows to send multiple e-mails. All e-mail configurations are input via the first operator input with each entry representing a different e-mail. The optional second input can be a file based dataset for the attachment. E-mail parameters that can be overwritten are: from, receiver, cc, bcc, subject and message. false nrRetries int The number of retries per email when send errors are encountered. 2 delayBetweenDeliveriesMS int The delay in milliseconds between sending two consecutive e-mails. This applies to the retry mechanism, but also to sending multiple e-mails. 2 <p>The identifier for this plugin is <code>SendEMail</code>.</p> <p>It can be found in the package <code>com.eccenca.di.mail</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#execute-spark-function","title":"Execute Spark function","text":"<p>Applies a specified Scala function to a specified field. E.g. when the inputField is \u2018name\u2019, the inputFunction is \u2018any =&gt; \u201cArrrrgh!\u201d and the alias is \u2018xxx\u2019,)\u2019 a query corresponding to \u2018Function existingField1, existingFiled2, \u2026 \u201cArrrrgh!\u201d as \u201cxxx\u201d\u2019  will be generated. If alias is empty the inputField will be overwritten, otherwise a new field will be added and the rest of the schema stays the same.</p> Parameter Type Description Default function MultilineStringParameter Scala function expression. inputField String Input field. empty string alias String Alias. no default <p>The identifier for this plugin is <code>SparkFunction</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.operator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#evaluate-template","title":"Evaluate template","text":"<p>Evaluates a template on a sequence of entities. Can be used after a transformation or directly after datasets that output a single table, such as CSV or Excel.</p> Parameter Type Description Default template TemplateParameter The template no default language String The template language. Currently, Jinja is supported. jinja outputAttribute String The attribute in the output that will hold the evaluated template. output fullEvaluation boolean If enabled, the entire input set will be evaluated at once. The template will receive a hierarchical \u2018entities\u2019 variable that can be iterated over. A single output entity will be generated that contains the evaluated template. false forwardInputAttributes boolean If true, the input attributes will be forwarded to the output. false <p>The identifier for this plugin is <code>Template</code>.</p> <p>It can be found in the package <code>com.eccenca.di.templating.operators</code>.</p> <p>The template operator supports the Jinja templating language. Documentation about Jinja can be found in the official Template Designer Documentation.</p> <p>Note that support for RDF properties is limited, because Jinja does not support some special characters (in particula colons) in variable names. This makes it impractical to access RDF properties. For this reason, the transformation that precedes the template operator needs to make sure that it generates attributes that are valid Jinja variable names.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#default-evaluation","title":"Default evaluation","text":"<p>By default, the template is evaluated separately for each entity. For each input entity, a output entity is generated that provides a single output attribute, which contains the evaluated template.</p> <p>Limitation: For the default evaluation, accessing nested paths is not supported. If the preceding transformation contains hierarchical mappings, only the attributes from the root mapping can be accessed.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#full-evaluation","title":"Full evaluation","text":"<p>If \u2018full evaluation\u2019 is enabled, the entire input set will be evaluated at once.</p> <p>The entities variable will contain all input entities and can be iterated over:</p> <pre><code>{% for entity in entities %}\n{{entity.property}}\n{% endfor %}\n</code></pre> <p>A single output entity will be generated that contains the evaluated template.</p> <p>If the input entities are hierarchical (typically the case if the input transformation is hierarchical), each entity will be hierarchical as well.</p> <p>Example iterating over an sequence of books that each contains a list of chapters:</p> <pre><code>{% for book in entities %}\nBook {{book.title}}\n{% for chapter in book.chapter %}\nChapter {{chapter.chapterNumber}}\n{% endfor %}\n{% endfor %}\n</code></pre> <p>In this example, the child mapping defines a <code>chapter</code> target property from which it is accessible from the root entities. If the child mapping allows multiple entities, the value of the property will be a list of entities.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#unpivot","title":"Unpivot","text":"<p>Given a list of table columns, transforms those columns into attribute-value pairs.</p> Parameter Type Description Default firstPivotProperty String The name of the first pivot column in the range. no default lastPivotProperty String the name of the last pivot column in the range. If left empty, all columns starting with the first pivot column are used. empty string attributeProperty String The URI of the output column used to hold the attribute. attribute valueProperty String The URI of the output column used to hold the value. value pivotColumns String Comma separated list of pivot column names. This property will override all inferred columns of the first two arguments. empty string <p>The identifier for this plugin is <code>Unpivot</code>.</p> <p>It can be found in the package <code>com.eccenca.di.unpivot</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-xml","title":"Parse XML","text":"<p>Takes exactly one input and reads either the defined inputPath or the first value of the first entity as XML document. Then executes the given output entity schema similar to the XML dataset to construct the result entities.</p> Parameter Type Description Default inputPath String The Silk path expression of the input entity that contains the XML document. If not set, the value of the first defined property will be taken. empty string basePath String The path to the elements to be read, starting from the root element, e.g., \u2018/Persons/Person\u2019. If left empty, all direct children of the root element will be read. empty string uriSuffixPattern String A URI pattern that is relative to the base URI of the input entity, e.g., /{ID}, where {path} may contain relative paths to elements. This relative part is appended to the input entity URI to construct the full URI pattern. empty string <p>The identifier for this plugin is <code>XmlParserOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#add-project-files","title":"Add project files","text":"<p>Adds file resources to the project that are piped into the input port.</p> Parameter Type Description Default fileName String File name of the uploaded file(s). If multiple files are uploaded, an index will be appended to the file name. If left empty, the existing file names will be used. empty string directory String Directory to which the files should be written. If left empty, the files will be uploaded to the project root directory. Note that all files will be written to this directory even if they have been read from a different project directory initially. empty string overwriteStrategy Enum The strategy to use if a file with the same name already exists. fail <p>The identifier for this plugin is <code>addProjectFiles</code>.</p> <p>It can be found in the package <code>org.silkframework.dataset.operations</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#delete-project-files","title":"Delete project files","text":"<p>Removes file resources from the project based on a regular expression.</p> Parameter Type Description Default filesRegex String The regex for filtering the file names. The regex needs to match the full path (i.e. from beginning to end, including sub-directories) in order for the file to be deleted. no default outputEntities boolean If enabled the operator outputs entities, one entity for each deleted file, with the path of the file as attribute \u2018filePath\u2019. false <p>The identifier for this plugin is <code>deleteProjectFiles</code>.</p> <p>It can be found in the package <code>org.silkframework.dataset.operations</code>.</p> <p>Removes file resources from the project based on a regular expression (regex).</p> <p>The project-relative path of each file of the current project is tested against a user given regular expression and the file is deleted if the expression matches this name. The file names include the sub-directory structure if present but do not start with a <code>/</code>. The regular expression has to match the full path of the file and is case sensitive.</p> <p>Given this list of example files of a project:</p> <pre><code>dataset.csv\nmy-dataset.xml\njson/example.json\njson/example_new.json\njson/data.xml\n</code></pre> <p>Here are some regular expressions with the expected result:</p> <ul> <li>The regex <code>dataset\\.csv</code> deletes only the first file.</li> <li>The regex <code>json/.*</code> deletes all files in the <code>json</code> sub-directory.</li> <li>The regex <code>new</code> deletes nothing.</li> <li>The regex <code>.*new.*</code> deletes the file <code>json/example_new.json</code> (and all other files with <code>new</code> in the path)</li> </ul> <p>We recommend testing your regular expression before using it. regex101.com is a nice service to test your regular expressions. This deep-link provides a test bed using the example files and the last expression from the list.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#download-file","title":"Download file","text":"<p>Downloads a file from a given URL.</p> Parameter Type Description Default url String The URL of the file to be downloaded. empty string accept String The accept header String. empty string requestTimeout int Request timeout in ms. The overall maximum time the request should take. 10000 connectionTimeout int Connection timeout in ms. The time until which a connection with the remote end must be established. 5000 readTimeout int Read timeout in ms. The max. time a request stays idle, i.e. no data is send or received. 10000 httpHeaders MultilineStringParameter Configure additional HTTP headers. One header per line. Each header entry follows the curl syntax. authorizationHeader String The authorization header. This is usually either \u2018Authorization\u2019 or \u2018Proxy-Authorization\u2019If left empty, no authorization header is sent. empty string authorizationHeaderValue PasswordParameter The authorization header value. Usually this has the form \u2018type secret\u2019, e.g. for OAuth \u2018bearer .\u2019This config parameter will be encrypted in the backend. <p>The identifier for this plugin is <code>downloadFile</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.rest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#upload-file-to-knowledge-graph","title":"Upload File to Knowledge Graph","text":"<p>Uploads an N-Triples or Turtle (limited support) file from the file repository to a \u2018Knowledge Graph\u2019 dataset. The output of this operatorcan be the input of datasets that support graph store file upload, e.g. \u2018Knowledge Graph\u2019. The file will be uploaded to the graph specified in that dataset.</p> Parameter Type Description Default fileNT Resource RDF file (N-Triples or Turtle) from the resource repository that should be uploaded to the Knowledge Graph. no default maxChunkSizeInMB IntOptionParameter The N-Triples file will be split into multiple chunks if the file size exceeds the max chunk size. For Turtle files this parameter is ignored since no chunking is supported. contentType Enum The MIME type of the serialization format of the RDF file. application/n-triples <p>The identifier for this plugin is <code>eccencaDataPlatformGraphStoreFileUploadOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.dataplatform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#execute-rest-requests","title":"Execute REST requests","text":"<p>REST operator that fetches and optionally merges data from a REST endpoint. It supports executing multiple requests either via input entities that each overwrite config parameters or via paging. If you only need to download a single file, the \u201cDownload file\u201d operator might be the better option. Most features are currently only supported for JSON REST APIs. From multiple requests the REST operator can produce a merged JSON result, i.e. for JSON it will concatenate all results in a JSON array. Alternatively multiple results can be written directly to file (of a JSON dataset), either as a merged JSON file or one file per request inside a ZIP file. By default the output of this operator is an entity with a single property \u2018result\u2019, which is the (concatenated) JSON string.</p> Parameter Type Description Default url String The URL to execute this request against. This can be overwritten at execution time via input. empty string method Enum One of the following HTTP methods: GET, POST, PUT, PATCH or DELETE. GET accept String The accept header String. empty string requestTimeout int Request timeout in ms. The overall maximum time the request should take. 10000 connectionTimeout int Connection timeout in ms. The time until which a connection with the remote end must be established. 5000 readTimeout int Read timeout in ms. The max. time a request stays idle, i.e. no data is send or received. 10000 contentType String The content-type header String. This can be set in case of PUT or POST. If another content type comes back, the task will fail. empty string content String The content that is send with a POST, PUT or PATCH request. For handling this payload dynamically this parameter must be overwritten via the task input. empty string httpHeaders MultilineStringParameter Configure additional HTTP headers. One header per line. Each header entry follows the curl syntax. readParametersFromInput boolean If this is set to true, specific parameters can be overwritten at execution time and one request per overwrite config will be executed. Else inputs are ignored and exactly one request will be executed. Parameters that can currently be overwritten: url, content false multipartFileParameter String If set to a non-empty String then instead of a normal POST a multipart/form-data file upload request is executed. This value is used as the form parameter name. empty string authorizationHeader String The authorization header. This is usually either \u2018Authorization\u2019 or \u2018Proxy-Authorization\u2019If left empty, no authorization header is sent. empty string authorizationHeaderValue PasswordParameter The authorization header value. Usually this has the form \u2018type secret\u2019, e.g. for OAuth \u2018bearer .\u2019This config parameter will be encrypted in the backend. delayBetweenRequests int The delay between requests in milliseconds. 0 retriesPerRequest int How often should a single request be retried if it fails. 3 abortOnRequestFail boolean If a single request fails, i.e. it reaches its max. retry count, should the execution then be aborted or the next requests be executed. true limit int If this is set to a number greater 0, then only this number of input REST configurations will be executed. Mainly used for debugging and executing a subset. 0 offset int How many input entries to skip. 0 maxFailedRequests int If set to greater 0, then the execution will abort if more than the given number of requests have failed. This should be used to fail early. If \u2018abort on request fail\u2019 is set to true, then this option has no effect. 0 pagingMethod Enum \\There are two paging methods currently supported:\\1. Next page full URL: The JSON response contains the full URL of the next page. This URL will be used for the subsequent request.\\2. Next page identifier: The JSON response contains the ID of the next page. This ID will be used as query parameter for the subsequent request.\\In both cases the path to the next page value in the response JSON must be defined via the \u2018Next page JSON path\u2019 parameter.\\In case of the \u2018Identifier next page parameter\u2019 paging method, also the parameter \u2018Next page ID query parameter\u2019 must be set.\\ none nextPageJsonPath String The path to the JSON value containing the next page value of the JSON response, e.g. paging/next. The path syntax follows the Silk path syntax, but only allows forward paths. empty string nextPageIdQueryParameter String The query parameter name for the next page ID that should be attached to the next page URI request. This is necessary for the \u2018Next page identifier\u2019 paging method. empty string outputResultAsFile boolean \\If a file based dataset is connected to the output of the REST operator, then this option can be enabled in order to overwrite the file resource\\of the connected dataset. This allows for handling the result of the REST request/s as a normal dataset. If a non-file based dataset\\is connected to this operator the execution will fail.\\If disabled, a single entity with a single property \u2018result\u2019 will be output that contains the (merged) result.\\ false urlProperty String If this is non-empty, a property is created in the root JSON object (if it exists) with the same name that has the request URL as value. This is mostly relevant if the request URL cannot be re-constructed from the response data. Only supported for JSON response data. empty string <p>The identifier for this plugin is <code>eccencaRestOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.rest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#core-parameter-overview","title":"Core parameter overview","text":"<ul> <li><code>URL</code>: The URL the request will be executed against. This value can be overwritten at execution time when the \u2018Read parameters from input\u2019 option          is enabled. This value will also be adapted when a paging approach is configured, see the paging section for more details.</li> <li><code>Method</code>: One of the following HTTP methods: GET, POST, PUT, PATCH or DELETE.</li> <li><code>Accept</code>: The ACCEPT header value for content negotiation, e.g. \u2018application/json\u2019.</li> <li><code>Content type</code>: The CONTENT-TYPE header value. This is usually used for POST, PUT or PATCH requests when the API endpoint                   supports multiple different MIME types and/or requires a content MIME type to be set. E.g. \u2018application/json\u2019</li> <li><code>Content</code>: The text content of a POST, PUT or PATCH request. This value can be overwritten at execution time when the              \u2018Read parameters from input\u2019 option is enabled.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#authorization","title":"Authorization","text":"<p>If the request needs authorization following parameters should be set, else there is no authorization header sent.</p> <ul> <li><code>Authorization header</code>: The header that is used for authorization, usually either \u2018Authorization\u2019 or \u2018Proxy-Authorization\u2019.</li> <li><code>Authorization header value</code>: The secret value for the authorization, i.e. password or token. This value will be encrypted                                 and cannot be accessed in the user interface anymore after saving it.                                 E.g. for OAuth the value would have the following form: <code>bearer &lt;TOKEN_VALUE&gt;</code>.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sending-multiple-requests","title":"Sending multiple requests","text":"<p>In the default configuration a single requests is sent. If multiple requests should be sent with different URLs and/or content, the configurations for these requests must be defined via the input port of the REST operator.</p> <ul> <li> <p>Read parameters from input:     The \u2018URL\u2019 and \u2018Content\u2019 parameter values are read from entities that are input via the input     port of the operator. The property names are \u2018url\u2019 and \u2018content\u2019 and only overwrite the     original parameter value if defined.</p> <p>For each input entity a separate request will be sent. - Limit: If set to a positive number, then only that number of input entities will be processed as requests. - Offset: If set to a positive number, then that many input entities will be ignored before processing them as requests.</p> </li> </ul> <p>If the option \u2018Read parameters from input\u2019 is enabled, it is currently always assumed that multiple requests will be sent. The responses must either be of type JSON, then the results are merged into a JSON array, or the \u2018Output result as file\u2019 option must be enabled in order to write a merged JSON or a ZIP file. See section \u2018Output options\u2019 for more details.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#paging","title":"Paging","text":"<p>If the REST endpoint does not return all results in a single response, multiple requests (one per page) must usually be sent in order to fetch all results. This is currently only supported for JSON requests.</p> <ul> <li> <p><code>Paging method</code>:    There are two paging methods currently supported:</p> <ol> <li><code>Next page full URL</code>: The JSON response contains the full URL of the next page. This URL will be used for the subsequent request URL.</li> <li><code>Next page identifier</code>: The JSON response contains the ID of the next page. This ID will be used as query parameter value for the subsequent request.</li> </ol> </li> </ul> <p>In both cases the path to the next page value in the response JSON must be defined via the \u2018Next page JSON path\u2019 parameter.    In case of the \u2018Next page identifier\u2019 paging method, also the parameter \u2018Next page ID query parameter\u2019 must be set. - <code>Next page JSON path</code>: The property path in the result JSON where the \u2018next page\u2019 URL/value is provided.    E.g. for following response structure, the value for this parameter would be <code>paging/next</code>:</p> <p><pre><code>  {\n    ...,\n    \"paging\": {\n      \"next\": \"Next ID\"\n    }\n  }\n</code></pre> - <code>Next page ID query parameter</code>: If the paging method is \u2018Next page identifier\u2019, this defines the query parameter name that should   be attached to the original request URL in combination with the \u2018next page\u2019 value of the current response in order   to request the next page.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#setting-http-headers","title":"Setting HTTP headers","text":"<ul> <li><code>HTTP headers</code>: This parameter allows to set HTTP headers of the request being made. Each line of the multi-line value should contain a single header, e.g.   <pre><code>Accept-Language: en-US,en;q=0.5\nCache-Control: max-age=0\n</code></pre></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sending-a-multipart-http-file-request","title":"Sending a multipart HTTP file request","text":"<p>If the content of a POST request should be sent as file content of a multipart HTTP request, instead of the request body, following parameter must be configured:</p> <ul> <li><code>Multi-part file parameter</code>: If set to a non-empty value then, instead of a normal POST request, a multipart/form-data                                file upload request will be executed.                                The value of this parameter is used as the form parameter name.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#output-options","title":"Output options","text":"<p>By default, the response body of a request is output as value of the \u2018result\u2019 property of a single output entity. If the response body needs to be processed this can e.g. be achieved with the \u2018Parse JSON\u2019 operator. Alternatively the response/s can be written to a file based dataset. Currently only text based datasets are supported.</p> <p>The results of multiple requests, see section \u2018Sending multiple requests\u2019 for details, can be written to a single, merged file (only supported for JSON) or to a ZIP archive, i.e. a file resource that must end in \u2018.zip\u2019. In the latter case an entry per request is added to the ZIP file. Currently, the following datasets support the processing of ZIP files: JSON, XML, CSV and RDF file.</p> <ul> <li><code>Output result as file</code>: If enabled, instead of outputting a single entity, the result/s will be written directly                            to the file of the file-based dataset that is connected to the output of this operator.</li> </ul> <p>If the option \u2018Read parameters from input\u2019 is enabled, it is currently always assumed that multiple requests will be sent. The responses must either be JSON, then the results are merged into a JSON array or the \u2018Output result as file\u2019 option must be enabled in order to write a merged JSON or ZIP file.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fine-tuning-timeouts","title":"Fine-tuning timeouts","text":"<p>If requests can take a much longer time than what can usually be expected, it is possible to increase the timeouts to control when a request should eventually fail.</p> <ul> <li><code>Request timeout</code>: The maximum overall time in milliseconds the request is allowed to take. Default: <code>10000</code>.</li> <li><code>Connection timeout</code>: The maximum time in milliseconds the request is allowed to establish a connection to the server. Default: <code>5000</code>.</li> <li><code>Read timeout</code>: The maximum time a request is allowed to stay idle, i.e. the time while it receives no data. Usually this    should be greater than the time span between the request being sent and the first data being received. Default: <code>10000</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#throttling-requests","title":"Throttling requests","text":"<p>If a lot of requests are sent via the \u2018Read parameters from input\u2019 option, it can make sense to throttle the number of requests sent in a specific time span.</p> <ul> <li><code>Delay between requests</code>: The delay between subsequent requests in milliseconds. Default: <code>0</code>.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#error-handling","title":"Error handling","text":"<p>Following parameters can be tuned in order to decide when an execution should be considered as failed.</p> <ul> <li><code>Retries per request</code>: How often a single request configuration (URL, content) should be retried before considering this                          request configuration as failed. Default: <code>3</code></li> <li><code>Abort when request fails</code>: When enabled, if a single request configuration eventually fails, i.e. it reaches its max. retry count,                               the overall execution of the REST operator will fail.</li> <li><code>Max failed requests</code>: If set to a value greater 0, the execution will abort if more than the given number of request configurations                          have failed (reached max. retries). This can be used if a number of failed requests can be tolerated.                          When \u2018Abort when request fails\u2019 is enabled, this option is ignored.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#propagating-the-request-url","title":"Propagating the request URL","text":"<p>If having the request URL in the response data is needed, following parameter needs to be configured:</p> <ul> <li><code>URL property</code>: If this parameter is non-empty the request URL will be added to the response JSON object. It will be added as value to                   a property with the specified name in the root level of the response JSON object.                   This is mostly relevant if the request URL cannot be re-constructed from the response data. Only supported for JSON responses.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#get-project-files","title":"Get project files","text":"<p>Get file resources from the project.</p> Parameter Type Description Default fileName String The path of the project file to retrieve. Leave empty if the file regex parameter should be used. empty string filesRegex String Optional regular expression for retrieving files. The regex needs to match the full path (i.e. from beginning to end, including sub-directories). empty string <p>The identifier for this plugin is <code>getProjectFiles</code>.</p> <p>It can be found in the package <code>org.silkframework.dataset.operations</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-construct-query","title":"SPARQL Construct query","text":"<p>A task that executes a SPARQL Construct query on a SPARQL enabled data source and outputs the SPARQL result. If the result should be written to the same RDF store it is read from, the SPARQL Update operator is preferable.</p> Parameter Type Description Default query SparqlCodeParameter A SPARQL 1.1 construct query no default tempFile boolean When copying directly to the same SPARQL Endpoint or when copying large amounts of triples, set to True by default true <p>The identifier for this plugin is <code>sparqlCopyOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.tasks</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-select-query","title":"SPARQL Select query","text":"<p>A task that executes a SPARQL Select query on a SPARQL enabled data source and outputs the SPARQL result. If the SPARQL source is defined on a specific graph, a FROM clause will be added to the query at execution time, except when there already exists a GRAPH or FROM clause in the query. FROM NAMED clauses are not injected.</p> Parameter Type Description Default selectQuery SparqlCodeParameter A SPARQL 1.1 select query no default limit String If set to a positive integer, the number of results is limited empty string optionalInputDataset SparqlEndpointDatasetParameter An optional SPARQL dataset that can be used for example data, so e.g. the transformation editor shows mapping examples. sparqlTimeout int SPARQL query timeout (select/update) in milliseconds. A value of zero means that there is no timeout set explicitly. If a value greater zero is specified this overwrites possible default timeouts. 0 <p>The identifier for this plugin is <code>sparqlSelectOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.tasks</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-update-query","title":"SPARQL Update query","text":"<p>A task that outputs SPARQL Update queries for every entity from the input based on a SPARQL Update template. The output of this operator should be connected to the SPARQL datasets to which the results should be written. In contrast to the SPARQL select operator, no FROM clause gets injected into the query.</p> Parameter Type Description Default sparqlUpdateTemplate SparqlCodeParameter \\This operator takes a SPARQL Update Query Template that depending on the templating mode (Simple/Velocity Engine) supports\\a set of templating features, e.g. filling in input values via placeholders in the template.\\Example for the \u2018Simple\u2019 mode:\\  DELETE DATA { ${} rdf:label ${\u201cPROP_FROM_ENTITY_SCHEMA2\u201d} }\\  INSERT DATA { ${} rdf:label ${\u201cPROP_FROM_ENTITY_SCHEMA3\u201d} }\\  \\  This will insert the URI serialization of the property value PROP_FROM_ENTITY_SCHEMA1 for the ${} expression.\\  And it will insert a plain literal serialization for the property values PROP_FROM_ENTITY_SCHEMA2/3 for the template literal expressions.\\  It is be possible to write something like ${\u201cPROP\u201d}^^http://someDatatype or ${\u201cPROP\u201d}@en.\\Example for the \u2018Velocity Engine\u2019 mode:\\  DELETE DATA { $row.uri(\u201cPROP_FROM_ENTITY_SCHEMA1\u201d) rdf:label $row.plainLiteral(\u201cPROP_FROM_ENTITY_SCHEMA2\u201d) }\\  #if ( $row.exists(\u201cPROP_FROM_ENTITY_SCHEMA1\u201d) )\\    INSERT DATA { $row.uri(\u201cPROP_FROM_ENTITY_SCHEMA1\u201d) rdf:label $row.plainLiteral(\u201cPROP_FROM_ENTITY_SCHEMA3\u201d) }\\  #end\\  Input values are accessible via various methods of the \u2018row\u2019 variable:\\  - uri(inputPath: String): Renders an input value as URI. Throws exception if the value is no valid URI.\\  - plainLiteral(inputPath: String): Renders an input value as plain literal, i.e. escapes problematic characters etc.\\  - rawUnsafe(inputPath: String): Renders an input value as is, i.e. no escaping is done. This should only be used \u2013 better never \u2013 if the input values can be trusted.\\  - exists(inputPath: String): Returns true if a value for the input path exists, else false.\\  The methods uri, plainLiteral and rawUnsafe throw an exception if no input value is available for the given input path.\\  In addition to input values, properties of the input and output tasks can be accessed via the inputProperties and outputProperties objects\\  in the same way as the row object, e.g.\\    $inputProperties.uri(\u201cgraph\u201d)\\  For more information about the Velocity Engine visit http://velocity.apache.org.\\ no default batchSize int How many entities should be handled in a single update request. 1 templatingMode Enum The templating mode. \u2018Simple\u2019 only allows simple URI and literal insertions, whereas \u2018Velocity Engine\u2019 supports complex templating. See \u2018Sparql Update Template\u2019 parameter description for examples and http://velocity.apache.org for details on the Velocity templates. simple <p>The identifier for this plugin is <code>sparqlUpdateOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.tasks</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#request-rdf-triples","title":"Request RDF triples","text":"<p>A task that requests all triples from an RDF dataset.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>tripleRequestOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.tripleRequest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize-units-of-measurement","title":"Normalize units of measurement","text":"<p>Custom task that will substitute numeric values and pertaining unit symbols with a SI-system-unit normalized representation in three columns:   * The normalized numeric value.   * The unit symbol of the SI-system-unit pertaining to the value.   * The origin unit symbol from which it was normalized (so we are able to reverse this action).</p> Parameter Type Description Default valueProperties String The names (comma-separated) of columns containing numeric values interpreted as quantities of the dimension indicated by the pertaining unit. no default unitProperties String The names (comma-separated) of dedicated columns containing the unit symbol for the pertaining value in the value column (the positions in this list have to align with the pertaining value columns). Either this param or \u2018static unit\u2019 has to be set. empty string staticUnits String Unit symbols (comma-separated) defining the unit for all values in the pertaining value column. If set, the \u2018unitProperty\u2019 param will be ignored and all values of the value column have to be numbers without unit symbols (the positions in this list have to align with the pertaining value columns). empty string targetUnits String Unit symbols (comma-separated) defining the target unit to which the value column will be converted (Note: Make sure the input unit can be converted to the target unit). By default the pertaining SI-base unit will be used as normalization unit (the positions in this list have to align with the pertaining value columns) empty string suppressErrors boolean If true, will ignore any parsing or value conversion error and return an empty result (might happen because of unknown unit symbols or non-numbers as values). Beware, the value will be lost completely! false configFilePath ResourceOption An absolute file path for a unit CSV configuration file (for syntax see \u2018configuration\u2019 param). If set, the \u2018configuration\u2019 param will be ignored. configuration MultilineStringParameter While all SI units and decimal prefixes are supported by default, custom or obsolete units have to be added via this configuration.\\       NOTE: when constructing formulae depending on other units defined in the configuration, make sure to order them dependently.\\       ALSO: Rational numbers are not supported by the UCUM syntax, express them as a fraction (see \u2018grain\u2019 example below).\\ # Example configuration, don\u2019t forget to remove the \u2018#\u2019 in front of each row.#      CSV COLUMNS:#       * unit name - the human readable name of the unit#       * override  - (true <p>The identifier for this plugin is <code>ucumNormalizationTask</code>.</p> <p>It can be found in the package <code>com.eccenca.di.measure</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-xml","title":"Validate XML","text":"<p>Validates an XML dataset against a provided XML schema (XSD) file.          Any errors are written to the output. Can be used in conjunction with the <code>Cancel Workflow</code> operator in order to stop the workflow if errors have been found.\u201d</p> Parameter Type Description Default file Resource The XSD file to be used for validating the XML. no default <p>The identifier for this plugin is <code>validateXsdOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#xslt","title":"XSLT","text":"<p>A task that converts an XML resource via an XSLT script and writes the transformed output into a file resource.</p> Parameter Type Description Default file Resource The XSLT file to be used for transforming XML. no default <p>The identifier for this plugin is <code>xsltOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#dataset-plugins","title":"Dataset Plugins","text":"<p>The following dataset plugins are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#deprecated","title":"Deprecated","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparksql-view","title":"SparkSQL view","text":"<p>Use the SQL endpoint dataset instead.</p> Parameter Type Description Default viewName String The name of the view. This specifies the table that can be queried by another virtual dataset or via JDBC (the \u2018default\u2019 schema is used for all virtual datasets). no default query String Optional SQL query on the selected table. Has no effect when used as an output dataset. empty string cache boolean Optional boolean option that selects if the table should be cached by Spark or not (default = true). true uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string charset String The source internal encoding, e.g., UTF8, ISO-8859-1 UTF-8 arraySeparator String The character that is used to separate the parts of array values. Write \u201cback slash t\u201d to specify the tab character. useCompatibleTypes boolean If true, basic types will be used for types that otherwise would result in client errors. This mainly that arrays will be stored as Strings separated by the separator defined above. If the view is only for use within a SparkContext, this can be set to false. true <p>The identifier for this plugin is <code>sparkView</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.spark.virtual</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#embedded","title":"Embedded","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#hive-database","title":"Hive database","text":"<p>Read from or write to an embedded Apache Hive endpoint.</p> Parameter Type Description Default schema String Name of the hive schema or namespace. empty string table String Name of the hive table. no default query String Optional query for projection and selection (e.g. \u201d SELECT * FROM table WHERE x = true\u201d. empty string uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string charset String The source internal encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>Hive</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#knowledge-graph","title":"Knowledge Graph","text":"<p>Read RDF from or write RDF to a Knowledge Graph embedded in Corporate Memory.</p> Parameter Type Description Default endpoint String The named endpoint within the eccenca DataPlatform. default graph GraphUriParameter The URI of the named graph. no default pageSize int The number of solutions to be retrieved per SPARQL query. 100000 pauseTime int The number of milliseconds to wait between subsequent query 0 retryCount int The number of retries if a query fails 3 retryPause int The number of milliseconds to wait until a failed query is retried. 1000 strategy Enum The strategy use for retrieving entities: simple: Retrieve all entities using a single query; subQuery: Use a single query, but wrap it for improving the performance on Virtuoso; parallel: Use a separate Query for each entity property. parallel clearGraphBeforeExecution boolean If set to true this will clear the specified graph before executing a workflow that writes to it. Note that this will always use the configured graph and ignore any overwritten values from the config port. false entityList MultilineStringParameter A list of entities to be retrieved. If not given, all entities will be retrieved. Multiple entities are separated by whitespace. sparqlTimeout int SPARQL query timeout (select/update) in milliseconds. A value of zero means that there is no timeout. If a value greater zero is specified this overwrites possible default timeouts. This timeout is also propagated to DataPlatform and may overwrite default timeouts there. 0 optimizedRetrieve boolean Optimized retrieval method to remove load from the underlying triple store. Query parallelism is limited and cheaper queries are executed against the backend. By putting the main work on DataIntegration side, the RDF backend is kept responsive. true <p>The identifier for this plugin is <code>eccencaDataPlatform</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.dataplatform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#in-memory-dataset","title":"In-memory dataset","text":"<p>A Dataset that holds all data in-memory.</p> Parameter Type Description Default clearGraphBeforeExecution boolean If set to true this will clear this dataset before it is used in a workflow execution. true <p>The identifier for this plugin is <code>inMemory</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#internal-dataset","title":"Internal dataset","text":"<p>Dataset for storing entities between workflow steps. The underlying dataset type can be configured using the <code>dataset.internal.*</code> configuration parameters.</p> Parameter Type Description Default graphUri String The RDF graph that is used for storing internal data null <p>The identifier for this plugin is <code>internal</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sql-endpoint","title":"SQL endpoint","text":"<p>Provides a JDBC endpoint that exposes workflow or transformation results as tables, which can be queried using SQL.</p> Parameter Type Description Default tableNamePrefix String Prefix of the table that will be shared. In the case of complex mappings more than one table will be created. If one name is given it will be used as a prefix for table names. If left empty the table names will be generated from the user name and time stamps and start with \u2018root\u2019, \u2018object-mapping\u2019 empty string cache boolean Optional boolean option that selects if the table should be cached by Spark or not (default = true). true arraySeparator String The character that is used to separate  the parts of array values. Write \\t to specify the tab character. useCompatibleTypes boolean If true, basic types will be used for unusual data types that otherwise may result in client errors. Try switching this on, if a client has weird error messages. (Default = true) true map Map Mapping of column names. Similar to aliases E.g. \u2018c1:c2\u2019 would rename column c1 into c2. <p>The identifier for this plugin is <code>sqlEndpoint</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.spark.endpoint</code>.</p> <p>SQL endpoint dataset parameters</p> <p>The dataset only requires that the tableNamePrefix parameter is given. This will be used as the prefix for the names of the generated tables. When a set of entities is written to the endpoint a view is generated for each entity type (defined by an \u2018rdf_type\u2019 attribute). That means that the mapping or data source that are used as input for the SQL endpoint need to have a type or require a user defined type mapping.</p> <p>The operator has a compatibility mode. This mode will avoid complex types such as Arrays. When arrays exist in the input they are converted to a String using the given arraySeparator. This avoids errors and warnings in some Jdbc clients that are unable to handle typed arrays and may make working with software like Excel easier.</p> <p>The parameter aliasMap of the endpoint allows the specification of column aliases. The map is a comma separated list of key-value pairs. Each key and value is denoted by <code>key:value</code>. An example for renaming 2 columns (source1, source2 to target1, target2) in the result would be: <code>source1:target1,source2:target2</code></p> <p>Note: Table and column (mapping target) names will be automatically converted to be valid in as many databases as possible. Table names will be shortened to 128 characters. Only a-z, A-Z, 0-9 and _ are allowed. Others will be replaced with an underscore. Column names undergo the same transformation but will be converted to lower case as well. The log will inform about changes. The table names will be generated based on the target type of each mapping. The user needs to make sure that each object mapping specifies a unique type. If two object mappings define the same type, only the last one will be written.</p> <p>SQL endpoint activity</p> <p>See [ActivityDocumentation] for a general description of the Data Integration activities. The activity will start automatically, when the SQL endpoint is used as a data sink and Data Integration is configured to make the SQL endpoint accessible remotely.</p> <p>When the activity is started and running it returns the server status and JDBC URL as its value.</p> <p>Stopping the activity will drop all views generated by the activity. It can be restarted by rerunning the workflow containing it as a sink.</p> <p>Remote client configuration (via JDBC and ODBC)</p> <p>Within Data Integration the SQL endpoint can be used as a source or sink like any other dataset. If the startThriftServer option is set to \u2018true\u2019 access via JDBC or ODBC is possible.</p> <p>ODBC and JDBC drivers can be used to connect to relational databases.</p> <p>When selecting a version of a driver the client operating system and its type (32bit/64 bit) are the most important factors. The version of the client drivers sometimes is the same as the server\u2019s. If no version of a driver is given, the newest driver of the vendor should work, as it should be backwards compatible.</p> <p>Any JDBC or ODBC client can connect to an SQL endpoint dataset. SparkSQL uses the same query processing as Hive, therefore the requirements for the client are:</p> <ul> <li>A JDBC driver compatible with Hive 1.2.1<sup>1</sup> (platform independent driver org.apache.hive.jdbc.HiveDriver is needed) or</li> <li>A JDBC driver compatible with Spark 2.3.3</li> <li>A Hive ODBC driver (ODBC driver for the client architecture and operating system needed)</li> </ul> <p>A detailed instruction to connect to a Hive or SparkSQL endpoint with various tools (e.g. SQuirreL, beeline, SQL Developer, \u2026) can be found at Apache HiveServer2 Clients. The database client DBeaver can connect to the SQL endpoint out of the box.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#variable-dataset-deprecated","title":"Variable dataset (deprecated)","text":"<p>Dataset that acts as a placeholder in workflows and is replaced at request time. This is deprecated, please use the \u2018replaceable input/output dataset config\u2019 in the node menu of the workflow editor instead.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>variableDataset</code>.</p> <p>It can be found in the package <code>org.silkframework.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#file","title":"File","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#alignment","title":"Alignment","text":"<p>Writes the alignment format specified at http://alignapi.gforge.inria.fr/format.html.</p> Parameter Type Description Default file WritableResource The alignment file. no default <p>The identifier for this plugin is <code>alignment</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#avro","title":"Avro","text":"<p>Read from or write to an Apache Avro file.</p> Parameter Type Description Default file WritableResource Path (e.g. relative like \u2018path/filename.avro\u2019 or absolute \u2018hdfs:///path/filename.avro\u2019). no default uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>avro</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#csv","title":"CSV","text":"<p>Read from or write to an CSV file.</p> Parameter Type Description Default file WritableResource The CSV file. This may also be a zip archive of multiple CSV files that share the same schema. no default properties String Comma-separated list of properties. If not provided, the list of properties is read from the first line. Properties that are no valid (relative or absolute) URIs will be encoded. empty string separator String The character that is used to separate values. If not provided, defaults to \u2018,\u2019, i.e., comma-separated values. \u201c\\t\u201d for specifying tab-separated values, is also supported. , arraySeparator String The character that is used to separate the parts of array values. Write \u201c\\t\u201d to specify the tab character. empty string quote String Character used to quote values. \u201c uri String Deprecated A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string charset String The file encoding, e.g., UTF-8, UTF-8-BOM, ISO-8859-1 UTF-8 regexFilter String A regex filter used to match rows from the CSV file. If not set all the rows are used. empty string linesToSkip int The number of lines to skip in the beginning, e.g. copyright, meta information etc. 0 maxCharsPerColumn int The maximum characters per column. Warning: System will request heap memory of that size (2 bytes per character) when reading the CSV. If there are more characters found, the parser will fail. 128000 ignoreBadLines boolean If set to true then the parser will ignore lines that have syntax errors or do not have to correct number of fields according to the current config. false quoteEscapeCharacter String Escape character to be used inside quotes, used to escape the quote character. It must also be used to escape itself, e.g. by doubling it, e.g. \u201c\u201d. If left empty, it defaults to quote. \u201c zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. ^(?!.[\\/\\\\]\\..$ clearBeforeExecution boolean If set to true this will clear the specified file before executing a workflow that writes to it. false trimWhitespaceAndNonPrintableCharacters boolean If set to true, this will trim whitespace and non-printable characters from the contents of the CSV dataset. false <p>The identifier for this plugin is <code>csv</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.csv</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel","title":"Excel","text":"<p>Read from or write to an Excel workbook in Open XML format (XLSX).</p> Parameter Type Description Default file WritableResource File name inside the resources directory. no default streaming boolean Streaming enables reading and writing large Excels files. Warning: Be careful to disable streaming for large datasets (&gt; 10MB), because of high memory consumption. true linesToSkip int The number of lines to skip in the beginning when reading files. 0 hasHeader boolean If true, the first line will be read as the table header, which defines the column names. If false, the first line will be read as data. In that case, the columns need to be adressed using #A, #B, etc. true outputObjectValues boolean Output results from object rules (URIs). true <p>The identifier for this plugin is <code>excel</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rdf","title":"RDF","text":"<p>Dataset which retrieves and writes all entities from/to an RDF file. For reading, the dataset is loaded in-memory and thus the size is restricted by the available memory. Large datasets should be loaded into an external RDF store and retrieved using the SPARQL dataset instead.</p> Parameter Type Description Default file WritableResource The RDF file. This may also be a zip archive of multiple RDF files. no default format String Optional RDF format. If left empty, it will be auto-detected based on the file extension. N-Triples is the only format that can be written, while other formats can only be read. empty string graph String The graph name to be read. If not provided, the default graph will be used. Must be provided if the format is N-Quads. empty string entityList MultilineStringParameter A list of entities to be retrieved. If not given, all entities will be retrieved. Multiple entities are separated by whitespace. zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. .* <p>The identifier for this plugin is <code>file</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#json","title":"JSON","text":"<p>Read from or write to a JSON or JSON Lines file.</p> Parameter Type Description Default file WritableResource JSON file. This may also be a zip archive of multiple JSON files that share the same schema. no default template JsonCodeParameter Template for writing JSON. The term {{output}} will be replaced by the written JSON. {{output}} navigateIntoArrays boolean Navigate into arrays automatically. If set to false, the <code>#array</code> path operator must be used to navigate into arrays. true basePath String The path to the elements to be read, starting from the root element, e.g., \u2018/Persons/Person\u2019. If left empty, all direct children of the root element will be read. empty string uriPattern String A URI pattern, e.g., http://namespace.org/{ID}, where {path} may contain relative paths to elements empty string maxDepth int Maximum depth of written JSON. This acts as a safe guard if a recursive structure is written. 15 streaming boolean Streaming allows for reading large JSON files. If streaming is enabled, backward paths are not supported. true zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. ^(?!.[\\/\\\\]\\..$ <p>The identifier for this plugin is <code>json</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.json</code>.</p> <p>Typically, this dataset is used to transform an JSON file to another format, e.g., to RDF.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#reading","title":"Reading","text":"<p>In addition to plain JSON files, JSON Lines files can also be read.</p> <p>For reading, the JSON dataset supports a number of special paths: - <code>#id</code> Is a special syntax for generating an id for a selected element. It can be used in URI patterns for entities which do not provide an identifier. Examples: <code>http://example.org/{#id}</code> or <code>http://example.org/{/pathToEntity/#id}</code>. - <code>#text</code> retrieves the text of the selected node. - The backslash can be used to navigate to the parent JSON node, e.g., <code>\\parent/key</code>. The name of the backslash key (here <code>parent</code>) is ignored.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#writing","title":"Writing","text":"<p>When writing JSON, all entities need to possess a unique URI. Writing multiple root entities with the same URI will result in multiple entries in the generated JSON. If multiple nested entities with the same URI are written, only the last entity with a given URI will be written.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#multi-csv-zip","title":"Multi CSV ZIP","text":"<p>Reads from or writes to multiple CSV files from/to a single ZIP file.</p> Parameter Type Description Default file WritableResource Zip file name inside the resources directory/repository. no default separator String The character that is used to separate values. If not provided, defaults to \u2018,\u2019, i.e., comma-separated values. \u201c\\t\u201d for specifying tab-separated values, is also supported. , arraySeparator String The character that is used to separate the parts of array values. Write \u201c\\t\u201d to specify the tab character. empty string quote String Character used to quote values. \u201c charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 linesToSkip int The number of lines to skip in the beginning, e.g. copyright, meta information etc. 0 maxCharsPerColumn int The maximum characters per column. If there are more characters found, the parser will fail. 128000 ignoreBadLines boolean If set to true then the parser will ignore lines that have syntax errors or do not have to correct number of fields according to the current config. false quoteEscapeCharacter String Escape character to be used inside quotes, used to escape the quote character. It must also be used to escape itself, e.g. by doubling it, e.g. \u201c\u201d. If left empty, it defaults to quote. \u201c append boolean If \u2018True\u2019 then files in the ZIP archive are only added or updated, all other files in the ZIP stay untouched. If \u2018False\u2019 then a new ZIP file will be created on every dataset write. true zipFileRegex String Filter file paths inside the ZIP file via this regex. By default sub folders or files not ending with .csv are ignored. ^[^/]*\\.csv$ clearBeforeExecution boolean If set to true this will clear the specified file before executing a workflow that writes to it. true trimWhitespaceAndNonPrintableCharacters boolean If set to true, this will trim whitespace and non-printable characters from the contents of the CSV dataset. false <p>The identifier for this plugin is <code>multiCsv</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.csv</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#orc","title":"ORC","text":"<p>Read from or write to an Apache ORC file.</p> Parameter Type Description Default file WritableResource Path (e.g. relative like \u2018path/filename.orc\u2019 or absolute \u2018hdfs:///path/filename.orc\u2019). no default uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string partition String Optional specification of the attribute for output partitioning empty string compression String Optional compression algorithm (e.g. snappy, zlib) snappy charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>orc</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parquet","title":"Parquet","text":"<p>Read from or write to an Apache Parquet file.</p> Parameter Type Description Default file WritableResource Path (e.g. relative like \u2018path/filename.orc\u2019 or absolute \u2018hdfs:///path/filename.parquet\u2019). no default uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string partition String Optional specification of the attribute for output partitioning empty string compression String Optional compression algorithm (e.g. snappy, zlib) empty string charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>parquet</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#xml","title":"XML","text":"<p>Read from or write to an XML file.</p> Parameter Type Description Default file WritableResource The XML file. This may also be a zip archive of multiple XML files that share the same schema. no default basePath String The base path when writing XML. For instance: /RootElement/Entity. Should no longer be used for reading XML! Instead, set the base path by specifying it as input type on the subsequent transformation or linking tasks. empty string uriPattern String A URI pattern, e.g., http://namespace.org/{ID}, where {path} may contain relative paths to elements empty string outputTemplate XmlCodeParameter The output template used for writing XML. Must be valid XML. The generated entity is identified through a processing instruction of the form . streaming boolean Streaming allows for reading large XML files. true maxDepth int Maximum depth of written XML. This acts as a safe guard if a recursive structure is written. 15 zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. ^(?!.[\\/\\\\]\\..$ <p>The identifier for this plugin is <code>xml</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p> <p>Typically, this dataset is used to transform an XML file to another format, e.g., to RDF. It can also be used to generate XML files.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#reading_1","title":"Reading","text":"<p>When this dataset is used as an input for another task (e.g., a transformation task), the input type of the consuming task selects the path where the entities to be read are located.</p> <p>Example:</p> <pre><code>&lt;Persons&gt;\n  &lt;Person&gt;\n    &lt;Name&gt;John Doe&lt;/Name&gt;\n    &lt;Year&gt;1970&lt;/Year&gt;\n  &lt;/Person&gt;\n  &lt;Person&gt;\n    &lt;Name&gt;Max Power&lt;/Name&gt;\n    &lt;Year&gt;1980&lt;/Year&gt;\n  &lt;/Person&gt;\n&lt;/Persons&gt;\n</code></pre> <p>A transformation for reading all persons of the above XML would set the input type to <code>/Person</code>. The transformation iterates all entities matching the given input path. In the above example the first entity to be read is:</p> <pre><code>&lt;Person&gt;\n  &lt;Name&gt;John Doe&lt;/Name&gt;\n  &lt;Year&gt;1970&lt;/Year&gt;\n&lt;/Person&gt;\n</code></pre> <p>All paths used in the consuming task are relative to this, e.g., the person name can be addressed with the path <code>/Name</code>.</p> <p>Path examples:</p> <ul> <li>The empty path selects the root element.</li> <li><code>/Person</code> selects all persons.</li> <li><code>/Person[Year = \"1970\"]</code> selects all persons which are born in 1970.</li> <li><code>/#id</code> Is a special syntax for generating an id for a selected element. It can be used in URI patterns for entities which do not provide an identifier. Examples: <code>http://example.org/{#id}</code> or <code>http://example.org/{/pathToEntity/#id}</code>.</li> <li>The wildcard * enumerates all direct children, e.g., <code>/Persons/*/Name</code>.</li> <li>The wildcard ** enumerates all direct and indirect children.</li> <li>The backslash can be used to navigate to the parent XML node, e.g., <code>\\Persons/SomeHeader</code>.</li> <li><code>#text</code> retrieves the text of the selected node.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#writing_1","title":"Writing","text":"<p>When writing XML, all entities need to possess a unique URI. Writing multiple root entities with the same URI will result in multiple entries in the generated XML. If multiple nested entities with the same URI are written, only the last entity with a given URI will be written.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remote","title":"Remote","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jdbc-endpoint","title":"JDBC endpoint","text":"<p>Connect to an existing JDBC endpoint.</p> Parameter Type Description Default url String JDBC URL, must contain the database as parameter, i.g. with ;database=DBNAME or /database depending on the vendor. no default table String Table name. Can be empty if the read-strategy is not set to read the full table. If non-empty it has to contain at least an existing table. empty string sourceQuery SqlCodeParameter Source query (e.g. \u2018SELECT TOP 10 * FROM table WHERE x = true\u2019. Warning: Uses Driver (mySql, HiveQL, MSSql, Postgres) specific syntax. Can be left empty when full tables are loaded. Note: Even if columns with spaces/special characters are named in the query, they need to be referred to URL-encoded in subsequent transformations. groupBy String Comma separated list of attributes appearing in the outer SELECT clause that should be grouped by. The attributes are matched case-insensitive. All other attributes will be grouped via an aggregation function that depends on the supported DBMS, e.g. (JSON) array aggregation. empty string orderBy String Optional column to sort the result set. empty string limit IntOptionParameter Optional limit of returned records. This limit should be pushed to the source. No value implies that no limit will be applied. 10 queryStrategy Enum The strategy decides how the source system is queried. access-complete-table writeStrategy Enum If this dataset is written to, it can be selected if data is overwritten or appended.\u2019 default multipleValuesStrategy Enum How multiple values per entity property are written. concatenateValuesStrategy clearTableBeforeExecution boolean If set to true this will clear the specified table before executing a workflow that writes to it. false user String Username. Must be empty in some cases e.g. if secret key and client id are used. If non-empty this will also overwrite any value set in the JDBC URL string. empty string password PasswordParameter Password. Can be empty in some cases e.g. secret key and client id are used or if it is just an empty string. The password must be set here and cannot be set in the JDBC URL connection string. tokenEndpoint String URL for retrieving tokens, when using MS SQL Active Directory token based authentication. Can be found in the Azure AD Admin Center under OAuth2 endpoint or cab be constructed with the general endpoint URL combined with the tenant id and the suffix /outh/v2/authortized. empty string spnName String Service Principal Name identifying the resource. Usually a static URL like https://database.windows.net. empty string clientId String Client id or application id. Client id used for MS SQL token based authentication. String seperated by - char. empty string clientSecret PasswordParameter Client secret. Client secret used for MS SQL token based authentication. Can be generated in Azure AD admin center. restriction String An SQL WHERE clause to filter the records to be retrieved. empty string retries int Optional number of retries per query 0 pause int Optional pause between queries in ms. 2000 charset String The source internal encoding, e.g., UTF-8, ISO-8859-1 UTF-8 forceSparkExecution boolean If set to true, Spark will be used for querying the database, even if the local execution manager is configured. false <p>The identifier for this plugin is <code>Jdbc</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.jdbc</code>.</p> <p>General usage</p> <p>The JDBC dataset supports connections to Hive, Microsoft SQL Server, MySQL, MariaDB, SnowFlake, Oracle Database, DB2 and PostgreSQL databases. A login, password and JDBC URL need to be provided. This dataset supports queries or simply schema and table names to define what to retrieve from a source DB. When the dataset is used as a sink, queries are ignored and only schema and table parameters are used. If the dataset is used as a sink for a hierarchical mapping, it behaves similarly to the SqlEndpoint: One table is created per entity type.</p> <p>The names of the written tables are generated as follows:</p> <ul> <li>The table name of the root mapping is defined by the table parameter of the dataset.   If the table name is empty, a name is generated from the first type of the mapping.   Special characters are removed and the name is truncated to a maximum of 128 characters.</li> <li>For each object mapping, the table name is generated from its type.</li> </ul> <p>JDBC Connection Strings/URLs</p> <p>Most of the dataset parameters are passed directly to the driver. Please make sure that you use the correct syntax for each DBMS, otherwise you may get unintuitive errors.</p> <p>Here are templates for supported database systems: <pre><code>oracle (external driver needed):\njdbc:oracle:thin:@{host}[:{port}]/{database}\n\npostgres (integrated):\njdbc:postgresql://{host}[:{port}]/[{database}]\n\nMySQL/MariaDB (integrated):\njdbc:{mariadb}://{host}[:{port}]/[{database}]\n\nSnowSQL (external driver needed):\njdbc:snowflake://{AWSAccount}.{AWS region}.snowflakecomputing.com?db={database}&amp;schema={schema}\n\nMSSqlServer (integrated):\njdbc:sqlserver://{host}[:{port}];databaseName={database}\n\nDB2 (external driver needed):\njdbc:db2//{host}[:{port}]/{database}\n\nTrino (external driver needed)\njdbc:trino//{host}:8080/catalog/schema\n</code></pre></p> <p>Read and write strategies</p> <p>There are multiple read and write strategies which can be selected depending on the purpose of the dataset in a workflow.</p> <p>Read strategies decide how the database is queried:</p> <ul> <li>full-table: Queries or wraps a complete table.   Only the DB schema and table name need to be set.</li> <li>query: The given source query is passed to the database.   The table name is not necessary in this case but a valid query in the SQL-dialect of the source database system must be provided.</li> </ul> <p>Write strategies decide how a new table is written:</p> <ul> <li>default: An error will occur if the table exists.   If not a new one will be created.</li> <li>overwrite: The old table will be removed and a new one will be created.</li> <li>append: Data will be appended to the existing table.   The schema of the data written has to be the same as the existing table schema.</li> </ul> <p>Optimized Writing</p> <p>Usually specific database systems have custom commands for loading large amounts of data, e.g. from a CSV file into a database table. For some DBMS and specific JDBC dataset configurations we support these optimized methods of loading data.</p> <p>Supported DBMS:</p> <ul> <li>MySQL and MariaDB (full support for versions 8.0.19+ and 10.4+, resp.):</li> <li>if older DBMS versions are used some dataset options like \u2018groupBy\u2019 might not be supported but equivalent queries will</li> <li>the same is true when older driver jars then the one provided by eccenca are used</li> <li>both use the MariaDB JDBC driver</li> <li>uses <code>LOAD DATA LOCAL INFILE</code> internally</li> <li>only applies when appending data to an existing table and having <code>Force Spark Execution</code> disabled</li> <li>Both the server parameter <code>local_infile</code> and the client parameter <code>allowLoadLocalInfile</code> must be enabled, e.g. by adding <code>allowLoadLocalInfile=true</code> to the JDBC URL.     For MySQL starting with version 8 the <code>local_infile</code> parameter is by default disabled!</li> <li> <p>If during writing to a MySQL/MariaDB a <code>[\u2026] You have an error in your SQL syntax [\u2026]</code> error is encountered make sure ANSIquotes are used.     <code>sql_mode=ANSI_QUOTES</code> can be set via a URL parameter to the JDBC connection string like:</p> <pre><code># MySQL\njdbc:mysql://&lt;host&gt;:&lt;port, eg. 3306&gt;/&lt;database&gt;?sessionVariables=sql_mode=ANSI_QUOTES\n\n# MariaDB\njdbc:mariadb://&lt;host&gt;:&lt;port, eg. 3306&gt;/&lt;database&gt;?sessionVariables=sql_mode=ANSI_QUOTES\n</code></pre> </li> </ul> <p>Registering JDBC drivers</p> <p>More 3rd party databases are supported via adding their JDBC drivers to the classpath of Data Integration. Drivers are usually provided by the database manufactures. If 32 bit and 64 bit versions are provided the latter is usually needed and should aways equal the bit-level of the JVM. To make sure that the drivers are loaded correctly, their class name (in case are jar contains multiple drivers) and location in the file system can be set with the <code>spark.sql.options.jdbc</code> option in the <code>dataintegration.conf</code> configuration file.</p> <p>An example for adding both the DB2 and MySQL drivers to the Data Integration configuration file <code>spark.sql.options.*</code> section:</p> <pre><code>spark.sql.options {\n  \u2026\n\n  # List of database identifiers to specify user provided JDBC drivers. The second part of the protocol of a JDBC URI (e.g. db2 from\n  # jdbc:db2://host:port)  is used to specify the driver. For each protocol on the list a jar classname and optional download\n  # location can be provided.\n  jdbc.drivers = \"db2,mysql\"\n\n  # Some database systems use licenses that are to loose or restrictive for us to ship the drivers. Therefore a path\n  # to a jar file containing the driver and the name of driver can be specified here.\n  jdbc.db2.jar = \"/home/user/Jars/db2jcc-db2jcc4.jar\"\n  jdbc.mysql.jar = \"/home/user/drivers/mysql.jar\"\n\n  # Name of the actual driver class for each db\n  jdbc.db2.name = \"com.ibm.db2.jcc.DB2Driver\"\n  jdbc.mysql.name = \"com.mysql.jdbc.Driver\"\n}\n</code></pre> <p>Driver Priority</p> <p>In general it will not work to upgrade a JDBC driver by providing an external driver for a database that is already packaged with eccenca Dataintegration.</p> <p>The driver delivered with eccenca Dataintegration will be prefered. Driver names (configured via e.g. <code>spark.sql.options.jdbc.drivers = \"mssql\"</code>) will be ignored if JDBC URLs starting with, in this example <code>jdbc:mssql...</code> , are already supported in the dataset.</p> <p>Recommended DBMS versions</p> <ul> <li>Microsoft SQL Server 2017: Older versions might work, but do not support the <code>groupBy</code> parameter.</li> <li>PostgreSQL 9.5: The <code>groupBy</code> parameter needs at least version 8.4.</li> <li>MySQL v8.0.19: Older versions do not support the <code>groupBy</code> parameter.</li> <li>DB2 v11.5.x: The <code>groupBy</code> feature needs at least version 9.7 to function.</li> <li>Oracle 12.2.x: The <code>groupBy</code> feature does not work for versions prior to 11g Release 2.</li> </ul> <p>These limitations are the same for JDBC drivers that are older than the fully supported databases. Queries can achieve a similar outcome if <code>groupBy</code> is not supported.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#snowflake-jdbc-endpoint","title":"Snowflake JDBC endpoint","text":"<p>Connect to Snowflake JDBC endpoint.</p> Parameter Type Description Default connection PluginObjectParameter Connection parameters read PluginObjectParameter Parameters related to reading from the database. write PluginObjectParameter Parameters related to writing to the database. queryExecution PluginObjectParameter Query execution parameters. <p>The identifier for this plugin is <code>SnowflakeJdbc</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.jdbc.databases.snowflake</code>.</p> <p>This dataset supports connections to the Snowflake JDBC endpoint.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#account-url-hostname","title":"Account URL hostname","text":"<p>The supplied account URL hostname needs to contain the account identifier. Refer to the Snowflake documentation on account identifiers for details.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#reading_2","title":"Reading","text":"<p>Either a table or a queries can be specified to retrieve data from Snowflake.</p> <p>Read strategies decide how the database is queried:</p> <ul> <li>full-table: Queries or wraps a complete table.   Only the table name need to be set.</li> <li>query: The given source query is passed to the database.   The table name is not necessary in this case but a valid query must be provided.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#writing_2","title":"Writing","text":"<p>When the dataset is used as a sink, queries are ignored and only schema and table parameters are used. If the dataset is used as a sink for a hierarchical mapping, one table is created per entity type.</p> <p>Write strategies decide how a new table is written:</p> <ul> <li>default: An error will occur if the table exists.   If not a new one will be created.</li> <li>overwrite: The old table will be removed and a new one will be created.</li> <li>append: Data will be appended to the existing table.   The schema of the data written has to be the same as the existing table schema.</li> </ul> <p>The names of the written tables are generated as follows:</p> <ul> <li>The table name of the root mapping is defined by the table parameter of the dataset.   If the table name is empty, a name is generated from the first type of the mapping.   Special characters are removed and the name is truncated to a maximum of 128 characters.</li> <li>For each object mapping, the table name is generated from its type.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel-google-drive","title":"Excel (Google Drive)","text":"<p>Read data from a remote Google Spreadsheet.</p> Parameter Type Description Default url String Link to the document (\u2018share with anyone having a link\u2019 must be enabled, URL parameters will be removed and corrected automatically). no default streaming boolean Streaming enables reading and writing large Excels files. Warning: Be careful to disable streaming for large datasets (&gt; 10MB), because of high memory consumption. true invalidateCacheAfter Duration Duration until file based cache is invalidated. PT5M linesToSkip int The number of lines to skip in the beginning when reading files. 0 <p>The identifier for this plugin is <code>googlespreadsheet</code>.</p> <p>It can be found in the package <code>com.eccenca.di.gdrive</code>.</p> <p>The dataset needs the document id of a \u201cshare via url\u201d sheet on Google Drive as input. It will automatically correct the URL and add the \u201cexport as xlsx\u201d option to a new URL that will be used to download an Excel Spreadsheet. The download will be cached and treated the same way as an xlsx file in the Excel Dataset.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#caching","title":"Caching","text":"<p>The advanced parameter <code>invalidateCacheAfter</code> allows the user to specify a duration of the file cache after which it is refreshed. A file based cache is created to avoid CAPTCHAs. During the caching and validation of the URL access occurs with random wait times between 1 and 5 seconds. The cache is invalidated after 5 minutes by default.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#neo4j","title":"Neo4j","text":"<p>Neo4j graph</p> Parameter Type Description Default uri String The URL to the Neo4j instance bolt://localhost:7687 user String The Neo4j username for basic authentication. neo4j password PasswordParameter The Neo4j password for basic authentication. PASSWORD_PARAMETER:UjbuVU/D0L+9QI2jGjfKiQ== database String Database (leave empty for default) empty string nodeLabel String Neo4j label for all entities to be covered by this dataset. When reading, all nodes with this label will be read. When writing, this label will be added to all generated nodes. If the dataset is cleared, only nodes with this label will be deleted. Any clearBeforeExecution boolean If set to true, all nodes with the specified label will be removed, before executing a workflow that writes to this graph. true <p>The identifier for this plugin is <code>neo4j</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.neo4j</code>.</p> <p>Supports reading and writing Neo4j graphs. The following sections outline how graphs are generated and read back.</p> <p>For more information about Neo4j, please refer to the Neo4j documentation.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#nodes","title":"Nodes","text":"<p>For each entity that is written to a Neo4j dataset, a node will be created. A property <code>uri</code> will be added to each generated node, which holds the URI of the original entity. In applications, the URI property should be used instead of the node identifiers, which are auto-generated in Neo4j and do not represent stable URIs.</p> <p>When reading nodes, the entity URIs will be generated based on that property. At the moment, it\u2019s not supported to read nodes that do not provide a <code>uri</code> property.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#labels","title":"Labels","text":"<p>Labels in Neo4j are used to group nodes into sets where all nodes that have a certain label belongs to the same set. Neo4j labels are comparable with classes in RDF (not to be confused with labels in RDF).</p> <p>When writing entities to the Neo4j dataset, the following labels will be added to each generated node:</p> <ul> <li>For each entity type (such as the type set in a mapping), a label will be added to the node in Neo4j.   Since types in eccenca DataIntegration are usually URIs, they will be converted according to the rules further down.</li> <li>The label as configured by the label parameter on the Neo4j dataset itself.   This is typically used to identify all entities that have been written by a certain Neo4j dataset specification in the project.   For instance, if two Neo4j dataset specifications are added to a project - both writing to the same Neo4j database - different labels can be set to distinguish both sets of entities.   In that respect it may be used to model a similar concept as graphs in RDF.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#relationships","title":"Relationships","text":"<p>A relationship connects two nodes in Neo4j. Hierarchical mappings will generate relationships for all object mappings.</p> <p>Relationships can be addressed with property paths in mappings. At the moment, only paths of length 1 are supported, i.e., it\u2019s not possible to use non-property paths.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#handling-of-uris","title":"Handling of URIs","text":"<p>In eccenca DataIntegration, URIs are typically used to uniquely identify classes and properties. While URIs are central in RDF, Neo4j does allow arbitrary names and does not have any special support for URIs.</p> <p>When generating Neo4j labels, properties and relationships, URIs will be shortened according to the following rules. - If a registered project prefix matches a URI, a name <code>{prefixName}_{localPart}</code> will be generated. For instance, <code>http://xmlns.com/foaf/0.1/name</code> will become <code>foaf_name</code>.   Note that underscores (<code>_</code>) are used instead of colons (<code>:</code>) to separate the namespace and the local name.   The reason is that colons are reserved in the Cypher query language and some tools don\u2019t escape properly and fail on databases that use colons in names. - If no project prefix matches a URI, the URI will be used verbatim. This will look ugly in Neo4j tools, so generally it\u2019s recommended to define prefixes for all used namespaces.</p> <p>When reading generated entities, the URIs of the classes and properties will be reconstructed based on the prefix table of the project. If the prefixes change between writing and reading, different URIs will be generated.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rdf-vs-neo4j-terminology","title":"RDF vs. Neo4j terminology","text":"<p>Neo4j uses a different terminology than RDF or description logic. For users familiar with RDF, the following table shows the correspondent terms for some central concepts. This is meant to help understanding and does not aim to provide a precise mapping as there are semantic differences between Neo4j and RDF.</p> RDF Neo4j resource node class label datatype property property object property relationship graph Do not exist in Neo4j, but labels can be used to mimic graphs.","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel-onedrive-office365","title":"Excel (OneDrive, Office365)","text":"<p>Read data from a remote onedrive or Office365 Spreadsheet.</p> Parameter Type Description Default url String Link to the document (\u2018share with anyone having a link\u2019 must be enabled). no default streaming boolean Streaming enables reading and writing large Excels files. Warning: Be careful to disable streaming for large datasets (&gt; 10MB), because of high memory consumption. true invalidateCacheAfter Duration Duration until file based cache is invalidated. PT5M linesToSkip int The number of lines to skip in the beginning when reading files. 0 <p>The identifier for this plugin is <code>office365preadsheet</code>.</p> <p>It can be found in the package <code>com.eccenca.di.office365</code>.</p> <p>The dataset needs the URL of a \u201cshare via link\u201d sheet on Office 365/OneDrive as input. It will automatically construct a direct download URL, cache the download file handle it like an XLSX file in the Excel Dataset.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#notes","title":"Notes","text":"<p>There are 2 types of URLs that can be shared: Onedrive links look like <code>https://1drv.ms/x/s!AucULvzmJ-dsdfsfgaIcyWP_XY_G4w?e=yx65uu</code></p> <p>Onedrive (based one sharepoint, for businesses) links look like <code>https://eccencagmbh-my.sharepoint.com/:x:/g/personal/person_eccenca_com/EdEMTEw1dclHiEZXyvy8P4YBit8wSyGsiwU5Kt__sQOZzw</code></p> <p>The first type should always work is not recommended for this dataset. The second type requires to set up an application in Microsoft EntraID (formerly Azure Active Directory). EntraID: https://docs.microsoft.com/azure/active-directory/develop/v2-overview Instructions and examples can be found here: https://github.com/Azure-Samples/ms-identity-msal-java-samples/tree/main/3-java-servlet-web-app/1-Authentication/sign-in</p> <p>After following the steps access to sharepoint/onedrive for business can be setup in the application.conf file for eccenca DataIntegration.</p> <p>Example:</p> <pre><code>com.eccenca.di.office365 = {\n    authority = \"https://login.microsoftonline.com/a0907dd1-f981-4c98-a8b9-1deb27bcf2cc/\"\n    clientId = \"4d14959d-3c62-4f90-a072-a96ca4b3fa9f\"\n    secret = \"Ceb8Q~QkMMV7TBK-ggB3nh22nUnqoDB1KTmkjj\"\n    scope = \"https://graph.microsoft.com/.default\"\n    tenantId = \"a0907dd1-f981-4c98-a8b9-1deb27bcf2cc\"\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#caching_1","title":"Caching","text":"<p>The advanced parameter <code>invalidateCacheAfter</code> allows the user to specify a duration of the file cache after which it is refreshed. A file based cache is created to avoid CAPTCHAs. During the caching and validation of the URL access occurs with random wait times between 1 and 5 seconds. The cache is invalidated after 5 minutes by default.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-endpoint","title":"SPARQL endpoint","text":"<p>Connect to an existing SPARQL endpoint.</p> Parameter Type Description Default endpointURI String The URI of the SPARQL endpoint, e.g., http://dbpedia.org/sparql no default login String Login required for authentication null password PasswordParameter Password required for authentication graph String Only retrieve entities from a specific graph null pageSize int The number of solutions to be retrieved per SPARQL query. 1000 entityList MultilineStringParameter A list of entities to be retrieved. If not given, all entities will be retrieved. Multiple entities are separated by whitespace. pauseTime int The number of milliseconds to wait between subsequent query 0 retryCount int The number of retries if a query fails 3 retryPause int The number of milliseconds to wait until a failed query is retried. 1000 queryParameters String Additional parameters to be appended to every request e.g. &amp;soft-limit=1 empty string strategy Enum The strategy use for retrieving entities: simple: Retrieve all entities using a single query; subQuery: Use a single query, but wrap it for improving the performance on Virtuoso; parallel: Use a separate Query for each entity property. parallel useOrderBy boolean Include useOrderBy in queries to enforce correct order of values. true clearGraphBeforeExecution boolean If set to true this will clear the specified graph before executing a workflow that writes to it. false sparqlTimeout int SPARQL query timeout (select/update) in milliseconds. A value of zero means that the timeout configured via property is used (e.g. configured via silk.remoteSparqlEndpoint.defaults.read.timeout.ms). To overwrite the configured value specify a value greater than zero. 0 <p>The identifier for this plugin is <code>sparqlEndpoint</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#uncategorized","title":"Uncategorized","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#internal-dataset-single-graph","title":"Internal dataset (single graph)","text":"<p>Dataset for storing entities between workflow steps. This variant does use the same graph for all internal datasets in a workflow. The underlying dataset type can be configured using the <code>dataset.internal.*</code> configuration parameters.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>LocalInternalDataset</code>.</p> <p>It can be found in the package <code>org.silkframework.execution.local</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#binary-file","title":"Binary file","text":"<p>Reads and writes binary files. A typical use-case for this dataset is to process PDF documents or images.</p> Parameter Type Description Default file WritableResource The file to read or write. no default zipFileRegex String If the file is a ZIP file, read files are filtered via this regex. If empty, the zip itself will be returned to readers. .* <p>The identifier for this plugin is <code>binaryFile</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset</code>.</p> <p>Reads and writes binary files. A typical use-case for this dataset is to process PDF documents or images using workflow operators that accept or output files. If an operator reads from this dataset that does not support files directly (such as transformation or linking tasks), it will only receive the file metadata, which includes the file path.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#zip-files","title":"ZIP files","text":"<p>This dataset can be used to compress/decompress ZIP files. If a ZIP file is configured, the behaviour is as follows: - Writing a ZIP file to this dataset will overwrite the configured ZIP file. - Writing one or many non-ZIP files will overwrite the dataset file with a ZIP that contains all written files. - When reading files, the dataset will return all files inside the ZIP that match the configured regex. If the regex is empty, the ZIP file itself will be returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replaceable-datasets","title":"Replaceable datasets","text":"<p>It can be used with the <code>replacable input</code> flag to replace the configured file in a workflow execution request. Same for the <code>replacable output</code> flag, which will return the file content as a result of a workflow execution request.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mime-type","title":"MIME type","text":"<p>The generic MIME type for files of this dataset is <code>application/octet-stream</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#text","title":"Text","text":"<p>Reads and writes plain text files.</p> Parameter Type Description Default file WritableResource The plain text file. May also be a zip archive containing multiple text files. no default charset String The file encoding, e.g., UTF-8, UTF-8-BOM, ISO-8859-1 UTF-8 typeName String A type name that represents this file. document property String The single property that holds the text. text zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. .* <p>The identifier for this plugin is <code>text</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.text</code>.</p> <p>Reads and writes plain text files.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#writing_3","title":"Writing","text":"<p>All values of each entity will be written as plain text. Multiple values per entity are separated by spaces. Each entity will be written to a new line.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#reading_3","title":"Reading","text":"<p>The entire text will be read as a single entity with a single property. Note that even if multiple entities have been written to this dataset before, those would still be read back as a single entity. The default type is <code>document</code>, the default path is <code>text</code>. Both values can be configured in the advanced section.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#distance-measures","title":"Distance Measures","text":"<p>The following distance measures are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characterbased","title":"Characterbased","text":"<p>Character-based distance measures compare strings on the character level. They are well suited for handling typographical errors.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#is-substring","title":"Is substring","text":"<p>Checks if a source value is a substring of a target value.</p> Parameter Type Description Default reverse boolean Reverse source and target inputs false <p>The identifier for this plugin is <code>isSubstring</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jaro-distance","title":"Jaro distance","text":"<p>Matches strings based on the Jaro distance metric.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>jaro</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p> <p>The Jaro distance measure calculates the similarity between two strings based on the number and order of common characters, the number of transpositions, and the length of the strings. The Jaro distance is 0 for a perfect match and 1 if there is no similarity between the given strings.</p> <p>For more information, please refer to: https://en.wikipedia.org/wiki/Jaro\u2013Winkler_distance.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_1","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jaro-winkler-distance","title":"Jaro-Winkler distance","text":"<p>Matches strings based on the Jaro-Winkler distance measure.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>jaroWinkler</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p> <p>The Jaro-Winkler distance measure is a variation of the Jaro distance metric. It takes into account the prefixes of the strings being compared and assigns higher weights to matching prefixes.</p> <p>For more information, please refer to: https://en.wikipedia.org/wiki/Jaro\u2013Winkler_distance.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_2","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalized-levenshtein-distance","title":"Normalized Levenshtein distance","text":"<p>Normalized Levenshtein distance. Divides the edit distance by the length of the longer string.</p> Parameter Type Description Default qGramsSize int The size of the q-grams to be indexed. Setting this to zero will disable indexing. 2 minChar char The minimum character that is used for indexing 0 maxChar char The maximum character that is used for indexing z <p>The identifier for this plugin is <code>levenshtein</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_3","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-0-for-equal-strings","title":"Returns 0 for equal strings:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John]</code></li> <li> <p>Target: <code>[John]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-14-if-two-strings-of-length-4-differ-by-one-edit-operation","title":"Returns 1/4 if two strings of length 4 differ by one edit operation:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John]</code></li> <li> <p>Target: <code>[Jxhn]</code></p> </li> <li> <p>Returns: \u2192 <code>0.25</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalizes-the-edit-distance-by-the-length-of-the-longer-string","title":"Normalizes the edit distance by the length of the longer string:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John]</code></li> <li> <p>Target: <code>[Jhn]</code></p> </li> <li> <p>Returns: \u2192 <code>0.25</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-the-maximum-distance-of-1-for-completely-different-strings","title":"Returns the maximum distance of 1 for completely different strings:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John]</code></li> <li> <p>Target: <code>[Clara]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#levenshtein-distance","title":"Levenshtein distance","text":"<p>Levenshtein distance. Returns a distance value between zero and the size of the string.</p> Parameter Type Description Default qGramsSize int The size of the q-grams to be indexed. Setting this to zero will disable indexing. 2 minChar char The minimum character that is used for indexing 0 maxChar char The maximum character that is used for indexing z <p>The identifier for this plugin is <code>levenshteinDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_4","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_1","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-0-for-equal-strings_1","title":"Returns 0 for equal strings:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John]</code></li> <li> <p>Target: <code>[John]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-1-for-strings-that-differ-by-one-edit-operation","title":"Returns 1 for strings that differ by one edit operation:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John]</code></li> <li> <p>Target: <code>[Jxhn]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-3-for-strings-that-differ-by-three-edit-operations","title":"Returns 3 for strings that differ by three edit operations:","text":"<ul> <li>Input values:</li> <li>Source: <code>[Saturday]</code></li> <li> <p>Target: <code>[Sunday]</code></p> </li> <li> <p>Returns: \u2192 <code>3.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#qgrams","title":"qGrams","text":"<p>String similarity based on q-grams (by default q=2).</p> Parameter Type Description Default q int No description 2 minChar char The minimum character that is used for indexing 0 maxChar char The maximum character that is used for indexing z <p>The identifier for this plugin is <code>qGrams</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_5","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_2","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-00-if-the-input-strings-are-equal","title":"Returns 0.0 if the input strings are equal:","text":"<ul> <li>Input values:</li> <li>Source: <code>[abcd]</code></li> <li> <p>Target: <code>[abcd]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-10-if-the-input-strings-do-not-share-a-single-q-gram","title":"Returns 1.0 if the input strings do not share a single q-gram:","text":"<ul> <li>Input values:</li> <li>Source: <code>[abcd]</code></li> <li> <p>Target: <code>[dcba]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-1-minus-the-matching-q-grams-divided-by-the-total-number-of-q-grams-generated-q-grams-in-this-example-a-ab-b-and-a-ac-c","title":"Returns 1 minus the matching q-grams divided by the total number of q-grams. Generated q-grams in this example: (#a, ab, b#) and (#a, ac, c#):","text":"<ul> <li>Input values:</li> <li>Source: <code>[ab]</code></li> <li> <p>Target: <code>[ac]</code></p> </li> <li> <p>Returns: \u2192 <code>0.8</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#starts-with","title":"Starts with","text":"<p>Returns success if the first string starts with the second string, failure otherwise.</p> Parameter Type Description Default reverse boolean Reverse source and target values false minLength int The minimum length of the string being contained. 2 maxLength int The potential maximum length of the strings that must match. If the max length is greater  than the length of the string to match, the full string must match. 2147483647 <p>The identifier for this plugin is <code>startsWith</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_6","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substring-comparison","title":"Substring comparison","text":"<p>Return 0 to 1 for strong similarity to weak similarity. Based on the paper: Stoilos, Giorgos, Giorgos Stamou, and Stefanos Kollias. \u201cA string metric for ontology alignment.\u201d The Semantic Web-ISWC 2005. Springer Berlin Heidelberg, 2005. 624-637.</p> Parameter Type Description Default granularity String The minimum length of a possible substring match. 3 <p>The identifier for this plugin is <code>substringDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_7","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#equality","title":"Equality","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#constant-similarity-value","title":"Constant similarity value","text":"<p>Always returns a constant similarity value.</p> Parameter Type Description Default value double No description 1.0 <p>The identifier for this plugin is <code>constantDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_8","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#string-equality","title":"String equality","text":"<p>Checks for equality of the string representation of the given values. Returns success if string values are equal, failure otherwise. For a numeric comparison of values use the \u2018Numeric Equality\u2019 comparator.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>equality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_9","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_3","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-distance-0-if-at-least-one-value-matches","title":"Returns distance 0, if at least one value matches:","text":"<ul> <li>Input values:</li> <li>Source: <code>[max, helmut]</code></li> <li> <p>Target: <code>[max]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-distance-1-if-no-value-matches","title":"Returns distance 1, if no value matches:","text":"<ul> <li>Input values:</li> <li>Source: <code>[max, helmut]</code></li> <li> <p>Target: <code>[john]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#greater-than","title":"Greater than","text":"<p>Checks if the source value is greater than the target value. If both strings are numbers, numerical order is used for comparison. Otherwise, alphanumerical order is used.</p> Parameter Type Description Default orEqual boolean Accept equal values false order Enum Per default, if both strings are numbers, numerical order is used for comparison. Otherwise, alphanumerical order is used. Choose a more specific order for improved performance. Autodetect reverse boolean Reverse source and target inputs false <p>The identifier for this plugin is <code>greaterThan</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_10","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#inequality","title":"Inequality","text":"<p>Returns success if values are not equal, failure otherwise.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>inequality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_11","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_4","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-distance-0-if-the-values-are-different","title":"Returns distance 0, if the values are different:","text":"<ul> <li>Input values:</li> <li>Source: <code>[max]</code></li> <li> <p>Target: <code>[john]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-distance-1-if-the-values-are-equal","title":"Returns distance 1, if the values are equal:","text":"<ul> <li>Input values:</li> <li>Source: <code>[max]</code></li> <li> <p>Target: <code>[max]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-multiple-values-are-provided-returns-0-if-at-least-one-value-does-not-match","title":"If multiple values are provided, returns 0, if at least one value does not match:","text":"<ul> <li>Input values:</li> <li>Source: <code>[max, helmut]</code></li> <li> <p>Target: <code>[max]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-multiple-values-are-provided-returns-1-if-all-value-match","title":"If multiple values are provided, returns 1, if all value match:","text":"<ul> <li>Input values:</li> <li>Source: <code>[max, max]</code></li> <li> <p>Target: <code>[max, max]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#lower-than","title":"Lower than","text":"<p>Checks if the source value is lower than the target value.</p> Parameter Type Description Default orEqual boolean Accept equal values false order Enum Per default, if both strings are numbers, numerical order is used for comparison. Otherwise, alphanumerical order is used. Choose a more specific order for improved performance. Autodetect reverse boolean Reverse source and target inputs false <p>The identifier for this plugin is <code>lowerThan</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_12","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-equality","title":"Numeric equality","text":"<p>Compares values numerically instead of their string representation as the \u2018String Equality\u2019 operator does. Allows to set the needed precision of the comparison. A value of 0.0 means that the values must represent exactly the same (floating point) value, values higher than that allow for a margin of tolerance.</p> Parameter Type Description Default precision double The range of tolerance in floating point number comparisons. Must be 0 or a non-negative number smaller than 1. 0.0 <p>The identifier for this plugin is <code>numericEquality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_13","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_5","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-0-for-equal-numbers","title":"Returns 0 for equal numbers:","text":"<ul> <li>Input values:</li> <li>Source: <code>[4.2]</code></li> <li> <p>Target: <code>[4.2]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-1-if-at-least-one-value-is-not-a-number","title":"Returns 1 if at least one value is not a number:","text":"<ul> <li>Input values:</li> <li>Source: <code>[1]</code></li> <li> <p>Target: <code>[one]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-0-for-numbers-within-the-configured-precision","title":"Returns 0 for numbers within the configured precision:","text":"<ul> <li>Parameters</li> <li> <p>precision: <code>0.1</code></p> </li> <li> <p>Input values:</p> </li> <li>Source: <code>[1.3]</code></li> <li> <p>Target: <code>[1.35]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-1-for-numbers-outside-the-configured-precision","title":"Returns 1 for numbers outside the configured precision:","text":"<ul> <li>Parameters</li> <li> <p>precision: <code>0.1</code></p> </li> <li> <p>Input values:</p> </li> <li>Source: <code>[1.3]</code></li> <li> <p>Target: <code>[1.5]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#relaxed-equality","title":"Relaxed equality","text":"<p>Return success if strings are equal, failure otherwise. Lower/upper case and differences like \u00f6/o, n/\u00f1, c/\u00e7 etc. are treated as equal.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>relaxedEquality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_14","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#language","title":"Language","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cjk-reading-distance","title":"CJK reading distance","text":"<p>CJK Reading Distance.</p> Parameter Type Description Default minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>cjkReadingDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.asian</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_15","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#korean-phoneme-distance","title":"Korean phoneme distance","text":"<p>Korean phoneme distance.</p> Parameter Type Description Default minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>koreanPhonemeDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.asian</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_16","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#korean-translit-distance","title":"Korean translit distance","text":"<p>Transliterated Korean distance.</p> Parameter Type Description Default minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>koreanTranslitDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.asian</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_17","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric","title":"Numeric","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#compare-physical-quantities","title":"Compare physical quantities","text":"<p>Computes the distance between two physical quantities. The distance is normalized to the SI base unit of the dimension. For instance for lengths, the distance will be in metres. Comparing incompatible units will yield a validation error.</p> Parameter Type Description Default numberFormat String The IETF BCP 47 language tag, e.g., \u2018en\u2019. en <p>The identifier for this plugin is <code>PhysicalQuantitiesDistance</code>.</p> <p>It can be found in the package <code>com.eccenca.di.measure</code>.</p> <p>SI units and common derived units are supported. The following section lists all supported units. By default, all quantities are normalized to their base unit. For instance, lengths will be normalized to metres.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#supported-units","title":"Supported units","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#time","title":"Time","text":"<p>Time is expressed in seconds (symbol: <code>s</code>). The following alternative symbols are supported: * <code>mo_s</code>: day29.53059 * <code>mo_g</code>: year/12.0 * <code>a</code>: day365.25 * <code>min</code>: min * <code>a_g</code>: year * <code>mo</code>: (day365.25)/12.0 * <code>mo_j</code>: (day365.25)/12.0 * <code>a_j</code>: day365.25 * <code>h</code>: h * <code>a_t</code>: day365.24219 * <code>d</code>: day</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#length","title":"Length","text":"<p>Length is expressed in metres (symbol: <code>m</code>). The following alternative symbols are supported: * <code>in</code>: c(cm254.0) * <code>nmi</code>: m1852.0 * <code>Ao</code>: dnm * <code>mil</code>: m(c(cm254.0)) * <code>yd</code>: ((c(cm254.0))12.0)3.0 * <code>AU</code>: m1.49597871E11 * <code>ft</code>: (c(cm254.0))12.0 * <code>pc</code>: m3.085678E16 * <code>fth</code>: ((c(cm254.0))12.0)6.0 * <code>mi</code>: ((c(cm254.0))12.0)5280.0 * <code>hd</code>: (c(cm254.0))4.0</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mass","title":"Mass","text":"<p>Mass is expressed in kilograms (symbol: <code>kg</code>). The following alternative symbols are supported: * <code>lb</code>: lb * <code>ston</code>: hlb20.0 * <code>t</code>: Mg * <code>stone</code>: lb14.0 * <code>u</code>: AMU * <code>gr</code>: (mg6479891.0)/100000.0 * <code>lcwt</code>: lb112.0 * <code>oz</code>: oz * <code>g</code>: g * <code>scwt</code>: hlb * <code>dr</code>: oz/16.0 * <code>lton</code>: (lb112.0)20.0</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#electric-current","title":"Electric current","text":"<p>Electric current is expressed in amperes (symbol: <code>A</code>). The following alternative symbols are supported: * <code>Bi</code>: daA * <code>Gb</code>: cm\u00b7(A/m)*250.0/[one?]</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#temperature","title":"Temperature","text":"<p>Temperature is expressed in kelvins (symbol: <code>K</code>). The following alternative symbols are supported: * <code>Cel</code>: \u2103</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#amount-of-substance","title":"Amount of substance","text":"<p>Amount of substance is expressed in moles (symbol: <code>mol</code>).</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#luminous-intensity","title":"Luminous intensity","text":"<p>Luminous intensity is expressed in candelas (symbol: <code>cd</code>).</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#area","title":"Area","text":"<p>Area is expressed in square metres (symbol: <code>m\u00b2</code>). The following alternative symbols are supported: * <code>m2</code>: m\u00b2 * <code>ar</code>: hm\u00b2 * <code>syd</code>: ((c(cm254.0))12.0)3.0\u00b2 * <code>cml</code>: [one?]/4.0\u00b7m(c(cm254.0))\u00b2 * <code>b</code>: hfm\u00b2 * <code>sft</code>: (c(cm254.0))12.0\u00b2 * <code>sin</code>: c(cm*254.0)\u00b2</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#volume","title":"Volume","text":"<p>Volume is expressed in cubic metres (symbol: <code>\u33a5</code>). The following alternative symbols are supported: * <code>st</code>: [\u33a5?] * <code>bf</code>: (c(cm254.0)\u00b3)144.0 * <code>cyd</code>: ((c(cm254.0))12.0)3.0\u00b3 * <code>cr</code>: ((c(cm254.0))12.0\u00b3)128.0 * <code>L</code>: L * <code>l</code>: l * <code>cin</code>: c(cm254.0)\u00b3 * <code>cft</code>: (c(cm254.0))*12.0\u00b3 * <code>m3</code>: \u33a5</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#energy","title":"Energy","text":"<p>Energy is expressed in joules (symbol: <code>J</code>). The following alternative symbols are supported: * <code>cal_IT</code>: (J41868.0)/10000.0 * <code>eV</code>: J1.602176487E-19 * <code>cal_m</code>: (J419002.0)/100000.0 * <code>cal</code>: m(J4184.0) * <code>cal_th</code>: m(J*4184.0)</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#angle","title":"Angle","text":"<p>Angle is expressed in radians (symbol: <code>rad</code>). The following alternative symbols are supported: * <code>circ</code>: [one?]\u00b7rad2.0 * <code>gon</code>: ([one?]\u00b7rad/180.0)0.9 * <code>deg</code>: [one?]\u00b7rad/180.0 * <code>'</code>: ([one?]\u00b7rad/180.0)/60.0 * <code>''</code>: (([one?]\u00b7rad/180.0)/60.0)/60.0</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#others","title":"Others","text":"<ul> <li><code>1/m</code>, derived units: <code>Ky</code>: c(1/m)</li> <li><code>kg/(m\u00b7s)</code>, derived units: <code>P</code>: g/(s\u00b7cm)</li> <li><code>bit/s</code>, derived units: <code>Bd</code>: bit/s</li> <li><code>bit</code>, derived units: <code>By</code>: bit*8.0</li> <li><code>Sv</code></li> <li><code>N</code></li> <li><code>\u03a9</code>, derived units: <code>Ohm</code>: \u03a9</li> <li><code>T</code>, derived units: <code>G</code>: T/10000.0</li> <li><code>sr</code>, derived units: <code>sph</code>: [one?]\u00b7sr*4.0</li> <li><code>F</code></li> <li><code>C/kg</code>, derived units: <code>R</code>: (C/kg)*2.58E-4</li> <li><code>cd/m\u00b2</code>, derived units: <code>sb</code>: cd/cm\u00b2, <code>Lmb</code>: cd/([one?]\u00b7cm\u00b2)</li> <li><code>Pa</code>, derived units: <code>bar</code>: Pa100000.0, <code>atm</code>: Pa101325.0</li> <li><code>kg/(m\u00b7s\u00b2)</code>, derived units: <code>att</code>: k(g\u00b7(m/s\u00b2)*9.80665)/cm\u00b2</li> <li><code>m\u00b2/s</code>, derived units: <code>St</code>: cm\u00b2/s</li> <li><code>A/m</code>, derived units: <code>Oe</code>: (A/m)*250.0/[one?]</li> <li><code>kg\u00b7m\u00b2/s\u00b2</code>, derived units: <code>erg</code>: cm\u00b2\u00b7g/s\u00b2</li> <li><code>kg/m\u00b3</code>, derived units: <code>g%</code>: g/dl</li> <li><code>mho</code></li> <li><code>V</code></li> <li><code>lx</code>, derived units: <code>ph</code>: lx/10000.0</li> <li><code>m/s\u00b2</code>, derived units: <code>Gal</code>: cm/s\u00b2, <code>m/s2</code>: m/s\u00b2</li> <li><code>m/s</code>, derived units: <code>kn</code>: m*1852.0/h</li> <li><code>m\u00b7kg/s\u00b2</code>, derived units: <code>gf</code>: g\u00b7(m/s\u00b2)9.80665, <code>lbf</code>: lb\u00b7(m/s\u00b2)9.80665, <code>dyn</code>: cm\u00b7g/s\u00b2</li> <li><code>m\u00b2/s\u00b2</code>, derived units: <code>RAD</code>: cm\u00b2\u00b7g/(s\u00b2\u00b7hg), <code>REM</code>: cm\u00b2\u00b7g/(s\u00b2\u00b7hg)</li> <li><code>C</code></li> <li><code>Gy</code></li> <li><code>Hz</code></li> <li><code>H</code></li> <li><code>lm</code></li> <li><code>W</code></li> <li><code>Wb</code>, derived units: <code>Mx</code>: Wb/1.0E8</li> <li><code>Bq</code>, derived units: <code>Ci</code>: Bq*3.7E10</li> <li><code>S</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_18","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_6","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#convert-si-unit-prefixes","title":"Convert SI unit prefixes:","text":"<ul> <li>Input values:</li> <li>Source: <code>[1 km]</code></li> <li> <p>Target: <code>[500 m]</code></p> </li> <li> <p>Returns: \u2192 <code>500.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#convert-imperial-and-metric-values","title":"Convert imperial and metric values:","text":"<ul> <li>Input values:</li> <li>Source: <code>[1 km]</code></li> <li> <p>Target: <code>[1 mi]</code></p> </li> <li> <p>Returns: \u2192 <code>609.344</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-if-the-compared-units-of-measurement-are-compatible","title":"Validate if the compared units of measurement are compatible:","text":"<ul> <li>Input values:</li> <li>Source: <code>[1 km]</code></li> <li> <p>Target: <code>[1 kg]</code></p> </li> <li> <p>Returns: \u2192 <code>NaN</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#date","title":"Date","text":"<p>The distance in days between two dates (\u2018YYYY-MM-DD\u2019 format).</p> Parameter Type Description Default requireMonthAndDay boolean If true, no distance value will be generated if months or days are missing (e.g., 2019-11). If false, missing month or day fields will default to 1. false <p>The identifier for this plugin is <code>date</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_19","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_7","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-0-if-both-dates-are-equal","title":"Returns 0 if both dates are equal:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2003-03-01]</code></li> <li> <p>Target: <code>[2003-03-01]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-1-if-both-dates-are-one-day-apart","title":"Returns 1 if both dates are one day apart:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2003-03-01]</code></li> <li> <p>Target: <code>[2003-03-02]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-the-number-of-days-if-both-dates-are-one-month-apart","title":"Returns the number of days if both dates are one month apart:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2003-03-01]</code></li> <li> <p>Target: <code>[2003-04-01]</code></p> </li> <li> <p>Returns: \u2192 <code>31.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-the-number-of-days-if-both-dates-are-one-year-apart","title":"Returns the number of days if both dates are one year apart:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2018-03-01]</code></li> <li> <p>Target: <code>[2019-03-01]</code></p> </li> <li> <p>Returns: \u2192 <code>365.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#time-of-day-is-ignored","title":"Time of day is ignored:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2003-03-01]</code></li> <li> <p>Target: <code>[2003-03-01T06:00:00]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-days-are-set-to-1-by-default","title":"Missing days are set to 1 by default:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2003-01]</code></li> <li> <p>Target: <code>[2003-01-01]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-months-are-set-to-1-by-default","title":"Missing months are set to 1 by default:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2003]</code></li> <li> <p>Target: <code>[2003-01-01]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-months-and-days-are-set-to-1-by-default","title":"Missing months and days are set to 1 by default:","text":"<ul> <li>Input values:</li> <li>Source: <code>[2018]</code></li> <li> <p>Target: <code>[2019]</code></p> </li> <li> <p>Returns: \u2192 <code>365.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-requiremonthandday-is-set-dates-without-a-day-and-month-will-not-match","title":"If \u2018requireMonthAndDay\u2019 is set, dates without a day and month will not match:","text":"<ul> <li>Parameters</li> <li> <p>requireMonthAndDay: <code>true</code></p> </li> <li> <p>Input values:</p> </li> <li>Source: <code>[2003]</code></li> <li> <p>Target: <code>[2003-03-01]</code></p> </li> <li> <p>Returns: \u2192 <code>Infinity</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-requiremonthandday-is-set-dates-without-a-day-will-not-match","title":"If \u2018requireMonthAndDay\u2019 is set, dates without a day will not match:","text":"<ul> <li>Parameters</li> <li> <p>requireMonthAndDay: <code>true</code></p> </li> <li> <p>Input values:</p> </li> <li>Source: <code>[2003-12]</code></li> <li> <p>Target: <code>[2003-03-01]</code></p> </li> <li> <p>Returns: \u2192 <code>Infinity</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#datetime","title":"DateTime","text":"<p>Distance between two date time values (xsd:dateTime format) in seconds.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>dateTime</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_20","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#inside-numeric-interval","title":"Inside numeric interval","text":"<p>Checks if a number is contained inside a numeric interval, such as \u20181900 - 2000\u2019.</p> Parameter Type Description Default separator String No description \u2014 <p>The identifier for this plugin is <code>insideNumericInterval</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_21","title":"Characteristics","text":"<p>This is a boolean distance measure, i.e., all distances are either 0 or 1.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-similarity","title":"Numeric similarity","text":"<p>Computes the numeric distance between two numbers.</p> Parameter Type Description Default minValue double The minimum number that is used for indexing -Infinity maxValue double The maximum number that is used for indexing Infinity <p>The identifier for this plugin is <code>num</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_22","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geographical-distance","title":"Geographical distance","text":"<p>Computes the geographical distance between two points. Author: Konrad H\u00f6ffner (MOLE subgroup of Research Group AKSW, University of Leipzig)</p> Parameter Type Description Default unit String No description km <p>The identifier for this plugin is <code>wgs84</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_23","title":"Characteristics","text":"<p>This distance measure is not normalized, i.e., all distances start at 0 (exact match) and increase the more different the values are.</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tokenbased","title":"Tokenbased","text":"<p>While character-based distance measures work well for typographical errors, there are a number of tasks where token-base distance measures are better suited:</p> <ul> <li>Strings where parts are reordered e.g. \u201cJohn Doe\u201d and \u201cDoe, John\u201d</li> <li>Texts consisting of multiple words</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cosine","title":"Cosine","text":"<p>Cosine Distance Measure.</p> Parameter Type Description Default k int No description 3 <p>The identifier for this plugin is <code>cosine</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_24","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#dice-coefficient","title":"Dice coefficient","text":"<p>Dice similarity coefficient.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>dice</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_25","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares sets of multiple values.Typically, incoming values are tokenized before being fed into this measure.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jaccard","title":"Jaccard","text":"<p>Jaccard similarity coefficient. Divides the matching tokens by the number of distinct tokens from both inputs.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>jaccard</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_26","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares sets of multiple values.Typically, incoming values are tokenized before being fed into this measure.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_8","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-0-for-equal-sets-of-values","title":"Returns 0 for equal sets of values:","text":"<ul> <li>Input values:</li> <li>Source: <code>[A, B, C]</code></li> <li> <p>Target: <code>[B, C, A]</code></p> </li> <li> <p>Returns: \u2192 <code>0.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-1-if-there-is-no-overlap-between-both-sets-of-tokens","title":"Returns 1 if there is no overlap between both sets of tokens:","text":"<ul> <li>Input values:</li> <li>Source: <code>[A, B, C]</code></li> <li> <p>Target: <code>[D, E, F]</code></p> </li> <li> <p>Returns: \u2192 <code>1.0</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-05-if-half-of-all-unique-tokens-overlap","title":"Returns 0.5 if half of all unique tokens overlap:","text":"<ul> <li>Input values:</li> <li>Source: <code>[A, B, C]</code></li> <li> <p>Target: <code>[A, B, D]</code></p> </li> <li> <p>Returns: \u2192 <code>0.5</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-23-if-one-third-of-all-unique-tokens-overlap","title":"Returns 2/3 if one third of all unique tokens overlap:","text":"<ul> <li>Input values:</li> <li>Source: <code>[John, Jane]</code></li> <li> <p>Target: <code>[John, Max]</code></p> </li> <li> <p>Returns: \u2192 <code>0.6666666666666666</code></p> </li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#soft-jaccard","title":"Soft Jaccard","text":"<p>Soft Jaccard similarity coefficient. Same as Jaccard distance but values within an levenhstein distance of \u2018maxDistance\u2019 are considered equivalent.</p> Parameter Type Description Default maxDistance int No description 1 <p>The identifier for this plugin is <code>softjaccard</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_27","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares sets of multiple values.Typically, incoming values are tokenized before being fed into this measure.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#token-wise-distance","title":"Token-wise distance","text":"<p>Token-wise string distance using the specified metric.</p> Parameter Type Description Default ignoreCase boolean No description true metricName String No description levenshtein splitRegex String No description [\\s\\d\\p{Punct}]+ stopwords String No description empty string stopwordWeight double Weight assigned to stopwords 0.01 nonStopwordWeight double Weight assigned to non-stopwords 0.1 useIncrementalIdfWeights boolean Use incremental IDF weights false matchThreshold double No description 0.0 orderingImpact double No description 0.0 adjustByTokenLength boolean No description false <p>The identifier for this plugin is <code>tokenwiseDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characteristics_28","title":"Characteristics","text":"<p>This distance measure is normalized, i.e., all distances are between 0 (exact match) and 1 (no similarity).</p> <p>Compares single values (as opposed to sequences of values). If multiple values are provided, all values are compared and the lowest distance is returned.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#transformations","title":"Transformations","text":"<p>The following transform and normalization functions are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#combine","title":"Combine","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#concatenate","title":"Concatenate","text":"<p>Concatenates strings from multiple inputs.</p> Parameter Type Description Default glue String Separator to be inserted between two concatenated strings. The text can contain escaped characters \\n, \\t and \\\\ that are replaced by a newline, tab or backslash respectively. empty string missingValuesAsEmptyStrings boolean Handle missing values as empty strings. false <p>The identifier for this plugin is <code>concat</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_9","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1","title":"Example 1:","text":"<ul> <li>Returns:</li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[a]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[a]</code>   2. <code>[b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>-</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[Last]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[First-Last]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5","title":"Example 5:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>-</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[Second, Third]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[First-Second, First-Third]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>-</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[]</code>   3. <code>[Second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[First--Second]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7","title":"Example 7:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>-</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[]</code>   3. <code>[Second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-8","title":"Example 8:","text":"<ul> <li>Parameters</li> <li>glue: <code>-</code></li> <li> <p>missingValuesAsEmptyStrings: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[]</code>   3. <code>[Second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[First--Second]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-9","title":"Example 9:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>\\n</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[Second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[First Second]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-10","title":"Example 10:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>\\t\\\\\\a</code></p> </li> <li> <p>Input values:   1. <code>[First]</code>   2. <code>[Second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[First \\\\aSecond]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#concatenate-multiple-values","title":"Concatenate multiple values","text":"<p>Concatenates multiple values received for an input. If applied to multiple inputs, yields at most one value per input. Optionally removes duplicate values.</p> Parameter Type Description Default glue String No description empty string removeDuplicates boolean No description false <p>The identifier for this plugin is <code>concatMultiValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_10","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_1","title":"Example 1:","text":"<ul> <li>Returns:</li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_1","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[a]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_1","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[a, b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_1","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>x</code></p> </li> <li> <p>Input values:   1. <code>[a, b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[axb]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_1","title":"Example 5:","text":"<ul> <li> <p>Input values:   1. <code>[a, b]</code>   2. <code>[1, 2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ab, 12]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_1","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>glue: <code>\\n\\t\\\\</code></p> </li> <li> <p>Input values:   1. <code>[a     \\b, c]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a     \\b     \\c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#concatenate-pairwise","title":"Concatenate pairwise","text":"<p>Concatenates the values of multiple inputs pairwise.</p> Parameter Type Description Default glue String Separator to be inserted between two concatenated strings. The text can contain escaped characters \\n, \\t and \\\\ that are replaced by a newline, tab or backslash respectively. empty string <p>The identifier for this plugin is <code>concatPairwise</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_11","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#values-of-two-inputs-are-concatenated-pairwise","title":"Values of two inputs are concatenated pairwise:","text":"<ul> <li> <p>Input values:   1. <code>[a, b, c]</code>   2. <code>[1, 2, 3]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a1, b2, c3]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#more-than-two-inputs-are-supported-as-well","title":"More than two inputs are supported as well:","text":"<ul> <li> <p>Input values:   1. <code>[a, b, c]</code>   2. <code>[1, 2, 3]</code>   3. <code>[x, y, z]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a1x, b2y, c3z]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-one-of-the-inputs-has-more-values-than-the-other-its-remaining-values-are-ignored","title":"If one of the inputs has more values than the other, its remaining values are ignored:","text":"<ul> <li> <p>Input values:   1. <code>[a, b, c]</code>   2. <code>[1, 2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a1, b2]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#empty-input-leads-to-empty-output","title":"Empty input leads to empty output:","text":"<ul> <li>Returns:</li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#a-single-input-is-just-forwarded","title":"A single input is just forwarded:","text":"<ul> <li> <p>Input values:   1. <code>[a]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#merge","title":"Merge","text":"<p>Merges the values of all inputs.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>merge</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_12","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_2","title":"Example 1:","text":"<ul> <li>Returns:</li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_2","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[a, b]</code>   2. <code>[c]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a, b, c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#conditional","title":"Conditional","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#contains-all-of","title":"Contains all of","text":"<p>Accepts two inputs. If the first input contains all of the second input values it returns \u2018true\u2019, else \u2018false\u2019 is returned.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>containsAllOf</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_13","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_3","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[A, B]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[true]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_3","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[A, D]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[false]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_2","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[D]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[false]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_2","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[A, B, C]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[true]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_2","title":"Example 5:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_2","title":"Example 6:","text":"<ul> <li> <p>Input values:   1. <code>[A]</code>   2. <code>[A]</code>   3. <code>[A]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_1","title":"Example 7:","text":"<ul> <li> <p>Input values:   1. <code>[A]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#contains-any-of","title":"Contains any of","text":"<p>Accepts two inputs. If the first input contains any of the second input values it returns \u2018true\u2019, else \u2018false\u2019 is returned.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>containsAnyOf</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_14","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_4","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[A, B]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[true]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_4","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[A, D]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[true]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_3","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[D]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[false]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_3","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[A, B, C]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[true]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_3","title":"Example 5:","text":"<ul> <li> <p>Input values:   1. <code>[A, B, C]</code>   2. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_3","title":"Example 6:","text":"<ul> <li> <p>Input values:   1. <code>[A]</code>   2. <code>[A]</code>   3. <code>[A]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_2","title":"Example 7:","text":"<ul> <li> <p>Input values:   1. <code>[A]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-contains","title":"If contains","text":"<p>Accepts two or three inputs. If the first input contains the given value, the second input is forwarded. Otherwise, the third input is forwarded (if present).</p> Parameter Type Description Default search String No description no default <p>The identifier for this plugin is <code>ifContains</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_15","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_5","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>search: <code>match</code></p> </li> <li> <p>Input values:   1. <code>[matching string]</code>   2. <code>[this is a match]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[this is a match]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_5","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>search: <code>match</code></p> </li> <li> <p>Input values:   1. <code>[different string]</code>   2. <code>[this is a match]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_4","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>search: <code>match</code></p> </li> <li> <p>Input values:   1. <code>[different string]</code>   2. <code>[this is a match]</code>   3. <code>[this is no match]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[this is no match]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-exists","title":"If exists","text":"<p>Accepts two or three inputs. If the first input provides a value, the second input is forwarded. Otherwise, the third input is forwarded (if present).</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ifExists</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_16","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_6","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[value]</code>   2. <code>[yes]</code>   3. <code>[no]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[yes]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_6","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[]</code>   2. <code>[yes]</code>   3. <code>[no]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[no]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_5","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[value]</code>   2. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-matches-regex","title":"If matches regex","text":"<p>Accepts two or three inputs. If any value of the first input matches the regex, the second input is forwarded. Otherwise, the third input is forwarded (if present).</p> Parameter Type Description Default regex String No description no default negate boolean No description false <p>The identifier for this plugin is <code>ifMatchesRegex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#negate-binary-not","title":"Negate binary (NOT)","text":"<p>Accepts one input, which is either \u2018true\u2019, \u20181\u2019 or \u2018false\u2019, \u20180\u2019 and negates it.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>negateTransformer</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_17","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_7","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[0, 1, false, true, False, True]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1, 0, true, false, true, false]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_7","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[falsee, true]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_6","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#conversion","title":"Conversion","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#convert-charset","title":"Convert charset","text":"<p>Convert the string from \u201csourceCharset\u201d to \u201ctargetCharset\u201d.</p> Parameter Type Description Default sourceCharset String No description ISO-8859-1 targetCharset String No description UTF-8 <p>The identifier for this plugin is <code>convertCharset</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conversion</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean-html","title":"Clean HTML","text":"<p>Cleans HTML using a tag white list and allows selection of HTML sections with XPath or CSS selector expressions.        If the tag or attribute white lists are left empty default white lists will be used (this behaviour can be changed).        To remove all HTML markup and retain text, keep the defaults and turn off the \u201cDefault tags and attributes\u201d toggle.        The operator takes two inputs: the page HTML and (optional) the page Url which may be needed to resolve relative links in the page HTML.</p> Parameter Type Description Default tagWhiteList StringIterableParameter Tags to keep in the cleaned output. attributeWhiteList StringIterableParameter Attributes to keep in the cleaned output. selectors StringIterableParameter CSS or XPath queries for selection of content. CSS selectors can be pipe separated for non-sequential execution. method Enum Selects use of XPath or CSS selectors. xPath defaultTagsAndAttributes boolean Use defaults for empty tag and attribute whitelists.\\If the attribute while list is empty, it will default to: \u201cclass\u201d, \u201cid\u201d, \u201chref\u201d, \u201csrc\u201d\\If the tag while list is empty, it will default to: \u201ca\u201d, \u201cb\u201d, \u201cblockquote\u201d, \u201cbr\u201d, \u201ccaption\u201d, \u201ccite\u201d, \u201ccode\u201d, \u201ccol\u201d, \u201ccolgroup\u201d, \u201cdd\u201d, \u201cdiv\u201d, \u201cdl\u201d, \u201cdt\u201d, \u201cem\u201d, \u201ch1\u201d, \u201ch2\u201d, \u201ch3\u201d, \u201ch4\u201d, \u201ch5\u201d, \u201ch6\u201d,\u201di\u201d, \u201cimg\u201d, \u201cli\u201d, \u201col\u201d, \u201cp\u201d, \u201cpre\u201d, \u201cq\u201d, \u201csmall\u201d, \u201cspan\u201d, \u201cstrike\u201d, \u201cstrong\u201d,\u201dsub\u201d, \u201csup\u201d, \u201ctable\u201d, \u201ctbody\u201d, \u201ctd\u201d, \u201ctfoot\u201d, \u201cth\u201d, \u201cthead\u201d, \u201ctr\u201d, \u201cu\u201d, \u201cul\u201d. true <p>The identifier for this plugin is <code>htmlCleaner</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.html</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#date_1","title":"Date","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-date","title":"Parse date","text":"<p>Parses and normalizes dates in different formats.</p> Parameter Type Description Default inputDateFormatId Option The input date/time format used for parsing the date/time string. w3c Date alternativeInputFormat String An input format string that should be used instead of the selected input format. Java DateFormat string. empty string inputLocale LocaleOptionParameter Optional locale for the (alternative) input format. If not set the system\u2019s locale will be used or the locale of the input format, if set. outputDateFormatId Option The output date/time format used for parsing the date/time string. w3c Date alternativeOutputFormat String An output format string that should be used instead of the selected output format. Java DateFormat string. empty string outputLocale LocaleOptionParameter Optional locale for the (alternative) output format. If not set the system\u2019s locale will be used or the locale of the output format, if set. <p>The identifier for this plugin is <code>DateTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_18","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_8","title":"Example 1:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>German style date format</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[20.03.1999]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1999-03-20]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_8","title":"Example 2:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c Date</code></li> <li> <p>outputDateFormatId: <code>German style date format</code></p> </li> <li> <p>Input values:   1. <code>[1999-03-20]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[20.03.1999]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_7","title":"Example 3:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>common ISO8601</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[2017-04-04T00:00:00.000+02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-04-04]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_4","title":"Example 4:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>common ISO8601</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[2017-04-04T00:00:00+02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-04-04]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_4","title":"Example 5:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>common ISO8601</code></li> <li> <p>outputDateFormatId: <code>dateTime with month abbr. (US)</code></p> </li> <li> <p>Input values:   1. <code>[2021-06-24T14:50:05.895+02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Jun-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_4","title":"Example 6:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>dateTime with month abbr. (US)</code></li> <li> <p>outputDateFormatId: <code>dateTime with month abbr. (DE)</code></p> </li> <li> <p>Input values:   1. <code>[24-Dec-2021 14:50:05 +02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Dez.-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_3","title":"Example 7:","text":"<ul> <li>Parameters</li> <li>alternativeInputFormat: <code>dd.MM.yyyy HH:mm.ss</code></li> <li> <p>alternativeOutputFormat: <code>yyyy-MM-dd'T'HH:mm.ss</code></p> </li> <li> <p>Input values:   1. <code>[20.03.1999 20:34.44]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1999-03-20T20:34.44]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-8_1","title":"Example 8:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>excelDateTime</code></li> <li> <p>outputDateFormatId: <code>xsdTime</code></p> </li> <li> <p>Input values:   1. <code>[12:20:00.000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[12:20:00.000]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-9_1","title":"Example 9:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c YearMonth</code></li> <li> <p>outputDateFormatId: <code>w3c Month</code></p> </li> <li> <p>Input values:   1. <code>[2020-01]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[--01]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-10_1","title":"Example 10:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c MonthDay</code></li> <li> <p>outputDateFormatId: <code>w3c Day</code></p> </li> <li> <p>Input values:   1. <code>[--12-31]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[---31]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-11","title":"Example 11:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c Date</code></li> <li> <p>outputDateFormatId: <code>w3c MonthDay</code></p> </li> <li> <p>Input values:   1. <code>[2020-12-31]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[--12-31]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-12","title":"Example 12:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c MonthDay</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[--12-31]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-13","title":"Example 13:","text":"<ul> <li>Parameters</li> <li>alternativeInputFormat: <code>yyyy-MM-dd HH:mm:ss.SSS</code></li> <li> <p>outputDateFormatId: <code>w3cDateTime</code></p> </li> <li> <p>Input values:   1. <code>[2020-02-22 16:34:14.000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2020-02-22T16:34:14]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-14","title":"Example 14:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>dateTime with month abbr. (DE)</code></li> <li>outputDateFormatId: <code>dateTime with month abbr. (US)</code></li> <li>inputLocale: <code>en_US</code></li> <li> <p>outputLocale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[24-Dec-2021 14:50:05 +02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Dez.-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-15","title":"Example 15:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>dateTime with month abbr. (US)</code></li> <li>outputDateFormatId: <code>dateTime with month abbr. (DE)</code></li> <li>inputLocale: <code>de</code></li> <li> <p>outputLocale: <code>en</code></p> </li> <li> <p>Input values:   1. <code>[24-Dez.-2021 14:50:05 +02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Dec-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-16","title":"Example 16:","text":"<ul> <li>Parameters</li> <li>outputLocale: <code>fr</code></li> <li>alternativeInputFormat: <code>MMM yyyy</code></li> <li>outputDateFormatId: <code>dateTime with month abbr. (DE)</code></li> <li>inputLocale: <code>de</code></li> <li>alternativeOutputFormat: <code>MMM uuuu</code></li> <li> <p>inputDateFormatId: <code>dateTime with month abbr. (US)</code></p> </li> <li> <p>Input values:   1. <code>[Dez. 2021]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[d\u00e9c. 2021]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-17","title":"Example 17:","text":"<ul> <li>Parameters</li> <li>alternativeInputFormat: <code>MMMM, uuuu</code></li> <li>alternativeOutputFormat: <code>MMMM, uuuu</code></li> <li>inputLocale: <code>en_US</code></li> <li> <p>outputLocale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[February, 2024]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Februar, 2024]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#compare-dates","title":"Compare dates","text":"<p>Compares two dates. Returns 1 if the comparison yields true and 0 otherwise. If there are multiple dates in both sets, the comparator must be true for all dates. For instance, {2014-08-02,2014-08-03} &lt; {2014-08-03} yields 0 as not all dates in the first set are smaller than in the second.</p> Parameter Type Description Default comparator Enum No description &lt; <p>The identifier for this plugin is <code>compareDates</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_19","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_9","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>comparator: <code>&lt;</code></p> </li> <li> <p>Input values:   1. <code>[2017-01-01]</code>   2. <code>[2017-01-02]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_9","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>comparator: <code>&lt;</code></p> </li> <li> <p>Input values:   1. <code>[2017-01-02]</code>   2. <code>[2017-01-01]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_8","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>comparator: <code>&gt;</code></p> </li> <li> <p>Input values:   1. <code>[2017-01-02]</code>   2. <code>[2017-01-01]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_5","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>comparator: <code>&gt;</code></p> </li> <li> <p>Input values:   1. <code>[2017-01-01]</code>   2. <code>[2017-01-02]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_5","title":"Example 5:","text":"<ul> <li>Parameters</li> <li> <p>comparator: <code>=</code></p> </li> <li> <p>Input values:   1. <code>[2017-01-01]</code>   2. <code>[2017-01-01]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_5","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>comparator: <code>=</code></p> </li> <li> <p>Input values:   1. <code>[2017-01-02]</code>   2. <code>[2017-01-01]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#current-date","title":"Current date","text":"<p>Outputs the current date.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>currentDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#date-to-timestamp","title":"Date to timestamp","text":"<p>Convert an xsd:dateTime to a timestamp. Returns the passed time since the Unix Epoch (1970-01-01).</p> Parameter Type Description Default unit Enum No description milliseconds <p>The identifier for this plugin is <code>datetoTimestamp</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_20","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_10","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[2017-07-03T21:32:52Z]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1499117572000]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_10","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[2017-07-03T21:32:52+01:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1499113972000]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_9","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>unit: <code>seconds</code></p> </li> <li> <p>Input values:   1. <code>[2017-07-03T21:32:52+01:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1499113972]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_6","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[2017-07-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1499040000000]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration","title":"Duration","text":"<p>Computes the time difference between two data times.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>duration</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration-in-days","title":"Duration in days","text":"<p>Converts an xsd:duration to days.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>durationInDays</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration-in-seconds","title":"Duration in seconds","text":"<p>Converts an xsd:duration to seconds.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>durationInSeconds</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration-in-years","title":"Duration in years","text":"<p>Converts an xsd:duration to years.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>durationInYears</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#number-to-duration","title":"Number to duration","text":"<p>Converts a number to an xsd:duration.</p> Parameter Type Description Default unit Enum No description day <p>The identifier for this plugin is <code>numberToDuration</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-date-pattern","title":"Parse date pattern","text":"<p>Parses a date based on a specified pattern, returning an xsd:date.</p> Parameter Type Description Default format String The date pattern used to parse the input values dd-MM-yyyy lenient boolean If set to true, the parser tries to use heuristics to parse dates with invalid fields (such as a day of zero). false locale LocaleOptionParameter Optional locale for the date format. If not set the system\u2019s locale will be used. <p>The identifier for this plugin is <code>parseDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_21","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_11","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>format: <code>dd.MM.yyyy</code></p> </li> <li> <p>Input values:   1. <code>[03.04.2015]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_11","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>format: <code>dd.MM.yyyy</code></p> </li> <li> <p>Input values:   1. <code>[3.4.2015]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_10","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>format: <code>yyyyMMdd</code></p> </li> <li> <p>Input values:   1. <code>[20150403]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_7","title":"Example 4:","text":"<ul> <li>Parameters</li> <li>format: <code>MMM yyyy</code></li> <li> <p>locale: <code>en</code></p> </li> <li> <p>Input values:   1. <code>[May 2024]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2024-05-01]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_6","title":"Example 5:","text":"<ul> <li>Parameters</li> <li>format: <code>MMM yyyy</code></li> <li> <p>locale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[Mai 2024]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2024-05-01]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_6","title":"Example 6:","text":"<ul> <li>Parameters</li> <li>format: <code>MMM yyyy</code></li> <li> <p>locale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[May 2024]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_4","title":"Example 7:","text":"<ul> <li>Parameters</li> <li>format: <code>yyyyMMdd</code></li> <li> <p>lenient: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[20150000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#timestamp-to-date","title":"Timestamp to date","text":"<p>Convert a timestamp to xsd:date format. Expects an integer that denotes the passed time since the Unix Epoch (1970-01-01)</p> Parameter Type Description Default format String Custom output format (e.g., \u2018yyyy-MM-dd\u2019). If left empty, a full xsd:dateTime (UTC) is returned. empty string unit Enum No description milliseconds <p>The identifier for this plugin is <code>timeToDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_22","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_12","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[1499117572000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-07-03T21:32:52Z]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_12","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>format: <code>yyyy-MM-dd</code></p> </li> <li> <p>Input values:   1. <code>[1499040000000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-07-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_11","title":"Example 3:","text":"<ul> <li>Parameters</li> <li>format: <code>yyyy-MM-dd</code></li> <li> <p>unit: <code>seconds</code></p> </li> <li> <p>Input values:   1. <code>[1499040000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-07-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-after","title":"Validate date after","text":"<p>Validates if the first input date is after the second input date. Outputs the first input if the validation is successful.</p> Parameter Type Description Default allowEqual boolean Allow both dates to be equal. false <p>The identifier for this plugin is <code>validateDateAfter</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_23","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_13","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[2015-04-02]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_13","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[2015-04-04]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-04]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_12","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>allowEqual: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[2015-04-03]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_8","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>allowEqual: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[2015-04-03]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-range","title":"Validate date range","text":"<p>Validates if dates are within a specified range.</p> Parameter Type Description Default minDate String Earliest allowed date in YYYY-MM-DD no default maxDate String Latest allowed data in YYYY-MM-DD no default <p>The identifier for this plugin is <code>validateDateRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-numeric-range","title":"Validate numeric range","text":"<p>Validates if a number is within a specified range.</p> Parameter Type Description Default min double Minimum allowed number no default max double Maximum allowed number no default <p>The identifier for this plugin is <code>validateNumericRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel_1","title":"Excel","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#abs","title":"Abs","text":"<p>Excel ABS(number): Returns the absolute value of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function ABS <p>The identifier for this plugin is <code>Excel_ABS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#acos","title":"Acos","text":"<p>Excel ACOS(number): Returns the inverse cosine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ACOS <p>The identifier for this plugin is <code>Excel_ACOS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#acosh","title":"Acosh","text":"<p>Excel ACOSH(number): Returns the inverse hyperbolic cosine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ACOSH <p>The identifier for this plugin is <code>Excel_ACOSH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#and","title":"And","text":"<p>Excel AND(argument1; argument2 \u2026argument30): Returns TRUE if all the arguments are considered TRUE, and FALSE otherwise.</p> Parameter Type Description Default functionName String The name of the Excel function AND <p>The identifier for this plugin is <code>Excel_AND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#asin","title":"Asin","text":"<p>Excel ASIN(number): Returns the inverse sine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ASIN <p>The identifier for this plugin is <code>Excel_ASIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#asinh","title":"Asinh","text":"<p>Excel ASINH(number): Returns the inverse hyperbolic sine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ASINH <p>The identifier for this plugin is <code>Excel_ASINH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#atan","title":"Atan","text":"<p>Excel ATAN(number): Returns the inverse tangent of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ATAN <p>The identifier for this plugin is <code>Excel_ATAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#atan2","title":"Atan2","text":"<p>Excel ATAN2(number_x; number_y): Returns the inverse tangent of the specified x and y coordinates. Number_x is the value for the x coordinate. Number_y is the value for the y coordinate.</p> Parameter Type Description Default functionName String The name of the Excel function ATAN2 <p>The identifier for this plugin is <code>Excel_ATAN2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#atanh","title":"Atanh","text":"<p>Excel ATANH(number): Returns the inverse hyperbolic tangent of the given number. (Angle is returned in radians.)</p> Parameter Type Description Default functionName String The name of the Excel function ATANH <p>The identifier for this plugin is <code>Excel_ATANH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#avedev","title":"Avedev","text":"<p>Excel AVEDEV(number1; number2; \u2026 number_30): Returns the average of the absolute deviations of data points from their mean. Displays the diffusion in a data set. Number_1; number_2; \u2026 number_30 are values or ranges that represent a sample. Each number can also be replaced by a reference.</p> Parameter Type Description Default functionName String The name of the Excel function AVEDEV <p>The identifier for this plugin is <code>Excel_AVEDEV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#average","title":"Average","text":"<p>Excel AVERAGE(number_1; number_2; \u2026 number_30): Returns the average of the arguments. Number_1; number_2; \u2026 number_30 are numerical values or ranges. Text is ignored.</p> Parameter Type Description Default functionName String The name of the Excel function AVERAGE <p>The identifier for this plugin is <code>Excel_AVERAGE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#averagea","title":"Averagea","text":"<p>Excel AVERAGEA(value_1; value_2; \u2026 value_30): Returns the average of the arguments. The value of a text is 0. Value_1; value_2; \u2026 value_30 are values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function AVERAGEA <p>The identifier for this plugin is <code>Excel_AVERAGEA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ceiling","title":"Ceiling","text":"<p>Excel CEILING(number; significance; mode): Rounds the given number to the nearest integer or multiple of significance. Significance is the value to whose multiple of ten the value is to be rounded up (.01, .1, 1, 10, etc.). Mode is an optional value. If it is indicated and non-zero and if the number and significance are negative, rounding up is carried out based on that value.</p> Parameter Type Description Default functionName String The name of the Excel function CEILING <p>The identifier for this plugin is <code>Excel_CEILING</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#choose","title":"Choose","text":"<p>Excel CHOOSE(index; value1; \u2026 value30): Uses an index to return a value from a list of up to 30 values. Index is a reference or number between 1 and 30 indicating which value is to be taken from the list. Value1; \u2026 value30 is the list of values entered as a reference to a cell or as individual values.</p> Parameter Type Description Default functionName String The name of the Excel function CHOOSE <p>The identifier for this plugin is <code>Excel_CHOOSE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean","title":"Clean","text":"<p>Excel CLEAN(text): Removes all non-printing characters from the string. Text refers to the text from which to remove all non-printable characters.</p> Parameter Type Description Default functionName String The name of the Excel function CLEAN <p>The identifier for this plugin is <code>Excel_CLEAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#code","title":"Code","text":"<p>Excel CODE(text): Returns a numeric code for the first character in a text string. Text is the text for which the code of the first character is to be found.</p> Parameter Type Description Default functionName String The name of the Excel function CODE <p>The identifier for this plugin is <code>Excel_CODE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#combin","title":"Combin","text":"<p>Excel COMBIN(count_1; count_2): Returns the number of combinations for a given number of objects. Count_1 is the total number of elements. Count_2 is the selected count from the elements. This is the same as the nCr function on a calculator.</p> Parameter Type Description Default functionName String The name of the Excel function COMBIN <p>The identifier for this plugin is <code>Excel_COMBIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#correl","title":"Correl","text":"<p>Excel CORREL(data_1; data_2): Returns the correlation coefficient between two data sets. Data_1 is the first data set. Data_2 is the second data set.</p> Parameter Type Description Default functionName String The name of the Excel function CORREL <p>The identifier for this plugin is <code>Excel_CORREL</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cos","title":"Cos","text":"<p>Excel COS(number): Returns the cosine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function COS <p>The identifier for this plugin is <code>Excel_COS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cosh","title":"Cosh","text":"<p>Excel COSH(number): Returns the hyperbolic cosine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function COSH <p>The identifier for this plugin is <code>Excel_COSH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#count","title":"Count","text":"<p>Excel COUNT(value_1; value_2; \u2026 value_30): Counts how many numbers are in the list of arguments. Text entries are ignored. Value_1; value_2; \u2026 value_30 are values or ranges which are to be counted.</p> Parameter Type Description Default functionName String The name of the Excel function COUNT <p>The identifier for this plugin is <code>Excel_COUNT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#counta","title":"Counta","text":"<p>Excel COUNTA(value_1; value_2; \u2026 value_30): Counts how many values are in the list of arguments. Text entries are also counted, even when they contain an empty string of length 0. If an argument is an array or reference, empty cells within the array or reference are ignored. value_1; value_2; \u2026 value_30 are up to 30 arguments representing the values to be counted.</p> Parameter Type Description Default functionName String The name of the Excel function COUNTA <p>The identifier for this plugin is <code>Excel_COUNTA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#covar","title":"Covar","text":"<p>Excel COVAR(data_1; data_2): Returns the covariance of the product of paired deviations. Data_1 is the first data set. Data_2 is the second data set.</p> Parameter Type Description Default functionName String The name of the Excel function COVAR <p>The identifier for this plugin is <code>Excel_COVAR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#degrees","title":"Degrees","text":"<p>Excel DEGREES(number): Converts the given number in radians to degrees.</p> Parameter Type Description Default functionName String The name of the Excel function DEGREES <p>The identifier for this plugin is <code>Excel_DEGREES</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#devsq","title":"Devsq","text":"<p>Excel DEVSQ(number_1; number_2; \u2026 number_30): Returns the sum of squares of deviations based on a sample mean. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample.</p> Parameter Type Description Default functionName String The name of the Excel function DEVSQ <p>The identifier for this plugin is <code>Excel_DEVSQ</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#even","title":"Even","text":"<p>Excel EVEN(number): Rounds the given number up to the nearest even integer.</p> Parameter Type Description Default functionName String The name of the Excel function EVEN <p>The identifier for this plugin is <code>Excel_EVEN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#exact","title":"Exact","text":"<p>Excel EXACT(text_1; text_2): Compares two text strings and returns TRUE if they are identical. This function is case- sensitive. Text_1 is the first text to compare. Text_2 is the second text to compare.</p> Parameter Type Description Default functionName String The name of the Excel function EXACT <p>The identifier for this plugin is <code>Excel_EXACT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#exp","title":"Exp","text":"<p>Excel EXP(number): Returns e raised to the power of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function EXP <p>The identifier for this plugin is <code>Excel_EXP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fact","title":"Fact","text":"<p>Excel FACT(number): Returns the factorial of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function FACT <p>The identifier for this plugin is <code>Excel_FACT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#false","title":"False","text":"<p>Excel FALSE(): Set the logical value to FALSE. The FALSE() function does not require any arguments.</p> Parameter Type Description Default functionName String The name of the Excel function FALSE <p>The identifier for this plugin is <code>Excel_FALSE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#find","title":"Find","text":"<p>Excel FIND(find_text; text; position): Looks for a string of text within another string. Where to begin the search can also be defined. The search term can be a number or any string of characters. The search is case-sensitive. Find_text is the text to be found. Text is the text where the search takes place. Position (optional) is the position in the text from which the search starts.</p> Parameter Type Description Default functionName String The name of the Excel function FIND <p>The identifier for this plugin is <code>Excel_FIND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#floor","title":"Floor","text":"<p>Excel FLOOR(number; significance; mode): Rounds the given number down to the nearest multiple of significance. Significance is the value to whose multiple of ten the number is to be rounded down (.01, .1, 1, 10, etc.). Mode is an optional value. If it is indicated and non-zero and if the number and significance are negative, rounding up is carried out based on that value.</p> Parameter Type Description Default functionName String The name of the Excel function FLOOR <p>The identifier for this plugin is <code>Excel_FLOOR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#forecast","title":"Forecast","text":"<p>Excel FORECAST(value; data_Y; data_X): Extrapolates future values based on existing x and y values. Value is the x value, for which the y value of the linear regression is to be returned. Data_Y is the array or range of known y\u0092s. Data_X is the array or range of known x\u0092s. Does not work for exponential functions.</p> Parameter Type Description Default functionName String The name of the Excel function FORECAST <p>The identifier for this plugin is <code>Excel_FORECAST</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fv","title":"Fv","text":"<p>Excel FV(rate; NPER; PMT; PV; type): Returns the future value of an investment based on periodic, constant payments and a constant interest rate. Rate is the periodic interest rate. NPER is the total number of periods. PMT is the annuity paid regularly per period. PV (optional) is the present cash value of an investment. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function FV <p>The identifier for this plugin is <code>Excel_FV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geomean","title":"Geomean","text":"<p>Excel GEOMEAN(number_1; number_2; \u2026 number_30): Returns the geometric mean of a sample. Number_1; number_2; \u2026 number_30 are numerical arguments or ranges that represent a random sample.</p> Parameter Type Description Default functionName String The name of the Excel function GEOMEAN <p>The identifier for this plugin is <code>Excel_GEOMEAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if","title":"If","text":"<p>Excel IF(test; then_value; otherwise_value): Returns different values based on the test value. Note that in this implementation it will not actually evaluate logical conditions. Then_value is the value that is returned if the test is TRUE. Otherwise_value (optional) is the value that is returned if the test is FALSE.</p> Parameter Type Description Default functionName String The name of the Excel function IF <p>The identifier for this plugin is <code>Excel_IF</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#int","title":"Int","text":"<p>Excel INT(number): Rounds the given number down to the nearest integer.</p> Parameter Type Description Default functionName String The name of the Excel function INT <p>The identifier for this plugin is <code>Excel_INT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#intercept","title":"Intercept","text":"<p>Excel INTERCEPT(data_Y; data_X): Calculates the y-value at which a line will intersect the y-axis by using known x-values and y-values. Data_Y is the dependent set of observations or data. Data_X is the independent set of observations or data. Names, arrays or references containing numbers must be used here. Numbers can also be entered directly.</p> Parameter Type Description Default functionName String The name of the Excel function INTERCEPT <p>The identifier for this plugin is <code>Excel_INTERCEPT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ipmt","title":"Ipmt","text":"<p>Excel IPMT(rate; period; NPER; PV; FV; type): Calculates the periodic amortization for an investment with regular payments and a constant interest rate. Rate is the periodic interest rate. Period is the period for which the compound interest is calculated. NPER is the total number of periods during which annuity is paid. Period=NPER, if compound interest for the last period is calculated. PV is the present cash value in sequence of payments. FV (optional) is the desired value (future value) at the end of the periods. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function IPMT <p>The identifier for this plugin is <code>Excel_IPMT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#irr","title":"Irr","text":"<p>Excel IRR(values; guess): Calculates the internal rate of return for an investment. The values represent cash flow values at regular intervals; at least one value must be negative (payments), and at least one value must be positive (income). Values is an array containing the values. Guess (optional) is the estimated value. If you can provide only a few values, you should provide an initial guess to enable the iteration.</p> Parameter Type Description Default functionName String The name of the Excel function IRR <p>The identifier for this plugin is <code>Excel_IRR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#large","title":"Large","text":"<p>Excel LARGE(data; rank_c): Returns the Rank_c-th largest value in a data set. Data is the cell range of data. Rank_c is the ranking of the value (2nd largest, 3rd largest, etc.) written as an integer.</p> Parameter Type Description Default functionName String The name of the Excel function LARGE <p>The identifier for this plugin is <code>Excel_LARGE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#left","title":"Left","text":"<p>Excel LEFT(text; number): Returns the first character or characters in a text string. Text is the text where the initial partial words are to be determined. Number (optional) is the number of characters for the start text. If this parameter is not defined, one character is returned.</p> Parameter Type Description Default functionName String The name of the Excel function LEFT <p>The identifier for this plugin is <code>Excel_LEFT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ln","title":"Ln","text":"<p>Excel LN(number): Returns the natural logarithm based on the constant e of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function LN <p>The identifier for this plugin is <code>Excel_LN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#log","title":"Log","text":"<p>Excel LOG(number; base): Returns the logarithm of the given number to the specified base. Base is the base for the logarithm calculation.</p> Parameter Type Description Default functionName String The name of the Excel function LOG <p>The identifier for this plugin is <code>Excel_LOG</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#log10","title":"Log10","text":"<p>Excel LOG10(number): Returns the base-10 logarithm of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function LOG10 <p>The identifier for this plugin is <code>Excel_LOG10</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#max","title":"Max","text":"<p>Excel MAX(number_1; number_2; \u2026 number_30): Returns the maximum value in a list of arguments. Number_1; number_2; \u2026 number_30 are numerical values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MAX <p>The identifier for this plugin is <code>Excel_MAX</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#maxa","title":"Maxa","text":"<p>Excel MAXA(value_1; value_2; \u2026 value_30): Returns the maximum value in a list of arguments. Unlike MAX, text can be entered. The value of the text is 0. Value_1; value_2; \u2026 value_30 are values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MAXA <p>The identifier for this plugin is <code>Excel_MAXA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#median","title":"Median","text":"<p>Excel MEDIAN(number_1; number_2; \u2026 number_30): Returns the median of a set of numbers. Number_1; number_2; \u2026 number_30 are values or ranges, which represent a sample. Each number can also be replaced by a reference.</p> Parameter Type Description Default functionName String The name of the Excel function MEDIAN <p>The identifier for this plugin is <code>Excel_MEDIAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mid","title":"Mid","text":"<p>Excel MID(text; start; number): Returns a text segment of a character string. The parameters specify the starting position and the number of characters. Text is the text containing the characters to extract. Start is the position of the first character in the text to extract. Number is the number of characters in the part of the text.</p> Parameter Type Description Default functionName String The name of the Excel function MID <p>The identifier for this plugin is <code>Excel_MID</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#min","title":"Min","text":"<p>Excel MIN(number_1; number_2; \u2026 number_30): Returns the minimum value in a list of arguments. Number_1; number_2; \u2026 number_30 are numerical values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MIN <p>The identifier for this plugin is <code>Excel_MIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mina","title":"Mina","text":"<p>Excel MINA(value_1; value_2; \u2026 value_30): Returns the minimum value in a list of arguments. Here text can also be entered. The value of the text is 0. Value_1; value_2; \u2026 value_30 are values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MINA <p>The identifier for this plugin is <code>Excel_MINA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mirr","title":"Mirr","text":"<p>Excel MIRR(values; investment; reinvest_rate): Calculates the modified internal rate of return of a series of investments. Values corresponds to the array or the cell reference for cells whose content corresponds to the payments. Investment is the rate of interest of the investments (the negative values of the array) Reinvest_rate is the rate of interest of the reinvestment (the positive values of the array).</p> Parameter Type Description Default functionName String The name of the Excel function MIRR <p>The identifier for this plugin is <code>Excel_MIRR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mod","title":"Mod","text":"<p>Excel MOD(dividend; divisor): Returns the remainder after a number is divided by a divisor. Dividend is the number which will be divided by the divisor. Divisor is the number by which to divide the dividend.</p> Parameter Type Description Default functionName String The name of the Excel function MOD <p>The identifier for this plugin is <code>Excel_MOD</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mode","title":"Mode","text":"<p>Excel MODE(number_1; number_2; \u2026 number_30): Returns the most common value in a data set. Number_1; number_2; \u2026 number_30 are numerical values or ranges. If several values have the same frequency, it returns the smallest value. An error occurs when a value does not appear twice.</p> Parameter Type Description Default functionName String The name of the Excel function MODE <p>The identifier for this plugin is <code>Excel_MODE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normdist","title":"Normdist","text":"<p>Excel NORMDIST(number; mean; STDEV; C): Returns the normal distribution for the given Number in the distribution. Mean is the mean value of the distribution. STDEV is the standard deviation of the distribution. C = 0 calculates the density function, and C = 1 calculates the distribution.</p> Parameter Type Description Default functionName String The name of the Excel function NORMDIST <p>The identifier for this plugin is <code>Excel_NORMDIST</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#norminv","title":"Norminv","text":"<p>Excel NORMINV(number; mean; STDEV): Returns the inverse of the normal distribution for the given Number in the distribution. Mean is the mean value in the normal distribution. STDEV is the standard deviation of the normal distribution.</p> Parameter Type Description Default functionName String The name of the Excel function NORMINV <p>The identifier for this plugin is <code>Excel_NORMINV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normsdist","title":"Normsdist","text":"<p>Excel NORMSDIST(number): Returns the standard normal cumulative distribution for the given Number.</p> Parameter Type Description Default functionName String The name of the Excel function NORMSDIST <p>The identifier for this plugin is <code>Excel_NORMSDIST</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normsinv","title":"Normsinv","text":"<p>Excel NORMSINV(number): Returns the inverse of the standard normal distribution for the given Number, a probability value.</p> Parameter Type Description Default functionName String The name of the Excel function NORMSINV <p>The identifier for this plugin is <code>Excel_NORMSINV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#not","title":"Not","text":"<p>Excel NOT(logical_value): Reverses the logical value. Logical_value is any value to be reversed.</p> Parameter Type Description Default functionName String The name of the Excel function NOT <p>The identifier for this plugin is <code>Excel_NOT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#nper","title":"Nper","text":"<p>Excel NPER(rate; PMT; PV; FV; type): Returns the number of periods for an investment based on periodic, constant payments and a constant interest rate. Rate is the periodic interest rate. PMT is the constant annuity paid in each period. PV is the present value (cash value) in a sequence of payments. FV (optional) is the future value, which is reached at the end of the last period. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function NPER <p>The identifier for this plugin is <code>Excel_NPER</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#npv","title":"Npv","text":"<p>Excel NPV(Rate; value_1; value_2; \u2026 value_30): Returns the net present value of an investment based on a series of periodic cash flows and a discount rate. Rate is the discount rate for a period. Value_1; value_2;\u2026 value_30 are values representing deposits or withdrawals.</p> Parameter Type Description Default functionName String The name of the Excel function NPV <p>The identifier for this plugin is <code>Excel_NPV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#odd","title":"Odd","text":"<p>Excel ODD(number): Rounds the given number up to the nearest odd integer.</p> Parameter Type Description Default functionName String The name of the Excel function ODD <p>The identifier for this plugin is <code>Excel_ODD</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#or","title":"Or","text":"<p>Excel OR(logical_value_1; logical_value_2; \u2026logical_value_30): Returns TRUE if at least one argument is TRUE. Returns the value FALSE if all the arguments have the logical value FALSE. Logical_value_1; logical_value_2; \u2026logical_value_30 are conditions to be checked. All conditions can be either TRUE or FALSE. If a range is entered as a parameter, the function uses the value from the range that is in the current column or row.</p> Parameter Type Description Default functionName String The name of the Excel function OR <p>The identifier for this plugin is <code>Excel_OR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pearson","title":"Pearson","text":"<p>Excel PEARSON(data_1; data_2): Returns the Pearson product moment correlation coefficient r. Data_1 is the array of the first data set. Data_2 is the array of the second data set.</p> Parameter Type Description Default functionName String The name of the Excel function PEARSON <p>The identifier for this plugin is <code>Excel_PEARSON</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#percentile","title":"Percentile","text":"<p>Excel PERCENTILE(data; alpha): Returns the alpha-percentile of data values in an array. Data is the array of data. Alpha is the percentage of the scale between 0 and 1.</p> Parameter Type Description Default functionName String The name of the Excel function PERCENTILE <p>The identifier for this plugin is <code>Excel_PERCENTILE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#percentrank","title":"Percentrank","text":"<p>Excel PERCENTRANK(data; value): Returns the percentage rank (percentile) of the given value in a sample. Data is the array of data in the sample.</p> Parameter Type Description Default functionName String The name of the Excel function PERCENTRANK <p>The identifier for this plugin is <code>Excel_PERCENTRANK</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pi","title":"Pi","text":"<p>Excel PI(): Returns the value of PI to fourteen decimal places.</p> Parameter Type Description Default functionName String The name of the Excel function PI <p>The identifier for this plugin is <code>Excel_PI</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pmt","title":"Pmt","text":"<p>Excel PMT(rate; NPER; PV; FV; type): Returns the periodic payment for an annuity with constant interest rates. Rate is the periodic interest rate. NPER is the number of periods in which annuity is paid. PV is the present value (cash value) in a sequence of payments. FV (optional) is the desired value (future value) to be reached at the end of the periodic payments. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function PMT <p>The identifier for this plugin is <code>Excel_PMT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#poisson","title":"Poisson","text":"<p>Excel POISSON(number; mean; C): Returns the Poisson distribution for the given Number. Mean is the middle value of the Poisson distribution. C = 0 calculates the density function, and C = 1 calculates the distribution.</p> Parameter Type Description Default functionName String The name of the Excel function POISSON <p>The identifier for this plugin is <code>Excel_POISSON</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#power","title":"Power","text":"<p>Excel POWER(base; power): Returns the result of a number raised to a power. Base is the number that is to be raised to the given power. Power is the exponent by which the base is to be raised.</p> Parameter Type Description Default functionName String The name of the Excel function POWER <p>The identifier for this plugin is <code>Excel_POWER</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ppmt","title":"Ppmt","text":"<p>Excel PPMT(rate; period; NPER; PV; FV; type): Returns for a given period the payment on the principal for an investment that is based on periodic and constant payments and a constant interest rate. Rate is the periodic interest rate. Period is the amortization period. NPER is the total number of periods during which annuity is paid. PV is the present value in the sequence of payments. FV (optional) is the desired (future) value. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function PPMT <p>The identifier for this plugin is <code>Excel_PPMT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#product","title":"Product","text":"<p>Excel PRODUCT(number 1 to 30): Multiplies all the numbers given as arguments and returns the product. Number 1 to number 30 are up to 30 arguments whose product is to be calculated, separated by semi-colons.</p> Parameter Type Description Default functionName String The name of the Excel function PRODUCT <p>The identifier for this plugin is <code>Excel_PRODUCT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#proper","title":"Proper","text":"<p>Excel PROPER(text): Capitalizes the first letter in all words of a text string. Text is the text to be converted.</p> Parameter Type Description Default functionName String The name of the Excel function PROPER <p>The identifier for this plugin is <code>Excel_PROPER</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pv","title":"Pv","text":"<p>Excel PV(rate; NPER; PMT; FV; type): Returns the present value of an investment resulting from a series of regular payments. Rate defines the interest rate per period. NPER is the total number of payment periods. PMT is the regular payment made per period. FV (optional) defines the future value remaining after the final installment has been made. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function PV <p>The identifier for this plugin is <code>Excel_PV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#radians","title":"Radians","text":"<p>Excel RADIANS(number): Converts the given number in degrees to radians.</p> Parameter Type Description Default functionName String The name of the Excel function RADIANS <p>The identifier for this plugin is <code>Excel_RADIANS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rand","title":"Rand","text":"<p>Excel RAND(): Returns a random number between 0 and 1.</p> Parameter Type Description Default functionName String The name of the Excel function RAND <p>The identifier for this plugin is <code>Excel_RAND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rank","title":"Rank","text":"<p>Excel RANK(value; data; type): Returns the rank of the given Value in a sample. Data is the array or range of data in the sample. Type (optional) is the sequence order, either ascending (0) or descending (1).</p> Parameter Type Description Default functionName String The name of the Excel function RANK <p>The identifier for this plugin is <code>Excel_RANK</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rate","title":"Rate","text":"<p>Excel RATE(NPER; PMT; PV; FV; type; guess): Returns the constant interest rate per period of an annuity. NPER is the total number of periods, during which payments are made (payment period). PMT is the constant payment (annuity) paid during each period. PV is the cash value in the sequence of payments. FV (optional) is the future value, which is reached at the end of the periodic payments. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period. Guess (optional) determines the estimated value of the interest with iterative calculation.</p> Parameter Type Description Default functionName String The name of the Excel function RATE <p>The identifier for this plugin is <code>Excel_RATE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replace","title":"Replace","text":"<p>Excel REPLACE(text; position; length; new_text): Replaces part of a text string with a different text string. This function can be used to replace both characters and numbers (which are automatically converted to text). The result of the function is always displayed as text. To perform further calculations with a number which has been replaced by text, convert it back to a number using the VALUE function. Any text containing numbers must be enclosed in quotation marks so it is not interpreted as a number and automatically converted to text. Text is text of which a part will be replaced. Position is the position within the text where the replacement will begin. Length is the number of characters in text to be replaced. New_text is the text which replaces text..</p> Parameter Type Description Default functionName String The name of the Excel function REPLACE <p>The identifier for this plugin is <code>Excel_REPLACE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rept","title":"Rept","text":"<p>Excel REPT(text; number): Repeats a character string by the given number of copies. Text is the text to be repeated. Number is the number of repetitions. The result can be a maximum of 255 characters.</p> Parameter Type Description Default functionName String The name of the Excel function REPT <p>The identifier for this plugin is <code>Excel_REPT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#right","title":"Right","text":"<p>Excel RIGHT(text; number): Defines the last character or characters in a text string. Text is the text of which the right part is to be determined. Number (optional) is the number of characters from the right part of the text.</p> Parameter Type Description Default functionName String The name of the Excel function RIGHT <p>The identifier for this plugin is <code>Excel_RIGHT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#roman","title":"Roman","text":"<p>Excel ROMAN(number; mode): Converts a number into a Roman numeral. The value range must be between 0 and 3999; the modes can be integers from 0 to 4. Number is the number that is to be converted into a Roman numeral. Mode (optional) indicates the degree of simplification. The higher the value, the greater is the simplification of the Roman numeral.</p> Parameter Type Description Default functionName String The name of the Excel function ROMAN <p>The identifier for this plugin is <code>Excel_ROMAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#round","title":"Round","text":"<p>Excel ROUND(number; count): Rounds the given number to a certain number of decimal places according to valid mathematical criteria. Count (optional) is the number of the places to which the value is to be rounded. If the count parameter is negative, only the whole number portion is rounded. It is rounded to the place indicated by the count.</p> Parameter Type Description Default functionName String The name of the Excel function ROUND <p>The identifier for this plugin is <code>Excel_ROUND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rounddown","title":"Rounddown","text":"<p>Excel ROUNDDOWN(number; count): Rounds the given number. Count (optional) is the number of digits to be rounded down to. If the count parameter is negative, only the whole number portion is rounded. It is rounded to the place indicated by the count.</p> Parameter Type Description Default functionName String The name of the Excel function ROUNDDOWN <p>The identifier for this plugin is <code>Excel_ROUNDDOWN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#roundup","title":"Roundup","text":"<p>Excel ROUNDUP(number; count): Rounds the given number up. Count (optional) is the number of digits to which rounding up is to be done. If the count parameter is negative, only the whole number portion is rounded. It is rounded to the place indicated by the count.</p> Parameter Type Description Default functionName String The name of the Excel function ROUNDUP <p>The identifier for this plugin is <code>Excel_ROUNDUP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#search","title":"Search","text":"<p>Excel SEARCH(find_text; text; position): Returns the position of a text segment within a character string. The start of the search can be set as an option. The search text can be a number or any sequence of characters. The search is not case-sensitive. The search supports regular expressions. Find_text is the text to be searched for. Text is the text where the search will take place. Position (optional) is the position in the text where the search is to start.</p> Parameter Type Description Default functionName String The name of the Excel function SEARCH <p>The identifier for this plugin is <code>Excel_SEARCH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sign","title":"Sign","text":"<p>Excel SIGN(number): Returns the sign of the given number. The function returns the result 1 for a positive sign, \u0096 1 for a negative sign, and 0 for zero.</p> Parameter Type Description Default functionName String The name of the Excel function SIGN <p>The identifier for this plugin is <code>Excel_SIGN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sin","title":"Sin","text":"<p>Excel SIN(number): Returns the sine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function SIN <p>The identifier for this plugin is <code>Excel_SIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sinh","title":"Sinh","text":"<p>Excel SINH(number): Returns the hyperbolic sine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function SINH <p>The identifier for this plugin is <code>Excel_SINH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#slope","title":"Slope","text":"<p>Excel SLOPE(data_Y; data_X): Returns the slope of the linear regression line. Data_Y is the array or matrix of Y data. Data_X is the array or matrix of X data.</p> Parameter Type Description Default functionName String The name of the Excel function SLOPE <p>The identifier for this plugin is <code>Excel_SLOPE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#small","title":"Small","text":"<p>Excel SMALL(data; rank_c): Returns the Rank_c-th smallest value in a data set. Data is the cell range of data. Rank_c is the rank of the value (2nd smallest, 3rd smallest, etc.) written as an integer.</p> Parameter Type Description Default functionName String The name of the Excel function SMALL <p>The identifier for this plugin is <code>Excel_SMALL</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sqrt","title":"Sqrt","text":"<p>Excel SQRT(number): Returns the positive square root of the given number. The value of the number must be positive.</p> Parameter Type Description Default functionName String The name of the Excel function SQRT <p>The identifier for this plugin is <code>Excel_SQRT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#standardize","title":"Standardize","text":"<p>Excel STANDARDIZE(number; mean; STDEV): Converts a random variable to a normalized value. Number is the value to be standardized. Mean is the arithmetic mean of the distribution. STDEV is the standard deviation of the distribution.</p> Parameter Type Description Default functionName String The name of the Excel function STANDARDIZE <p>The identifier for this plugin is <code>Excel_STANDARDIZE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stdev","title":"Stdev","text":"<p>Excel STDEV(number_1; number_2; \u2026 number_30): Estimates the standard deviation based on a sample. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample based on an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function STDEV <p>The identifier for this plugin is <code>Excel_STDEV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stdeva","title":"Stdeva","text":"<p>Excel STDEVA(value_1; value_2; \u2026 value_30): Calculates the standard deviation of an estimation based on a sample. Value_1; value_2; \u2026 value_30 are values or ranges representing a sample derived from an entire population. Text has the value 0.</p> Parameter Type Description Default functionName String The name of the Excel function STDEVA <p>The identifier for this plugin is <code>Excel_STDEVA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stdevp","title":"Stdevp","text":"<p>Excel STDEVP(number_1; number_2; \u2026 number_30): Calculates the standard deviation based on the entire population. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample based on an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function STDEVP <p>The identifier for this plugin is <code>Excel_STDEVP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stdevpa","title":"Stdevpa","text":"<p>Excel STDEVPA(value_1; value_2; \u2026 value_30): Calculates the standard deviation based on the entire population. Value_1; value_2; \u2026 value_30 are values or ranges representing a sample derived from an entire population. Text has the value 0.</p> Parameter Type Description Default functionName String The name of the Excel function STDEVPA <p>The identifier for this plugin is <code>Excel_STDEVPA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substitute","title":"Substitute","text":"<p>Excel SUBSTITUTE(text; search_text; new text; occurrence): Substitutes new text for old text in a string. Text is the text in which text segments are to be exchanged. Search_text is the text segment that is to be replaced (a number of times). New text is the text that is to replace the text segment. Occurrence (optional) indicates how many occurrences of the search text are to be replaced. If this parameter is missing, the search text is replaced throughout.</p> Parameter Type Description Default functionName String The name of the Excel function SUBSTITUTE <p>The identifier for this plugin is <code>Excel_SUBSTITUTE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sum","title":"Sum","text":"<p>Excel SUM(number_1; number_2; \u2026 number_30): Adds all the numbers in a range of cells. Number_1; number_2;\u2026 number_30 are up to 30 arguments whose sum is to be calculated. You can also enter a range using cell references.</p> Parameter Type Description Default functionName String The name of the Excel function SUM <p>The identifier for this plugin is <code>Excel_SUM</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumproduct","title":"Sumproduct","text":"<p>Excel SUMPRODUCT(array 1; array 2; \u2026array 30): Multiplies corresponding elements in the given arrays, and returns the sum of those products. Array 1; array 2;\u2026array 30 are arrays whose corresponding elements are to be multiplied. At least one array must be part of the argument list. If only one array is given, all array elements are summed.</p> Parameter Type Description Default functionName String The name of the Excel function SUMPRODUCT <p>The identifier for this plugin is <code>Excel_SUMPRODUCT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumsq","title":"Sumsq","text":"<p>Excel SUMSQ(number_1; number_2; \u2026 number_30): Calculates the sum of the squares of numbers (totaling up of the squares of the arguments) Number_1; number_2;\u2026 number_30 are up to 30 arguments, the sum of whose squares is to be calculated.</p> Parameter Type Description Default functionName String The name of the Excel function SUMSQ <p>The identifier for this plugin is <code>Excel_SUMSQ</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumx2my2","title":"Sumx2my2","text":"<p>Excel SUMX2MY2(array_X; array_Y): Returns the sum of the difference of squares of corresponding values in two arrays. Array_X is the first array whose elements are to be squared and added. Array_Y is the second array whose elements are to be squared and subtracted.</p> Parameter Type Description Default functionName String The name of the Excel function SUMX2MY2 <p>The identifier for this plugin is <code>Excel_SUMX2MY2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumx2py2","title":"Sumx2py2","text":"<p>Excel SUMX2PY2(array_X; array_Y): Returns the sum of the sum of squares of corresponding values in two arrays. Array_X is the first array whose arguments are to be squared and added. Array_Y is the second array, whose elements are to be added and squared.</p> Parameter Type Description Default functionName String The name of the Excel function SUMX2PY2 <p>The identifier for this plugin is <code>Excel_SUMX2PY2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumxmy2","title":"Sumxmy2","text":"<p>Excel SUMXMY2(array_X; array_Y): Adds the squares of the variance between corresponding values in two arrays. Array_X is the first array whose elements are to be subtracted and squared. Array_Y is the second array, whose elements are to be subtracted and squared.</p> Parameter Type Description Default functionName String The name of the Excel function SUMXMY2 <p>The identifier for this plugin is <code>Excel_SUMXMY2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tan","title":"Tan","text":"<p>Excel TAN(number): Returns the tangent of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function TAN <p>The identifier for this plugin is <code>Excel_TAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tanh","title":"Tanh","text":"<p>Excel TANH(number): Returns the hyperbolic tangent of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function TANH <p>The identifier for this plugin is <code>Excel_TANH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tdist","title":"Tdist","text":"<p>Excel TDIST(number; degrees_freedom; mode): Returns the t-distribution for the given Number. Degrees_freedom is the number of degrees of freedom for the t-distribution. Mode = 1 returns the one-tailed test, Mode = 2 returns the two-tailed test.</p> Parameter Type Description Default functionName String The name of the Excel function TDIST <p>The identifier for this plugin is <code>Excel_TDIST</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#true","title":"True","text":"<p>Excel TRUE(): Sets the logical value to TRUE. The TRUE() function does not require any arguments.</p> Parameter Type Description Default functionName String The name of the Excel function TRUE <p>The identifier for this plugin is <code>Excel_TRUE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#trunc","title":"Trunc","text":"<p>Excel TRUNC(number; count): Truncates a number to an integer by removing the fractional part of the number according to the precision specified in Tools &gt; Options &gt; OpenOffice.org Calc &gt; Calculate. Number is the number whose decimal places are to be cut off. Count is the number of decimal places which are not cut off.</p> Parameter Type Description Default functionName String The name of the Excel function TRUNC <p>The identifier for this plugin is <code>Excel_TRUNC</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#var","title":"Var","text":"<p>Excel VAR(number_1; number_2; \u2026 number_30): Estimates the variance based on a sample. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample based on an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function VAR <p>The identifier for this plugin is <code>Excel_VAR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#vara","title":"Vara","text":"<p>Excel VARA(value_1; value_2; \u2026 value_30): Estimates a variance based on a sample. The value of text is 0. Value_1; value_2; \u2026 value_30 are values or ranges representing a sample derived from an entire population. Text has the value 0.</p> Parameter Type Description Default functionName String The name of the Excel function VARA <p>The identifier for this plugin is <code>Excel_VARA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#varp","title":"Varp","text":"<p>Excel VARP(Number_1; number_2; \u2026 number_30): Calculates a variance based on the entire population. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function VARP <p>The identifier for this plugin is <code>Excel_VARP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#varpa","title":"Varpa","text":"<p>Excel VARPA(value_1; value_2; .. .value_30): Calculates the variance based on the entire population. The value of text is 0. Value_1; value_2; \u2026 value_30 are values or ranges representing an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function VARPA <p>The identifier for this plugin is <code>Excel_VARPA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#extract","title":"Extract","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#regex-extract","title":"Regex extract","text":"<p>Extracts occurrences of a regex \u201cregex\u201d in a string. If there is at least one capture group, it will return the string of the first capture group instead.</p> Parameter Type Description Default regex String Regular expression no default extractAll boolean If true, all matches are extracted. If false, only the first match is extracted. false <p>The identifier for this plugin is <code>regexExtract</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.extraction</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_24","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-the-first-match","title":"returns the first match:","text":"<ul> <li>Parameters</li> <li> <p>regex: <code>[a-z]{2,4}123</code></p> </li> <li> <p>Input values:   1. <code>[afe123_abc123]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[afe123]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-all-matches-if-extractall-true","title":"returns all matches, if extractAll = true:","text":"<ul> <li>Parameters</li> <li>regex: <code>[a-z]{2,4}123</code></li> <li> <p>extractAll: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[afe123_abc123]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[afe123, abc123]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-an-empty-list-if-nothing-matches","title":"returns an empty list if nothing matches:","text":"<ul> <li>Parameters</li> <li> <p>regex: <code>^[a-z]{2,4}123</code></p> </li> <li> <p>Input values:   1. <code>[abcdef123]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#returns-the-match-of-the-first-capture-group-that-matches","title":"returns the match of the first capture group that matches:","text":"<ul> <li>Parameters</li> <li> <p>regex: <code>^([a-z]{2,4})123([a-z]+)</code></p> </li> <li> <p>Input values:   1. <code>[abcd123xyz]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[abcd]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_7","title":"Example 5:","text":"<ul> <li>Parameters</li> <li> <p>regex: <code>\"bedeutungen\"\\s*:\\s*\\[\\s*(?:\"([^\"]*)\"(?:\\s*,\\s*\"([^\"]*)\")*)*\\s*\\]</code></p> </li> <li> <p>Input values:   1. <code>[\"bedeutungen\" : [ ]]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#filter","title":"Filter","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#filter-by-length","title":"Filter by length","text":"<p>Removes all strings that are shorter than \u2018min\u2019 characters and longer than \u2018max\u2019 characters.</p> Parameter Type Description Default min int No description 0 max int No description 2147483647 <p>The identifier for this plugin is <code>filterByLength</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#filter-by-regex","title":"Filter by regex","text":"<p>Removes all strings that do NOT match a regex. If \u2018negate\u2019 is true, only strings will be removed that match the regex.</p> Parameter Type Description Default regex String No description no default negate boolean No description false <p>The identifier for this plugin is <code>filterByRegex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-empty-values","title":"Remove empty values","text":"<p>Removes empty values.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeEmptyValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_25","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_14","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[value1, , value2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value1, value2]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_14","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[, ]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-stopwords-remote-stopword-list","title":"Remove stopwords (remote stopword list)","text":"<p>Removes stopwords from all values. The stopword list is retrieved via a http connection (e.g. https://sites.google.com/site/kevinbouge/stopwords-lists/stopwords_de.txt). Each line in the stopword list contains a stopword. The separator defines a regex that is used for detecting words.</p> Parameter Type Description Default stopWordListUrl String No description no default separator String No description [\\s-]+ <p>The identifier for this plugin is <code>removeRemoteStopwords</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-stopwords","title":"Remove stopwords","text":"<p>Removes stopwords from all values. Each line in the stopword list contains a stopword. The separator defines a regex that is used for detecting words.</p> Parameter Type Description Default stopwordList Resource No description no default separator String No description [\\s-]+ <p>The identifier for this plugin is <code>removeStopwords</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-values","title":"Remove values","text":"<p>Removes values that contain words from a blacklist. The blacklist values are separated with commas.</p> Parameter Type Description Default blacklist String No description no default <p>The identifier for this plugin is <code>removeValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geo","title":"Geo","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#retrieve-coordinates","title":"Retrieve coordinates","text":"<p>Retrieves geographic coordinates using Nominatim.</p> Parameter Type Description Default additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>RetrieveCoordinates</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#retrieve-latitude","title":"Retrieve latitude","text":"<p>Retrieves geographic coordinates using Nominatim and returns the latitude.</p> Parameter Type Description Default additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>RetrieveLatitude</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#retrieve-longitude","title":"Retrieve longitude","text":"<p>Retrieves geographic coordinates using Nominatim and returns the longitude.</p> Parameter Type Description Default additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>RetrieveLongitude</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#linguistic","title":"Linguistic","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#nysiis","title":"NYSIIS","text":"<p>NYSIIS phonetic encoding.</p> Parameter Type Description Default refined boolean No description true <p>The identifier for this plugin is <code>NYSIIS</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#metaphone","title":"Metaphone","text":"<p>Metaphone phonetic encoding.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>metaphone</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize-chars","title":"Normalize chars","text":"<p>Replaces diacritical characters with non-diacritical ones (eg, \u00f6 -&gt; o), plus some specialities like transforming \u00e6 -&gt; ae, \u00df -&gt; ss.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>normalizeChars</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#soundex","title":"Soundex","text":"<p>Soundex algorithm.</p> Parameter Type Description Default refined boolean No description true <p>The identifier for this plugin is <code>soundex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stem","title":"Stem","text":"<p>Stems a string using the Porter Stemmer.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>stem</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#metadata","title":"Metadata","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#file-hash","title":"File hash","text":"<p>Calculates the hash sum of a file. The hash sum is cached so that subsequent calls to this operator are fast. Note that initially and every time the specified resource has been updated, this operator might take a long time (depending on the file size). This operator supports using different hash algorithms from the Secure Hash Algorithms family (SHA, e.g. SHA256) and two algorithms from the Message-Digest Algorithm family (MD2 / MD5). Please be aware that some of these algorithms are not secure regarding collision- and other attacks. Note: This transform operator ignores any inputs.</p> Parameter Type Description Default file ResourceOption File for which the hash sum will be calculated. If left empty, the file of the input dataset is used. algorithm String The hash algorithm to be used. SHA256 <p>The identifier for this plugin is <code>fileHash</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.metadata</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#input-file-attributes","title":"Input file attributes","text":"<p>Retrieves a metadata attribute from the input file (such as the file name).</p> Parameter Type Description Default attribute Enum File attribute to be retrieved from the input dataset. name <p>The identifier for this plugin is <code>inputFileAttributes</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.metadata</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#input-task-attributes","title":"Input task attributes","text":"<p>Retrieves individual attributes from the input task (such as the modified date) or the entire task as JSON.</p> Parameter Type Description Default path String Path to retrieve from the JSON, such as \u2018metadata/modified\u2019. If left empty, the entire JSON will be returned. empty string <p>The identifier for this plugin is <code>inputTaskAttributes</code>.</p> <p>It can be found in the package <code>org.silkframework.serialization.json.transformer</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize","title":"Normalize","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-non-alphabetic-characters","title":"Strip non-alphabetic characters","text":"<p>Strips all non-alphabetic characters from a string. Spaces are retained.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>alphaReduce</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#camel-case","title":"Camel case","text":"<p>Converts a string to camel case. Upper camel case is the default, lower camel case can be chosen.</p> Parameter Type Description Default isDromedary boolean If true, lower camel case (aka. dromedary case) is used, otherwise upper camel case is used. false <p>The identifier for this plugin is <code>camelCase</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_26","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#a-sentence-with-several-words-is-converted-to-a-single-word-written-in-uppercamelcase","title":"A sentence with several words is converted to a single word written in UpperCamelCase:","text":"<ul> <li>Parameters</li> <li> <p>isDromedary: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[hello world]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[HelloWorld]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#a-sentence-with-several-words-is-converted-to-a-single-word-written-in-lowercamelcase","title":"A sentence with several words is converted to a single word written in lowerCamelCase:","text":"<ul> <li>Parameters</li> <li> <p>isDromedary: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[hello world]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[helloWorld]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#a-single-lowercase-letter-is-converted-to-uppercamelcase-ie-capitalized","title":"A single lowercase letter is converted to UpperCamelCase, i.e. capitalized:","text":"<ul> <li>Parameters</li> <li> <p>isDromedary: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[h]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[H]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#a-single-lowercase-letter-is-converted-to-lowercamelcase-aka-dromedary-case-ie-uncapitalized","title":"A single lowercase letter is converted to lowerCamelCase (aka. dromedary case), i.e. uncapitalized:","text":"<ul> <li>Parameters</li> <li> <p>isDromedary: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[h]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[h]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#an-empty-space-is-removed-the-dromedarylower-case-is-irrelevant-here","title":"An empty space is removed. The dromedary/lower case is irrelevant here:","text":"<ul> <li>Parameters</li> <li> <p>isDromedary: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[ ]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#an-empty-space-is-removed-the-upper-case-is-irrelevant-here","title":"An empty space is removed. The upper case is irrelevant here:","text":"<ul> <li>Parameters</li> <li> <p>isDromedary: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[ ]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#capitalize","title":"Capitalize","text":"<p>Capitalizes the string i.e. converts the first character to upper case. If \u2018allWords\u2019 is set to true, all words are capitalized and not only the first character.</p> Parameter Type Description Default allWords boolean No description false <p>The identifier for this plugin is <code>capitalize</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_27","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_15","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>allWords: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[capitalize me]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Capitalize me]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_15","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>allWords: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[capitalize me]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Capitalize Me]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#extract-physical-quantity","title":"Extract physical quantity","text":"<p>Extracts physical quantities, such as length or weight values. Values are expected of the form \u2018{Number}{UnitPrefix}{Symbol}\u2019 and are converted to the base unit.</p> <p>Example:</p> <ul> <li>Given a value \u201810km, 3mg\u2019.</li> <li>If the symbol parameter is set to \u2018m\u2019, the extracted value is 10000.</li> <li>If the symbol parameter is set to \u2018g\u2019, the extracted value is 0.001.</li> </ul> Parameter Type Description Default symbol String The symbol of the dimension, e.g., \u2018m\u2019 for meter. empty string numberFormat String The IETF BCP 47 language tag, e.g. \u2018en\u2019. en filter String Only extracts from values that contain the given regex (case-insensitive). empty string index int If there are multiple matches, retrieve the value with the given index (zero-based). 0 <p>The identifier for this plugin is <code>extractPhysicalQuantity</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean-html_1","title":"Clean HTML","text":"<p>Cleans HTML using a tag white list and allows selection of HTML sections with XPath or CSS selector expressions.        If the tag or attribute white lists are left empty default white lists will be used (this behaviour can be changed).        To remove all HTML markup and retain text, keep the defaults and turn off the \u201cDefault tags and attributes\u201d toggle.        The operator takes two inputs: the page HTML and (optional) the page Url which may be needed to resolve relative links in the page HTML.</p> Parameter Type Description Default tagWhiteList StringIterableParameter Tags to keep in the cleaned output. attributeWhiteList StringIterableParameter Attributes to keep in the cleaned output. selectors StringIterableParameter CSS or XPath queries for selection of content. CSS selectors can be pipe separated for non-sequential execution. method Enum Selects use of XPath or CSS selectors. xPath defaultTagsAndAttributes boolean Use defaults for empty tag and attribute whitelists.\\If the attribute while list is empty, it will default to: \u201cclass\u201d, \u201cid\u201d, \u201chref\u201d, \u201csrc\u201d\\If the tag while list is empty, it will default to: \u201ca\u201d, \u201cb\u201d, \u201cblockquote\u201d, \u201cbr\u201d, \u201ccaption\u201d, \u201ccite\u201d, \u201ccode\u201d, \u201ccol\u201d, \u201ccolgroup\u201d, \u201cdd\u201d, \u201cdiv\u201d, \u201cdl\u201d, \u201cdt\u201d, \u201cem\u201d, \u201ch1\u201d, \u201ch2\u201d, \u201ch3\u201d, \u201ch4\u201d, \u201ch5\u201d, \u201ch6\u201d,\u201di\u201d, \u201cimg\u201d, \u201cli\u201d, \u201col\u201d, \u201cp\u201d, \u201cpre\u201d, \u201cq\u201d, \u201csmall\u201d, \u201cspan\u201d, \u201cstrike\u201d, \u201cstrong\u201d,\u201dsub\u201d, \u201csup\u201d, \u201ctable\u201d, \u201ctbody\u201d, \u201ctd\u201d, \u201ctfoot\u201d, \u201cth\u201d, \u201cthead\u201d, \u201ctr\u201d, \u201cu\u201d, \u201cul\u201d. true <p>The identifier for this plugin is <code>htmlCleaner</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.html</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#lower-case","title":"Lower case","text":"<p>Converts a string to lower case.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>lowerCase</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_28","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#transforms-all-values-to-lower-case","title":"Transforms all values to lower case:","text":"<ul> <li> <p>Input values:   1. <code>[JoHN, LeNA]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[john, lena]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-blanks","title":"Remove blanks","text":"<p>Remove whitespace from a string.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeBlanks</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-duplicates","title":"Remove duplicates","text":"<p>Removes duplicated values, making a value sequence distinct.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeDuplicates</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-parentheses","title":"Remove parentheses","text":"<p>Remove all parentheses including their content, e.g., transforms \u2018Berlin (City)\u2019 -&gt; \u2018Berlin\u2019.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeParentheses</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-special-chars","title":"Remove special chars","text":"<p>Remove special characters (including punctuation) from a string.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeSpecialChars</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sort-words","title":"Sort words","text":"<p>Sorts all words in each value lexicographically.</p> Parameter Type Description Default splitRegex String The regular expression used to split values into words. \\s+ glue String Separator to be inserted between sorted words. <p>The identifier for this plugin is <code>sortWords</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_29","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_16","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_16","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[c a b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a b c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_13","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[Hans Hansa    Hamburg, M\u00fcnchen Marburg]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Hamburg Hans Hansa, Marburg M\u00fcnchen]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-uri-prefix","title":"Strip URI prefix","text":"<p>Strips the URI prefix and decodes the remainder. Leaves values unchanged which are not a valid URI.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>stripUriPrefix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_30","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_17","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/some/path/to/value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_17","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[urn:scheme:value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_14","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/some/path/to/encoded%20v%C3%A4lue]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[encoded v\u00e4lue]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_9","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#trim","title":"Trim","text":"<p>Remove leading and trailing whitespaces.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>trim</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#upper-case","title":"Upper case","text":"<p>Converts a string to upper case.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>upperCase</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fix-uri","title":"Fix URI","text":"<p>Generates valid absolute URIs from the given values. Already valid absolute URIs are left untouched.</p> Parameter Type Description Default uriPrefix String No description urn:url-encoded-value: <p>The identifier for this plugin is <code>uriFix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_31","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#non-absolute-uris-are-prefixed-with-the-configured-uri-prefix","title":"Non-absolute URIs are prefixed with the configured URI prefix:","text":"<ul> <li> <p>Input values:   1. <code>[ab]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[urn:url-encoded-value:ab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#uri-reserved-characters-are-encoded","title":"URI reserved characters are encoded:","text":"<ul> <li> <p>Input values:   1. <code>[a&amp;b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[urn:url-encoded-value:a%26b]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#valid-absolute-uris-are-forwarded-unchanged","title":"Valid absolute URIs are forwarded unchanged:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/some/path]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[http://example.org/some/path]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#query-parameters-and-fragments-are-left-unchanged","title":"Query parameters and fragments are left unchanged:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/path?query=some+stuff#hashtag]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[http://example.org/path?query=some+stuff#hashtag]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#valid-urns-are-forwarded-unchanged","title":"Valid URNs are forwarded unchanged:","text":"<ul> <li> <p>Input values:   1. <code>[urn:valid:uri]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[urn:valid:uri]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#special-characters-are-encoded","title":"Special characters are encoded:","text":"<ul> <li> <p>Input values:   1. <code>[http://www.broken domain.com/broken weird path \u00e4\u00f6\u00fc/nice/path/andNowSomeFragment#fragment\u00e4\u00f6\u00fc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[http://www.broken%20domain.com/broken%20weird%20path%20%C3%A4%C3%B6%C3%BC/nice/path/andNowSomeFragment#fragment%C3%A4%C3%B6%C3%BC]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#hash-signs-are-only-encoded-if-they-dont-denote-a-fragment","title":"Hash signs are only encoded if they don\u2019t denote a fragment:","text":"<ul> <li> <p>Input values:   1. <code>[http://domain/##path#]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[http://domain/#%23path%23]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#invalid-uris-are-fully-encoded","title":"Invalid URIs are fully encoded:","text":"<ul> <li> <p>Input values:   1. <code>[http : invalid URI]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[urn:url-encoded-value:http+%3A+invalid+URI]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#leading-and-trailing-spaces-are-removed","title":"Leading and trailing spaces are removed:","text":"<ul> <li> <p>Input values:   1. <code>[  http://domain.com/[squareBrackets] ]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[http://domain.com/%5BsquareBrackets%5D]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-10_2","title":"Example 10:","text":"<ul> <li> <p>Input values:   1. <code>[100%]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[urn:url-encoded-value:100%25]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#encode-url","title":"Encode URL","text":"<p>URL encodes the string.</p> Parameter Type Description Default encoding String The character encoding. UTF-8 <p>The identifier for this plugin is <code>urlEncode</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_32","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_18","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[ab]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_18","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[a&amp;b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a%26b]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_15","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/some/path]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[http%3A%2F%2Fexample.org%2Fsome%2Fpath]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric_1","title":"Numeric","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize-physical-quantity","title":"Normalize physical quantity","text":"<p>Normalizes physical quantities. Can either convert to a configured unit or to SI base units. For instance for lengths, values will be converted to metres if no target unit is configured. Will output the pure numeric value without the unit. If one input is provided, the physical quantities are parsed from the provided strings of the form \u201c1 km\u201d. If two inputs are provided, the numeric values are parsed from the first input and the units are parsed from the second inputs.</p> Parameter Type Description Default targetUnit String Target unit. Can be left empty to convert to the respective SI base units. empty string numberFormat String The IETF BCP 47 language tag, e.g., \u2018en\u2019. en <p>The identifier for this plugin is <code>PhysicalQuantitiesNormalizer</code>.</p> <p>It can be found in the package <code>com.eccenca.di.measure</code>.</p> <p>SI units and common derived units are supported. The following section lists all supported units. By default, all quantities are normalized to their base unit. For instance, lengths will be normalized to metres.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#supported-units_1","title":"Supported units","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#time_1","title":"Time","text":"<p>Time is expressed in seconds (symbol: <code>s</code>). The following alternative symbols are supported: * <code>mo_s</code>: day29.53059 * <code>mo_g</code>: year/12.0 * <code>a</code>: day365.25 * <code>min</code>: min * <code>a_g</code>: year * <code>mo</code>: (day365.25)/12.0 * <code>mo_j</code>: (day365.25)/12.0 * <code>a_j</code>: day365.25 * <code>h</code>: h * <code>a_t</code>: day365.24219 * <code>d</code>: day</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#length_1","title":"Length","text":"<p>Length is expressed in metres (symbol: <code>m</code>). The following alternative symbols are supported: * <code>in</code>: c(cm254.0) * <code>nmi</code>: m1852.0 * <code>Ao</code>: dnm * <code>mil</code>: m(c(cm254.0)) * <code>yd</code>: ((c(cm254.0))12.0)3.0 * <code>AU</code>: m1.49597871E11 * <code>ft</code>: (c(cm254.0))12.0 * <code>pc</code>: m3.085678E16 * <code>fth</code>: ((c(cm254.0))12.0)6.0 * <code>mi</code>: ((c(cm254.0))12.0)5280.0 * <code>hd</code>: (c(cm254.0))4.0</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mass_1","title":"Mass","text":"<p>Mass is expressed in kilograms (symbol: <code>kg</code>). The following alternative symbols are supported: * <code>lb</code>: lb * <code>ston</code>: hlb20.0 * <code>t</code>: Mg * <code>stone</code>: lb14.0 * <code>u</code>: AMU * <code>gr</code>: (mg6479891.0)/100000.0 * <code>lcwt</code>: lb112.0 * <code>oz</code>: oz * <code>g</code>: g * <code>scwt</code>: hlb * <code>dr</code>: oz/16.0 * <code>lton</code>: (lb112.0)20.0</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#electric-current_1","title":"Electric current","text":"<p>Electric current is expressed in amperes (symbol: <code>A</code>). The following alternative symbols are supported: * <code>Bi</code>: daA * <code>Gb</code>: cm\u00b7(A/m)*250.0/[one?]</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#temperature_1","title":"Temperature","text":"<p>Temperature is expressed in kelvins (symbol: <code>K</code>). The following alternative symbols are supported: * <code>Cel</code>: \u2103</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#amount-of-substance_1","title":"Amount of substance","text":"<p>Amount of substance is expressed in moles (symbol: <code>mol</code>).</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#luminous-intensity_1","title":"Luminous intensity","text":"<p>Luminous intensity is expressed in candelas (symbol: <code>cd</code>).</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#area_1","title":"Area","text":"<p>Area is expressed in square metres (symbol: <code>m\u00b2</code>). The following alternative symbols are supported: * <code>m2</code>: m\u00b2 * <code>ar</code>: hm\u00b2 * <code>syd</code>: ((c(cm254.0))12.0)3.0\u00b2 * <code>cml</code>: [one?]/4.0\u00b7m(c(cm254.0))\u00b2 * <code>b</code>: hfm\u00b2 * <code>sft</code>: (c(cm254.0))12.0\u00b2 * <code>sin</code>: c(cm*254.0)\u00b2</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#volume_1","title":"Volume","text":"<p>Volume is expressed in cubic metres (symbol: <code>\u33a5</code>). The following alternative symbols are supported: * <code>st</code>: [\u33a5?] * <code>bf</code>: (c(cm254.0)\u00b3)144.0 * <code>cyd</code>: ((c(cm254.0))12.0)3.0\u00b3 * <code>cr</code>: ((c(cm254.0))12.0\u00b3)128.0 * <code>L</code>: L * <code>l</code>: l * <code>cin</code>: c(cm254.0)\u00b3 * <code>cft</code>: (c(cm254.0))*12.0\u00b3 * <code>m3</code>: \u33a5</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#energy_1","title":"Energy","text":"<p>Energy is expressed in joules (symbol: <code>J</code>). The following alternative symbols are supported: * <code>cal_IT</code>: (J41868.0)/10000.0 * <code>eV</code>: J1.602176487E-19 * <code>cal_m</code>: (J419002.0)/100000.0 * <code>cal</code>: m(J4184.0) * <code>cal_th</code>: m(J*4184.0)</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#angle_1","title":"Angle","text":"<p>Angle is expressed in radians (symbol: <code>rad</code>). The following alternative symbols are supported: * <code>circ</code>: [one?]\u00b7rad2.0 * <code>gon</code>: ([one?]\u00b7rad/180.0)0.9 * <code>deg</code>: [one?]\u00b7rad/180.0 * <code>'</code>: ([one?]\u00b7rad/180.0)/60.0 * <code>''</code>: (([one?]\u00b7rad/180.0)/60.0)/60.0</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#others_1","title":"Others","text":"<ul> <li><code>1/m</code>, derived units: <code>Ky</code>: c(1/m)</li> <li><code>kg/(m\u00b7s)</code>, derived units: <code>P</code>: g/(s\u00b7cm)</li> <li><code>bit/s</code>, derived units: <code>Bd</code>: bit/s</li> <li><code>bit</code>, derived units: <code>By</code>: bit*8.0</li> <li><code>Sv</code></li> <li><code>N</code></li> <li><code>\u03a9</code>, derived units: <code>Ohm</code>: \u03a9</li> <li><code>T</code>, derived units: <code>G</code>: T/10000.0</li> <li><code>sr</code>, derived units: <code>sph</code>: [one?]\u00b7sr*4.0</li> <li><code>F</code></li> <li><code>C/kg</code>, derived units: <code>R</code>: (C/kg)*2.58E-4</li> <li><code>cd/m\u00b2</code>, derived units: <code>sb</code>: cd/cm\u00b2, <code>Lmb</code>: cd/([one?]\u00b7cm\u00b2)</li> <li><code>Pa</code>, derived units: <code>bar</code>: Pa100000.0, <code>atm</code>: Pa101325.0</li> <li><code>kg/(m\u00b7s\u00b2)</code>, derived units: <code>att</code>: k(g\u00b7(m/s\u00b2)*9.80665)/cm\u00b2</li> <li><code>m\u00b2/s</code>, derived units: <code>St</code>: cm\u00b2/s</li> <li><code>A/m</code>, derived units: <code>Oe</code>: (A/m)*250.0/[one?]</li> <li><code>kg\u00b7m\u00b2/s\u00b2</code>, derived units: <code>erg</code>: cm\u00b2\u00b7g/s\u00b2</li> <li><code>kg/m\u00b3</code>, derived units: <code>g%</code>: g/dl</li> <li><code>mho</code></li> <li><code>V</code></li> <li><code>lx</code>, derived units: <code>ph</code>: lx/10000.0</li> <li><code>m/s\u00b2</code>, derived units: <code>Gal</code>: cm/s\u00b2, <code>m/s2</code>: m/s\u00b2</li> <li><code>m/s</code>, derived units: <code>kn</code>: m*1852.0/h</li> <li><code>m\u00b7kg/s\u00b2</code>, derived units: <code>gf</code>: g\u00b7(m/s\u00b2)9.80665, <code>lbf</code>: lb\u00b7(m/s\u00b2)9.80665, <code>dyn</code>: cm\u00b7g/s\u00b2</li> <li><code>m\u00b2/s\u00b2</code>, derived units: <code>RAD</code>: cm\u00b2\u00b7g/(s\u00b2\u00b7hg), <code>REM</code>: cm\u00b2\u00b7g/(s\u00b2\u00b7hg)</li> <li><code>C</code></li> <li><code>Gy</code></li> <li><code>Hz</code></li> <li><code>H</code></li> <li><code>lm</code></li> <li><code>W</code></li> <li><code>Wb</code>, derived units: <code>Mx</code>: Wb/1.0E8</li> <li><code>Bq</code>, derived units: <code>Ci</code>: Bq*3.7E10</li> <li><code>S</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_33","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_19","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[1 km]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1000.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_19","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[1.0000     ft]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0.3048]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_16","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[1.0lb]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0.45359237]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_10","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[1000000000.0 nm]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_8","title":"Example 5:","text":"<ul> <li> <p>Input values:   1. <code>[-1E6 m]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[-1000000.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_7","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>numberFormat: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[1.000,5 m]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1000.5]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_5","title":"Example 7:","text":"<ul> <li> <p>Input values:   1. <code>[1,000.5 m]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1000.5]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-8_2","title":"Example 8:","text":"<ul> <li>Parameters</li> <li> <p>targetUnit: <code>mi</code></p> </li> <li> <p>Input values:   1. <code>[1 km]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0.621371192237334]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-9_2","title":"Example 9:","text":"<ul> <li>Parameters</li> <li> <p>targetUnit: <code>m</code></p> </li> <li> <p>Input values:   1. <code>[1 kg]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-10_3","title":"Example 10:","text":"<ul> <li> <p>Input values:   1. <code>[100.0]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-11_1","title":"Example 11:","text":"<ul> <li> <p>Input values:   1. <code>[1]</code>   2. <code>[km]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1000.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-12_1","title":"Example 12:","text":"<ul> <li> <p>Input values:   1. <code>[1, 10000]</code>   2. <code>[km, mm]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1000.0, 10.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-13_1","title":"Example 13:","text":"<ul> <li> <p>Input values:   1. <code>[1, 10000, 10]</code>   2. <code>[km, mm]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#aggregate-numbers","title":"Aggregate numbers","text":"<p>Aggregates all numbers in this set using a mathematical operation.</p> Parameter Type Description Default operator String One of \u2018+\u2019, \u2018*\u2019, \u2018min\u2019, \u2018max\u2019, \u2018average\u2019. no default <p>The identifier for this plugin is <code>aggregateNumbers</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#compare-numbers","title":"Compare numbers","text":"<p>Compares the numbers of two sets. Returns 1 if the comparison yields true and 0 otherwise. If there are multiple numbers in both sets, the comparator must be true for all numbers. For instance, {1,2} &lt; {2,3} yields 0 as not all numbers in the first set are smaller than in the second.</p> Parameter Type Description Default comparator Enum No description &lt; <p>The identifier for this plugin is <code>compareNumbers</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#count-values","title":"Count values","text":"<p>Counts the number of values.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>count</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_34","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_20","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[value1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_20","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[value1, value2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#extract-physical-quantity_1","title":"Extract physical quantity","text":"<p>Extracts physical quantities, such as length or weight values. Values are expected of the form \u2018{Number}{UnitPrefix}{Symbol}\u2019 and are converted to the base unit.</p> <p>Example:</p> <ul> <li>Given a value \u201810km, 3mg\u2019.</li> <li>If the symbol parameter is set to \u2018m\u2019, the extracted value is 10000.</li> <li>If the symbol parameter is set to \u2018g\u2019, the extracted value is 0.001.</li> </ul> Parameter Type Description Default symbol String The symbol of the dimension, e.g., \u2018m\u2019 for meter. empty string numberFormat String The IETF BCP 47 language tag, e.g. \u2018en\u2019. en filter String Only extracts from values that contain the given regex (case-insensitive). empty string index int If there are multiple matches, retrieve the value with the given index (zero-based). 0 <p>The identifier for this plugin is <code>extractPhysicalQuantity</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#format-number","title":"Format number","text":"<p>Formats a number according to a user-defined pattern.   The pattern syntax is documented at:   https://docs.oracle.com/javase/8/docs/api/java/text/DecimalFormat.html</p> Parameter Type Description Default pattern String No description no default locale String No description en <p>The identifier for this plugin is <code>formatNumber</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_35","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_21","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code>000</code></p> </li> <li> <p>Input values:   1. <code>[1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[001]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_21","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code>000000.000</code></p> </li> <li> <p>Input values:   1. <code>[123.78]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[000123.780]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_17","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code>###,###.###</code></p> </li> <li> <p>Input values:   1. <code>[123456.789]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[123,456.789]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_11","title":"Example 4:","text":"<ul> <li>Parameters</li> <li>pattern: <code>###.###,###</code></li> <li> <p>locale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[123456.789]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[123.456,789]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_9","title":"Example 5:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code># apples</code></p> </li> <li> <p>Input values:   1. <code>[10]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[10 apples]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_8","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code>000'0'</code></p> </li> <li> <p>Input values:   1. <code>[1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0010]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_6","title":"Example 7:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code>0</code></p> </li> <li> <p>Input values:   1. <code>[1.0]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-8_3","title":"Example 8:","text":"<ul> <li>Parameters</li> <li> <p>pattern: <code>0.0</code></p> </li> <li> <p>Input values:   1. <code>[0000123.4]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[123.4]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#logarithm","title":"Logarithm","text":"<p>Transforms all numbers by applying the logarithm function. Non-numeric values are left unchanged.</p> Parameter Type Description Default base int No description 10 <p>The identifier for this plugin is <code>log</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-operation","title":"Numeric operation","text":"<p>Applies a numeric operation to the values of multiple input operators.  Uses double-precision floating-point numbers for computation.</p> Parameter Type Description Default operator String The operator to be applied to all values. One of \u2018+\u2019, \u2018-\u2018, \u2018*\u2019, \u2018/\u2019 no default <p>The identifier for this plugin is <code>numOperation</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_36","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_22","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>+</code></p> </li> <li> <p>Input values:   1. <code>[1]</code>   2. <code>[1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_22","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>-</code></p> </li> <li> <p>Input values:   1. <code>[1]</code>   2. <code>[1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[0.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_18","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>*</code></p> </li> <li> <p>Input values:   1. <code>[5]</code>   2. <code>[6]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[30.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_12","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>/</code></p> </li> <li> <p>Input values:   1. <code>[5]</code>   2. <code>[2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2.5]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_10","title":"Example 5:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>+</code></p> </li> <li> <p>Input values:   1. <code>[1]</code>   2. <code>[no number]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_9","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>*</code></p> </li> <li> <p>Input values:   1. <code>[1]</code>   2. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_7","title":"Example 7:","text":"<ul> <li>Parameters</li> <li> <p>operator: <code>+</code></p> </li> <li> <p>Input values:   1. <code>[1, 1]</code>   2. <code>[1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[3.0]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-reduce","title":"Numeric reduce","text":"<p>Strip all non-numeric characters from a string.</p> Parameter Type Description Default keepPunctuation boolean No description true <p>The identifier for this plugin is <code>numReduce</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_37","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_23","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>keepPunctuation: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[some1.2Value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[12]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_23","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>keepPunctuation: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[some1.2Value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1.2]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parser","title":"Parser","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-date_1","title":"Parse date","text":"<p>Parses and normalizes dates in different formats.</p> Parameter Type Description Default inputDateFormatId Option The input date/time format used for parsing the date/time string. w3c Date alternativeInputFormat String An input format string that should be used instead of the selected input format. Java DateFormat string. empty string inputLocale LocaleOptionParameter Optional locale for the (alternative) input format. If not set the system\u2019s locale will be used or the locale of the input format, if set. outputDateFormatId Option The output date/time format used for parsing the date/time string. w3c Date alternativeOutputFormat String An output format string that should be used instead of the selected output format. Java DateFormat string. empty string outputLocale LocaleOptionParameter Optional locale for the (alternative) output format. If not set the system\u2019s locale will be used or the locale of the output format, if set. <p>The identifier for this plugin is <code>DateTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_38","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_24","title":"Example 1:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>German style date format</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[20.03.1999]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1999-03-20]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_24","title":"Example 2:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c Date</code></li> <li> <p>outputDateFormatId: <code>German style date format</code></p> </li> <li> <p>Input values:   1. <code>[1999-03-20]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[20.03.1999]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_19","title":"Example 3:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>common ISO8601</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[2017-04-04T00:00:00.000+02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-04-04]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_13","title":"Example 4:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>common ISO8601</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[2017-04-04T00:00:00+02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2017-04-04]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_11","title":"Example 5:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>common ISO8601</code></li> <li> <p>outputDateFormatId: <code>dateTime with month abbr. (US)</code></p> </li> <li> <p>Input values:   1. <code>[2021-06-24T14:50:05.895+02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Jun-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_10","title":"Example 6:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>dateTime with month abbr. (US)</code></li> <li> <p>outputDateFormatId: <code>dateTime with month abbr. (DE)</code></p> </li> <li> <p>Input values:   1. <code>[24-Dec-2021 14:50:05 +02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Dez.-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_8","title":"Example 7:","text":"<ul> <li>Parameters</li> <li>alternativeInputFormat: <code>dd.MM.yyyy HH:mm.ss</code></li> <li> <p>alternativeOutputFormat: <code>yyyy-MM-dd'T'HH:mm.ss</code></p> </li> <li> <p>Input values:   1. <code>[20.03.1999 20:34.44]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1999-03-20T20:34.44]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-8_4","title":"Example 8:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>excelDateTime</code></li> <li> <p>outputDateFormatId: <code>xsdTime</code></p> </li> <li> <p>Input values:   1. <code>[12:20:00.000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[12:20:00.000]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-9_3","title":"Example 9:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c YearMonth</code></li> <li> <p>outputDateFormatId: <code>w3c Month</code></p> </li> <li> <p>Input values:   1. <code>[2020-01]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[--01]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-10_4","title":"Example 10:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c MonthDay</code></li> <li> <p>outputDateFormatId: <code>w3c Day</code></p> </li> <li> <p>Input values:   1. <code>[--12-31]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[---31]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-11_2","title":"Example 11:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c Date</code></li> <li> <p>outputDateFormatId: <code>w3c MonthDay</code></p> </li> <li> <p>Input values:   1. <code>[2020-12-31]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[--12-31]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-12_2","title":"Example 12:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>w3c MonthDay</code></li> <li> <p>outputDateFormatId: <code>w3c Date</code></p> </li> <li> <p>Input values:   1. <code>[--12-31]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-13_2","title":"Example 13:","text":"<ul> <li>Parameters</li> <li>alternativeInputFormat: <code>yyyy-MM-dd HH:mm:ss.SSS</code></li> <li> <p>outputDateFormatId: <code>w3cDateTime</code></p> </li> <li> <p>Input values:   1. <code>[2020-02-22 16:34:14.000]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2020-02-22T16:34:14]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-14_1","title":"Example 14:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>dateTime with month abbr. (DE)</code></li> <li>outputDateFormatId: <code>dateTime with month abbr. (US)</code></li> <li>inputLocale: <code>en_US</code></li> <li> <p>outputLocale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[24-Dec-2021 14:50:05 +02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Dez.-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-15_1","title":"Example 15:","text":"<ul> <li>Parameters</li> <li>inputDateFormatId: <code>dateTime with month abbr. (US)</code></li> <li>outputDateFormatId: <code>dateTime with month abbr. (DE)</code></li> <li>inputLocale: <code>de</code></li> <li> <p>outputLocale: <code>en</code></p> </li> <li> <p>Input values:   1. <code>[24-Dez.-2021 14:50:05 +02:00]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[24-Dec-2021 14:50:05 +02:00]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-16_1","title":"Example 16:","text":"<ul> <li>Parameters</li> <li>outputLocale: <code>fr</code></li> <li>alternativeInputFormat: <code>MMM yyyy</code></li> <li>outputDateFormatId: <code>dateTime with month abbr. (DE)</code></li> <li>inputLocale: <code>de</code></li> <li>alternativeOutputFormat: <code>MMM uuuu</code></li> <li> <p>inputDateFormatId: <code>dateTime with month abbr. (US)</code></p> </li> <li> <p>Input values:   1. <code>[Dez. 2021]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[d\u00e9c. 2021]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-17_1","title":"Example 17:","text":"<ul> <li>Parameters</li> <li>alternativeInputFormat: <code>MMMM, uuuu</code></li> <li>alternativeOutputFormat: <code>MMMM, uuuu</code></li> <li>inputLocale: <code>en_US</code></li> <li> <p>outputLocale: <code>de</code></p> </li> <li> <p>Input values:   1. <code>[February, 2024]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Februar, 2024]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-float","title":"Parse float","text":"<p>Parses and normalizes float values.</p> Parameter Type Description Default commaAsDecimalPoint boolean No description false thousandSeparator boolean No description false bracketsForNegative boolean No description false <p>The identifier for this plugin is <code>FloatTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-geo-coordinate","title":"Parse geo coordinate","text":"<p>Parses and normalizes geo coordinates.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>GeoCoordinateParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-geo-location","title":"Parse geo location","text":"<p>Parses and normalizes geo locations like continents, countries, states and cities.</p> Parameter Type Description Default parseTypeId Enum What type of location should be parsed. no default fullStateName boolean Set to true if the full state name should be output instead of the 2-letter code. true <p>The identifier for this plugin is <code>GeoLocationParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-integer","title":"Parse integer","text":"<p>Parses integer values.</p> Parameter Type Description Default commaAsDecimalPoint boolean Use comma as decimal point (uses a point, otherwise) false thousandSeparator boolean Use comma or point to separate digits false <p>The identifier for this plugin is <code>IntegerParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-isin","title":"Parse ISIN","text":"<p>Parses International Securities Identification Numbers (ISIN) values and fails if the String is no valid ISIN.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>IsinParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-skos-term","title":"Parse SKOS term","text":"<p>Parses values from a SKOS ontology.</p> Parameter Type Description Default surfaceFormToRepresentationMapping Map No description no default <p>The identifier for this plugin is <code>SkosTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.discoverer</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-string","title":"Parse string","text":"<p>Parses string values, basically an identity function.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>StringParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean-html_2","title":"Clean HTML","text":"<p>Cleans HTML using a tag white list and allows selection of HTML sections with XPath or CSS selector expressions.        If the tag or attribute white lists are left empty default white lists will be used (this behaviour can be changed).        To remove all HTML markup and retain text, keep the defaults and turn off the \u201cDefault tags and attributes\u201d toggle.        The operator takes two inputs: the page HTML and (optional) the page Url which may be needed to resolve relative links in the page HTML.</p> Parameter Type Description Default tagWhiteList StringIterableParameter Tags to keep in the cleaned output. attributeWhiteList StringIterableParameter Attributes to keep in the cleaned output. selectors StringIterableParameter CSS or XPath queries for selection of content. CSS selectors can be pipe separated for non-sequential execution. method Enum Selects use of XPath or CSS selectors. xPath defaultTagsAndAttributes boolean Use defaults for empty tag and attribute whitelists.\\If the attribute while list is empty, it will default to: \u201cclass\u201d, \u201cid\u201d, \u201chref\u201d, \u201csrc\u201d\\If the tag while list is empty, it will default to: \u201ca\u201d, \u201cb\u201d, \u201cblockquote\u201d, \u201cbr\u201d, \u201ccaption\u201d, \u201ccite\u201d, \u201ccode\u201d, \u201ccol\u201d, \u201ccolgroup\u201d, \u201cdd\u201d, \u201cdiv\u201d, \u201cdl\u201d, \u201cdt\u201d, \u201cem\u201d, \u201ch1\u201d, \u201ch2\u201d, \u201ch3\u201d, \u201ch4\u201d, \u201ch5\u201d, \u201ch6\u201d,\u201di\u201d, \u201cimg\u201d, \u201cli\u201d, \u201col\u201d, \u201cp\u201d, \u201cpre\u201d, \u201cq\u201d, \u201csmall\u201d, \u201cspan\u201d, \u201cstrike\u201d, \u201cstrong\u201d,\u201dsub\u201d, \u201csup\u201d, \u201ctable\u201d, \u201ctbody\u201d, \u201ctd\u201d, \u201ctfoot\u201d, \u201cth\u201d, \u201cthead\u201d, \u201ctr\u201d, \u201cu\u201d, \u201cul\u201d. true <p>The identifier for this plugin is <code>htmlCleaner</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.html</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replace_1","title":"Replace","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel-map","title":"Excel map","text":"<p>Replaces values based on a map of values read from a file in Open XML format (XLSX). The XLSX file may contain several sheets of the form:</p> <p>mapFrom,mapTo , \u2026 and more <p>An empty string can be created in Excel and alternatives by inserting =\u201d\u201d in the input line of a cell.</p> <p>If there are multiple values for a single key, all values will be returned for the given key.</p> <p>Note that the mapping table will be cached in memory. If the Excel file is updated (even while transforming), the map will be reloaded within seconds.</p> Parameter Type Description Default excelFile Resource Excel file inside the resources directory containing one or more sheets with mapping tables. no default sheetName String The sheet that contains the mapping table or empty if the first sheet should be taken. empty string skipLines int How many rows to skip before reading the mapping table. By default the expected header row is skipped. 1 strict boolean If set to true, the operator throws validation errors for values it cannot map. If set to false, the chosen conflict strategy will be applied for missing values. true conflictStrategy Enum Determines how values that cannot be found in the mapping table are treated. Only has an effect if \u2018strict\u2019 is set to false. If \u2018retain\u2019 is chosen, the original value will be forwarded. If \u2018remove\u2019 is chosen, no value will be output. retain <p>The identifier for this plugin is <code>excelMap</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#map","title":"Map","text":"<p>Replaces values based on a map of values.</p> Parameter Type Description Default map Map A map of values no default default String Default if the map defines no value no default <p>The identifier for this plugin is <code>map</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_39","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_25","title":"Example 1:","text":"<ul> <li>Parameters</li> <li>map: <code>Key1:Value1,Key2:Value2</code></li> <li> <p>default: <code>Undefined</code></p> </li> <li> <p>Input values:   1. <code>[Key1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Value1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_25","title":"Example 2:","text":"<ul> <li>Parameters</li> <li>map: <code>Key1:Value1,Key2:Value2</code></li> <li> <p>default: <code>Undefined</code></p> </li> <li> <p>Input values:   1. <code>[Key1X]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Undefined]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#map-with-default","title":"Map with default","text":"<p>Takes two inputs. Tries to map the first input based on the map of values parameter config. If the input value is not found in the map, it takes the value of the second input. The indexes of the mapped value and the default value match. If there are less default values than values to map, the last default value is replicated to match the count.</p> Parameter Type Description Default map Map A map of values no default <p>The identifier for this plugin is <code>mapWithDefaultInput</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#regex-replace","title":"Regex replace","text":"<p>Replace all occurrences of a regex \u201cregex\u201d with \u201creplace\u201d in a string.</p> Parameter Type Description Default regex String The regular expression to search for no default replace String The string that will replace each match empty string <p>The identifier for this plugin is <code>regexReplace</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replace_2","title":"Replace","text":"<p>Replace all occurrences of a string \u201csearch\u201d with \u201creplace\u201d in a string.</p> Parameter Type Description Default search String The string to search for no default replace String The string that will replace each match no default <p>The identifier for this plugin is <code>replace</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#selection","title":"Selection","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#coalesce-first-non-empty-input","title":"Coalesce (first non-empty input)","text":"<p>Forwards the first non-empty input, i.e. for which any value(s) exist. A single empty string is considered a value.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>coalesce</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.selection</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_40","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_26","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[]</code>   2. <code>[]</code>   3. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_26","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[]</code>   2. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_20","title":"Example 3:","text":"<ul> <li>Returns:</li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_14","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[]</code>   2. <code>[first]</code>   3. <code>[second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[first]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_12","title":"Example 5:","text":"<ul> <li> <p>Input values:   1. <code>[]</code>   2. <code>[first A, first B]</code>   3. <code>[second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[first A, first B]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_11","title":"Example 6:","text":"<ul> <li> <p>Input values:   1. <code>[first]</code>   2. <code>[second]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[first]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#regex-selection","title":"Regex selection","text":"<p>This transformer takes 3 inputs. The first input should have exactly one value that should be passed out again untouched. The second input has at least two Regex values - two in order to make sense. The third input should have exactly one value which is checked against the regexes.</p> <p>The result of the transformer is a sequence with the same length of number of regexes. For the output value (of the first input) is set to each position in this sequence where the related regex also matched.</p> <p>If <code>oneOnly</code> is true only the position of the first matching regex will be set to the output value.</p> Parameter Type Description Default oneOnly boolean No description false <p>The identifier for this plugin is <code>regexSelect</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.selection</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sequence","title":"Sequence","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#count-values_1","title":"Count values","text":"<p>Counts the number of values.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>count</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_41","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_27","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[value1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_27","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[value1, value2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#get-value-by-index","title":"Get value by index","text":"<p>Returns the value found at the specified index. Fails or returns an empty result depending on failIfNoFound is set or not.        Please be aware that this will work only if the data source supports some kind of ordering like XML or JSON. This        is probably not a good idea to do with RDF models.</p> <pre><code>   If emptyStringToEmptyResult is true then instead of a result with an empty String, an empty result is returned.\n</code></pre> Parameter Type Description Default index int No description no default failIfNotFound boolean No description false emptyStringToEmptyResult boolean No description false <p>The identifier for this plugin is <code>getValueByIndex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.sequence</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sort","title":"Sort","text":"<p>Sorts values lexicographically.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>sort</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.sequence</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_42","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_28","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_28","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[c, a, b]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a, b, c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_21","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[Hans, Hansa, Hamburg]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Hamburg, Hans, Hansa]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sequence-values-to-indexes","title":"Sequence values to indexes","text":"<p>Transforms the sequence of values to their respective indexes in the sequence.   Example:    - (\u201ca\u201d, \u201cb\u201d, \u201cc\u201d) becomes (0, 1, 2)</p> <p>If there is more than one input, the values are numbered from the first input on and continued for the next inputs.   Applied against an RDF source the order might not be deterministic.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>toSequenceIndex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.sequence</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substring","title":"Substring","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-postfix","title":"Strip postfix","text":"<p>Strips a postfix of a string.</p> Parameter Type Description Default postfix String No description no default <p>The identifier for this plugin is <code>stripPostfix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_43","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_29","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>postfix: <code>Postfix</code></p> </li> <li> <p>Input values:   1. <code>[valuePostfix]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_29","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>postfix: <code>Postfix</code></p> </li> <li> <p>Input values:   1. <code>[Value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-prefix","title":"Strip prefix","text":"<p>Strips a prefix of a string.</p> Parameter Type Description Default prefix String No description no default <p>The identifier for this plugin is <code>stripPrefix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_44","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_30","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>prefix: <code>prefix</code></p> </li> <li> <p>Input values:   1. <code>[prefixValue]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_30","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>prefix: <code>prefix</code></p> </li> <li> <p>Input values:   1. <code>[ValueWithoutPrefix]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ValueWithoutPrefix]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-uri-prefix_1","title":"Strip URI prefix","text":"<p>Strips the URI prefix and decodes the remainder. Leaves values unchanged which are not a valid URI.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>stripUriPrefix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_45","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_31","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/some/path/to/value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_31","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[urn:scheme:value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_22","title":"Example 3:","text":"<ul> <li> <p>Input values:   1. <code>[http://example.org/some/path/to/encoded%20v%C3%A4lue]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[encoded v\u00e4lue]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_15","title":"Example 4:","text":"<ul> <li> <p>Input values:   1. <code>[value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substring_1","title":"Substring","text":"<p>Returns a substring between \u2018beginIndex\u2019 (inclusive) and \u2018endIndex\u2019 (exclusive). If \u2018endIndex\u2019 is 0 (default), it is ignored and the entire remaining string starting with \u2018beginIndex\u2019 is returned. If \u2018endIndex\u2019 is negative, -endIndex characters are removed from the end.</p> Parameter Type Description Default beginIndex int The beginning index, inclusive. 0 endIndex int The end index, exclusive. Ignored if set to 0, i.e., the entire remaining string starting with \u2018beginIndex\u2019 is returned. If negative, -endIndex characters are removed from the end 0 stringMustBeInRange boolean If true, only strings will be accepted that are within the start and end indices, throwing a validating error if an index is out of range. true <p>The identifier for this plugin is <code>substring</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_46","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_32","title":"Example 1:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>0</code></li> <li> <p>endIndex: <code>1</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[a]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_32","title":"Example 2:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>2</code></li> <li> <p>endIndex: <code>3</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_23","title":"Example 3:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>3</code></li> <li> <p>endIndex: <code>3</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_16","title":"Example 4:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>2</code></li> <li> <p>endIndex: <code>4</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_13","title":"Example 5:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>2</code></li> <li>endIndex: <code>4</code></li> <li> <p>stringMustBeInRange: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[c]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_12","title":"Example 6:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>10</code></li> <li>endIndex: <code>20</code></li> <li> <p>stringMustBeInRange: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-7_9","title":"Example 7:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>0</code></li> <li> <p>endIndex: <code>-1</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-8_5","title":"Example 8:","text":"<ul> <li>Parameters</li> <li>beginIndex: <code>1</code></li> <li> <p>endIndex: <code>0</code></p> </li> <li> <p>Input values:   1. <code>[abc]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[bc]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#trim_1","title":"Trim","text":"<p>Remove leading and trailing whitespaces.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>trim</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#until-character","title":"Until character","text":"<p>Extracts the substring until the character given.</p> Parameter Type Description Default untilCharacter char No description no default <p>The identifier for this plugin is <code>untilCharacter</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_47","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_33","title":"Example 1:","text":"<ul> <li>Parameters</li> <li> <p>untilCharacter: <code>c</code></p> </li> <li> <p>Input values:   1. <code>[abcde]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[ab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_33","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>untilCharacter: <code>c</code></p> </li> <li> <p>Input values:   1. <code>[abab]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[abab]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#template","title":"Template","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#evaluate-template_1","title":"Evaluate template","text":"<p>Evaluates a template. Input values can be addressed using the variables \u2018input1\u2019, \u2018input2\u2019, etc. Global variables are available in the \u2018global\u2019 scope, e.g., \u2018global.myVar\u2019.</p> Parameter Type Description Default template TemplateParameter The template no default language String The template language. Currently, Jinja is supported. jinja <p>The identifier for this plugin is <code>TemplateTransformer</code>.</p> <p>It can be found in the package <code>com.eccenca.di.templating.operators</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_48","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_34","title":"Example 1:","text":"<ul> <li>Parameters</li> <li>template: `Hello {{input1}} {{input2}},</li> </ul> <p>How are you today?`</p> <ul> <li> <p>Input values:   1. <code>[John]</code>   2. <code>[Doe]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 `[Hello John Doe,</p> <p>How are you today?]`</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_34","title":"Example 2:","text":"<ul> <li>Parameters</li> <li> <p>template: <code>Hello {{badVariable}} {{input1}}</code></p> </li> <li> <p>Input values:   1. <code>[John]</code>   2. <code>[Doe]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_24","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>template: <code>Hello {{input01}}</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_17","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>template: <code>Hello {{input1}}</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_14","title":"Example 5:","text":"<ul> <li>Parameters</li> <li> <p>template: <code>Hello {{input1}}</code></p> </li> <li> <p>Input values:   1. <code>[A, B]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Hello AB]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_13","title":"Example 6:","text":"<ul> <li>Parameters</li> <li> <p>template: <code>Hello {% for value in input1 %}{{value}}, {% endfor %}how are you doing?</code></p> </li> <li> <p>Input values:   1. <code>[Bob, Eve]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Hello Bob, Eve, how are you doing?]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tokenization","title":"Tokenization","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#camel-case-tokenizer","title":"Camel case tokenizer","text":"<p>Tokenizes a camel case string. That is it splits strings between a lower case character and an upper case character.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>camelcasetokenizer</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.tokenization</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_49","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_35","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[camelCaseString]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[camel, Case, String]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_35","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[nocamelcase]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[nocamelcase]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#get-value-by-index_1","title":"Get value by index","text":"<p>Returns the value found at the specified index. Fails or returns an empty result depending on failIfNoFound is set or not.        Please be aware that this will work only if the data source supports some kind of ordering like XML or JSON. This        is probably not a good idea to do with RDF models.</p> <pre><code>   If emptyStringToEmptyResult is true then instead of a result with an empty String, an empty result is returned.\n</code></pre> Parameter Type Description Default index int No description no default failIfNotFound boolean No description false emptyStringToEmptyResult boolean No description false <p>The identifier for this plugin is <code>getValueByIndex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.sequence</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tokenize","title":"Tokenize","text":"<p>Tokenizes all input values.</p> Parameter Type Description Default regex String The regular expression used to split values. \\s <p>The identifier for this plugin is <code>tokenize</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.tokenization</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_50","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#by-default-splits-values-at-whitespaces","title":"By default, splits values at whitespaces:","text":"<ul> <li> <p>Input values:   1. <code>[Hello World]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[Hello, World]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#optionally-splits-values-at-the-provided-regex","title":"Optionally, splits values at the provided regex:","text":"<ul> <li>Parameters</li> <li> <p>regex: <code>,</code></p> </li> <li> <p>Input values:   1. <code>[.175,.050]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[.175, .050]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validation","title":"Validation","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-after_1","title":"Validate date after","text":"<p>Validates if the first input date is after the second input date. Outputs the first input if the validation is successful.</p> Parameter Type Description Default allowEqual boolean Allow both dates to be equal. false <p>The identifier for this plugin is <code>validateDateAfter</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_51","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_36","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[2015-04-02]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_36","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[2015-04-04]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-04]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_25","title":"Example 3:","text":"<ul> <li>Parameters</li> <li> <p>allowEqual: <code>true</code></p> </li> <li> <p>Input values:   1. <code>[2015-04-03]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[2015-04-03]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_18","title":"Example 4:","text":"<ul> <li>Parameters</li> <li> <p>allowEqual: <code>false</code></p> </li> <li> <p>Input values:   1. <code>[2015-04-03]</code>   2. <code>[2015-04-03]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-range_1","title":"Validate date range","text":"<p>Validates if dates are within a specified range.</p> Parameter Type Description Default minDate String Earliest allowed date in YYYY-MM-DD no default maxDate String Latest allowed data in YYYY-MM-DD no default <p>The identifier for this plugin is <code>validateDateRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-number-of-values","title":"Validate number of values","text":"<p>Validates that the number of values lies in a specified range.</p> Parameter Type Description Default min int Minimum allowed number of values 0 max int Maximum allowed number of values 1 <p>The identifier for this plugin is <code>validateNumberOfValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_52","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_37","title":"Example 1:","text":"<ul> <li>Parameters</li> <li>min: <code>0</code></li> <li> <p>max: <code>1</code></p> </li> <li> <p>Input values:   1. <code>[value1]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[value1]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_37","title":"Example 2:","text":"<ul> <li>Parameters</li> <li>min: <code>0</code></li> <li> <p>max: <code>1</code></p> </li> <li> <p>Input values:   1. <code>[value1, value2]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-numeric-range_1","title":"Validate numeric range","text":"<p>Validates if a number is within a specified range.</p> Parameter Type Description Default min double Minimum allowed number no default max double Maximum allowed number no default <p>The identifier for this plugin is <code>validateNumericRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-regex","title":"Validate regex","text":"<p>Validates if all values match a regular expression.</p> Parameter Type Description Default regex String regular expression \\w* <p>The identifier for this plugin is <code>validateRegex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#value","title":"Value","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#constant","title":"Constant","text":"<p>Generates a constant value.</p> Parameter Type Description Default value String The constant value to be generated empty string <p>The identifier for this plugin is <code>constant</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_53","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#always-outputs-the-specified-value","title":"Always outputs the specified value:","text":"<ul> <li>Parameters</li> <li> <p>value: <code>John</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[John]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#constant-uri","title":"Constant URI","text":"<p>Generates a constant URI.</p> Parameter Type Description Default value Uri The constant URI to be generated http://www.w3.org/2002/07/owl#Class <p>The identifier for this plugin is <code>constantUri</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#current-date_1","title":"Current date","text":"<p>Outputs the current date.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>currentDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#dataset-parameter","title":"Dataset parameter","text":"<p>Reads a meta data parameter from a dataset in Corporate Memory. If authentication is enabled, workbench.superuser must be configured.</p> Parameter Type Description Default project ProjectReference The project of the dataset. cmem dataset TaskReference The dataset the meta data parameter is read from. no default key String No description no default lang String No description empty string <p>The identifier for this plugin is <code>datasetParameter</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.datasetParam</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#default-value","title":"Default Value","text":"<p>Generates a default value, if the input values are empty. Forwards any non-empty values.</p> Parameter Type Description Default value String The default value to be generated, if input values are empty default <p>The identifier for this plugin is <code>defaultValue</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_54","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#forwards-input-values","title":"Forwards input values:","text":"<ul> <li> <p>Input values:   1. <code>[input value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[input value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#outputs-the-default-value-if-the-inputs-are-empty","title":"Outputs the default value, if the inputs are empty:","text":"<ul> <li>Parameters</li> <li> <p>value: <code>default value</code></p> </li> <li> <p>Input values:   1. <code>[]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[default value]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#empty-value","title":"Empty value","text":"<p>Generates an empty value.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>emptyValue</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#input-hash","title":"Input hash","text":"<p>Calculates the hash sum of the input values. Generates a single hash sum for all input values combined. This operator supports using different hash algorithms from the Secure Hash Algorithms family (SHA, e.g. SHA256) and two algorithms from the Message-Digest Algorithm family (MD2 / MD5). Please be aware that some of these algorithms are not secure regarding collision- and other attacks.</p> Parameter Type Description Default algorithm String The hash algorithm to be used. SHA256 <p>The identifier for this plugin is <code>inputHash</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_55","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_38","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[input value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[f708c2afff0ed197e8551c4dd549ee5b848e0b407106cbdb8e451c8cd1479362]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#random-number","title":"Random number","text":"<p>Generates a set of random numbers.</p> Parameter Type Description Default min double The smallest number that could be generated. 0.0 max double The largest number that could be generated. 100.0 minCount int The minimum number of values to generate in each set. 1 maxCount int The maximum number of values to generate in each set. 1 <p>The identifier for this plugin is <code>randomNumber</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#read-parameter","title":"Read parameter","text":"<p>Reads a parameter from a Java Properties file.</p> Parameter Type Description Default resource Resource The Java properties file to read the parameter from. no default parameter String The name of the parameter. no default <p>The identifier for this plugin is <code>readParameter</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#uuid","title":"UUID","text":"<p>Generates UUIDs. If no input value is provided, a random UUID (type 4) is generated using a cryptographically strong pseudo random number generator. If input values are provided, a name-based UUID (type 3) is generated for each input value. Each input value will generate a separate UUID. For building a UUID from multiple inputs, the Concatenate operator can be used.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>uuid</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_56","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_39","title":"Example 1:","text":"<ul> <li> <p>Input values:   1. <code>[input value]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[cee963a2-8f70-3e97-b51a-85ef732e66dd]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_38","title":"Example 2:","text":"<ul> <li> <p>Input values:   1. <code>[\u00fc\u00f6\u00e4!, \u00ea\u00e9\u00e8]</code></p> </li> <li> <p>Returns:</p> </li> </ul> <p>\u2192 <code>[690802dd-a317-335f-807c-e4e1e32b7b5b, 925cbd7f-377b-3fbd-8f4c-ca41529b74ad]</code></p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#aggregations","title":"Aggregations","text":"<p>The following aggregation functions are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#average_1","title":"Average","text":"<p>Computes the weighted average.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>average</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_57","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#computes-the-arithmetic-mean-of-all-similarity-scores","title":"Computes the arithmetic mean of all similarity scores:","text":"<ul> <li>Input values: [0.4, 0.5, 0.9]</li> <li>Returns: <code>0.6</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#multiplies-individual-similarity-scores-with-their-weight-before-averaging","title":"Multiplies individual similarity scores with their weight before averaging:","text":"<ul> <li>Weights: [1, 1, 2]</li> <li>Input values: [0.3, 0.5, 0.6]</li> <li>Returns: <code>0.5</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-scores-always-lead-to-an-output-of-none","title":"Missing scores always lead to an output of none:","text":"<ul> <li>Input values: [-1.0, (none), 1.0]</li> <li>Returns: <code>(none)</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#first-non-empty-score","title":"First non-empty score","text":"<p>Forwards the first input that provides a non-empty similarity score.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>firstNonEmpty</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_58","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#the-first-defined-score-is-returned-even-if-its-not-the-highest-score","title":"The first defined score is returned, even if it\u2019s not the highest score:","text":"<ul> <li>Input values: [(none), 0.2, 0.5]</li> <li>Returns: <code>0.2</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geometric-mean","title":"Geometric mean","text":"<p>Compute the (weighted) geometric mean.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>geometricMean</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_59","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_40","title":"Example 1:","text":"<ul> <li>Weights: [1, 2, 1]</li> <li>Input values: [0.0, 0.0, 0.0]</li> <li>Returns: <code>0.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_39","title":"Example 2:","text":"<ul> <li>Weights: [1, 2, 1]</li> <li>Input values: [1.0, 1.0, 1.0]</li> <li>Returns: <code>1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_26","title":"Example 3:","text":"<ul> <li>Weights: [2, 1]</li> <li>Input values: [0.5, 1.0]</li> <li>Returns: <code>0.629961</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_19","title":"Example 4:","text":"<ul> <li>Weights: [2, 1, 5]</li> <li>Input values: [0.5, 1.0, 0.7]</li> <li>Returns: <code>0.672866</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_15","title":"Example 5:","text":"<ul> <li>Weights: [10, 2, 3]</li> <li>Input values: [0.1, 0.9, 0.2]</li> <li>Returns: <code>0.153971</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-scores-always-lead-to-an-output-of-none_1","title":"Missing scores always lead to an output of none:","text":"<ul> <li>Input values: [-1.0, (none), 1.0]</li> <li>Returns: <code>(none)</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#handle-missing-values","title":"Handle missing values","text":"<p>Generates a default similarity score, if no similarity score is provided (e.g., due to missing values). Using this operator can have a performance impact, since it lowers the efficiency of the underlying computation.</p> Parameter Type Description Default defaultValue double The default value to be generated, if no similarity score is provided. Must be a value between -1 (inclusive) and 1 (inclusive). \u20181\u2019 represents boolean true and \u2018-1\u2019 represents boolean false. -1.0 <p>The identifier for this plugin is <code>handleMissingValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_60","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#forwards-input-similarity-scores","title":"Forwards input similarity scores:","text":"<ul> <li>Input values: [0.1]</li> <li>Returns: <code>0.1</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#outputs-the-default-score-if-no-input-score-is-provided","title":"Outputs the default score, if no input score is provided:","text":"<ul> <li>Parameters</li> <li> <p>defaultValue: <code>1.0</code></p> </li> <li> <p>Input values: [(none)]</p> </li> <li>Returns: <code>1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#or_1","title":"Or","text":"<p>At least one input score must be within the threshold. Selects the maximum score.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>max</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_61","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#selects-the-maximum-similarity-score","title":"Selects the maximum similarity score:","text":"<ul> <li>Input values: [0.5, 0.0]</li> <li>Returns: <code>0.5</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#selects-the-maximum-similarity-score_1","title":"Selects the maximum similarity score:","text":"<ul> <li>Input values: [-1.0, -0.5, -0.3]</li> <li>Returns: <code>-0.3</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-scores-default-to-a-similarity-score-of-1","title":"Missing scores default to a similarity score of -1:","text":"<ul> <li>Input values: [(none)]</li> <li>Returns: <code>-1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#weights-are-ignored","title":"Weights are ignored:","text":"<ul> <li>Weights: [1000, 0]</li> <li>Input values: [1.0, 0.0]</li> <li>Returns: <code>1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#and_1","title":"And","text":"<p>All input scores must be within the threshold. Selects the minimum score.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>min</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_62","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#selects-the-minimum-similarity-score","title":"Selects the minimum similarity score:","text":"<ul> <li>Input values: [1.0, 0.0]</li> <li>Returns: <code>0.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#selects-the-minimum-similarity-score_1","title":"Selects the minimum similarity score:","text":"<ul> <li>Input values: [-1.0, 0.0, 0.5, 1.0]</li> <li>Returns: <code>-1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-scores-default-to-a-similarity-score-of-1_1","title":"Missing scores default to a similarity score of -1:","text":"<ul> <li>Input values: [1.0, (none), -0.5]</li> <li>Returns: <code>-1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#weights-are-ignored_1","title":"Weights are ignored:","text":"<ul> <li>Weights: [1000, 0]</li> <li>Input values: [1.0, 0.0]</li> <li>Returns: <code>0.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#negate","title":"Negate","text":"<p>Negates the result of the input comparison. A single input is expected. Using this operator can have a performance impact, since it lowers the efficiency of the underlying computation.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>negate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#euclidian-distance","title":"Euclidian distance","text":"<p>Calculates the Euclidian distance.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>quadraticMean</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_63","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-1_41","title":"Example 1:","text":"<ul> <li>Weights: [1, 1, 1]</li> <li>Input values: [1.0, 1.0, 1.0]</li> <li>Returns: <code>1.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-2_40","title":"Example 2:","text":"<ul> <li>Weights: [1, 1]</li> <li>Input values: [1.0, 0.0]</li> <li>Returns: <code>0.707107</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-3_27","title":"Example 3:","text":"<ul> <li>Weights: [1, 1, 1]</li> <li>Input values: [0.4, 0.5, 0.6]</li> <li>Returns: <code>0.506623</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-4_20","title":"Example 4:","text":"<ul> <li>Weights: [1, 1]</li> <li>Input values: [0.0, 0.0]</li> <li>Returns: <code>0.0</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-5_16","title":"Example 5:","text":"<ul> <li>Weights: [2, 1, 1]</li> <li>Input values: [1.0, 0.0, 0.0]</li> <li>Returns: <code>0.707107</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#example-6_14","title":"Example 6:","text":"<ul> <li>Weights: [1, 2, 3]</li> <li>Input values: [0.4, 0.5, 0.6]</li> <li>Returns: <code>0.538516</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#missing-scores-always-lead-to-an-output-of-none_2","title":"Missing scores always lead to an output of none:","text":"<ul> <li>Input values: [-1.0, (none), 1.0]</li> <li>Returns: <code>(none)</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#scale","title":"Scale","text":"<p>Scales a similarity score by a factor.</p> Parameter Type Description Default factor double All input similarity values are multiplied with this factor. 1.0 <p>The identifier for this plugin is <code>scale</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#examples_64","title":"Examples","text":"<p>Notation: List of values are represented via square brackets. Example: <code>[first, second]</code> represents a list of two values \u201cfirst\u201d and \u201csecond\u201d.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#scales-similarity-scores-by-the-specified-factor","title":"Scales similarity scores by the specified factor:","text":"<ul> <li>Parameters</li> <li> <p>factor: <code>0.5</code></p> </li> <li> <p>Input values: [1.0]</p> </li> <li>Returns: <code>0.5</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ignores-missing-values","title":"Ignores missing values:","text":"<ul> <li>Input values: [(none)]</li> <li>Returns: <code>(none)</code></li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#throws-a-validation-error-if-more-than-one-input-is-provided","title":"Throws a validation error if more than one input is provided:","text":"<ul> <li>Input values: [0.1, 0.2]</li> <li>Returns: <code>(none)</code></li> </ul> <ol> <li> <p>Hive 1.2.1 is ODPi runtime compliant\u00a0\u21a9</p> </li> </ol>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/docker-orchestration/","title":"Docker Orchestration","text":"","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#introduction","title":"Introduction","text":"<p>This page describes the configuration for the <code>docker compose</code> based orchestration.</p> <p>The Docker Orchestration (hereafter simply orchestration) is configured via environment files. In this document we provide an overview on how the environment files are loaded, how to modify the configuration inside those files and available configuration parameters.</p>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#environment-files-loading-sequence","title":"Environment Files: Loading Sequence","text":"<p>The environment files are supplied in the\u00a0CONFIGFILE environment variable to the make targets inside the orchestration. For example, in\u00a0Scenario: Single Node Cloud Installation\u00a0we have created a <code>prod.env</code> environment file and created the Corporate Memory instance using <code>prod.env</code> configuration:</p> <pre><code>$ CONFIGFILE=environments/prod.env make clean-pull-start-bootstrap\n</code></pre> <p>When you run <code>make clean-pull-start-bootstrap</code> target, the Makefile will evaluate and export the environment variables from the <code>environments/default.env</code>, your <code>${CONFIGFILE}</code> or <code>environments/config.env</code> and <code>environments/scripted-env.mk</code>:</p> <pre><code>$ cat Makefile\n...\ninclude ${CONFIGFILE_BASE_DIRECTORY}/default.env\ninclude ${CONFIGFILE}\ninclude ${CONFIGFILE_BASE_DIRECTORY}/scripted-env.mk\nexport\n...\n</code></pre> <p>The files are loaded exactly in this order and the later env files will overwrite the environment variables from the former env files. In other words, your <code>${CONFIGFILE}</code> will have precedence over <code>environments/default.env</code>. While <code>environments/scripted-env.mk</code> has precedence over both\u00a0<code>environments/default.env</code> and your <code>${CONFIGFILE}</code>.</p>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#configuring-docker-orchestration","title":"Configuring Docker Orchestration","text":"<p>To configure the orchestration according to your requirements, you need simply to create a file inside <code>environments/</code> directory and set the necessary variables there. For example, to replicate the minimum configuration from <code>config.env</code>, you can do the following:</p> <pre><code>$ echo \"create empty environments/prod.env file\"\n$ touch environments/prod.env\n$ echo \"inject necessary variables into the prod.env\"\n$ echo \"CMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" &gt;&gt; environments/prod.env\n$ echo \"STARDOG_PASSWORD=admin\" &gt;&gt; environments/prod.env\n$ echo \"TRUSTSTOREPASS=Aimeik5Ocho5riuC\" &gt;&gt; environments/prod.env\n</code></pre> <p>This configuration will be sufficient to run the orchestration locally as described in\u00a0Scenario: Local Installation:</p> <pre><code>$ CONFIGFILE=environments/prod.env make clean-pull-start-bootstrap\n</code></pre>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#available-configuration-variables","title":"Available Configuration Variables","text":"<p>All available configuration environment variables are listed in <code>environments/default.env</code> file. In this section we describe the default value and purpose of each of those variables.</p>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#docker-settings","title":"Docker Settings","text":"Variable Default Value Description Docker DOCKER_CMD_ADD (optional) Additional command line arguments to be supplied to <code>docker compose</code> such as\u00a0<code>--tlscacert</code>,\u00a0<code>--tlscert</code>,\u00a0<code>--tlskey</code> or\u00a0<code>--tlsverify</code> ECC_HOST (internal)","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#deployment-settings","title":"Deployment Settings","text":"Variable Default Value Description DEPLOYPROTOCOL http Deploy protocol: http or https DEPLOYHOST docker.localhost Deploy host such as docker.localhost or corporate-memory.example.com PORT 80 Port for apache2 to listen on, for SSL configuration see section below. DEST $(dir $(abspath Makefile)) Directory where the orchestration is located (by default resolves to the directory where this Makefile is located) APACHE_BASE_FILE docker-compose.apache2-exposed.yml <code>docker compose</code> extension file for apache2, see SSL configuration section below for an example APACHE_CONFIG default.conf Apache2 virtual host configuration SSLCONF ssl.default.conf Apache2 virtual host configuration for SSL setup HTTP_PORT 80 APACHE_HTTP_PORT is used as a standard port 80 in SSL setup LETSENCRYPT_MAIL administration@eccenca.com email to be used when requesting letsencrypt certificates DATAINTEGRATION_BASE_FILE docker-compose.dataintegration-base.yml <code>docker compose</code> extension file for Build (DataIntegration),\u00a0see SSL configuration section below for an example TRUSTSTOREPASS (empty) Truststore password, see self-signed certificates configuration section below for an example","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#project-settings","title":"Project Settings","text":"Variable Default Value Description BOOTSTRAP false \u201cfalse\u201d or \u201ctrue\u201d, indicates whether to load the Corporate Memory bootstrap data PROJECT_NAME_SUFFIX (empty) (optional) will append to the <code>docker compose</code> project environment variable\u00a0COMPOSE_PROJECT_NAME","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#explore-frontend-datamanager-settings","title":"Explore frontend (DataManager) Settings","text":"Variable Default Value Description DATAMANAGER_CONFIG_WORKSPACES_DEFAULT_NAME CMEM Orchestration Name of the default Explore frontend (DataManager) workspace DATAMANAGER_CONFIG_APPPRESENTATION_HEADERNAME eccenca Corporate Memory Explore frontend (DataManager) header name DATAMANAGER_CONFIG_APPPRESENTATION_WINDOWTITLE eccenca Corporate Memory Explore frontend (DataManager) windows title","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#explore-backend-dataplatform-settings","title":"Explore backend (DataPlatform) Settings","text":"Variable Default Value Description AUTHORIZATION_ABOX_PREFIX http://eccenca.com/ ABox prefix defines a prefix for access control lists, changing this can break authorization in the Corporate Memory instance AUTHORIZATION_ABOX_ADMINGROUP elds-admins Default admin group for the Corporate Memory users DATAPLATFORM_CONTEXTPATH /dataplatform Context path for the dataplatform, meaning that dataplatform will run under <code>http://dataplatform.host/dataplatform</code> DATAPLATFORM_JAVA_TOOL_OPTIONS -Xms512m -Xmx2g Java options, modify to increase memory allocation","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#build-dataintegration-settings","title":"Build (DataIntegration) Settings","text":"Variable Default Value Description DATAINTEGRATION_EXECUTOR LocalExecutionManager Build (DataIntegration)\u00a0execution.manager.plugin parameter, see\u00a0Build (DataIntegration)\u00a0manual for more details INTERNAL_BASE_URL ${DEPLOYPROTOCOL}://${DEPLOYHOST} Used for\u00a0DATAPLATFORM_URL and\u00a0OAUTH_TOKEN_URL\u00a0variables DATAPLATFORM_URL ${INTERNAL_BASE_URL}${DATAPLATFORM_CONTEXTPATH} Build (DataIntegration) eccencaDataPlatform.url\u00a0parameter,\u00a0see\u00a0Build (DataIntegration)\u00a0manual for more details OAUTH_AUTHORIZATION_URL ${DEPLOY_BASE_URL}/auth/realms/cmem/protocol/openid-connect/auth Build (DataIntegration)\u00a0oauth.authorizationUrl parameter,\u00a0see\u00a0Build (DataIntegration)\u00a0manual for more details OAUTH_TOKEN_URL ${INTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/token Build (DataIntegration) oauth.tokenUrl\u00a0parameter,\u00a0see\u00a0Build (DataIntegration)\u00a0manual for more details DATAINTEGRATION_PRODUCTION_CONFIG_FILE /opt/cmem/eccenca-DataIntegration/dist/etc/dataintegration/conf/dataintegration.conf Path to Build (DataIntegration) configuration file DATAINTEGRATION_JAVA_TOOL_OPTIONS -Xmx4g Java options, modify to increase memory allocation","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#keycloak-settings","title":"Keycloak Settings","text":"Variable Default Value Description PROXY_ADDRESS_FORWARDING false Keycloak proxy forwarding, necessary for SSL configuration KEYCLOAK_AUTH_URL_INTERNAL <code>http://keycloak:8080/auth/realms/cmem/protocol/openid-connect/token</code> (internal) used in scripts/utils.sh to restore Build (DataIntegration) projects CMEM_SERVICE_ACCOUNT_CLIENT_ID (internal) used in scripts/utils.sh to restore Build (DataIntegration) projects","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#stardog-settings","title":"Stardog Settings","text":"Variable Default Value Description STARDOG_SEARCHINDEX_ENABLE true Enable or disable stardog search index STARDOG_SERVER_JAVA_ARGS -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g Java options, modify to increase memory allocation","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#component-versions","title":"Component Versions","text":"Variable Default Value Description DI_VERSION develop Build (DataIntegration) docker image version to be used DP_VERSION develop Explore backend (DataPlatform) docker image version to be used DM_VERSION develop Explore frontend (DataManager) docker image version to be used APACHE2_VERSION v2.6.0 Apache2 docker image version to be used KEYCLOAK_VERSION v6.0.1-2 Keycloak docker image version to be used POSTGRES_VERSION 11.5-alpine Postgresql docker image version to be used STARDOG_VERSION v7.2.0-1 Stardog docker image version to be used","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#ssl-configuration-with-letsencrypt-example","title":"SSL Configuration with Letsencrypt Example","text":"<p>A complete example on how to deploy the Corporate Memory instance on Hetzner with Letsencrypt certificates is described in\u00a0Scenario: Single Node Cloud Installation</p> <pre><code>#!/bin/bash\n\nCMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nSTARDOG_PASSWORD=admin\n# change DEPLOYHOST to your own value! the one you have configured in your DNS\nDEPLOYHOST=corporate-memory.eccenca.dev\nPROXY_ADDRESS_FORWARDING=true\nDATAINTEGRATION_JAVA_TOOL_OPTIONS=-Xmx4g\nDATAPLATFORM_JAVA_TOOL_OPTIONS=-Xms2g -Xmx4g\nSTARDOG_SERVER_JAVA_ARGS=-Xms2g -Xmx2g -XX:MaxDirectMemorySize=4g\n\n# letsencrypt:\nSSLCONF=ssl.letsencrypt.conf\n# change MAIL to your own value! use a common system administration mailbox here\nLETSENCRYPT_MAIL=administration@eccenca.com\n\nDI_VERSION=v20.03\nDP_VERSION=v20.03\nDM_VERSION=v20.03\nAPACHE2_VERSION=v2.6.0\nKEYCLOAK_VERSION=v6.0.1-2\nPOSTGRES_VERSION=11.5-alpine\nSTARDOG_VERSION=v7.2.0-1\n\n#################################\n# Do NOT CHANGE these settings. #\n# ###############################\n# NOTE:\n#  - these settings differ from http setup but should not be altered\n#\nDEPLOYPROTOCOL=https\nPORT=443\nAPACHE_BASE_FILE=docker-compose.apache2-ssl.yml\nAPACHE_CONFIG=default.ssl.conf\nPROXY_ADDRESS_FORWARDING=true\n</code></pre>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#ssl-configuration-with-self-signed-certificates-example","title":"SSL Configuration with Self-Signed Certificates Example","text":"<pre><code>#!/bin/bash\n\nCMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nSTARDOG_PASSWORD=admin\nTRUSTSTOREPASS=Aimeik5Ocho5riuC\n\n# Set this to your deployhost\nDEPLOYHOST=corporate.memory\nDATAINTEGRATION_BASE_FILE=docker-compose.dataintegration-ssl.yml\n\nDI_VERSION=v20.03\nDP_VERSION=v20.03\nDM_VERSION=v20.03\nAPACHE2_VERSION=v2.6.0\nKEYCLOAK_VERSION=v6.0.1-2\nPOSTGRES_VERSION=11.5-alpine\nSTARDOG_VERSION=v7.2.0-1\n\n#################################\n# Do NOT CHANGE these settings. #\n# ###############################\n# NOTE:\n#  - these settings differ from http setup but should not be altered\n#\nDEPLOYPROTOCOL=https\nPORT=443\nAPACHE_BASE_FILE=docker-compose.apache2-ssl.yml\nAPACHE_CONFIG=default.ssl.conf\nPROXY_ADDRESS_FORWARDING=true\n</code></pre>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/explore/","title":"Explore (DataManager/DataPlatform)","text":"<p>This page describes how to configure the eccenca Explore component which is done in two parts:</p> <ol> <li>The Explore frontend (DataManager) is configured visually through the  Workspace configuration module.</li> <li>The Explore backend (DataPlatform), all details are described on the respective sub-page</li> </ol> <p>eccenca Explore frontend (DataManager) is a single-page JavaScript application, which means the application consists of a single HTML page which loads all needed web resources in the browser after loading the page itself.</p> <p>In the context of Explore frontend (DataManager), these web resources are:</p> <ul> <li>The application including its configuration (<code>app*.js</code>, <code>config.js</code>)</li> <li>Styles (<code>*.css</code>)</li> <li>Web fonts for typography as well as for icons (<code>*.woff</code>, <code>*.ttf</code>, <code>*.eot</code>)</li> <li>Images (e.g. logos) (<code>*.png</code>, <code>*.svg</code>)</li> </ul> <p>Explore frontend (DataManager) communicates with different API endpoints in order to retrieve and manipulate data.</p> <p>The features of Explore frontend (DataManager) include:</p> <ul> <li>Dataset Manager to create and update datasets and its meta data</li> <li>Vocabulary Manager to install and remove Vocabulary descriptions</li> <li>Data browser to explore and manage graph-based data</li> <li>Taxonomy Editor to manage and create SKOS based taxonomies</li> <li>Query editor to query graph-based data via SPARQL queries</li> <li>Access control</li> <li>Compliance of W3C standards such as\u00a0RDF,\u00a0Linked Data\u00a0and\u00a0SPARQL</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/","title":"Explore backend (DataPlatform)","text":"<p>This manual describes how to install and set up eccenca Explore backend (DataPlatform). It is intended for system administrators, who are responsible for installing, configuring, maintaining and supporting the deployment of Explore backend (DataPlatform). To use this manual, system administrators should have knowledge about Linux (Ubuntu) and the installation environment on which Explore backend (DataPlatform) is deployed.</p> <p>The following subsections describe different configuration topics in detail. Every subsection is presented with a property key overview and a details section with additional explanations:</p> <ul> <li>Explore backend general configuration</li> <li>OAuth specific configuration</li> <li>Triple Store specific configuration<ul> <li>Ontotext GraphDB</li> <li>HTTP</li> <li>In-Memory</li> <li>AWS Neptune</li> <li>Openlink Virtuoso</li> </ul> </li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/","title":"General","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#deployment-options-for-explore-container","title":"Deployment options for explore container","text":"<p>Property: deploy.apiPrefix</p> <p>API prefix for former dataplatform endpoints i.e. /dataplatform</p> Category Value Default /dataplatform Required false Valid values string Environment DEPLOY_APIPREFIX <p>Property: deploy.post-logout-redirect-uri</p> <p>URI where to redirect to when the user logs out</p> Category Value Default / Required false Valid values string Environment DEPLOY_POST_LOGOUT_REDIRECT_URI","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#options-for-additional-prometheus-metrics-endpoint","title":"Options for additional prometheus metrics endpoint","text":"<p>Property: deploy.additional-prometheus-endpoint.enabled</p> Category Value Default false Required false Valid values string Environment DEPLOY_ADDITIONAL_PROMETHEUS_ENDPOINT_ENABLED <p>Property: deploy.additional-prometheus-endpoint.port</p> Category Value Default 9091 Required false Valid values string Environment DEPLOY_ADDITIONAL_PROMETHEUS_ENDPOINT_PORT <p>Property: deploy.additional-prometheus-endpoint.context</p> Category Value Default /metrics Required false Valid values string Environment DEPLOY_ADDITIONAL_PROMETHEUS_ENDPOINT_CONTEXT","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#license","title":"License","text":"<p>By default, DataPlatform is subject to the eccenca free Personal, Evaluation and Development License Agreement (PEDAL), a license intended for non-commercial usage. When your delivery includes a dedicated license file, you have to configure DataPlatform to enable your license. To change the default configuration, you have several options. If the properties under license are not provided the default license included (PEDAL) is used.</p> <p>In case a dedicated license file is used, different configuration options can overwrite each other. The license is read in the following sequence:</p> <ol> <li>license.key property</li> <li>license.file property</li> <li>license.asc file in the same folder, where the application is started from (in Standalone Mode)</li> <li>Fallback to eccenca free Personal, Evaluation and Development License Agreement (PEDAL)</li> </ol> <p>Property: license.key</p> <p>Use this property to specify the license key as a YAML multiline string value of the license.key property.</p> <pre><code>key: |\n    -----BEGIN PGP MESSAGE-----\n    ...\n    ...\n    -----END PGP MESSAGE-----\n</code></pre> Category Value Default none Required false Valid values PGP Key (Message) Conflicts with license.file Environment LICENSE_KEY <p>Property: license.file</p> <p>Use this property to specify the location of the license file</p> Category Value Default none Required false Valid values location of the license file Conflicts with license.key Environment LICENSE_FILE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#general-platform-settings-for-dataplatform","title":"General platform settings for DataPlatform","text":"<p>This section provides general configuration settings.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#configuration-of-caching","title":"Configuration of Caching","text":"<p>DataPlatform provides caching support which is enabled by default with an in-memory Caffeine cache.</p> <p>Property: spring.cache.type</p> <p>Use this property to define the type of cache to use. The default type (INFINISPAN) provides a cache based on infinispan which can be further configured under the custom properties \u201cspring.cache.infinispan\u201d</p> <p>To disable caching, set the type to NONE (not recommended).</p> Category Value Default INFINISPAN Required true Valid values INFINISPAN, NONE Environment SPRING_CACHE_TYPE <p>Property: spring.cache.infinispan.mode</p> Category Value Default LOCAL Required false Valid values string Environment SPRING_CACHE_INFINISPAN_MODE <p>Property: spring.mvc.pathmatch.matching-strategy</p> Category Value Default ant_path_matcher Required false Valid values string Environment SPRING_MVC_PATHMATCH_MATCHING_STRATEGY <p>Property: spring.thymeleaf.prefix</p> Category Value Default classpath:/public/ Required false Valid values string Environment SPRING_THYMELEAF_PREFIX <p>Property: spring.thymeleaf.mode</p> Category Value Default HTML Required false Valid values string Environment SPRING_THYMELEAF_MODE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#configuration-of-servlet-container","title":"Configuration of Servlet Container","text":"<p>Multipart upload limits config You may need to set the following parameter values to 2048MB for implementations that cannot handle large requests</p> <p>Property: spring.servlet.multipart.max-file-size</p> <p>Use this property to define the maximum size of an uploaded file in number of bytes. Values can use the suffixed \u201cMB\u201d or \u201cKB\u201d (e.g. \u20181024MB\u2019).</p> <p>Note: If DataPlatform is deployed in a Servlet container, make sure to also configure support for large file sizes.</p> Category Value Default 4096MB Required false Valid values string Environment SPRING_SERVLET_MULTIPART_MAX_FILE_SIZE <p>Property: spring.servlet.multipart.max-request-size</p> <p>Use this property to define the maximum size of HTTP request in number of bytes. Values can use the suffixed \u201cMB\u201d or \u201cKB\u201d (e.g. \u20181024MB\u2019).</p> Category Value Default 4096MB Required false Valid values string Environment SPRING_SERVLET_MULTIPART_MAX_REQUEST_SIZE <p>Property: spring.servlet.multipart.location</p> <p>Temporary storage used for multipart upload. This defaults to system property java.io.tmpdir.</p> Category Value Default none Required false Valid values string Environment SPRING_SERVLET_MULTIPART_LOCATION <p>Property: spring.jackson.default-property-inclusion</p> Category Value Default non_null Required false Valid values string Environment SPRING_JACKSON_DEFAULT_PROPERTY_INCLUSION <p>Property: management.info.env.enabled</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_INFO_ENV_ENABLED <p>Property: management.endpoints.web.base-path</p> Category Value Default /dataplatform/actuator Required false Valid values string Environment MANAGEMENT_ENDPOINTS_WEB_BASE_PATH <p>Property: management.endpoints.web.exposure.include</p> Category Value Default * Required false Valid values string Environment MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE <p>Property: management.endpoints.enabled-by-default</p> Category Value Default false Required false Valid values string Environment MANAGEMENT_ENDPOINTS_ENABLED_BY_DEFAULT <p>Property: management.endpoint.health.enabled</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_ENDPOINT_HEALTH_ENABLED <p>Property: management.endpoint.health.show-details</p> Category Value Default when_authorized Required false Valid values string Environment MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS <p>Property: management.endpoint.info.enabled</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_ENDPOINT_INFO_ENABLED <p>Property: management.health.diskspace.enabled</p> Category Value Default false Required false Valid values string Environment MANAGEMENT_HEALTH_DISKSPACE_ENABLED <p>Property: management.health.livenessstate.enabled</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_HEALTH_LIVENESSSTATE_ENABLED <p>Property: management.health.readinessstate.enabled</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_HEALTH_READINESSSTATE_ENABLED <p>Property: management.health.sparql.enabled</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_HEALTH_SPARQL_ENABLED <p>Property: management.health.sparql.fixedDelayInMilliseconds</p> Category Value Default 5000 Required false Valid values string Environment MANAGEMENT_HEALTH_SPARQL_FIXEDDELAYINMILLISECONDS <p>Property: management.health.sparql.timeoutInMilliseconds</p> Category Value Default 5000 Required false Valid values string Environment MANAGEMENT_HEALTH_SPARQL_TIMEOUTINMILLISECONDS <p>Property: management.influx.metrics.export.enabled</p> Category Value Default false Required false Valid values string Environment MANAGEMENT_INFLUX_METRICS_EXPORT_ENABLED <p>Activate Micrometer Tracing capability with Brave.</p> <p>Property: management.tracing.enabled</p> <p>Whether tracing is enabled. If not then IDs for i.e. queries are generated via UUID mechanism. Backend store \u201cneptune\u201d is not compatible with tracing enabled.</p> Category Value Default true Required false Valid values string Environment MANAGEMENT_TRACING_ENABLED","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#openapi-specification-and-swagger-ui","title":"OpenAPI Specification and Swagger UI","text":"<p>You can activate endpoints to expose an OpenAPI compliant specification of the available DataPlatform APIs. Developers can make use of this information to understand the API and to bootstrap client integration code.</p> <p>The servers URLs can be customized by setting the environment variable OPENAPI_SERVER_URLS on the machine or in the docker container that runs DataManager:</p> <pre><code>export OPENAPI_SERVER_URLS=\"https://my-custom.domain.com:443/dataplatform\"\n</code></pre> <p>Configuration example:</p> <pre><code>springdoc:\n  swagger-ui:\n   enabled: true\n  api-docs:\n   enabled: true\n</code></pre> <p>Property: springdoc.api-docs.enabled</p> <p>Use this property to enable and expose endpoint that provide the OpenAPI compliant specification of the DataPlatform APIs. The following endpoints will become available when this option is set to true:</p> <ul> <li>/v3/api-docs <li>/v3/api-docs.yaml <li>/v3/api-docs/swagger-config Category Value Default false Required false Valid values boolean Environment SPRINGDOC_API_DOCS_ENABLED <p>Property: springdoc.swagger-ui.enabled</p> <p>Use this property to enable and expose a Swagger UI browser interface that can be used to explore and interact with the APIs. The following endpoints will become available when this option is set to true:</p> <ul> <li>/swagger-ui.html Category Value Default false Required false Valid values boolean Environment SPRINGDOC_SWAGGER_UI_ENABLED","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#cross-origin-resource-sharing-cors","title":"Cross-origin resource sharing (CORS)","text":"<p>DataPlatform supports Cross-origin resource sharing (CORS).</p> <p>Configuration example:</p> <pre><code>http:\n  cors:\n    allowedOrigins:\n    - http://example.org\n    - https://example.com\n</code></pre> <p>Property: http.cors.allowedOrigins</p> <p>Use this property to define the list of allowed origins. The values must be either specific origins, e.g. http://example.org, or * for all origins.</p> Category Value Default [*] Required false Valid values list of strings Environment HTTP_CORS_ALLOWEDORIGINS <p>Property: http.cors.allowedMethods</p> <p>Use this property to define the list of allowed HTTP methods. The special value * allows all methods.</p> Category Value Default [OPTIONS, HEAD, GET, POST, PUT, DELETE, PATCH] Required false Valid values list of strings (of OPTIONS, HEAD, GET, POST,  PUT, DELETE, PATCH) Environment HTTP_CORS_ALLOWEDMETHODS <p>Property: http.cors.allowedHeaders</p> <p>Use this property to define the list of allowed HTTP headers. The special value * may be used to allow all headers.</p> Category Value Default [Authorization, X-Requested-With, Content-Type, Content-Length, ETag] Required false Valid values list of strings Environment HTTP_CORS_ALLOWEDHEADERS <p>Property: http.cors.exposedHeaders</p> <p>Use this property to define the list of headers that an actual response might have and can be exposed.</p> Category Value Default [WWW-Authenticate, Link, ETag] Required false Valid values list of strings Environment HTTP_CORS_EXPOSEDHEADERS <p>Property: http.cors.allowCredentials</p> <p>Use this property to define whether the browser should send credentials, such as cookies along with cross domain requests.</p> Category Value Default false Required false Valid values boolean Environment HTTP_CORS_ALLOWCREDENTIALS <p>Property: http.cors.maxAge</p> <p>Use this property to define how long in seconds the response from a pre-flight request can be cached by clients.</p> Category Value Default 3600 Required false Valid values non-negative integer Environment HTTP_CORS_MAXAGE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#http-client-settings","title":"HTTP client settings","text":"<p>Java 11 HTTP client settings for HTTP access to the backend store.</p> <p>Property: httpclient.connectionPoolSize</p> <p>The maximum number of connections to keep in the HTTP/1.1 keep alive cache. A value of 0 means that the cache is unbounded</p> Category Value Default 200 Required false Valid values string Environment HTTPCLIENT_CONNECTIONPOOLSIZE <p>Property: httpclient.keepalive.timeout</p> <p>The number of seconds to keep idle HTTP/1.1 connections alive in the keep alive cache</p> Category Value Default 1200 Required false Valid values string Environment HTTPCLIENT_KEEPALIVE_TIMEOUT","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#authorization","title":"Authorization","text":"<p>DataPlatform supports authorization of RDF named graphs and actions. Authorization for clients and/or users is specified by the access conditions model which is described in section Access conditions. You can configure root access for a specific group of users who are given unrestricted access regardless of the defined access conditions. Refer to section Root Access for more information.</p> <p>Property: authorization.rootAccess</p> <p>Use this property to enable or disable root access. DataPlatform allows root access for a specific administrator group (see property authorization.abox.adminGroup). You can toggle root access using the property authorization.rootAccess. Regardless of the access conditions declared in the access conditions model (see Access conditions), all members of the administrator group are permitted to read and write all graphs of all endpoints and are allowed to perform all actions.</p> <p>For example, the following configuration grants root access to any user in the group admins:</p> <pre><code>authorization:\n  rootAccess: true\n  abox:\n    adminGroup: admins\n</code></pre> Category Value Default true Required false Valid values boolean Environment AUTHORIZATION_ROOTACCESS <p>Use the following configuration options to specify options for collecting user information</p> <p>Property: authorization.userInfoGraph.active</p> <p>Use this property to enable/disable collection of user information of logged-in users</p> Category Value Default true Required false Valid values string Environment AUTHORIZATION_USERINFOGRAPH_ACTIVE <p>Property: authorization.userInfoGraph.ignored-account-names</p> <p>Logins of the following account names are not collected</p> Category Value Default [service-account-cmem-service-account] Required false Valid values string Environment AUTHORIZATION_USERINFOGRAPH_IGNORED_ACCOUNT_NAMES <p>Use the following configuration options to specify values used by DataPlatform when working with RDF data, such as default URIs and prefixes.</p> <p>Property: authorization.abox.adminGroup</p> <p>Use this property to configure the group that gets root access if enabled (see section Root access).</p> Category Value Default elds-admins Required false Valid values string Environment AUTHORIZATION_ABOX_ADMINGROUP <p>Property: authorization.abox.publicGroup</p> <p>Use this property to configure the URI of the public user group (see section Public access). Note: If you change this property, you also need to change existing URI descriptions and existing access conditions.</p> Category Value Default https://vocab.eccenca.com/auth/PublicGroup Required false Valid values string Environment AUTHORIZATION_ABOX_PUBLICGROUP <p>Property: authorization.abox.anonymousUser</p> <p>Use this property to configure the URI of the public user (see section Public access). Note: If you change this property, you also need to change existing URI descriptions and existing access conditions.</p> Category Value Default https://vocab.eccenca.com/auth/AnonymousUser Required false Valid values string Environment AUTHORIZATION_ABOX_ANONYMOUSUSER","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#access-conditions","title":"Access conditions","text":"<p>IMPORTANT: The following properties are deprecated and have no function anymore!</p> <p>Property: authorization.abox.accessConditions.url</p> <p>DEPRECATED Use this property to set the URL of the access conditions model file. This can be either a remote (http://\u2026) or a local (file:\u2026) .rdf file. Refer to section Access conditions for more information on the access conditions model.</p> Category Value Default none Required false Valid values string Environment AUTHORIZATION_ABOX_ACCESSCONDITIONS_URL <p>Property: authorization.abox.accessConditions.graph</p> <p>DEPRECATED Use this property to set the graph containing the access conditions model. Note: If you change this property, you also need to change the corresponding shape definitions for access conditions (more precisely, the UI SPARQL queries).</p> Category Value Default https://ns.eccenca.com/data/ac/ Required false Valid values string Conflicts with url Environment AUTHORIZATION_ABOX_ACCESSCONDITIONS_GRAPH","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#sparql-endpoints","title":"SPARQL endpoints","text":"<p>SPARQL endpoints declare how DataPlatform connects to a SPARQL-capable store or service. This includes stores that are capable of reading and writing RDF such as Virtuoso as well as read-only services like remote SPARQL HTTP endpoints (e.g. DBpedia).</p> <p>With the default configuration, DataPlatform uses an in-memory database. This means, that no persistent storage is available, unless a store supporting data persistence is configured.</p> <p>The following example showcases a setup in which for each Resource all rdfs:label, Literals with language es, then en and in the end those without a language are evaluated. If nothing matches here, skos:prefLabel is examined in the same way</p> <pre><code>proxy:\n  endpointIds:\n    - my_stardog\n  labelProperties:\n    - \"http://www.w3.org/2000/01/rdf-schema#label\"\n    - \"http://www.w3.org/2004/02/skos/core#prefLabel\"\n  languagePreferences:\n    - \"es\"\n    - \"en\"\n    - \"\"\n</code></pre> <p>Property: proxy.defaultBaseIri</p> <p>Base IRI for this Corporate Memory instance. If not set falls back to environment variable DEPLOY_BASE_URL, further fallback to https://fallback.eccenca.com/</p> Category Value Default https://fallback.eccenca.com/ Required false Valid values URI Environment PROXY_DEFAULTBASEIRI <p>Property: proxy.labelProperties</p> <p>Use this property to specify which RDF properties should be used to provide label values when matching IRIs against a search term during rewriting SELECT-queries. Note: This configuration property affects modification of SELECT-queries for search triggered by the search-string query parameter. Results of SELECT-queries when the resolveLabels property is set to LABELS</p> Category Value Default [http://www.w3.org/2004/02/skos/core#prefLabel, http://www.w3.org/2000/01/rdf-schema#label, http://purl.org/dc/terms/title, http://www.w3.org/ns/shacl#name] Required false Valid values list of Properties Environment PROXY_LABELPROPERTIES <p>Property: proxy.descriptionProperties</p> <p>Use this property to specify which RDF properties should be used to provide description values when matching IRIs against a search term during rewriting SELECT-queries. Note: This configuration property affects modification of SELECT-queries for search triggered by the search-string query parameter. Results of SELECT-queries when the resolveLabels property is set to LABELS</p> Category Value Default [http://purl.org/dc/terms/description, http://www.w3.org/2000/01/rdf-schema#comment] Required false Valid values list of Properties Environment PROXY_DESCRIPTIONPROPERTIES <p>Property: proxy.languagePreferences</p> <p>Specifies base language preferences for this instance.</p> <p>Note: This configuration property affects results of SELECT-queries when the resolveLabels property is set to LABELS.</p> Category Value Default [en, , de, fr] Required false Valid values string Environment PROXY_LANGUAGEPREFERENCES <p>Property: proxy.languagePreferencesAnyLangFallback</p> <p>Allows the fallback to ignoring the languagePreferences, in case none of the configured match the data.</p> Category Value Default true Required false Valid values string Environment PROXY_LANGUAGEPREFERENCESANYLANGFALLBACK <p>Property: proxy.maxCBDDepth</p> <p>The Concise Boundary Description is used for viewing and editing resoures. By default up to a max of 5 Blank nodes are traversed for calculation. Increasing the max fetch will support deeper constructs, but will also add to loading time.</p> Category Value Default 5 Required false Valid values string Environment PROXY_MAXCBDDEPTH <p>Property: proxy.maxCBDStatements</p> <p>The max amount of statements which the Concise Bound Description can contain. (S)CBDs surpassing this will not load but return an error</p> Category Value Default 1000000 Required false Valid values string Environment PROXY_MAXCBDSTATEMENTS <p>Property: proxy.shapedMaxValueCount</p> <p>Maximum Values for shaped Resources When a resource is shaped by shacl forms, shapedMaxValueCount limits the number of values returned per <code>shacl:PropertyShape</code>. The default needs to be larger than the DataManager setting for for \u2018propertyLimit\u2019, which is up to 25. Changing this value allows custom endpoints to fetch more data. Increasing this value will increase response time</p> Category Value Default 26 Required false Valid values string Environment PROXY_SHAPEDMAXVALUECOUNT <p>Property: proxy.cacheExpiration</p> <p>Cache Expiration - Caches in DataPlatform have a default expiration time which can be set</p> Category Value Default PT30M Required false Valid values ISO 8601 duration format string i.e. PT30M, PT1D Environment PROXY_CACHEEXPIRATION <p>Property: proxy.cacheSelectiveInvalidation</p> <p>Indicates whether the DataPlatform caches should selectively invalidate based upon the result of the done operations (insofar as determinable) or not</p> Category Value Default true Required false Valid values boolean Environment PROXY_CACHESELECTIVEINVALIDATION <p>Property: proxy.queryMonitorMaxMemoryInMb</p> <p>Maximum amount of memory entries in the query monitor can take up.</p> Category Value Default 30 Required false Valid values Value in MB Environment PROXY_QUERYMONITORMAXMEMORYINMB <p>Property: proxy.shaclBatchResultsMemoryBoundaryInMb</p> <p>Maximum amount of memory entries for shacl batch validation results can take up.</p> Category Value Default 100 Required false Valid values Value in MB Environment PROXY_SHACLBATCHRESULTSMEMORYBOUNDARYINMB <p>Property: proxy.fetchValuesStrategy</p> <p>Value Fetch Strategy Determines how the Knowledge Graph is walked for values for specific resources. Used for resolving titles &amp; comments and loading shaped resources. - RESOURCE_IN_VALUES uses a SPARQL <code>VALUES (?resource ) { (:resource1)(:resource2)}</code> - FILTER_ONLY Uses SPARQL uses a SPARQL <code>FILTER (?resource in (:resource1, :resource2))</code></p> Category Value Default RESOURCE_IN_VALUES Required false Valid values RESOURCE_IN_VALUES, FILTER_ONLY Environment PROXY_FETCHVALUESSTRATEGY <p>Property: proxy.gspUploadGzipContentLimit</p> <p>The limit of data for the GSP zip-bomb check in bytes. If this limit is exceeded the upload is aborted</p> Category Value Default 5368709120 Required false Valid values string Environment PROXY_GSPUPLOADGZIPCONTENTLIMIT <p>Property: proxy.proxy-sparql-streaming-format</p> <p>The format in which internally SPARQL results are fetched from the store. For streaming either JSON or XML</p> Category Value Default xml Required false Valid values JSON,XML Environment PROXY_PROXY_SPARQL_STREAMING_FORMAT","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#llm-assistant-supported","title":"LLM Assistant Supported","text":"<p>Leverage generative AI (LLMs) for ontology creation and exploration (SPARQL query generation).</p> <p>Property: assist.enabled</p> <p>Activate LLM Assistant features.</p> Category Value Default false Required false Valid values boolean Environment ASSIST_ENABLED <p>Property: assist.openAiApiKey</p> <p>Your OpenAI API key to use.</p> Category Value Default none Required true Valid values string Environment ASSIST_OPENAIAPIKEY <p>Property: assist.baseUrl</p> <p>The access token URL for the LLM API. Used to configure an openAI API compatible alternative service (e.g. OpenAI in MS Azure). If empty OpenAI API will be used.</p> Category Value Default none Required false Valid values string Environment ASSIST_BASEURL <p>Property: assist.chatModel</p> <p>The name of the model to use for chat interactions. Defaults to \u201cgpt-4o-2024-08-06\u201d. Use a model fine tuned for function calling.</p> Category Value Default gpt-4o-2024-08-06 Required false Valid values string Environment ASSIST_CHATMODEL <p>Property: assist.embeddingModel</p> <p>The name of the model to use for retrieving embeddings. Defaults to \u201ctext-embedding-3-large\u201d.</p> Category Value Default text-embedding-3-large Required false Valid values string Environment ASSIST_EMBEDDINGMODEL","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#syncing-graph-via-git-repositories","title":"Syncing graph via git repositories","text":"<p>DataPlatform can sync graphs between git repositories and the backend store. Changes of graphs in the backend are transferred to the git repository on each update / write of the graph. Changes of the graph in the git repository are synchronized to the store on a scheduled basis.</p> <p>Only HTTP git repositories with basic authentication can be used. A local public bare repository reachable from DataPlatform can be used in the DataPlatform configuration (for testing purposes).</p> <p>For details how to provide the correct git authentication refer to https://www.codeaffine.com/2014/12/09/jgit-authentication/.</p> <p>Note</p> <p>All properties need to be written as camel case (e.g. \u201cgitSync\u201d), hyphens as separators must not be used.</p> <p>An example git DataPlatform configuration using a gitlab git repository looks like:</p> <pre><code>gitSync:\n  enabled: true\n  remoteUrl: https://gitlab-ci-token:abcMyCiTokenxy5@gitlab.example.com/username/gitsync.git\n  user: username\n  password: abcMyCiTokenxy5\n  branch: master\n  scheduledPullCron: \"0 */5 * * * *\"\n</code></pre> <p>Property: gitSync.enabled</p> <p>Activates / Deactivates git graph sync feature</p> Category Value Default false Required false Valid values boolean Environment GITSYNC_ENABLED <p>Property: gitSync.dataFolder</p> <p>The folder inside the repositories where Corporate Memory places the synchronized files</p> Category Value Default data Required false Valid values string Environment GITSYNC_DATAFOLDER <p>Property: gitSync.remoteUrl</p> <p>A remote git repository (http, local) - configured http repositories in graph configuration take precedence over this</p> Category Value Default none Required false Valid values HTTP or local repository which can be reached from DataPlatform Environment GITSYNC_REMOTEURL <p>Property: gitSync.branch</p> <p>The main branch on which the git sync takes place - the sync may create new branches on conflict. The branch must exist before using the feature.</p> Category Value Default main Required false Valid values An existing branch in the repository Environment GITSYNC_BRANCH <p>Property: gitSync.user</p> <p>The git username for simple user/password authentification - may be empty for local repository (s. remoteUrl) w/o authentification</p> Category Value Default none Required false Valid values Existing git repository user Environment GITSYNC_USER <p>Property: gitSync.password</p> <p>The git password for simple user/password authentification - may be empty for local repository (s. remoteUrl) w/o authentification</p> Category Value Default none Required false Valid values Existing git repository password Environment GITSYNC_PASSWORD <p>Property: gitSync.committerName</p> <p>The committer name which appears in the commit message on system commits</p> Category Value Default eccenca DataPlatform Required false Valid values string Environment GITSYNC_COMMITTERNAME <p>Property: gitSync.committerEmail</p> <p>The committer email which appears in the commit message  on system commits</p> Category Value Default info@eccenca.com Required false Valid values string Environment GITSYNC_COMMITTEREMAIL <p>Property: gitSync.scheduledPullCron</p> <p>Schedules Pull Frequency - Configured git repositories for sync are pulled regularly to check for external updates of synchronized graphs. This setting sets the frequency of the pull.</p> Category Value Default 0 */30 * * * * Required false Valid values Cron setting according to https://docs.spring.io/spring-framework/docs/current/reference/html/integration.html#scheduling-cron-expression Environment GITSYNC_SCHEDULEDPULLCRON","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#application-logging","title":"Application logging","text":"<p>By default, DataPlatform only logs to the console. You can change the log level or configure logging into a file.</p> <p>There are multiple levels of logging you can choose from that are explained in the table below.</p> <p>Available log levels are: TRACE, DEBUG, INFO, WARN, ERROR, FATAL and OFF. The default root log level is WARN. It is also possible to set the log level per package. Per default only console output is activated. To enable file output, specify a log file (auto-rotating, 10Mb file size). Possible log settings for specific modules: Query Logging: com.eccenca.elds.backend.sparql.query.logging: DEBUG</p> <p>The levels can also be configured on runtime via the loggers HTTP endpoint as described in section Application loggers of the Developer Manual.</p> <pre><code>logging:\n  level:\n    root: WARN\n    com.eccenca.elds.backend: DEBUG\n    org.springframework: INFO\n  file: /var/logs/dataplatform.log\n</code></pre> <p>Use these properties to specify where you want to store your logging file. Specifying a file leads to both, logging to standard output and the file. File output creates an auto-rotating file with 10 MB file size each.</p> <p>Property: logging.file.name</p> <p>Log file name (for instance, <code>myapp.log</code>). Names can be an exact location or relative to the current directory.</p> Category Value Default none Required false Valid values string (file name) - empty to disable file output Environment LOGGING_FILE_NAME <p>Property: logging.file.path</p> <p>Location of the log file. For instance, <code>/var/log</code>.</p> Category Value Default none Required false Valid values string (file path) - empty to disable file output Environment LOGGING_FILE_PATH <p>Property: logging.config</p> <p>Logging for DataPlatform can also be configured with Logback, which, for example, allows a more granular control on file rolling strategies. For further information on configuration options, refer to the Logback\u2019s Configuration manual section and the Spring Boot\u2019s Configure Logback for Logging manual section.</p> <p>Use this property to specify where the Logback configuration is located.</p> <pre><code>logging:\n  configuration: ELDS_HOME/etc/dataplatform/logback.xml\n</code></pre> Category Value Default none Required false Valid values string (file path) Environment LOGGING_CONFIG <p>Property: logging.level.audit</p> Category Value Default INFO Required false Valid values string Environment LOGGING_LEVEL_AUDIT <p>Property: logging.level.com.eccenca.elds.backend</p> Category Value Default INFO Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND <p>Property: logging.level.org.springframework</p> Category Value Default WARN Required false Valid values string Environment LOGGING_LEVEL_ORG_SPRINGFRAMEWORK <p>Property: logging.level.com.eccenca.elds.backend.webapp.web.filter.SimpleCorsFilter</p> Category Value Default WARN Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_WEBAPP_WEB_FILTER_SIMPLECORSFILTER <p>Property: logging.level.com.eccenca.elds.backend.webapp.web.GlobalControllerExceptionHandler</p> Category Value Default TRACE Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_WEBAPP_WEB_GLOBALCONTROLLEREXCEPTIONHANDLER <p>Property: logging.level.com.eccenca.elds.backend.cache.logging</p> Category Value Default WARN Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_CACHE_LOGGING <p>Property: logging.level.org.hibernate.search.backend.lucene.impl</p> Category Value Default ERROR Required false Valid values string Environment LOGGING_LEVEL_ORG_HIBERNATE_SEARCH_BACKEND_LUCENE_IMPL","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#audit-trail-logging","title":"Audit trail logging","text":"<p>DataPlatform is able to log the access of each user to named graphs in form of an audit trail log under the logger name audit.</p> <pre><code>auditTrail:\n  enabled: true\n  auditedGraphs:\n  - \"example.org/data\"\n  - \"aksw.org\"\n</code></pre> <p>Property: audit-trail.enabled</p> <p>Use this property to enable logging of read and write access to every graph access. If auditTrail.auditedGraphs is specified, only those graphs are logged. Note: If audit trail logging is enabled, RDF upload over the Graph Store Protocol interface is limited to triple formats. Any attempt to upload a quad format results in an HTTP 415 error.</p> Category Value Default false Required false Valid values boolean Environment AUDIT_TRAIL_ENABLED <p>Property: audit-trail.auditedGraphs</p> <p>Use this property to specify graphs whose read and write access you want to be logged. Omit this value to log access to all graphs.</p> Category Value Default none Required false Valid values List of graph IRIs Environment AUDIT_TRAIL_AUDITEDGRAPHS <p>Limits the size of the query response</p> <p>Property: sparql.query.limit</p> Category Value Default 100000 Required false Valid values string Environment SPARQL_QUERY_LIMIT","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#embedded-tomcat","title":"Embedded Tomcat","text":"<p>The URL under which DataPlatform is accessible has the following form: PROTOCOL://HOST:PORT/CONTEXT_PATH where:</p> <pre><code>- PROTOCOL: http or https depending on SSL configuration (see section SSL support)\n- HOST: The hostname pointing to the server where DataPlatform is installed\n- PORT: The TCP port where the embedded server is available (see the property server.port)\n- CONTEXT_PATH: The context path under which DataPlatform is available (see the property server.servlet.contextPath)\n</code></pre> <pre><code>server:\n  port: 9090\n  servlet:\n    contextPath: /dataplatform\n</code></pre> <p>Property: server.port</p> <p>Use this property to set the TCP port where the embedded server is available.</p> Category Value Default 9090 Required false Valid values integer Environment SERVER_PORT <p>Property: server.error.include-stacktrace</p> Category Value Default NEVER Required false Valid values string Environment SERVER_ERROR_INCLUDE_STACKTRACE <p>Property: server.servlet.contextPath</p> <p>Use this property to define the context path under which DataPlatform is available. If this property is provided, use a leading slash.</p> Category Value Default none Required false Valid values string Environment SERVER_SERVLET_CONTEXTPATH <p>Tomcat servlet settings</p> <p>Property: server.servlet.session.cookie.same-site</p> Category Value Default Lax Required false Valid values string Environment SERVER_SERVLET_SESSION_COOKIE_SAME_SITE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#https-support-for-standalone-mode","title":"HTTPS support for standalone mode","text":"<p>If DataPlatform is executed in standalone mode (see Standalone), the embedded servlet container can be configured to support one-way (server certification) or two-way (server and client certification) SSL. A KeyStore is required for one-way SSL and both a KeyStore as well as a TrustStore are required for two-way SSL.</p> <p>Refer to the Oracle documentation to see how to create KeyStore and TrustStore files.</p> <p>Configuration example:</p> <pre><code>server:\n  ssl:\n    key-store: ./key-store.jks\n    key-store-password: jks-password\n    client-auth: NEED\n</code></pre> <p>Property: server.ssl.key-store</p> <p>Use this property to define the path to the KeyStore used for one-way or two-way SSL authentication.</p> <p>In case of two-way authentication, a TrustStore must also be configured. This configuration must be provided as Java system properties either directly in the execution command or as part of the JAVA_TOOL_OPTIONS environment variable, e.g.:</p> <pre><code>JAVA_TOOL_OPTIONS=-Djavax.net.ssl.trustStore=path_to_trust_store.jks -Djavax.net.ssl.trustStorePassword=trust_store_password (ADD TO EXISTING JAVA_TOOL_OPTIONS)\n</code></pre> Category Value Default none Required false Valid values string Environment SERVER_SSL_KEY_STORE <p>Property: server.ssl.key-store-password</p> <p>Use this property to set the password to unlock the KeyStore used for one-way or two-way SSL authentication.</p> Category Value Default none Required false Valid values string Environment SERVER_SSL_KEY_STORE_PASSWORD <p>Property: server.ssl.client-auth</p> <p>Use this property to define the client identification policy.</p> <p>If WANT is set, client identification is optional. If NEED is set, client identification is mandatory, so unauthenticated clients are refused.</p> Category Value Default none Required false Valid values none, NEED, WANT Environment SERVER_SSL_CLIENT_AUTH","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#https-support-for-proxy-deployment","title":"HTTPS support for proxy deployment","text":"<p>If DataPlatform is running behind a proxy server (e.g. Apache) then you must use all of the following properties to enforce HTTPS.</p> <p>Configuration recommendation:</p> <pre><code>  server:\n    tomcat:\n     remoteIpHeader: x-forwarded-for\n     protocolHeader: x-forwarded-proto\n\n    security:\n      requireSsl: true\n</code></pre> <p>Note: This configuration recommendation provides settings for headers most commonly used by proxies. Make sure to add all three properties in order to enforce HTTPS.</p> <p>Property: server.tomcat.remoteIpHeader</p> <p>Use this property to set the request header which is required to identify the originating IP address of the client connecting to DataPlatform through an HTTP proxy.</p> Category Value Default none Required false Valid values string Environment SERVER_TOMCAT_REMOTEIPHEADER <p>Property: server.tomcat.protocolHeader</p> <p>Use this property to set the request header which is required to identify the originating protocol of an HTTP request through an HTTP proxy.</p> Category Value Default none Required false Valid values string Environment SERVER_TOMCAT_PROTOCOLHEADER <p>Property: server.tomcat.max-swallow-size</p> Category Value Default -1 Required false Valid values string Environment SERVER_TOMCAT_MAX_SWALLOW_SIZE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#scheduler-for-asynchronous-operations","title":"Scheduler for asynchronous operations","text":"<p>Schedulers Configuration (Thread Pools) for asynchronous operations like background file uploads</p> <p>Property: scheduler.bulkLoadPoolSize</p> <p>Bulk upload Pool Size - Limits how many (bulk/large) uploads via GSP / bulk load can be run in parallel in file upload.</p> Category Value Default 1 Required false Valid values integer Environment SCHEDULER_BULKLOADPOOLSIZE <p>Property: scheduler.analyticalPoolSize</p> <p>Limits how many analytical requests can be run in parallel. Analytical requests  can have longer runtimes than retrieval requests.</p> Category Value Default 10 Required false Valid values integer Environment SCHEDULER_ANALYTICALPOOLSIZE <p>Property: scheduler.backgroundQueryPoolSize</p> <p>Limits how many background query requests can be run in parallel. This applies to scheduled processes which query the triple store</p> Category Value Default 4 Required false Valid values integer Environment SCHEDULER_BACKGROUNDQUERYPOOLSIZE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#asynchronous-file-uploads","title":"Asynchronous file uploads","text":"<p>Files can be asynchronously uploaded to the backend store in multiple steps which include an analysis of the uploaded file. Please s. API documentation under /api/upload/ for further information.</p> <p>Property: files.maxStorageSingleFileSizeMb</p> <p>Maximum size of one stored file (as uploaded i.e. can also be compressed size) Value in Mb</p> Category Value Default 3000 Required false Valid values string Environment FILES_MAXSTORAGESINGLEFILESIZEMB <p>Property: files.minStorageTempSpaceLeftMb</p> <p>Minimum storage space left on temp device of DataPlatform for file uploads Value in Mb</p> Category Value Default 3000 Required false Valid values string Environment FILES_MINSTORAGETEMPSPACELEFTMB <p>Property: files.maintenanceExpirationDuration</p> <p>Cron setting for housekeeping / maintenance job Stored files and saved analysis will be deleted if older than maintenanceExpirationDuration</p> Category Value Default P1D Required false Valid values string Environment FILES_MAINTENANCEEXPIRATIONDURATION <p>Property: files.storageDirectory</p> <p>A folder where the storage service places the files for later analysis and upload to the store. The folder is cleansed regularly. A temp folder (java.io.tmpdir) is created and used if not set.</p> Category Value Default none Required false Valid values string Environment FILES_STORAGEDIRECTORY","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-full/#store-configuration","title":"Store configuration","text":"<p>Store properties for connecting to a triple store backend. Please see specific sections in documentation for each backend.</p> <p>Property: store.type</p> <p>One of the supported types of backends DataPlatform can connect to</p> Category Value Default none Required true Valid values MEMORY, HTTP, GRAPHDB, VIRTUOSO, NEPTUNE Environment STORE_TYPE <p>Property: store.owlImportsResolution</p> <p>Use this property to enable OWL imports resolution</p> Category Value Default true Required false Valid values string Environment STORE_OWLIMPORTSRESOLUTION <p>Property: store.authorization</p> <p>Strategies to realize authorization for an RDF endpoint</p> Category Value Default none Required false Valid values NONE, REWRITE_FROM Environment STORE_AUTHORIZATION <p>Property: store.queryTimeoutGeneral</p> <p>Query timeout as duration which is active if no timeout in request has been set</p> Category Value Default PT1H Required false Valid values ISO 8601 duration format Environment STORE_QUERYTIMEOUTGENERAL","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-graphdb-full/","title":"GraphDB","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-graphdb-full/#configuration-for-connecting-to-graphdb-backend","title":"Configuration for connecting to GraphDB backend","text":"<p>Configuration example:</p> <p>This example configures a connection with HTTPS to a remote graphdb store (https://remote:7200) using the workbench import directory which is shared with the GraphDB instance. The repository will be created on startup of CMEM.</p> <pre><code>store:\n  type: graphdb\n  owlImportsResolution: true\n  authorization: REWRITE_FROM\n  graphdb:\n    host: \"remote\"\n    port: 7200\n    ssl-enabled: true\n    repository: \"newRepository\"\n    username: \"admin\"\n    password: \"admin\"\n    importDirectory: \"/shared/mount\"\n    useDirectTransfer: false\n    createRepositoryOnStartup: true\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cgraphdb\u201d</p> Category Value Default graphdb Required true Valid values GRAPHDB Environment STORE_TYPE","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-graphdb-full/#specific-settings-for-graphdb","title":"Specific settings for GraphDB","text":"<p>Property: store.graphdb.host</p> <p>The host of the GraphDB database</p> Category Value Default none Required true Valid values string Environment STORE_GRAPHDB_HOST <p>Property: store.graphdb.port</p> <p>The port of the GraphDB database</p> Category Value Default 7200 Required false Valid values integer Environment STORE_GRAPHDB_PORT <p>Property: store.graphdb.ssl-enabled</p> <p>Whether SSL is enabled or not (http vs. https)</p> Category Value Default false Required false Valid values boolean Environment STORE_GRAPHDB_SSL_ENABLED <p>Property: store.graphdb.repository</p> <p>The name of the repository to connect to</p> Category Value Default cmem Required false Valid values string Environment STORE_GRAPHDB_REPOSITORY <p>Property: store.graphdb.username</p> <p>The user name to connect with</p> Category Value Default user Required true Valid values string Environment STORE_GRAPHDB_USERNAME <p>Property: store.graphdb.password</p> <p>The credentials of the given user</p> Category Value Default password Required true Valid values string Environment STORE_GRAPHDB_PASSWORD <p>Property: store.graphdb.importDirectory</p> <p>Import directory to be utilized in the \u201cworkbench import with shared folder\u201d approach. Not relevant when <code>useDirectTransfer</code> is true. Must be set when <code>useDirectTransfer</code> is false.</p> Category Value Default none Required false Valid values string Environment STORE_GRAPHDB_IMPORTDIRECTORY <p>Property: store.graphdb.useDirectTransfer</p> <p>Set to true to use the native Graph Store API endpoint. Set to false to use the GraphDB workbench import. The import directory must be set then.</p> Category Value Default true Required false Valid values boolean Environment STORE_GRAPHDB_USEDIRECTTRANSFER <p>Property: store.graphdb.create-repository-on-startup</p> <p>Whether to create the given repository on startup if it does not exist</p> Category Value Default true Required false Valid values boolean Environment STORE_GRAPHDB_CREATE_REPOSITORY_ON_STARTUP <p>Property: store.graphdb.gdbBaseIndex</p> <p>The iri of the lucene index to be used for searches. If the default index is used, Explore backend (DataPlatform) syncs the index with the configured <code>proxy.labelProperties</code></p> Category Value Default http://www.ontotext.com/connectors/lucene/instance#cmembaseindex Required false Valid values Valid URI of lucene index Environment STORE_GRAPHDB_GDBBASEINDEX <p>Property: store.graphdb.graphDbChangeTrackingActive</p> <p>Whether to make use of GraphDB change tracking during SPARQL updates (s. https://graphdb.ontotext.com/documentation/10.0/change-tracking.html). This setting is relevant in regards to selectively evicting DP caches depending on the outcome of the SPARQL update s. also <code>proxy.cacheSelectiveInvalidation</code></p> Category Value Default false Required false Valid values boolean Environment STORE_GRAPHDB_GRAPHDBCHANGETRACKINGACTIVE <p>Property: store.graphdb.graphDbChangeTrackingMaxQuadMemory</p> <p>Maximum amount of quads of change tracking result which will be loaded in memory</p> Category Value Default 1000 Required false Valid values int Environment STORE_GRAPHDB_GRAPHDBCHANGETRACKINGMAXQUADMEMORY","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-http-full/","title":"HTTP","text":"","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-http-full/#configuration-for-connecting-to-arbitrary-sparql-http-backend","title":"Configuration for connecting to arbitrary SPARQL HTTP backend","text":"<p>Use the following set of properties to connect to arbitrary HTTP SPARQL services.</p> <p>Configuration example:</p> <pre><code>store:\n  type: http\n  authorization: REWRITE_FROM\n  http:\n    query-endpoint-url: \"http://localhost:7200/repositories/cmem\"\n    update-endpoint-url: \"http://localhost:7200/repositories/cmem/statements\"\n    graph-store-endpoint-url: \"http://localhost:7200/repositories/cmem/rdf-graphs/service\"\n    username: \"user\"\n    password: \"password\"\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201chttp\u201d</p> Category Value Default http Required true Valid values HTTP Environment STORE_TYPE <p>Property: store.authorization</p> Category Value Default REWRITE_FROM Required false Valid values string Environment STORE_AUTHORIZATION <p>Specific settings for HTTP. At least query and update endpoints must be provided.</p> <p>Property: store.http.query-endpoint-url</p> <p>Use this property to configure the endpoint to which SPARQL 1.1 queries are sent.</p> Category Value Default http://localhost:7200/repositories/cmem Required true Valid values string Environment STORE_HTTP_QUERY_ENDPOINT_URL <p>Property: store.http.update-endpoint-url</p> <p>Use this property to configure the endpoint to which SPARQL 1.1 updates are sent.</p> Category Value Default http://localhost:7200/repositories/cmem/statements Required true Valid values string Environment STORE_HTTP_UPDATE_ENDPOINT_URL <p>Property: store.http.graph-store-endpoint-url</p> <p>Use this property to configure the endpoint to SPARQL 1.1 Graph Store Protocol requests are sent.</p> Category Value Default http://localhost:7200/repositories/cmem/rdf-graphs/service Required false Valid values string Environment STORE_HTTP_GRAPH_STORE_ENDPOINT_URL <p>Property: store.http.username</p> <p>Basic authentication is used if this parameter is provided.</p> Category Value Default user Required false Valid values string Environment STORE_HTTP_USERNAME <p>Property: store.http.password</p> <p>Basic authentication is used if this parameter is provided.</p> Category Value Default password Required false Valid values string Environment STORE_HTTP_PASSWORD <p>Property: store.http.graphListQuery</p> <p>Defines how the raw list of graphs is retrieved, and therefore which graphs are visible to the system. Graph must be bound to variable ?g !</p> Category Value Default SELECT distinct ?g {graph ?g {?s ?p ?o}} Required false Valid values Valid SPARQL query with bound variable \u201cg\u201d Environment STORE_HTTP_GRAPHLISTQUERY","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-inmemory-full/","title":"In-Memory","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-inmemory-full/#configuration-for-connecting-to-internal-memory-backend","title":"Configuration for connecting to internal memory backend","text":"<p>You can configure a in-memory SPARQL backend. Based on Jena Models, in-memory backends do not provide persistent storage. Hence, shutting down a DataPlatform configured with an in-memory backend deletes your data and therefore you should use it only for testing purposes.</p> <p>Configuration example:</p> <p>This example configures an in-memory store which initializes with the triples contained in the given file.</p> <pre><code>store:\n  type: memory\n  authorization: REWRITE_FROM\n  memory:\n    files:\n      - \"/data/data.trig\"\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cmemory\u201d</p> Category Value Default memory Required true Valid values MEMORY Environment STORE_TYPE <p>Property: store.authorization</p> Category Value Default REWRITE_FROM Required false Valid values string Environment STORE_AUTHORIZATION <p>Specific settings for in-memory backend</p> <p>Property: store.memory.files</p> <p>list of files in file URI scheme</p> Category Value Default none Required false Valid values A list of files Environment STORE_MEMORY_FILES","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-neptune-full/","title":"Neptune","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-neptune-full/#configuration-for-connecting-to-neptune-backend","title":"Configuration for connecting to Neptune backend","text":"<p>Configuration example:</p> <p>This example configures a connection to a neptune instance in the AWS region eu-central-1. Authentication is enabled so it is assumed that CMEM runs on a EC2 VM with configured role for authentication to neptune. Files (uncompressed) greater than 100MB are uploaded via S3 based bulk loader. The S3 bucket is accessed in this case via an access point which is configured here. The EC2 role CMEM runs under has write access to the bucket. One of the role the neptune cluster runs under is configured in this setting and has read access to the bucket. On bulk load the loading runs parallel in the setting HIGH which causes higher cpu load but better performance.</p> <p>Also the example disables DPs generation of tracing IDs by micrometer tracing - Neptune needs UUIDs as IDs for queries etc.</p> <pre><code>store:\n  type: neptune\n  authorization: REWRITE_FROM\n  neptune:\n    host: \"neptune123.eu-central-1.neptune.amazonaws.com\"\n    port: 8182\n    aws:\n      region: \"eu-central-1\"\n      authEnabled: true\n    s3:\n      bucketNameOrAPAlias: \"nap1-nnipjzugs1ar45n11n316mzagiw6heuc1a-s3alias\"\n      iamRoleArn: \"arn:aws:iam::887770733838:role/NeptuneLoadFromS3\"\n      bulkLoadThresholdInMb: 100\n      bulkLoadParallelism: HIGH\n\nmanagement.tracing.enabled: false\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cneptune\u201d</p> Category Value Default neptune Required true Valid values NEPTUNE Environment STORE_TYPE <p>Configuration of a neptune instance connection.</p> <p>Property: store.neptune.host</p> <p>The name of one of the writer endpoints of the neptune instance.</p> Category Value Default none Required true Valid values string Environment STORE_NEPTUNE_HOST <p>Property: store.neptune.port</p> <p>The port of the neptune instance.</p> Category Value Default 8182 Required false Valid values integer Environment STORE_NEPTUNE_PORT <p>Settings for the connection to the Amazon Cloud</p> <p>Property: store.neptune.aws.region</p> <p>The region where the neptune instance is located i.e. \u201ceu-central-1\u201d s. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions</p> Category Value Default eu-central-1 Required true Valid values One of the AWS regions Environment STORE_NEPTUNE_AWS_REGION <p>Property: store.neptune.aws.authEnabled</p> <p>Whether the neptune instance is configured with enabled IAM authentication. In case of enabled authentication the credentials need to be accessible to the JVM of the dataplatform. Deployment on EC2 and assigning a role to the VM is sufficient. Other ways to achieve this are described in https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html</p> Category Value Default true Required true Valid values boolean Environment STORE_NEPTUNE_AWS_AUTHENABLED <p>Settings for S3 bucket connection and upload of large files to the neptune instance. The neptune store blocks all HTTP requests with size &gt;150MB. To upload larger files a graph file is temporarily stored in a S3 bucket and uploaded via Neptune Bulk Loader. The S3 bucket needs to be in the same region as the neptune cluster. For more information s. https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html. If no S3 upload is necessary then the limit of 150 MB on HTTPS uploads apply for neptune. The whole section can be left out of the configuration.</p> <p>Property: store.neptune.s3.bucketNameOrAPAlias</p> <p>The name of the bucket or access point -&gt; the role CMEM runs under needs write access to the bucket</p> Category Value Default none Required false Valid values string Environment STORE_NEPTUNE_S3_BUCKETNAMEORAPALIAS <p>Property: store.neptune.s3.iamRoleArn</p> <p>The name of the role the neptune loader accesses the bucket -&gt; the role needs read access to the bucket s. https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html</p> Category Value Default none Required false Valid values string Environment STORE_NEPTUNE_S3_IAMROLEARN <p>Property: store.neptune.s3.bulkLoadThresholdInMb</p> <p>The threshold on uncompressed graph data when bulk upload is applied with a maximum / default of 150 MB</p> Category Value Default 150 Required false Valid values string Environment STORE_NEPTUNE_S3_BULKLOADTHRESHOLDINMB <p>Property: store.neptune.s3.bulkLoadParallelism</p> <p>The degree of parallelism (CPU) for the neptune loader, possible values are LOW, MEDIUM, HIGH, OVERSUBSCRIBE, default of HIGH</p> Category Value Default HIGH Required false Valid values LOW, MEDIUM, HIGH, OVERSUBSCRIBE Environment STORE_NEPTUNE_S3_BULKLOADPARALLELISM","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-oauth-full/","title":"OAuth","text":"","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-oauth-full/#authentication","title":"Authentication","text":"<p>Access to DataPlatform resources is restricted using OAuth 2.0.</p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-oauth-full/#configuration-example","title":"Configuration example","text":"<pre><code>spring:\n  security:\n    oauth2:\n      resourceserver:\n        anonymous: true\n        jwt:\n          issuerUri: http://keycloak/auth/realms/cmem\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-oauth-full/#oauth-20-resource-server","title":"OAuth 2.0 Resource Server","text":"<p>In order to protect access to it\u2019s resources, DataPlatform acts as an OAuth 2.0 resource server accepting and responding to a protected resource request using a JSON Web Token (JWT).</p> <p>The OAuth 2.0 specification as well as the JSON Web Token specification don\u2019t define any mandatory claims to be contained in a JWT access token. However, if the property spring.security.oauth2.resourceserver.jwt.issuer-uri is set, the iss (issuer) claim is required to be contained in the JWT. It\u2019s value must be equal to the configured issuer URI. Additionally, in order to identify the requesting principal, either the username claim or the clientId claim must be contained in the JWT.</p> <p>Property: spring.security.oauth2.resourceserver.anonymous</p> <p>Use this property to allow anonymous access to protected resources.</p> Category Value Default false Required false Valid values boolean Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_ANONYMOUS <p>Property: spring.security.oauth2.resourceserver.jwt.issuerUri</p> <p>Use this property to specify the URI that an OpenID Connect Provider asserts as its Issuer Identifier if it supports OpenID Connect discovery. If this property is set, the iss (issuer) claim is required to be contained in the JWT. The value of the claim has to be the same value as the configured issuer URI.</p> <p>Note: If the authorization server is down when DataPlatform queries it (given appropriate timeouts), then startup will fail. Also, if the authorization server doesn\u2019t support the Provider Configuration endpoint, or if DataPlatform must be able to start up independently from the authorization server, use the property jwk-set-uri instead.</p> Category Value Default http://docker.localhost/auth/realms/cmem Required false Valid values URI to OpenID Connect Provider Conflicts with spring.security.oauth2.resourceserver.jwt.jwkSetUri Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_ISSUERURI <p>Property: spring.security.oauth2.resourceserver.jwt.jwkSetUri</p> <p>Use this property to specify the JSON Web Key URI to use to verify the JWT token.</p> Category Value Default none Required false Valid values URI to OpenID Connect Provider Conflicts with spring.security.oauth2.resourceserver.jwt.issuerUri Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWKSETURI <p>Use the following configuration options to specify the claims conveyed by a JWT used to access protected resources of DataPlatform. If nothing is configured, a default configuration is provided with the following configuration</p> <p>Property: spring.security.oauth2.resourceserver.jwt.claims.username</p> <p>Use this property to specify the claim providing the account name of the user accessing a protected resource.</p> Category Value Default preferred_username Required false Valid values string Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_CLAIMS_USERNAME <p>Property: spring.security.oauth2.resourceserver.jwt.claims.groups</p> <p>Use this property to specify the claim identifying the roles (authorities) of the user accessing a protected resource.</p> Category Value Default groups Required false Valid values string Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_CLAIMS_GROUPS <p>Property: spring.security.oauth2.resourceserver.jwt.claims.clientId</p> <p>Use this property to specify the claim providing the OAuth 2.0 client ID to which a token was issued.</p> Category Value Default clientId Required false Valid values string Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_CLAIMS_CLIENTID","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-oauth-full/#oauth-20-client-configuration","title":"OAuth 2.0 client configuration","text":"<p>In order to protect access to it\u2019s resources, DataPlatform acts as an OAuth 2.0 Client which provides authentication its own clients by means of a session cookie. For this type of authentication a JSON Web Token (JWT) is not necessary. The registration which is configured is named \u201ckeycloak\u201d and provides a login page redirecting to a keycloak backend. For specific customizations please s. https://docs.spring.io/spring-security/reference/servlet/oauth2/client/index.html</p> <p>One authentication backend is configured named \u2018keycloak\u2019. The login page is accessible under \u2018{basepath}/oauth2/authorization/keycloak\u2019</p> <p>Property: spring.security.oauth2.client.registration.keycloak.client-id</p> Category Value Default dataintegration Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_CLIENT_ID <p>Property: spring.security.oauth2.client.registration.keycloak.authorization-grant-type</p> Category Value Default authorization_code Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_AUTHORIZATION_GRANT_TYPE <p>Property: spring.security.oauth2.client.registration.keycloak.client-authentication-method</p> Category Value Default basic Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_CLIENT_AUTHENTICATION_METHOD <p>Property: spring.security.oauth2.client.registration.keycloak.redirectUri</p> Category Value Default {baseUrl}/login/oauth2/code/{registrationId} Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_REDIRECTURI <p>Property: spring.security.oauth2.client.registration.keycloak.scope</p> Category Value Default [openid, profile, email] Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_SCOPE <p>Property: spring.security.oauth2.client.registration.keycloak.provider.keycloak.issuer-uri</p> Category Value Default http://docker.localhost/auth/realms/cmem Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_PROVIDER_KEYCLOAK_ISSUER_URI <p>Property: spring.security.oauth2.client.registration.keycloak.provider.keycloak.user-name-attribute</p> Category Value Default preferred_username Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_PROVIDER_KEYCLOAK_USER_NAME_ATTRIBUTE","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-virtuoso-full/","title":"Virtuoso","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/dataplatform/application-virtuoso-full/#configuration-for-connecting-to-virtuoso-backend","title":"Configuration for connecting to Virtuoso backend","text":"<p>Configuration example:</p> <p>This example configures a connection with HTTPS to a remote Virtuoso store (https://remote:8080).</p> <pre><code>store:\n  type: virtuoso\n  authorization: REWRITE_FROM\n  virtuoso:\n    host: \"remote\"\n    ssl-enabled: true\n    port: 8080\n    username: \"admin\"\n    password: \"admin\"\n    databasePort: 1111\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cvirtuoso\u201d</p> Category Value Default virtuoso Required true Valid values VIRTUOSO Environment STORE_TYPE <p>Specific settings for Virtuoso</p> <p>Property: store.virtuoso.host</p> <p>The host of the Virtuoso server</p> Category Value Default none Required false Valid values string Environment STORE_VIRTUOSO_HOST <p>Property: store.virtuoso.port</p> <p>The HTTP port of the Virtuoso server</p> Category Value Default 8080 Required false Valid values integer Environment STORE_VIRTUOSO_PORT <p>Property: store.virtuoso.databasePort</p> <p>The database port of the Virtuoso server for direct access to the JDBC database</p> Category Value Default 1111 Required false Valid values integer Environment STORE_VIRTUOSO_DATABASEPORT <p>Property: store.virtuoso.ssl-enabled</p> <p>Whether SSL is enabled or not (http vs. https)</p> Category Value Default false Required false Valid values string Environment STORE_VIRTUOSO_SSL_ENABLED <p>Property: store.virtuoso.username</p> <p>The user name to connect with</p> Category Value Default none Required false Valid values string Environment STORE_VIRTUOSO_USERNAME <p>Property: store.virtuoso.password</p> <p>The credentials of the given user</p> Category Value Default none Required false Valid values string Environment STORE_VIRTUOSO_PASSWORD","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/explore/graph-resource-pattern/","title":"Graph Resource Pattern","text":"<p>This chapter specifies the JSON object used to provide a Business Knowledge Editor search (query) configuration. The <code>GraphResourcePattern</code> is part of an optional advanced configuration of the Business Knowledge Editor module. It can be used to provide search filter / facets in order to tailor the search results presented to the end user.</p> <p>The <code>GraphResourcePattern</code> object reference is provided in different ways depending on your preferences:</p> Type ScriptJSON SchemaGraphResourcePattern <pre><code>export type GraphResourcePattern = {\n    paths: Path[];\n    pathFilters?: PathVariableFilter[];\n};\n\nexport type Path = {\n    subjectVarName?: string;\n    objectVarName?: string;\n    inverted: boolean;\n    predicate: string;\n};\n\nexport type PathVariableFilter = {\n    varname?: string;\n    varIsAnyOneOfResource?: string[];\n    varIsAnyOneOfLiteral?: Literal[];\n    isNoneOfResource?: string[];\n    isNoneOfLiteral?: Literal[];\n    literalFilters?: PathVariableLiteralFilter[];\n};\n\nexport type Literal = {\n    value: string;\n    lang?: string;\n    datatype?: string;\n};\n\nexport type PathVariableLiteralFilter = {\n    operation:\n        | \"GreaterThan\"\n        | \"LessThan\"\n        | \"GreaterEqualsThan\"\n        | \"LessEqualThan\"\n        | \"NotEquals\"\n        | \"Contains\"\n        | \"Regex\";\n    value: Literal;\n};\n</code></pre> <pre><code>{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\" : \"GraphResourcePattern\",\n    \"description\" : \"Description of the GraphResourcePattern JSON data structure\",\n    \"properties\": {\n        \"pathFilters\": {\n            \"items\": {\n                \"properties\": {\n                    \"isNoneOfLiteral\": {\n                        \"items\": {\n                            \"properties\": {\n                                \"datatype\": {\n                                    \"type\": \"string\"\n                                },\n                                \"lang\": {\n                                    \"type\": \"string\"\n                                },\n                                \"value\": {\n                                    \"type\": \"string\"\n                                }\n                            },\n                            \"type\": \"object\"\n                        },\n                        \"type\": \"array\"\n                    },\n                    \"isNoneOfResource\": {\n                        \"items\": {\n                            \"type\": \"string\"\n                        },\n                        \"type\": \"array\"\n                    },\n                    \"literalFilters\": {\n                        \"items\": {\n                            \"properties\": {\n                                \"operation\": {\n                                    \"enum\": [\n                                        \"Contains\",\n                                        \"GreaterEqualsThan\",\n                                        \"GreaterThan\",\n                                        \"LessEqualThan\",\n                                        \"LessThan\",\n                                        \"NotEquals\",\n                                        \"Regex\"\n                                    ],\n                                    \"type\": \"string\"\n                                },\n                                \"value\": {\n                                    \"properties\": {\n                                        \"datatype\": {\n                                            \"type\": \"string\"\n                                        },\n                                        \"lang\": {\n                                            \"type\": \"string\"\n                                        },\n                                        \"value\": {\n                                            \"type\": \"string\"\n                                        }\n                                    },\n                                    \"type\": \"object\"\n                                }\n                            },\n                            \"type\": \"object\"\n                        },\n                        \"type\": \"array\"\n                    },\n                    \"varIsAnyOneOfLiteral\": {\n                        \"items\": {\n                            \"properties\": {\n                                \"datatype\": {\n                                    \"type\": \"string\"\n                                },\n                                \"lang\": {\n                                    \"type\": \"string\"\n                                },\n                                \"value\": {\n                                    \"type\": \"string\"\n                                }\n                            },\n                            \"type\": \"object\"\n                        },\n                        \"type\": \"array\"\n                    },\n                    \"varIsAnyOneOfResource\": {\n                        \"items\": {\n                            \"type\": \"string\"\n                        },\n                        \"type\": \"array\"\n                    },\n                    \"varname\": {\n                        \"type\": \"string\"\n                    }\n                },\n                \"type\": \"object\"\n            },\n            \"type\": \"array\"\n        },\n        \"paths\": {\n            \"items\": {\n                \"properties\": {\n                    \"inverted\": {\n                        \"type\": \"boolean\"\n                    },\n                    \"objectVarName\": {\n                        \"type\": \"string\"\n                    },\n                    \"predicate\": {\n                        \"type\": \"string\"\n                    },\n                    \"subjectVarName\": {\n                        \"type\": \"string\"\n                    }\n                },\n                \"type\": \"object\"\n            },\n            \"type\": \"array\"\n        }\n    },\n    \"type\": \"object\"\n}\n</code></pre> <pre><code>{\n    \"paths\": [\n        {\n            \"subjectVarName\": \"subResource\",\n            \"objectVarName\": \"resource\",\n            \"predicate\": \"http://example.com/vocab/hasParent\"\n        },\n        {\n            \"subjectVarName\": \"resource\",\n            \"predicate\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\n            \"objectVarName\": \"class\"\n        }\n    ],\n    \"pathFilters\": [\n        {\n            \"varname\": \"class\",\n            \"varIsAnyOneOfResource\": [\n                \"http://example.com/vocab/Company\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>A human friendly representation can be generatled using json-schema-for-humans.</p> <p>A valid configuration must use a <code>subjectVarName</code> called <code>resource</code>. This is the binding that yields results.</p> <p>This configuration produces the following result, it only shows results where:</p> <ul> <li><code>resource</code> is of type <code>http://example.com/vocab/Company</code></li> <li>a <code>subResource</code> exists which is related to <code>resource</code> via the <code>http://example.com/vocab/hasParent</code> property</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/keycloak/","title":"Keycloak","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#introduction","title":"Introduction","text":"<p>This page describes important steps in order to configure Keycloak as an authentication backend for Corporate Memory. The screenshots displayed in this documentation were taken from Keycloak v20 using the <code>keycloak.v2</code> theme.</p> <p>Info</p> <p>You do not need these instruction in case you followed the documentation on\u00a0Scenario: Local Installation\u00a0or\u00a0Scenario: Single Node Cloud Installation\u00a0(in this case, everything was done automatically). However, in case you need to integrate Corporate Memory with an existing Keycloak, this page may help you. Please also have the\u00a0Keycloak - Server Administration Guide\u00a0ready\u00a0</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#realm-configuration","title":"Realm configuration","text":"<p>Warning</p> <p>A realm can be im-/exported. However, exported realms will not contain user credentials.</p> <p>To create a realm, use the drop down menu for choosing a realm on the left side.</p> <ul> <li>Create a realm <code>cmem</code><ul> <li>Select Realm settings</li> <li>General tab:</li> <li>Change HTML Display name to\u00a0<code>&lt;span class=\"ecc-logo\"&gt;&lt;/span&gt;Corporate Memory</code></li> <li>Themes tab</li> <li>Switch realm\u2019s login theme to\u00a0<code>eccenca</code></li> <li>Switch realm\u2019s account theme to\u00a0<code>eccenca</code></li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#client-configuration","title":"Client configuration","text":"<p>Clients are used to link users and groups managed in Keycloak to Corporate Memory. There are two different clients used by Corporate Memory:</p> <ul> <li>The first client is used to authenticate a user for using the web interface (usually named <code>cmem</code>).</li> <li>The other client is used as a technical user with the command line interface (typically named <code>cmem-service-account</code>).   Depending on the environment, there might be other use cases, when running background schedules, then a third client, also as technical user, might be useful.</li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#access-conditions-roles-and-groups","title":"Access conditions, roles and groups","text":"<p>Corporate Memory uses access conditions which are related to users or groups. This is described at Access Conditions. To use groups from Keycloak in Corporate Memory access conditions, all Keycloak client configurations need to have attached mappers:</p> <ul> <li> <p>For the web interface client (<code>cmem</code>), the user groups need to get attached to the client.   This is done by a Group Membership mapper (described below).   With this mapper each group of a user is assigned for the authentication process, so Corporate Memory is aware of the user and group IDs for setting up access conditions.</p> </li> <li> <p>For the technical account clients (such as <code>cmem-service-account</code>), Keycloak does not allow to add groups directly to a client.   To work around this limitation, we are using ROLES instead.   By creating a mapper to re-define roles from groups, we allow Corporate Memory to read roles as groups attached to the client token.</p> </li> </ul> <p>In the default setup in helm or <code>docker compose</code> deployments, we often refer to the <code>elds-admins</code> group, acting as a super-admin / root group. Every user in this group has all possible rights in Corporate Memory, no matter which access conditions are available. This is configured in the Explore backend (DataPlatform) configuration or as an environment variable <code>AUTHORIZATION_ABOX_ADMINGROUP=elds-admins</code> (see also Explore backend authorization configuration).</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#option-1-import-the-needed-clients-from-a-json-export","title":"Option 1: Import the needed clients from a JSON export","text":"<p>To import a pre-configured <code>cmem</code> client for using the web interface, follow these steps:</p> <p></p> <ul> <li>Login to Keycloak and select the Corporate Memory realm (<code>cmem</code>).</li> <li>Download the client configuration for using the web interface (<code>cmem.json</code>).</li> <li>Select Clients, then Import client.</li> <li>Browse for the downloaded <code>cmem.json</code> and select it.</li> <li>Save new client.</li> </ul> <p>To import a pre-configured <code>cmem-service-account</code> client, repeat the process with the client configuration with credentials for the technical account (<code>cmem-service-account</code>) (<code>cmem-service-account.json</code>). After importing add the <code>elds-admins</code> role mapper to the client. See in the manual section of Add the <code>cmem-service-account</code> client</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#option-2-create-client-configurations-manually","title":"Option 2: Create client configurations manually","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#add-the-cmem-client-for-using-the-web-interface","title":"Add the <code>cmem</code> client for using the web interface","text":"<p>This client is intended for the usage with Explore and Build (DataIntegration) (user login):</p> <p></p> <ul> <li>Client type: OpenID Connect</li> <li>Client ID: i.e. <code>cmem</code>, you need to remember this and use this later</li> <li>Name and Description: fill as you like</li> <li>Select Next</li> <li>Client authentication: Off</li> <li>Authorization: Off</li> <li>Enable\u00a0Standard Flow Enabled\u00a0(enables OAuth 2.0 Authorization Code Flow)</li> <li>Before v23.1:<ul> <li>Additionally enable\u00a0Implicit Flow Enabled</li> </ul> </li> <li>Save</li> </ul> <p></p> <p>The dialog above closes and you land on the configuration page of this client:</p> <ul> <li>Valid redirect URIs: Add the correct URL pattern (e.g., wildcard\u00a0<code>https://cmem.example.net/*</code>\u00a0works) to\u00a0<code>Valid Redirect URIs</code>\u00a0(<code>*</code>\u00a0for testing purposes can be used as well) and Save</li> <li>Switch the Tabs to Client scopes and click on the first scope (i.e.: <code>cmem-dedicated</code>)</li> </ul> <p> </p> <ul> <li>Click Configure a new mapper<ul> <li>Select Mapper Type\u00a0Group Membership</li> <li>Name <code>groups</code></li> <li>Token Claim Name <code>groups</code></li> <li>Disable\u00a0Full group path</li> <li>Disable\u00a0Add to ID token</li> <li>Enable\u00a0Add to access token</li> <li>Enable\u00a0Add to user info</li> </ul> </li> <li>Save</li> </ul> <p></p> <ul> <li>In Corporate Memory configuration until v22.2:<ul> <li>Configure this client ID under\u00a0<code>js.config.workspaces.default.authorization.oauth2.clientId</code>\u00a0in DataManager\u2019s configuration file (Datamanager needs implicit flow)</li> <li>Configure  this client ID under\u00a0<code>oauth.clientId = \"cmem\"</code>\u00a0in DataManager\u2019s configuration file (Dataintegration needs standard flow)</li> </ul> </li> <li>In Corporate Memory configuration from v23.1:<ul> <li>Configure this client ID in the environments with the name <code>OAUTH_CLIENT_ID</code> in <code>/environments/config.env</code> (defaults to <code>cmem</code> if not set)</li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#add-the-cmem-service-account-client","title":"Add the <code>cmem-service-account</code> client","text":"<p>This client is intended for internal use by Build (DataIntegration) (scheduler super-user) and data import purposes (cmemc).</p> <p>This descriptions can also be used to create clients with different permissions than admins. For this, just create a different role name later, and create an access condition with this groups name in Corporate Memory as it is described in Access Conditions.</p> <p> </p> <ul> <li>Client type: OpenID Connect</li> <li>Client ID: i.e. <code>cmem-service-account</code>, you need to remember this and use this later</li> <li>Name and Description: fill as you like</li> <li>click Next</li> <li>Client authentication: On</li> <li>Authorization: Off</li> <li>Authentication flow: only enable <code>Service accounts roles</code>, the rest can be disabled</li> <li> <p>Save</p> </li> <li> <p>Go to\u00a0Credentials\u00a0and configure\u00a0Client Id and Secret, copy the client secret for later usage</p> </li> </ul> <p></p> <ul> <li>Go to\u00a0Roles\u00a0and click Create role to create the\u00a0<code>elds-admins</code>\u00a0role</li> </ul> <p> </p> <ul> <li>Click Action and select Add associated roles</li> </ul> <p></p> <ul> <li>Select Filter by client from the filter pull-down-menu</li> </ul> <p></p> <ul> <li>In this dialog select the client by name which you are currently configuring (here <code>cmem-service-account</code>)\u00a0and then Assign</li> </ul> <p></p> <ul> <li>Go back to Client details e.g., by using the top navigation</li> <li>In the Roles tab you now see your created role here</li> </ul> <p></p> <ul> <li>Switch the Tabs to Client scopes and click on the first scope (i.e.: <code>cmem-service-account-dedicated</code>)</li> </ul> <p></p> <ul> <li>select Add mapper -&gt; By configuration</li> </ul> <p></p> <ul> <li>select Mapper Type\u00a0<code>User Client Role</code><ul> <li>Name <code>roles</code></li> <li>Client ID select the client you are currently configuring from the pull-down-menu (here <code>cmem-service-account</code>)</li> <li>Enable Multivalued</li> <li>Token Claim Name <code>groups</code></li> <li>Enable Add to ID token</li> <li>Enable\u00a0Add to access token</li> <li>Enable\u00a0Add to user info</li> </ul> </li> <li>Save</li> </ul> <p></p> <p></p> <ul> <li>After Save go back to Client details</li> <li>Go to Service account roles tab <li>Select the link in the center To manage detail and group mappings, click on the username service-account-YOUR_CLIENT_ID</li> <p></p> <ul> <li>Go to tab Role mapping and select Assign role</li> </ul> <p></p> <ul> <li>Change the filter to Filter by clients and select the new Client ID, i.e <code>cmem-service-account</code></li> <li>Click Assign</li> </ul> <p> </p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#corporate-memory-configuration-after-setting-up-clients","title":"Corporate Memory configuration after setting up clients","text":"<ul> <li>If Build (DataIntegration) schedulers are required, configure this client id and secret under the properties <code>workbench.superuser.client</code>\u00a0and\u00a0<code>workbench.superuser.clientSecret</code>\u00a0in Build (DataIntegration)\u2019s configuration file or</li> <li> <p>in <code>docker compose</code>-orchestration you can edit this in the environment as:</p> <pre><code>  CMEM_SERVICE_ACCOUNT_CLIENT_ID=cmem-service-account\n  CMEM_SERVICE_ACCOUNT_CLIENT_SECRET=YourSecret\n  DATAINTEGRATION_CMEM_SERVICE_CLIENT=cmem-service-account\n  DATAINTEGRATION_CMEM_SERVICE_CLIENT_SECRET=YourSecret\n</code></pre> </li> <li> <p>in helm this value is defined by:</p> <pre><code>  DATAINTEGRATION_CMEM_SERVICE_CLIENT_SECRET: {{ .Values.global.cmemClientSecret }}\n  DATAINTEGRATION_CMEM_SERVICE_CLIENT: {{ .Values.global.cmemClientId }}\n</code></pre> </li> <li> <p>For cmemc you can configure this with <code>OAUTH_CLIENT_ID</code> and <code>OAUTH_CLIENT_SECRET</code>.</p> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#groups-configuration","title":"Groups configuration","text":"<ul> <li>Go to\u00a0Groups and add the following groups:<ul> <li><code>elds-admins</code></li> <li>These groups are used only to assign them to user accounts (clients have roles-to-group mappers).</li> <li>Any groups provided by your user management system (e.g. LDAP) that must be recognized/mapped by Keycloak</li> <li>Corporate Memory does not come with any other groups. Those are optional and can be defined here.</li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#users-configuration","title":"Users configuration","text":"<ul> <li>This applies to the\u00a0Docker Orchestration, for other setups consult the\u00a0Keycloak manual.</li> <li>Go to\u00a0<code>Users</code></li> <li>Add the following users and assign their groups respectively (for each user go to credentials, add password and disable\u00a0<code>Temporary</code>)<ul> <li><code>admin:admin</code><ul> <li>groups:\u00a0<code>elds-admins</code></li> </ul> </li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/","title":"Changing Passwords and Keys","text":"<p>This page describes how to change passwords and keys for a new deployment (esp. in the context of a\u00a0Single Node Cloud Installation).</p> <p>Assuming your instance runs at\u00a0<code>https://cmem.example.com/</code>\u00a0in a default installation Keycloak is deployed at\u00a0<code>https://cmem.example.com/auth</code>\u00a0(this may vary depending on your setup).</p> <p>This is your starting page:</p> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#change-credentials-of-keycloak-admin-account","title":"Change credentials of Keycloak admin account","text":"<p>To change the admin user\u2019s password go to \u201cAdministration Console\u201d and login with username/password admin/admin</p> <p>In the upper right corner go to \u201cManage account\u201d</p> <p></p> <p>In the \u201cAccount Security\u201d field go to \u201cSigning In\u201d</p> <p></p> <p>Click on update in \u201cBasic Authentication\u201d to set a new admin password for :</p> <p></p> <p>Set a new password.</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#change-credentials-of-cmem-service-account","title":"Change\u00a0credentials of cmem-service-account","text":"<p>Make sure the realm Cmem is selected, go to Clients in left sidebar and edit\u00a0cmem-service-account:</p> <p></p> <p>Switch to \u201cCredentials\u201d tab and press \u201cRegenerate Secret\u201d Button.</p> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#change-credentials-of-user-accounts","title":"Change credentials of user accounts","text":"<p>In default configuration, there are two users: user and admin. Both are configured with different groups to have different permissions inside Corporate Memory.</p> <p>To change the default passwords, select the Cmem Realm and open Users in the left sidebar:</p> <p></p> <p></p> <p>Then, select \u201cView all users\u201d and choose an account you want to change the password for (we start with admin)</p> <p></p> <p>Here you can change the password and unselect Temporary. Then press \u201cReset Password\u201d</p> <p></p> <p>Now proceed with the other account(s).</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#persisting","title":"Persisting","text":"<p>Warning</p> <p>This step is applicable only in case your deployment is based on\u00a0the\u00a0Single Node Cloud Installation.</p> <p>In order to persist this setup go back to your terminal inside the installation directory.</p> <p>The following make targets will create a database dump, store it in\u00a0<code>data/backups/keycloak/latest.sql</code>\u00a0and replace the initial database dump\u00a0<code>conf/postgres/docker-entrypoint-initdb.d/keycloak_db.sql</code>.</p> <pre><code>make keycloak-backup keycloak-restore\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/","title":"Configure Corporate Memory with an external Keycloak","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#introduction","title":"Introduction","text":"<p>Maybe you already operate a central Keycloak deployment in your infrastructure or you want to deploy multiple stages of Corporate Memory with a single Keycloak. Very often this results in a Keycloak which is deployed in a different domain than your Corporate Memory (such as Corporate Memory is available on <code>https://cmem.example.com</code> and Keycloak is available on <code>https://keycloak.example.com</code>). For this scenario, this page provides additional configuration requirements.</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#configuration","title":"Configuration","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#infrastructure","title":"Infrastructure","text":"<p>Depending on your infrastructure around Corporate Memory, you need to change some provisioned HTTP header on the following services:</p> <ul> <li>Headers for Keycloak URLs:<ul> <li><code>Access-Control-Allow-Origin: https://cmem.example.com</code></li> </ul> </li> <li>Headers for Corporate Memory URLs:<ul> <li><code>Access-Control-Allow-Origin: *</code></li> </ul> </li> </ul> <p>For example, if you are using our helm charts, adapt the followin ingress annotations:</p> <pre><code>  # KEYCLOAK ingress\n  nginx.ingress.kubernetes.io/enable-cors: \"true\"\n  nginx.ingress.kubernetes.io/cors-allow-origin: \"https://cmem.example.com\"\n</code></pre> <pre><code>  # Corporate Memory ingress\n  nginx.ingress.kubernetes.io/enable-cors: \"true\"\n  nginx.ingress.kubernetes.io/cors-allow-origin: \"*\"\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#keycloak","title":"Keycloak","text":"<p>You have to allow the Corporate Memory domain in the Keycloak settings.</p> <p>In your realm, in Clients go to i.e. <code>cmem</code> client and add <code>https://cmem.example.com/*</code> to Valid redirect URIs:</p> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#corporate-memory","title":"Corporate Memory","text":"<p>When running the Corporate Memory docker orchestration, you can configure the Keycloak through editing <code>environments/config.env</code>. Then just add the variables below. You can get those from the <code>.well-known</code> url from your instance, e.g. <code>https://keycloak.example.com/auth/realms/cmem/.well-known/openid-configuration</code>:</p> <pre><code>OAUTH_AUTHORIZATION_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/auth\nOAUTH_TOKEN_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/token\nOAUTH_JWK_SET_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/certs\nOAUTH_USERINFO_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/userinfo\nOAUTH_LOGOUT_REDIRECT_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/logout?redirect_uri=${EXTERNAL_BASE_URL}\nOAUTH_CLIENT_ID=cmem\n</code></pre> <p></p> <p>By using variables in the Build (DataIntegration) configuration file <code>dataintegration.conf</code>, this environment is used automatically in the docker orchestration:</p> <pre><code>oauth.clientId = ${OAUTH_CLIENT_ID}\noauth.authorizationUrl = ${OAUTH_AUTHORIZATION_URL}\noauth.tokenUrl = ${OAUTH_TOKEN_URL}\noauth.logoutRedirectUrl = ${OAUTH_LOGOUT_REDIRECT_URL}\n</code></pre> <p>By using variables in the Explore backend (DataPlatform) configuration file <code>application.yml</code>, this environment is used automatically in the docker orchestration:</p> <pre><code>spring.security.oauth2:\n  resourceserver:\n    anonymous: \"${DATAPLATFORM_ANONYMOUS}\"\n    jwt:\n      jwk-set-uri: \"${OAUTH_JWK_SET_URL}\"\n  client:\n    registration:\n      keycloak:\n        client-id: \"${OAUTH_CLIENT_ID}\"\n        authorization-grant-type: \"authorization_code\"\n        client-authentication-method: \"basic\"\n        redirectUri: \"${DEPLOY_BASE_URL: 'http://localhost' }/dataplatform/login/oauth2/code/{registrationId}\"\n        scope: # openid is mandatory as spring somehow does not add it to the userinfo request\n          - openid\n          - profile\n          - email\n    provider:\n      keycloak:\n        jwk-set-uri: \"${OAUTH_JWK_SET_URL}\"\n        authorization-uri: \"${OAUTH_AUTHORIZATION_URL}\"\n        token-uri: \"${OAUTH_TOKEN_URL}\"\n        user-info-uri: \"${OAUTH_USERINFO_URL}\"\n        user-name-attribute: \"preferred_username\"\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#cmemc","title":"cmemc","text":"<p>In cmemc you need to change the Keycloak endpoint, which is used for authentication before connecting to Corporate Memory:</p> <pre><code>KEYCLOAK_BASE_URI=https://keycloak.example.com/\nKEYCLOAK_REALM_ID=cmem\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#helm-charts","title":"Helm charts","text":"<p>In the helm charts, we assumed you deploy Keycloak by using the official charts, either via operator, or via helm charts. In either way you can configure the base realm path in the global value section:</p> <pre><code>  # keycloak base url (e.g. https://cmem.example.com/auth)\n  keycloakBaseUrl: https://keycloak.example.com/auth/\n  # keycloak cmem realm url (e.g. https://cmem.example.com/auth/realms/cmem)\n  keycloakIssuerUrl: https://keycloak.example.com/auth/realms/cmem\n  # keycloak oauth client id (used for DataPlatform connection and DataIntegration cmem service client)\n  oauthClientId: cmem\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/","title":"Label Resolution and Full-Text Search","text":""},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#introduction","title":"Introduction","text":"<p>Label resolution translates resource identifiers (URI/IRI) into human readable labels. This resolution and, by extension, the full text search is configurable for different scenarios.</p>"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#configuration","title":"Configuration","text":"<p>eccenca Explore backend (DataPlatform) offers three configuration options:\u00a0<code>labelProperties</code> (line 2)\u00a0<code>languagePreferences</code>\u00a0(line 5) and\u00a0<code>languagePreferencesAnyLangFallback</code>\u00a0(line 8).</p> <pre><code>proxy:\n  labelProperties:\n  - \"http://www.w3.org/2000/01/rdf-schema#label\"\n  - \"http://www.w3.org/2004/02/skos/core#prefLabel\"\n  languagePreferences:\n  - \"en\"\n  - \"\"\n  languagePreferencesAnyLangFallback: true\n</code></pre> <p>These properties define not only which properties and languages should be considered, but also the precedence of languages and properties over each other.</p> <p>The retrieval process can be simplified to the following procedure:</p> <ul> <li>First, when determining the label for a resource, the language is evaluated, then the property is considered.</li> <li>Consequently, for a resource in the default case:<ol> <li>An english\u00a0value for\u00a0<code>rdfs:label</code>\u00a0is searched.</li> <li>A literal of the property\u00a0<code>rdfs:label</code>\u00a0without a language tag is searched (which is why there is an entry\u00a0<code>\"\"</code>).</li> <li>An english value of\u00a0<code>skos:prefLabel</code>\u00a0is searched.</li> <li>A literal of the property\u00a0<code>skos:prefLabel</code>\u00a0without a language tag is searched.</li> <li>If nothing is found, Explore backend (DataPlatform) tries to create a prefixed URI, otherwise the last segment of the resource identifier is used.</li> </ol> </li> </ul> <p>Additionally, in case more than one label could be retrieved, for example by conflicting values, the alphabetically first entry is used.</p>"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#example","title":"Example","text":"<p>How labels are resolved is best explained using these default settings and some examples.</p> <pre><code>:Resource1 rdfs:label \"Leipzig\"@en.\n:Resource2 :someOtherProperty \"Berlin\"@en.\n:Resource3 rdfs:label \"Stuttgart\"@de\n:Resource4 rdfs:label \"Hanover\"@en\n:Resource4 rdfs:label \"Another Label for Hanover\"@en\n</code></pre> <ul> <li>For\u00a0<code>:Resource1</code>\u00a0the label will be\u00a0<code>Leipzig</code> as the english\u00a0<code>rdfs:label</code> will be retrieved.</li> <li>For\u00a0<code>:Resource2</code> the label cannot be retrieved from the Knowledge Graph since no known property is used. Hence the fallback.</li> <li>For <code>:Resource3</code> the label will be retrieved as\u00a0<code>Stuttgart</code>, if the <code>languagePreferencesAnyLangFallback</code>\u00a0is\u00a0<code>true</code>.</li> <li>While there is a well-known property used, none of the used languages match. Using the fallback, the alphabetically first match is retrieved in this case.</li> <li>For\u00a0<code>:Resource4</code> multiple label candidates could be determined.</li> <li>In this case,\u00a0<code>Another Label for Hanover</code>\u00a0is retrieved as it is the first value in the alphanumerical comparison.</li> </ul>"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#client-api","title":"Client API","text":"<p>The label resolution functionality can also be used by client systems. This functionality is exposed as an\u00a0API endpoint\u00a0(<code>&lt;dp_url&gt;/api/explore/title</code>).</p>"},{"location":"deploy-and-configure/configuration/production-ready-settings/","title":"Production-Ready Settings","text":"<p>If you plan to deploy\u00a0Corporate Memory\u00a0in a non-trusted environment, you need to take care about some final configuration steps.</p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#restrict-redirect-urls","title":"Restrict Redirect URLs","text":"<p>As stated in the Keycloak Server Administration Guide:</p> <p>Make your registered redirect URIs as specific as possible. Registering vague redirect URIs for Authorization Code Flows may allow malicious clients to impersonate another client with broader access.</p> <p>Corporate Memory uses the <code>cmem</code> client to authenticate against Keycloak, so adjust the Valid Redirect URIs and Valid Logout Redirect URIs fields for this client.</p> <p>Select<code>cmem</code> realm, then Clients\u00a0\u2192\u00a0<code>cmem</code>\u00a0and enter your deploy URL, e.g., <code>https://cmem.example.net/*</code>. As valid-logout-redirect-uri we suggest the base basic URL of your deployment. e.g. <code>https://cmem.example.net/</code>. Once you restrict these URLs in Keycloak you might see error messages in your keycloak log indicating that those redirect uri\u2019s are not valid. Please update the settings accordingly.</p> <p></p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#explore-backend-dataplatform-valid-post-redirect-settings","title":"Explore backend (DataPlatform) valid post redirect settings","text":"<p>For Explore backend (DataPlatform) you set this in <code>application.yml</code> or as environment variable</p> <p><pre><code>deploy.post-logout-redirect-uri: \"${DEPLOY_BASE_URL}\"\n</code></pre> <pre><code>DEPLOY_POST_LOGOUT_REDIRECT_URI=${DEPLOY_BASE_URL}\n</code></pre></p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#build-dataintegration-valid-post-redirect-settings","title":"Build (DataIntegration) valid post redirect settings","text":"<p>For in Build backend (DataIntegration) you set this in <code>dataintegration.conf</code>. The following parameter are relevant that for:</p> <ul> <li>The first (<code>endSessionUrl</code>) is the keycloak logout url, like <code>KEYCLOAK_URL/auth/realms/cmem/protocol/openid-connect/logout</code></li> <li>Number two (<code>logoutRedirectUrl</code>) sets the URL where the redirect should happen to, after a successful logout.</li> <li>And the last (<code>idToken</code>) is required now and always default to <code>true</code>.</li> </ul> <p>This is part of the OIDC flow.</p> <pre><code>oauth.endSessionUrl = ${OAUTH_LOGOUT_URL}\noauth.logoutRedirectUrl = ${OAUTH_LOGOUT_REDIRECT_URL}\noauth.idToken = true\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#password-policies","title":"Password Policies","text":"<p>If you create users in Keycloak, make sure these users have strong passwords. To enforce this, setting up password policies can help.</p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#cookie-settings","title":"Cookie Settings","text":"","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#keycloak","title":"Keycloak","text":"<p>In Keycloak you should enforce the secure flag for Keycloak cookies. Select <code>cmem</code> realm, then Realm settings\u00a0\u2192 General\u00a0and change Require SSL to <code>All requests</code>. If you are running Corporate Memory without SSL for testing, you will no longer be able to login after this step.</p> <p>Once this is done, make sure Explore backend (DataPlatform) and Build (DataIntegration) use\u00a0<code>HTTPS</code>\u00a0to connect to Keycloak. See the usage of\u00a0<code>DATAPLATFORM_AUTH_URL</code>,\u00a0<code>OAUTH_AUTHORIZATION_URL</code> and\u00a0<code>OAUTH_TOKEN_URL</code>.</p> <p></p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#explore-backend-dataplatform","title":"Explore backend (DataPlatform)","text":"<p>For Explore backend (DataPlatform) you can uncomment these cookie setting in <code>application.yml</code>.</p> <pre><code>## This is important to set flags for DP session cookies\nserver.servlet.session.cookie.same-site: Strict\n\n# If this is enabled it only allows usage of cookies if TLS connection are available\nserver.servlet.session.cookie.secure: true\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#build-dataintegration","title":"Build (DataIntegration)","text":"<p>Similar to Explore backend (DataPlatform), you can also set cookie settings for Build (DataIntegration) inside <code>productions.conf</code> for <code>docker compose</code> deployments or in <code>dataintegration.conf</code> in helm deployments</p> <pre><code># sets \"secure\" flag in PLAY_SESSION cookie\n# https://www.playframework.com/documentation/2.8.x/SettingsSession\nplay.http.session.secure = ${DATAINTEGRATION_SECURE_COOKIE}\n</code></pre> <p>In the Play documentation, you can find further information, i.e. also setting <code>sameSite = \"lax\"</code>or <code>strict</code>. By default Build (DataIntegration) sets this to <code>lax</code></p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#cors-settings","title":"CORS Settings","text":"","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#explore-backend-dataplatform_1","title":"Explore backend (DataPlatform)","text":"<p>Explore backend (DataPlatform) uses\u00a0<code>http.cors.allowedOrigins *</code>\u00a0as the default setting. It is recommended to set custom values for the following headers:</p> <ul> <li><code>Access-Control-Allow-Origin</code>:\u00a0 specifies which domains can access a\u00a0site\u2019s resources. For example, if ABC Corp. has domains\u00a0<code>ABC.com</code>\u00a0and\u00a0<code>XYZ.com</code>, then its developers can use this header to securely grant\u00a0<code>XYZ.com</code>\u00a0access to ABC.com\u2019s resources.</li> <li><code>Access-Control-Allow-Methods</code>: specifies which HTTP request methods (<code>GET</code>, <code>PUT</code>, <code>DELETE</code>, etc.) can be used to access resources. This\u00a0header lets developers further enhance security by specifying what methods are valid when XYZ accesses ABC\u2019s resources.</li> </ul> <p>Detailed configuration options can be found\u00a0here.</p> <p>This is an example section from Explore backend (DataPlatform) <code>application.yml</code>:</p> <pre><code>## Cross-Origin Resource Sharing (CORS) settings\nhttp:\n  cors:\n    allowedOrigins:\n      - \"https://cmem.example.net\"\n    allowedMethods:\n      - \"OPTIONS\"\n      - \"HEAD\"\n      - \"GET\"\n      - \"POST\"\n      - \"PUT\"\n      - \"DELETE\"\n      - \"PATCH\"\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#dataintegration","title":"DataIntegration","text":"<p>DataIntegration uses\u00a0<code>cors.config.allowOrigins *</code>\u00a0as the default setting. It is recommended to set custom values for the <code>Access-Control-Allow-Origin</code>\u00a0header. It specifies which domains can access a\u00a0site\u2019s resources. For example, if ABC Corp. has the domains\u00a0<code>ABC.com</code>\u00a0and\u00a0<code>XYZ.com</code>, you can use this header to securely grant\u00a0<code>XYZ.com</code>\u00a0access to <code>ABC.com</code>\u2019s resources. Detailed configuration options can be found\u00a0here.</p> <p>This is an example section from <code>dataintegration.conf</code>:</p> <pre><code>## Cross-Origin Resource Sharing (CORS) settings\n# CORS configuration ###\ncors.enabled = true\n# List of domains that are allowed to do requests.\n# Wildcard '*' means \"All domains\".\ncors.config.allowOrigins = \"*\"\n# Support cookies, auth etc. for the configured domain under allowOrigins.\n# If set to true, allowOrigins must not have '*' configured.\ncors.config.allowCredentials = false\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/","title":"Quad Store Configuration","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#virtuoso-setup","title":"Virtuoso setup","text":"<p>This section only covers setup options that have a direct impact on the interoperability between Explore and Virtuoso. There are far more options which can significantly improve the overall performance. For an overview of all configuration files and options of Virtuoso refer to the OpenLink Virtuoso Universal Server Documentation. Ensure to add the suggested parameters to the corresponding subsections of the configuration file before starting the server.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#compatibility","title":"Compatibility","text":"<ul> <li>Virtuoso 7.2.4.2\u00a0- Explore is compatible with\u00a0Virtuoso 7.2.4.2. Compatibility with other versions is not guaranteed.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#configuration","title":"Configuration","text":"<p>The following options apply to the Virtuoso configuration file\u00a0<code>virtuoso.ini</code>. Check these options before starting Explore and adjust them if necessary.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#ssl-support","title":"SSL support","text":"<p>Mandatory configuration if\u00a0<code>sparqlEndpoints.virtuoso[i].sslEnabled=true</code>. The server must have a valid certificate, which must be trusted by the system where Explore backend (DataPlatform) runs. In this case, the\u00a0<code>sparqlEndpoints.virtuoso[i].port</code>\u00a0property must point to the\u00a0<code>SSLServerPort</code>.</p> <pre><code>[Parameters]\nSSLServerPort      = 2111\n;; Make sure that &lt;PATH_TO_CERTS_DIR&gt; is contained by DirsAllowed\nSSLCertificate     = &lt;PATH_TO_CERTS_DIR&gt;/public-cert.pem\nSSLPrivateKey      = &lt;PATH_TO_CERTS_DIR&gt;/private-key.pem\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#transaction-size-limit-optional","title":"Transaction size limit (optional)","text":"<p>This option has impact on the maximum file size which can be uploaded using the SPARQL Graph Store endpoint and executing update queries.\\ The maximum value you can set for this option is\u00a0<code>2147483000</code>\u00a0bytes.</p> <pre><code>[Parameters]\nTransactionAfterImageLimit = &lt;size_limit_in_bytes&gt; # default 50000000\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#database-buffers","title":"Database buffers","text":"<p>These options determine the amount of RAM used by Virtuoso to cache database files. To choose the appropriate values consult the\u00a0<code>virtuoso.ini</code>\u00a0file.</p> <pre><code>[Parameters]\nNumberOfBuffers          = &lt;size&gt;\nMaxDirtyBuffers          = &lt;size&gt;\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#resultset-max-rows","title":"ResultSet max rows","text":"<p>This setting is used to limit the number of rows in a result. The effective limit is the lowest value of this setting and the SPARQL query\u00a0<code>LIMIT</code>\u00a0clause value (if present). The exact value is situation-related and depends on the size of the datasets and the consequential number of returned rows. We recommend to set this option to\u00a0<code>0</code>\u00a0to disable the limitation.</p> <pre><code>[SPARQL]\nResultSetMaxRows = 0 # default 10000\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#stardog-setup","title":"Stardog setup","text":"<p>This section only covers limitations and options which have a direct impact on the interoperability between Explore and Stardog. For an overview of all configuration options of Stardog refer to the official\u00a0Stardog documentation. Make sure to add the parameters described in this section to the\u00a0<code>stardog.properties</code> configuration file\u00a0before starting the server.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#compatibility_1","title":"Compatibility","text":"<ul> <li>Stardog 7.2.1 - Explore is compatible with\u00a0Stardog version 7.1.1. Compatibility with newer versions is not guaranteed.</li> <li>Stardog 6.2.3 (deprecated) -\u00a0Explore is compatible with\u00a0Stardog version 6.2.3.</li> </ul> <p>!!! note     Support for 6.2.3 is deprecated and will be removed in later Explore releases.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#configuration_1","title":"Configuration","text":"<ul> <li>Search enabled\\     Explore relies on the Stardog Semantic Search, which has to be enabled by setting:<ul> <li><code>search.enabled=true</code>\\     You can set this property using either Stardog Studio or the\u00a0<code>stardog-admin</code>\u00a0commands.\\     Refer to the\u00a0Stardog documentation\u00a0for more detailed information.</li> </ul> </li> <li> <p>Server side named graph security\\     If the\u00a0<code>PROVISIONED</code>\u00a0access control strategy is used for the configured endpoint, you have to set the property\u00a0<code>security.named.graphs=true</code>\u00a0for the configured database as explained in the\u00a0Stardog documentation. Additionally, the following properties are required:</p> <ul> <li><code>password.length.max</code>: For the provisioned mode to work properly. This property should have a value of at least 64.</li> <li><code>password.regex</code>: The default value configured in Stardog is not compatible with the passwords generated by Explore. The regex should be\u00a0<code>[\\\\w+\\\\/=]+</code></li> </ul> </li> <li> <p>SSL support\\     Mandatory configuration if\u00a0<code>sparqlEndpoints.stardog[i].sslEnabled=true</code>. The server must have a valid certificate which must be trusted by the system where Explore runs. In this case, the\u00a0<code>sparqlEndpoints.stardog[i].port</code>\u00a0property must point to the SSL port (which default value is\u00a0<code>5821</code>).\\     Consult the\u00a0Configuring Stardog to use SSL\u00a0section of Stardog\u2019s manual for more information on the topic.</p> </li> <li>Query timeout override\\     In order to allow Explore to override the query timeout for individual queries, you have to ensure that the property\u00a0<code>query.timeout.override.enabled</code>\u00a0for the database is set to\u00a0<code>true</code>\u00a0(which is the default).\\     Consult the\u00a0Configuring Query Management\u00a0section of Stardog\u2019s manual for further information.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#limitations","title":"Limitations","text":"<ul> <li>Quad format upload\\     The Graph Store Protocol implementation for Stardog does not support uploading of RDF quad data (TriG, N-Quads).</li> <li>Initial connection\\     The first request to Explore can take several seconds due to connection startup to the Stardog server.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#caveats","title":"Caveats","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#datatype-canonicalization","title":"Datatype canonicalization","text":"<p>By default Stardog performs canonicalization of XSD integer datatypes as explained in the\u00a0Stardog documentation. That means that the datatype of all literals declaring a sub-datatype of\u00a0<code>xsd:integer</code>\u00a0is generalized on load or insert:</p> original datatype of literal new datatype of canonicalized literal <code>xsd:int</code> <code>xsd:integer</code> <code>xsd:short</code> <code>xsd:integer</code> <code>xsd:byte</code> <code>xsd:integer</code> <code>xsd:nonNegativeInteger</code> <code>xsd:integer</code> <code>xsd:positiveInteger</code> <code>xsd:integer</code> <code>xsd:nonPositiveInteger</code> <code>xsd:integer</code> <code>xsd:negativeInteger</code> <code>xsd:integer</code> <code>xsd:unsignedLong</code> <code>xsd:integer</code> <code>xsd:unsignedInt</code> <code>xsd:integer</code> <code>xsd:unsignedShort</code> <code>xsd:integer</code> <code>xsd:unsignedByte</code> <code>xsd:integer</code> <p>For example, when ingesting the following RDF data</p> <pre><code>@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\nxsd:long &lt;urn:example:value&gt; \"42\"^^xsd:long .\nxsd:int &lt;urn:example:value&gt; \"42\"^^xsd:int .\nxsd:short &lt;urn:example:value&gt; \"42\"^^xsd:short .\nxsd:byte &lt;urn:example:value&gt; \"42\"^^xsd:byte .\nxsd:nonNegativeInteger &lt;urn:example:value&gt; \"42\"^^xsd:nonNegativeInteger .\nxsd:positiveInteger &lt;urn:example:value&gt; \"42\"^^xsd:positiveInteger .\nxsd:nonPositiveInteger &lt;urn:example:value&gt; \"-42\"^^xsd:nonPositiveInteger .\nxsd:negativeInteger &lt;urn:example:value&gt; \"-42\"^^xsd:negativeInteger .\nxsd:unsignedLong &lt;urn:example:value&gt; \"42\"^^xsd:unsignedLong .\nxsd:unsignedInt &lt;urn:example:value&gt; \"42\"^^xsd:unsignedInt .\nxsd:unsignedShort &lt;urn:example:value&gt; \"42\"^^xsd:unsignedShort .\nxsd:unsignedByte &lt;urn:example:value&gt; \"42\"^^xsd:unsignedByte .\n</code></pre> <p>in a Stardog database with\u00a0<code>index.literals.canonical</code>\u00a0set to\u00a0<code>true</code>\u00a0(default), it will be stored as</p> <pre><code>@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\nxsd:long &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:int &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:short &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:byte &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:nonNegativeInteger &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:positiveInteger &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:nonPositiveInteger &lt;urn:example:value&gt; \"-42\"^^xsd:integer .\nxsd:negativeInteger &lt;urn:example:value&gt; \"-42\"^^xsd:integer .\nxsd:unsignedLong &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:unsignedInt &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:unsignedShort &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:unsignedByte &lt;urn:example:value&gt; \"42\"^^xsd:integer .\n</code></pre> <p>If data is to be copied between CMEM setups backed by Stardog, the\u00a0<code>index.literals.canonical</code>\u00a0options of the corresponding databases must be set to the same value on both setups.</p> <p>It is recommended to only turn this canonicalization off when it is strictly necessary, due to a negative impact on database performance without it.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#graphdb-setup","title":"GraphDB setup","text":"<p>This section covers only limitations and options which have a direct impact on the interoperability between Explore and GraphDB. For an overview of all configuration options of GraphDB refer to the official\u00a0GraphDB documentation.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#compatibility_2","title":"Compatibility","text":"<ul> <li>GraphDB 9.2.0 - Explore is compatible with\u00a0GraphDB version 9.2.0. Compatibility with newer versions is not guaranteed.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#configuration_2","title":"Configuration","text":"<p>No specific configuration changes are needed in order to run with GraphDB.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/reverse-proxy/","title":"Reverse Proxy","text":"<p>A reverse proxy forwards all requests from the users to the called service and returns the result to the users. Corporate Memory is tested with an Apache HTTP server as a reverse proxy.</p> <p>Reverse proxy is a necessary component in the Corporate Memory deployment. It enables you to:</p> <ul> <li>define routes for all the components within one domain name</li> <li>expose only ports 80 and 443 to the outside network, all other communication would be performed in the internal network</li> <li>ease configuration and management of the SSL certificates</li> </ul> <p>This also enables you to activate the\u00a0Linked Data delivery mode\u00a0of Explore. The Linked Data delivery mode is able to serve Linked Data that uses the same namespace as the configured domain name as resolvable URIs including content negotiation.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/reverse-proxy/#example-configuration-for-apache-http-server","title":"Example Configuration for Apache HTTP server","text":"<p>apache configuration template</p> <pre><code>&lt;VirtualHost *:80&gt;\n    ServerName corporate-memory.example.com\n    Redirect permanent / https://corporate-memory.example.com/\n&lt;/Virtualhost&gt;\n&lt;VirtualHost *:443&gt;\n\n    ServerName corporate-memory.example.com\n    ServerAlias www.corporate-memory.example.com\n    ProxyPreserveHost On\n\n    ProxyPass /auth https://keycloak.host/auth retry=0\n    ProxyPassReverse /auth https://keycloak.host/auth\n\n    ProxyPass /dataplatform https://dataplatform.host/dataplatform retry=0\n    ProxyPassReverse /dataplatform https://dataplatform.host/dataplatform\n\n    RewriteEngine  on\n    RewriteRule    \"^/dataintegration$\"  \"/dataintegration/\" [R]\n\n    RewriteCond %{HTTP:Upgrade} =websocket [NC]\n    RewriteRule \"/dataintegration/(.*)\" wss://dataintegration.host/dataintegration/$1 [P,L]\n    RewriteCond %{HTTP:Upgrade} !=websocket [NC]\n    RewriteRule \"/dataintegration/(.*)\" https://dataintegration.host/dataintegration/$1 [P,L]\n\n    ProxyPassReverse /dataintegration https://dataintegration.host/dataintegration\n\n    ProxyPass / https://datamanager.host/ retry=0\n    ProxyPassReverse / https://datamanager.host/\n\n    # https://github.com/gitlabhq/gitlabhq/issues/8924\n    AllowEncodedSlashes NoDecode\n\n    # Network timeout in seconds for proxied requests (default 300)\n    # http://serverfault.com/questions/500467/apache2-proxy-timeout/583266\n    ProxyTimeout 1200\n\n    #######################\n    # SSL\n    #######################\n    SSLEngine on\n    SSLCertificateFile /etc/apache2/ssl/ssl-bundle.crt\n    SSLCertificateKeyFile /etc/apache2/ssl/cert.key\n    SSLCertificateChainFile /etc/apache2/ssl/cert.cabundle\n&lt;/VirtualHost&gt;\n</code></pre> <p>Information about the runtime environment which is used to run Explore and Build (DataIntegration), is hidden.</p> <p>Note</p> <p>Keep in mind that you have to adjust the location paths/URLs in the Explore and Build (DataIntegration) configuration files.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/reverse-proxy/#linked-data-delivery-mode","title":"Linked Data delivery mode","text":"<p>The Linked Data delivery mode is able to serve data that uses the same namespace as the configured domain name as resolvable URIs including content negotiation.</p> <p>Therefore you can use the following template (e.g.:\u00a0https://corporate-memory.example.com):</p> <ul> <li>https://dataplatform.corporate-memory.example.com\u00a0(DataPlatform)</li> <li>https://dataplatform.corporate-memory.example.com/vocabulary/example/\u00a0(a custom vocabulary)</li> <li>with HTTPS enforcement (recommended)</li> </ul> <p>apache sample config for linked data delivery</p> <pre><code>&lt;VirtualHost *:80&gt;\n  ServerName dataplatform.corporate-memory.example.com\n  Redirect permanent / https://dataplatform.corporate-memory.example.com/\n&lt;/Virtualhost&gt;\n\n&lt;VirtualHost *:443&gt;\n\n  ServerName dataplatform.corporate-memory.example.com\n\n  RewriteEngine On\n  ProxyPreserveHost On\n\n  Define reverse_url\\\n  \"http://DATAPLATFORM_SERVICE_HOST:8080/proxy/default/graph?owlImportsResolution=false&amp;graph=\"\n\n  # Return complete graphs for URIs ending with slash\n  RewriteCond %{REQUEST_METHOD} ^(.*)\n  RewriteRule \"^/vocabulary/(.+)/$\"\\\n    \"${reverse_url}%{REQUEST_SCHEME}://%{HTTP_HOST}%{REQUEST_URI}\"\\\n    [P]\n\n  # Everything else is just plain proxied\n  RewriteRule \"^/(.*)$\" \"http://DATAPLATFORM_SERVICE_HOST:8080/$1\" [P]\n\n  # Reverse proxy only needed for login via browser (e.g. with DataManager)\n  ProxyPassReverse \"/\" \"http://DATAPLATFORM_SERVICE_HOST:8080/\"\n\n  # ssl-config\n  SSLEngine on\n  SSLCertificateFile /etc/apache2/ssl/ssl-bundle.crt\n  SSLCertificateKeyFile /etc/apache2/ssl/cert.key\n  SSLCertificateChainFile /etc/apache2/ssl/cert.cabundle\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note</p> <p>Keep in mind, that the vocabulary namespace is using\u00a0<code>https</code>\u00a0as protocol, too.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/installation/","title":"Installation","text":"<p>This page describes proven deployment scenarios for eccenca Corporate Memory.</p> <p>All Corporate Memory components are distributed as Docker images and can be obtained from eccenca\u2019s Artifactory service. To run them you need a Docker enabled Linux server. In addition to that, eccenca provides distribution archives for all components which contain configuration examples (YAML) as well as JAR/WAR artifacts.</p>"},{"location":"deploy-and-configure/installation/#operating-systems-os","title":"Operating Systems (OS)","text":"<p>Corporate Memory is tested on Ubuntu 18.04 (backward compatible with 16.04 and 14.04) and RHEL 7.7.</p> <p>Special note on RHEL SELinux Support: there is no limitation for RedHat SELinux. We recommend to keep the SELinux in\u00a0enforced\u00a0mode. You can keep the default setting of the\u00a0<code>/etc/selinux/config</code>\u00a0file.</p> sample config /etc/selinux/config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=enforcing\n# SELINUXTYPE= can take one of three values:\n#     targeted - Targeted processes are protected,\n#     minimum - Modification of targeted policy. Only selected processes are protected.\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n</code></pre>"},{"location":"deploy-and-configure/installation/#docker-compose-based-orchestration-deployment","title":"Docker compose based Orchestration deployment","text":"<p>Docker Compose\u00a0is a convenient way to provision several Docker containers locally for development setups or on remote servers for single node setups.</p> <p>eccenca is heavily using <code>docker compose</code> for all kinds of internal and customer deployments. For more details on how to use <code>docker compose</code> based orchestration refer to\u00a0Scenario: Local Installation\u00a0and\u00a0Scenario: Single Node Cloud Installation.</p>"},{"location":"deploy-and-configure/installation/#dataintegration","title":"DataIntegration","text":""},{"location":"deploy-and-configure/installation/#running-on-a-spark-cluster","title":"Running on a Spark Cluster","text":"<p>eccenca Build (DataIntegration) supports the execution of Build (DataIntegration) workflows in a cluster environment with Apache Spark.</p>"},{"location":"deploy-and-configure/installation/#prerequisites","title":"Prerequisites","text":"<p>For the execution of Build (DataIntegration) in a Spark cluster the following software components from the Hadoop eco-system are recommended:</p> <ul> <li>Scala 2.11 or 2.10</li> <li>Apache Spark 2.1.2 (compiled for Scala 2.11)</li> <li>Apache Hadoop 2.7 (HDFS)</li> <li>Apache Hive 1.2, with a relational data bases as meta store (e.g. Derby)</li> </ul> <p>Recent versions of the following Hadoop distributions are generally supported as well:</p> <ul> <li>Hortonworks (HDP 2.5)</li> <li>Cloudera (CDH 5.8)</li> <li>Oracle Big Data Lite (4.6)</li> <li>Microsoft HDInsight (based on HDP)</li> </ul>"},{"location":"deploy-and-configure/installation/#installation_1","title":"Installation","text":"<p>A Spark application can run in three different modes:</p> <ul> <li>local mode</li> <li>client mode</li> <li>cluster mode</li> </ul> <p>The local mode is for running Spark applications on one local machine. In the client mode the Build (DataIntegration) application will run outside of the cluster and create Spark Jobs to be executed in the cluster at run time. The cluster mode requires that the application using Spark runs completely in the cluster and is managed by the software running on the cluster (e.g. Spark, Apache Yarn, Mesos). Build (DataIntegration) supports local mode (for testing), client mode (for production, only with clusters managed by Spark) or cluster mode on Yarn (for production, integrates best with other distributed applications).</p> <p>When running Build (DataIntegration) in a cluster, the same installation procedure and prerequisites apply as for the local installation. The application can be installed outside the cluster or on any cluster node. A number of configuration options have to be set to be able to connect to and use a Spark cluster. The necessary configuration options are described in\u00a0Build (DataIntegration).</p>"},{"location":"deploy-and-configure/installation/#explore","title":"Explore","text":""},{"location":"deploy-and-configure/installation/#scaling","title":"Scaling","text":"<p>Run multiple Explore instances with the same configuration to enable high-availability and/or high-performance setups.</p>"},{"location":"deploy-and-configure/installation/#prerequisites_1","title":"Prerequisites","text":"<p>For running multiple Explore instances the following prerequisites apply:</p> <ul> <li>The same application configuration properties must be used by all scaled instances.</li> <li>If access control for any SPARQL endpoint is active, a shared Redis cache used by all Explore instances is required.</li> </ul>"},{"location":"deploy-and-configure/installation/#limitations","title":"Limitations","text":"<p>When running multiple Explore instances it is not possible to use a shared Virtuoso backend with provisioned access control active.</p>"},{"location":"deploy-and-configure/installation/#troubleshooting","title":"Troubleshooting","text":"<p>In case Explore failed to start, check the logs for error messages pointing to faulty parameters in the configuration. Since not every faulty behavior is apparent from reading the logs, the following checks can help you to verify the configuration:</p> <ul> <li>Check the\u00a0<code>http(s)://&lt;servername:port&gt;/actuator/health/</code>\u00a0endpoint to verify if the SPARQL proxy service endpoints are configured properly.</li> </ul> <p>Note: Refer to the\u00a0Spring documentation\u00a0on how to set active profiles.</p>"},{"location":"deploy-and-configure/installation/#plugins","title":"Plugins","text":"<p>In some cases Explore needs to be extended with plugins. Extensions are necessary when drivers cannot be included due to licensing restrictions or when plugins are delivered separately.</p> <p>In this case, you have to update the .war file of Explore by placing the plugin .jar files in the same directory, or by stating the path via the configuration option.</p> <p>To include plugins that are located in the same directory as the\u00a0<code>eccenca-DataPlatform.war</code>\u00a0file, execute the .war file with the option\u00a0<code>-u</code>\u00a0or\u00a0<code>--update-war</code>:</p> <pre><code># with plugins located in the same folder as the WAR file\njava -jar ${JAVA_TOOL_OPTIONS} eccenca-DataPlatform.war --update-war\n</code></pre> <p>If the plugins to be included are not located in the same folder as the .war file, you can specify a directory containing the plugins as the argument of the\u00a0<code>-u</code>\u00a0or\u00a0<code>--update-war</code>\u00a0option.</p> <pre><code>java -jar ${JAVA_TOOL_OPTIONS} eccenca-DataPlatform.war -u /data/plugins\n</code></pre> <p>The last command repackages the\u00a0<code>eccenca-DataPlatform.war</code>\u00a0by including all plugins (.jar) located in the specified directory.</p> <p>Note: Make sure that only the\u00a0<code>eccenca-DataPlatform.war</code>\u00a0file is in the directory since multiple .war files can cause problems.</p> <p>Note: During the update procedure, the directory\u00a0<code>WEB-INF</code>\u00a0is created. Due to security concerns the update mechanism does not delete this directory. You can delete it after the update process is finished.</p>"},{"location":"deploy-and-configure/installation/migrating-stores/","title":"Migrating Stores","text":""},{"location":"deploy-and-configure/installation/migrating-stores/#sizing-and-deployment","title":"Sizing and Deployment","text":"<ul> <li>size and deploy of the new store (refer to the capacity planning / sizing considerations, refer to the docker container / orchestration)</li> <li>store specific config (e.g. search-all-graphs in SD)</li> </ul>"},{"location":"deploy-and-configure/installation/migrating-stores/#transferring-data-and-configuration","title":"Transferring Data and Configuration","text":"<ul> <li>backing-up / exporting and restore / import of graphs, DI-projects, configuration (if any)<ul> <li>graphs<ul> <li>blacklisting the DI projects graphs</li> </ul> </li> <li>config<ul> <li>DM<ul> <li>stardog text match support (this is a DM parameter!)</li> <li>search queries</li> <li>navigation</li> </ul> </li> </ul> </li> <li>DP<ul> <li>configure the resp. store</li> </ul> </li> <li>DI<ul> <li>nothing to do \u2026 just duplicate / copy the configuration as-is</li> <li><code>cmemc admin workspace export / import</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"deploy-and-configure/installation/migrating-stores/#test-and-validation","title":"Test and Validation","text":"<ul> <li>best practice:<ul> <li>run all (SELECT) queries in the query catalog and compare results (e.g. with <code>cmemc</code>)<ul> <li>theoretically this could also be applied to INSERT queries (by re-writing into SELECTS in case you want / need to omit altering your graphs)</li> </ul> </li> <li>count all triples in all graphs on both instances before/after export/import (<code>cmemc graph count --all</code>)</li> </ul> </li> </ul>"},{"location":"deploy-and-configure/installation/migrating-stores/#optimizing-your-setup","title":"Optimizing Your Setup","text":"<ul> <li>optimizing customization (e.g. queries in SHAPES; DI; DM-config)<ul> <li>\u201ctextmatch\u201d / \u201clucene\u201d queries need to be migrated (a query can be helpful to find these queries\u2026)</li> <li>performance comparisons could be automated via <code>cmemc query replay</code><ul> <li>identify query that won\u2019t run or run slow</li> </ul> </li> </ul> </li> <li>general query best practices<ul> <li>\u2192 query optimization guide<ul> <li>use <code>VALUE</code> instead of <code>FILTER (?x IN (...))</code> (esp. on GDB)</li> </ul> </li> </ul> </li> </ul>"},{"location":"deploy-and-configure/installation/scenario-local-installation/","title":"Introduction","text":"<p>This page describes a <code>docker compose</code> based orchestration running on your local machine and accessible via browser.</p> <p>The code examples in this section assumes that you have POSIX-compliant shell (linux, macOS or WSL for Windows).</p>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#requirements","title":"Requirements","text":"<ul> <li>Access credentials to eccenca Artifactory and eccenca Docker Registry \u2192\u00a0contact us to get yours</li> <li>docker\u00a0and\u00a0docker compose\u00a0(v2) installed locally</li> <li>git\u00a0installed locally</li> <li>jq\u00a0installed locally</li> <li>make - build tools (apt-get install make) installed locally</li> <li>At least 4 CPUs and 12GB of RAM (recommended: 16GB) dedicated to docker</li> </ul>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#wsl-installation-and-configuration","title":"WSL installation and configuration","text":"<p>For all you need to start Powershell started as administrator. Alternatively you can also install a WSL distribution from Microsoft Store.</p> <p>Install WSL, then restart your Windows machine.</p> <pre><code>wsl --install\n</code></pre> <p>List available distributions</p> <pre><code>wsl --list --online\n</code></pre> <p>Install a distribution. Chose from the <code>Name</code> column. Here we use a Debian based distribution like Debian or any Ubuntu. However other Distributions might work as well.</p> <pre><code>wsl --install Debian\n</code></pre> <p>Ensure you use WSL version 2 (this is necessary to use <code>systemd</code> services).</p> <pre><code>wsl -l -v\n</code></pre> <p>Install version 2 components (this requires a windows restart)</p> <pre><code>dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n</code></pre> <p>Enter WSL machine</p> <pre><code>wsl -d Debian\n</code></pre> <p>Enable <code>generateHosts = false</code> in your <code>/etc/hosts</code> file to make sure that this file won\u2019t be overwritten from the host system on every restart.</p> <p>To be able to use <code>systemd</code> services and commands make sure <code>/etc/wsl.conf</code> is available with this content (should be the default with WSL v2):</p> <pre><code>[boot]\nsystemd=true\n</code></pre> <p>(Optional) If you need to restart your WSL use in Powershell:</p> <pre><code>wsl --shutdown\n</code></pre> <p>(Optional) you can create a <code>.wslconfig</code> file under <code>C:\\users\\&lt;your username&gt;</code> to specify some system resources like:</p> <pre><code>[wsl2]\nmemory=16GB # restrict ram WSL can use\nprocessors=4 # restrict cpu-cores\nswap=8GB # set swap size\nswapFile=C:/Users/&lt;your username&gt;/wsl/Debianswap.vhdx  # location to swap-file\n</code></pre> <p>Alternatively, (with WSL v2) you may use the graphical configuration application WSL Settings.</p>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#setup-check-installation-environment","title":"Setup &amp; Check Installation Environment","text":"<p>For docker we recommend to use the linux docker within WSL. Follow the instruction in Server Provisioning in Scenario: Single Node Cloud Installation. Alternatively, you may use docker for desktop and enable WSL integration in the settings.</p> <p>Open a terminal window, create a directory, copy and extract docker orchestration there.</p> <pre><code># Create eccenca-corporate-memory directory in your ${HOME} and set as a\n# working dir.\nsudo apt install -y curl jq make git unzip gpg\n\nmkdir ${HOME}/eccenca-corporate-memory &amp;&amp; cd ${HOME}/eccenca-corporate-memory\n\n# download the Corporate Memory orchestration distribution\ncurl https://releases.eccenca.com/docker-orchestration/latest.zip \\\n    &gt; cmem-orchestration.zip\n\n# unzip the orchestration and move the unzipped directory \nunzip cmem-orchestration.zip\nrm cmem-orchestration.zip\nmv cmem-orchestration-v* cmem-orchestration\ncd cmem-orchestration\ngit init &amp;&amp; git add . &amp;&amp; git commit -m \"stub\"\n</code></pre> <p>Check your local environment:</p> <pre><code># Run the following command to check your docker server version.\n# To have the current security patches, always update your docker version\n# to the latest one.\n\ndocker version | grep -i version\n# Docker version: 27.5.1, build 9f9e405\n\n# Check docker compose version, should be at least v2.*.*\n# update to the latest version if necessary\n\ndocker compose version\n# Docker Compose version v2.32.4\n\n# login into eccenca docker registry\n\ndocker login docker-registry.eccenca.com\n# Username: yourusername\n# Password:\n# Login Succeeded\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#installation","title":"Installation","text":"<p>To install Corporate Memory, you need to modify your local hosts file (located in /etc/hosts), minimal configuration is as follows:</p> <pre><code>##\n# Host Database\n#\n# localhost is used to configure the loopback interface\n# when the system is booting.  Do not change this entry.\n##\n127.0.0.1 localhost\n127.0.0.1 docker.localhost\n</code></pre> <p>Corporate Memory uses Ontotext GraphDB triple store as default backend. Graphdb is available as free version and does not requires a license. If you have a license for Ontotext GraphDB you can copy the file to the <code>license</code> folder inside Corporate Memory\u2019s root folder.</p> <pre><code>cp YOUR_SE_LICENSE_FILE \\\n  ${HOME}/cmem-orchestration-VERSION/licenses/graphdb-se.license\n# or\ncp YOUR_EE_LICENSE_FILE \\\n  ${HOME}/cmem-orchestration-VERSION/licenses/graphdb-ee.license\n</code></pre> <p>Then change the file <code>environments/config.env</code> to use the correct version:</p> <pre><code># Use Free, 'se' or 'ee' or adjust the mountpoint in \n# compose/docker-compose.store.graphdb.yaml\nGRAPHDB_LICENSE=se\n</code></pre> <p>Run the command to clean workspace, pull the images, start the Corporate Memory instance and load initial data:</p> <pre><code># Pulling the images will take time\n\nmake clean-pull-start-bootstrap\n</code></pre> <p>You should see the output as follows:</p> <pre><code>make[1]: Entering directory '/home/ttelleis/cmem-dist/cmem-orchestration'\nThe target cleans up everything and esp. REMOVES ALL YOUR DATA. Do you want to continue?\n\nYou can avoid this question / interruption by setting CO_I_KNOW_WHAT_I_DO to true.\nType '1' for Yes or type '2' for No.\n\n1) Yes\n2) No\n#? 1\nmake check-env kill stop down rm-log-dir\nmake[2]: Entering directory '/home/ttelleis/cmem-dist/cmem-orchestration'\n/usr/bin/docker compose  kill\nno container to kill/usr/bin/docker compose  stop\n/usr/bin/docker compose  down --volumes --remove-orphans || exit 0\n/usr/bin/docker compose  up -d\n[+] Running 12/12\n \u2714 Network dockerlocalhost_default              Created                   0.1s\n \u2714 Volume \"dockerlocalhost_store_volume\"        Created                   0.0s\n \u2714 Volume \"dockerlocalhost_import_volume\"       Created                   0.0s\n \u2714 Volume \"dockerlocalhost_postgres_volume\"     Created                   0.0s\n \u2714 Container dockerlocalhost-store-1            Started                   1.0s\n \u2714 Container dockerlocalhost-apache2-1          Started                   1.3s\n \u2714 Container dockerlocalhost-cmemc-1            Started                   1.2s\n \u2714 Container dockerlocalhost-datamanager-1      Started                   1.3s\n \u2714 Container dockerlocalhost-postgres-1         Healthy                   6.6s\n \u2714 Container dockerlocalhost-keycloak-1         Healthy                  47.8s\n \u2714 Container dockerlocalhost-dataplatform-1     Started                  48.3s\n \u2714 Container dockerlocalhost-dataintegration-1  Started                  48.3s\n/home/ttelleis/cmem-dist/cmem-orchestration//scripts/waitForSuccessfulStart.dist.sh\nWaiting for healthy orchestration.................. done\nRemove existing bootstrap data from triple store and import shipped data from DP\nchmod a+r conf/cmemc/cmemc.ini\ndocker compose run -i --rm --env \"OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" --volume /home/ttelleis/cmem-dist/cmem-orchestration/data:/data --volume /home/ttelleis/cmem-dist/cmem-orchestration/conf/cmemc/cmemc.ini:/config/cmemc.ini cmemc -c cmem admin store bootstrap --import\nUpdate or import bootstrap data ... done\nmake[1]: Leaving directory '/home/ttelleis/cmem-dist/cmem-orchestration'\n\nCMEM-Orchestration successfully started with store graphdb.\nPlease open http://docker.localhost:80 for validation.\nRun make logs to see log output\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#initial-login-test","title":"Initial Login / Test","text":"<p>Open your browser and navigate to\u00a0http://docker.localhost</p> account password description <code>admin</code> <code>admin</code> Is member of the global admin group (can see and do anything) <p>After successful login, you will see Corporate Memory interface. You can now proceed to the\u00a0Getting Started\u00a0section.</p>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#backup","title":"Backup","text":"<p>To create a backup you have to prepare the backup folders. Make sure these folders exists and have write permissions. Run this:</p> <pre><code># assuming you are currently in the the cmem-orchestration folder\nmkdir -p data/backups/graphs data/backups/workspace\nchmod 777 data/backups/graphs data/backups/workspace\n\nmake backup\nmkdir -p data/backups/keycloak\n# Started Keycloak database backup to data/backups/keycloak/keycloak.sql ...\n# Finished Keycloak database backup.\nmv data/backups/keycloak/keycloak.sql data/backups/keycloak/2024-07-26_14-15.sql\nln -sf 2024-07-26_14-15.sql data/backups/keycloak/latest.sql\nmkdir -p data/backups/workspace\ndocker compose run -i --rm --env \"OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" --volume /home/ttelleis/cmem-dist/cmem-orchestration/data:/data --volume /home/ttelleis/cmem-dist/cmem-orchestration/conf/cmemc/cmemc.ini:/config/cmemc.ini cmemc -c cmem admin workspace export /data/backups/workspace/2024-07-26_14-15.zip\n# Export workspace to /data/backups/workspace/2024-07-26_14-15.zip ... done\nln -sf 2024-07-26_14-15.zip data/backups/workspace/latest.zip\nmkdir -p data/backups/python-packages\nzip -r data/backups/python-packages/2024-07-26_14-15.zip data/python-packages\n  adding: data/python-packages/ (stored 0%)\nln -sf 2024-07-26_14-15.zip data/backups/python-packages/latest.zip\nmkdir -p data/backups/graphs\ndocker compose run -i --rm --env \"OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" --volume /home/ttelleis/cmem-dist/cmem-orchestration/data:/data --volume /home/ttelleis/cmem-dist/cmem-orchestration/conf/cmemc/cmemc.ini:/config/cmemc.ini cmemc -c cmem admin store export /data/backups/graphs/2024-07-26_14-15.zip\n# Exporting graphs backup to /data/backups/graphs/2024-07-26_14-15.zip ... done\nln -sf 2024-07-26_14-15.zip data/backups/graphs/latest.zip\nzip -r data/backups/2024-07-26_14-15.zip data/backups/keycloak/2024-07-26_14-15.sql data/backups/workspace/2024-07-26_14-15.zip data/backups/graphs/2024-07-26_14-15.zip data/backups/python-packages/2024-07-26_14-15.zip\n  adding: data/backups/keycloak/2024-07-26_14-15.sql (deflated 82%)\n  adding: data/backups/workspace/2024-07-26_14-15.zip (stored 0%)\n  adding: data/backups/graphs/2024-07-26_14-15.zip (stored 0%)\n  adding: data/backups/python-packages/2024-07-26_14-15.zip (stored 0%)\nln -sf 2024-07-26_14-15.zip data/backups/latest.zip\n</code></pre> <p>The full backup is now at <code>data/backups/latest.zip</code>.</p>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#caveats","title":"Caveats","text":"<p>In case you have problems starting and receive error messages like Port 80 already assigned. Then check if a apache2 service is running and remove it.</p> <pre><code>sudo service apache2 status\nsudo service stop apache2\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/","title":"Scenario: Single Node Cloud Installation","text":""},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#introduction","title":"Introduction","text":"<p>This page describes a docker-compose based orchestration running on a server instance accessible publicly via browser (SSL enabled via letsencrypt).</p>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#requirements","title":"Requirements","text":"<ul> <li>ssh access to a server instance\u00a0(Debian 11) with a public IP address</li> <li>A resolvable domain name to this server</li> <li>Terminal with ssh client installed locally</li> <li>An eccenca partner account for the docker registry as well as the release artifact area</li> </ul>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#server-provisioning","title":"Server Provisioning","text":"<p>In this step, you install necessary software on the server and execute the following commands as root:</p> <pre><code>sudo apt-get update\n\n# install ntp and set timezone\nsudo apt-get install -y ntp\nsudo timedatectl set-timezone Europe/Berlin\n\n# install needed packages\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg2 \\\n    software-properties-common gnupg lsb-release gettext zip unzip git \\ \n    make vim jq\n\n# install docker and docker-compose\ncurl -fsSL https://download.docker.com/linux/debian/gpg \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb \\\n    [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] \\\n    https://download.docker.com/linux/debian $(lsb_release -cs) stable\" \\\n    | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io \\\n    docker-compose-plugin\n\n# (optional) add a user to docker group\n# may require logout/login to reload group assignments\n# sudo usermod -a -G docker admin\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#installation","title":"Installation","text":"<p>Info</p> <p>For username and password in curl command use the credentials to access eccenca Artifactory and docker registry.</p> <p>Connect to the server and navigate to the directory with the Corporate Memory docker orchestration:</p> <pre><code># login to the eccenca docker registry\ndocker login docker-registry.eccenca.com\n\n# download the Corporate Memory orchestration distribution\ncd /opt\ncurl https://releases.eccenca.com/docker-orchestration/latest.zip \\\n    &gt; cmem-orchestration.zip\n\n# unzip the orchestration and move the unzipped directory to\n# /opt/cmem-orchestration\nunzip cmem-orchestration.zip\nrm cmem-orchestration.zip\nmv cmem-orchestration-v* /opt/cmem-orchestration\n\n# configure git in order to commit changes to the orchestration\ncd /opt/cmem-orchestration\ngit config --global user.email \"you@example.com\" &amp;&amp; git init &amp;&amp; git add . \\\n    &amp;&amp; git commit -m \"stub\"\n</code></pre> <p>The Corporate Memory docker orchestration is configured with environment files.</p> <p>You will need to create an environment file at\u00a0<code>/opt/cmem-orchestration/environments/prod.env</code>. For now, you can use the provided file\u00a0<code>config.ssl-letsencrypt.env</code>\u00a0as a template.</p> <p>Info</p> <p>You need to change the lines with\u00a0DEPLOYHOST and\u00a0LETSENCRYPT_MAIL to your actual values.</p> <pre><code>cd /opt/cmem-orchestration/environments\ncp config.ssl-letsencrypt.env prod.env\n\n# change DEPLOYHOST and LETSENCRYPT_MAIL values\nvi prod.env\n</code></pre> <p>In addition that, you need to remove the default config and link it to your prod.env</p> <pre><code>cd /opt/cmem-orchestration/environments\n\nrm config.env\nln -s prod.env config.env\n</code></pre> <p>To see all available configuration options refer to\u00a0Docker Orchestration configuration\u00a0page.</p> <p>Next, request SSL certificates from\u00a0letsencrypt\u00a0service:</p> <pre><code>cd /opt/cmem-orchestration\nmake letsencrypt-create\n</code></pre> <p>Change\u00a0<code>CMEM_BASE_URI</code> according to your <code>DEPLOYHOST</code>.</p> <pre><code># update cmemc configuration\nrm conf/cmemc/cmemc.ini\ncat &lt;&lt;EOF &gt; conf/cmemc/cmemc.ini\n[cmem]\nCMEM_BASE_URI=https://corporate-memory.eccenca.dev/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nEOF\n</code></pre> <p>Finally deploy the Corporate Memory instance:</p> <pre><code>make clean-pull-start-bootstrap\nmake tutorials-import\n</code></pre> <p>Optional: you can install cmem as a systemd service for this use these commands as root or sudo:</p> <pre><code>cp /opt/cmem-orchestration/conf/systemd/cmem-orchestration.service \\\n    /etc/systemd/system\nsystemctl enable cmem-orchestration\nsystemctl start cmem-orchestration\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#validation-and-finalisation","title":"Validation and Finalisation","text":"<p>Open your browser and navigate to the host you have created in DNS server, e.g.\u00a0<code>https://corporate-memory.eccenca.dev</code></p> <p>Click CONTINUE WITH LOGIN and use one of these default accounts:</p> account password description <code>admin</code> <code>admin</code> Is member of the global admin group (can see and do anything) <p>After successful login, you will see Corporate Memory interface. You can now proceed to the\u00a0 Getting Started\u00a0section.</p> <p>Do not forget to change the passwords of your deployment, especially if it is available from the public internet. For this, take a look at\u00a0Change Passwords and Keys.</p> <pre><code>cp /opt/cmem-orchestration/conf/systemd/cmem-orchestration.service \\\n    /etc/systemd/system\nsystemctl enable cmem-orchestration\nsystemctl start cmem-orchestration\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#backup","title":"Backup","text":"<p>To create a backup you have to prepare the backup folders. Make sure these folders exists and have write permissions. Run this:</p> <p><pre><code># assuming you are currently in the the cmem-orchestration folder\nmkdir -p data/backups/graphs data/backups/workspace\nchmod 777 data/backups/graphs data/backups/workspace\n\nmake backup\nmkdir -p data/backups/keycloak\nStarted Keycloak database backup to data/backups/keycloak/keycloak.sql ...\nFinished Keycloak database backup.\nmv data/backups/keycloak/keycloak.sql data/backups/keycloak/2024-07-26_14-15.sql\nln -sf 2024-07-26_14-15.sql data/backups/keycloak/latest.sql\nmkdir -p data/backups/workspace\ndocker compose run -i --rm --env \"OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" --volume /home/ttelleis/cmem-dist/cmem-orchestration/data:/data --volume /home/ttelleis/cmem-dist/cmem-orchestration/conf/cmemc/cmemc.ini:/config/cmemc.ini cmemc -c cmem admin workspace export /data/backups/workspace/2024-07-26_14-15.zip\nExport workspace to /data/backups/workspace/2024-07-26_14-15.zip ... done\nln -sf 2024-07-26_14-15.zip data/backups/workspace/latest.zip\nmkdir -p data/backups/python-packages\nzip -r data/backups/python-packages/2024-07-26_14-15.zip data/python-packages\n  adding: data/python-packages/ (stored 0%)\nln -sf 2024-07-26_14-15.zip data/backups/python-packages/latest.zip\nmkdir -p data/backups/graphs\ndocker compose run -i --rm --env \"OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" --volume /home/ttelleis/cmem-dist/cmem-orchestration/data:/data --volume /home/ttelleis/cmem-dist/cmem-orchestration/conf/cmemc/cmemc.ini:/config/cmemc.ini cmemc -c cmem admin store export /data/backups/graphs/2024-07-26_14-15.zip\nExporting graphs backup to /data/backups/graphs/2024-07-26_14-15.zip ... done\nln -sf 2024-07-26_14-15.zip data/backups/graphs/latest.zip\nzip -r data/backups/2024-07-26_14-15.zip data/backups/keycloak/2024-07-26_14-15.sql data/backups/workspace/2024-07-26_14-15.zip data/backups/graphs/2024-07-26_14-15.zip data/backups/python-packages/2024-07-26_14-15.zip\n  adding: data/backups/keycloak/2024-07-26_14-15.sql (deflated 82%)\n  adding: data/backups/workspace/2024-07-26_14-15.zip (stored 0%)\n  adding: data/backups/graphs/2024-07-26_14-15.zip (stored 0%)\n  adding: data/backups/python-packages/2024-07-26_14-15.zip (stored 0%)\nln -sf 2024-07-26_14-15.zip data/backups/latest.zip\n</code></pre> The full backup is now at <code>data/backups/latest.zip</code>.</p>"},{"location":"deploy-and-configure/requirements/","title":"Requirements","text":"<p>This page lists software and hardware requirements for eccenca Corporate Memory deployments. For a general overview of a deployment setup please refer to the\u00a0System Architecture.</p>"},{"location":"deploy-and-configure/requirements/#minimal-setup","title":"Minimal Setup","text":"<p>A minimal single-node deployment for testing/evaluation purposes means:</p> <ul> <li>no memory consuming linking and transformation workflows,</li> <li>nearly no concurrent users.</li> </ul> <p>Depending on how much RAM is dedicated to the triple store, Knowledge Graphs up to several million triples can be built and served.</p> <ul> <li>Operating System / Hardware<ul> <li>Bare metal server or VM with Debian based linux OS (see\u00a0Installation\u00a0for details)</li> <li>16 GB RAM</li> <li>100 GB free disk space (10 GB for docker images + data + logs over time)</li> <li>docker and docker compose (we deliver an orchestration including all needed components)</li> </ul> </li> </ul> <p>For an example of a single-node installation refer to the following scenarios:</p> <ul> <li>Scenario: Local Installation</li> <li>Scenario: Single Node Cloud Installation</li> </ul>"},{"location":"deploy-and-configure/requirements/#typical-setup","title":"Typical Setup","text":"<p>In a typical deployment all components are installed in a kubernetes cluster.</p> <p>The following numbers are based on existing customer deployments running Knowledge Graphs up to 300 million triples.</p> Component CPU Memory eccenca Explore &gt;= 2 cores<sup>1</sup> &gt;= 4 GB RAM eccenca Build (DataIntegration) &gt;= 4 cores<sup>2</sup> &gt;= 8 GB RAM<sup>2</sup> Triple / Quad Store &gt;= 8 cores<sup>1</sup> &gt;= 16 GB RAM<sup>3</sup> Keycloak incl. PostgreSQL<sup>4</sup> 2 cores &gt;= 2 GB RAM <p>For GraphDB always also have a look at GraphDB recommendations.</p>"},{"location":"deploy-and-configure/requirements/#clients","title":"Clients","text":""},{"location":"deploy-and-configure/requirements/#browser-web-client","title":"Browser / Web Client","text":"<p>We support all (LTS/ESR) versions of the below listed browsers that are actively supported be the respective publishers:</p> <ul> <li>Microsoft Edge &gt; v88.0</li> <li>Google Chrome or Chromium &gt; v92.0</li> <li>Firefox &gt; v78.0</li> </ul> <p>Note</p> <p>Internet Explorer 11 as well as Safari Browser are not officially supported. IE11 is reported not to work.</p>"},{"location":"deploy-and-configure/requirements/#command-line-client-cmemc","title":"Command Line Client (cmemc)","text":"<p>cmemc is supported by the following Python versions: </p> <p>There is also a docker image available.</p> <ol> <li> <p>Needs to be scaled with concurrent users.\u00a0\u21a9\u21a9</p> </li> <li> <p>Depends on the Build (DataIntegration) workflows.\u00a0\u21a9\u21a9</p> </li> <li> <p>Needs to be scaled with the amount of triples.\u00a0\u21a9</p> </li> <li> <p>In cloud deployments, this could / will be a cloud service.\u00a0\u21a9</p> </li> </ol>"},{"location":"deploy-and-configure/system-architecture/","title":"System Architecture","text":"<p>This page describes the overall system architecture of eccenca Corporate Memory and its components.</p> <p></p> <p>eccenca Corporate Memory consists of three core components:</p> <ul> <li>eccenca Build</li> <li>eccenca Explore, and</li> <li>cmemc (Corporate Memory Control)</li> </ul> <p>Build is the Corporate Memory component which enables integration of datasets into a single consistent knowledge graph. Datasets in their original format are mapped to RDF schemata and then linked to and persisted into a knowledge graph. The data integration is performed semi-automatically based on domain-specific integration rules and vocabularies (OWL ontologies). Corporate Memory supports multiple kinds of source integration data sources such as SQL databases or files of different formats. These files can be processed with Build.</p> <p>Explore is a single-page JavaScript application which enables creating and managing knowledge graphs based on established W3C standards. It is a generic data browser suitable to edit, explore and query the created knowledge graph. Explore provides convenient options to create specific data views by using\u00a0Shapes Constraint Language\u00a0(SHACL).</p> <p>Explore also acts as the semantic middleware application which provides a unified access to semantic graph data. Additionally, Explore manages authorization of the users according to the access control lists defined in the Triple Store. The knowledge graph is stored in a quad store connected to Explore. This can either be a physical store like\u00a0GraphDB,\u00a0Virtuoso\u00a0or a remotely accessible SPARQL 1.1 compliant HTTP endpoint.</p> <p>Keycloak provides authentication. Keycloak can act as an authentication broker for already existing, external OpenId Connect or SAML infrastructures. In addition to that, Keycloak supports a wide variety of internal user management configuration scenarios and the option to connect to an external LDAP server for user and group synchronization. Keycloak uses the embedded Java-based relational database H2 as a default to store its configuration data. However, it is highly recommended to\u00a0use a relational database for production use instead. Refer to the\u00a0Keycloak manual\u00a0for further information on possible setups.</p> <p>cmemc\u00a0(Corporate Memory Control) is the eccenca Corporate Memory Command Line Interface (CLI). cmemc is intended for System Administrators and Linked Data Experts who wants to automate and remote control activities on Corporate Memory.</p>"},{"location":"develop/","title":"Develop","text":"<p>API documentation and programming recipes.</p> <p> Intended audience: Software Developers and Linked Data Experts</p> <ul> <li> <p> Java</p> <p>Accessing Graphs with Java Applications covers how to connect to Corporate Memory using a Java program.</p> </li> <li> <p> Python</p> <p>For Python developers, we offer a Plugin SDK as well as an API for accessing and manipulating Corporate Memory Instances (cmem-cmempy).</p> </li> <li> <p> OpenAPI specification</p> <p>API Specifications are available for</p> <ul> <li>Build (DataIntegration) (Build) and</li> <li>Explore backend (Explore/Consume).</li> </ul> </li> </ul>"},{"location":"develop/accessing-graphs-with-java-applications/","title":"Accessing Graphs with Java Applications","text":"","tags":["Java"]},{"location":"develop/accessing-graphs-with-java-applications/#introduction","title":"Introduction","text":"<p>This short recipe covers how to connect to Corporate Memory using a Java program. Such program can connect to Corporate Memory at any time autonomously, independently of whether a user is logged in or not.</p>","tags":["Java"]},{"location":"develop/accessing-graphs-with-java-applications/#java-example","title":"Java Example","text":"<p>This example assumes that there is a Corporate Memory instance runnning at <code>http://docker.localhost</code>, and the programmer has access to its files. The process is very simple:</p> <ol> <li>Obtain a Bearer token.<ol> <li>Go to the file <code>cmem-orchestration/environments/config.env</code>, and get the client secret from variable <code>CMEM_SERVICE_ACCOUNT_CLIENT_SECRET</code>.</li> <li>With the client secret, connect to to the OpenID endpoint to obtain the Bearer token.</li> </ol> </li> <li>Use the Bearer token to connect to Corporate Memory, and, for example, execute a query.</li> </ol> <p>The following code provides a simple implementation of the process:</p> JavaCMEMHTTPClient.java<pre><code>package com.eccenca.cmem.client;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.http.HttpEntity;\nimport org.apache.http.HttpResponse;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.ClientProtocolException;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClientBuilder;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\nimport org.json.JSONObject;\n\npublic class HTTPClient {\n\n  public static void main(String[] args) throws ClientProtocolException, IOException {\n\n    // We assume that the Corporate Memory instance is running at docker.localhost\n    String openidConnectEndpoint = \"http://docker.localhost/auth/realms/cmem/protocol/openid-connect/token\";\n\n    // Get the client secret to obtain the bearer token from file cmem-orchestration/environments/config.env,\n    // variable CMEM_SERVICE_ACCOUNT_CLIENT_SECRET\n    String clientSecret = \"...\";\n\n    // Create an HTTP Client\n    CloseableHttpClient client = HttpClientBuilder.create().build();\n\n    // POST request to obtain the bearer token for later authorization\n    HttpPost httpPostToken = new HttpPost(openidConnectEndpoint);\n    httpPostToken.setHeader(\"Content-type\", \"application/x-www-form-urlencoded\");\n    List &lt; NameValuePair &gt; params = new ArrayList &lt; NameValuePair &gt; ();\n    params.add(new BasicNameValuePair(\"grant_type\", \"client_credentials\"));\n    params.add(new BasicNameValuePair(\"client_id\", \"cmem-service-account\"));\n    params.add(new BasicNameValuePair(\"client_secret\", clientSecret));\n    httpPostToken.setEntity(new UrlEncodedFormEntity(params));\n\n    // Parse the JSON response to obtain the bearer token\n    HttpResponse httpResponseToken = client.execute(httpPostToken);\n    HttpEntity httpEntity = httpResponseToken.getEntity();\n    String responseBody = EntityUtils.toString(httpEntity);\n    JSONObject obj = new JSONObject(responseBody);\n    String bearerToken = \"Bearer \" + obj.getString(\"access_token\");\n\n    // POST request to query the default SPARQL endpoint with the bearer token obtained above\n    HttpPost httpPostQuery = new HttpPost(\"http://docker.localhost/dataplatform/proxy/default/sparql\");\n    httpPostQuery.setHeader(\"Accept\", \"application/sparql-results+json\");\n    httpPostQuery.setHeader(\"Content-type\", \"application/x-www-form-urlencoded\");\n    httpPostQuery.setHeader(\"Authorization\", bearerToken);\n    final ArrayList &lt; NameValuePair &gt; postParameters = new ArrayList &lt; NameValuePair &gt; ();\n    postParameters.add(new BasicNameValuePair(\"query\", \"SELECT * WHERE {?s ?p ?o} LIMIT 10\"));\n    httpPostQuery.setEntity(new UrlEncodedFormEntity(postParameters));\n\n    // The response (variable responseBodyQuery bellow) should have some bindings:\n    //  {\n    //     \"head\": {\n    //       \"vars\": [ \"s\" , \"p\" , \"o\" ]\n    //     } ,\n    //     \"results\": {\n    //       \"bindings\": [\n    HttpResponse httpResponseQuery = client.execute(httpPostQuery);\n    HttpEntity httpEntityQuery = httpResponseQuery.getEntity();\n    String responseBodyQuery = EntityUtils.toString(httpEntityQuery);\n  }\n}\n</code></pre>","tags":["Java"]},{"location":"develop/cmemc-scripts/","title":"cmemc - Python Scripts","text":"","tags":["cmemc","Python","Automate"]},{"location":"develop/cmemc-scripts/#introduction","title":"Introduction","text":"<p>As a more lightweight and fault-tolerant alternative to using cmempy directly, we recommend to call cmemc commands directly in your Python automation scripts.</p> <p>The advantages of this approach are:</p> <ul> <li>You can test and use your calls in the command line before integrating them.</li> <li>You don\u2019t have to worry about internal details and have a well-documented and stable interface.</li> <li>Authorization is done in the same way, cmemc is doing this.</li> </ul>","tags":["cmemc","Python","Automate"]},{"location":"develop/cmemc-scripts/#installation","title":"Installation","text":"<p>cmemc is published as an Apache 2 licensed open source python package at pypi.org, hence you are able to install it with a simple pip command:</p> <pre><code>pip install cmem-cmemc\n</code></pre>","tags":["cmemc","Python","Automate"]},{"location":"develop/cmemc-scripts/#configure-a-connection","title":"Configure a connection","text":"<p>Assuming you have already configured your cmemc connection, using it in Python scripts is quite easy.</p> <pre><code>from os import environ\n\n# use the cmemc connection\nenviron[\"CMEMC_CONNECTION\"] = \"my-dev-cmem\"\n</code></pre> <p>This will tell cmemc to use the configured connection <code>my-dev-cmem</code>.</p>","tags":["cmemc","Python","Automate"]},{"location":"develop/cmemc-scripts/#running-commands","title":"Running commands","text":"<p>In order to execute a command and process the results, you can use this wrapper function:</p> <pre><code>import json\n\nfrom click.testing import CliRunner\n\nfrom cmem_cmemc.cli import cli\n\n\ndef cmemc(params: list[str]) -&gt; str | list[str] | list[dict]:\n    \"\"\"Run a cmemc command and provide the output.\n\n    When using the --raw option, output is a parsed list of dictionaries.\n    When using the --id-only option, output is a parsed list of strings.\n    Returns plain text output otherwise.\n\n    Have a look at click API documentation for more options:\n    https://click.palletsprojects.com/en/stable/api/#click.testing.CliRunner\n    \"\"\"\n    result = CliRunner().invoke(cli, params)\n    if result.exit_code != 0:\n        raise RuntimeError(result.exception)\n    output: str = result.stdout\n    if \"--id-only\" in params:\n        return output.splitlines()\n    if \"--raw\" in params:\n        return json.loads(output)\n    return output\n</code></pre> <p>This function will use the configured connection to execute a command and return the result. Please note that the output can bei either a multi-line string, a list of strings or a list of dictionaries. This is because we made sure that <code>--raw</code> outputs JSON and <code>--id-only</code> only identifier.</p> <p>As a last step, you can iterate and process the results:</p> <pre><code># specify the command as a list of arguments - in this case: list all existing worflows\ncommand = [\"workflow\", \"list\", \"--raw\"]\n\n# iterate and process the output\nfor workflow in cmemc(command):\n    project_id = workflow.get(\"projectId\")\n    workflow_id = workflow.get(\"id\")\n    print(f\"Workflow '{workflow_id}' in '{project_id}'\")\n</code></pre>","tags":["cmemc","Python","Automate"]},{"location":"develop/cmemc-scripts/#using-a-shebang-to-create-an-executable-file","title":"Using a shebang to create an executable file","text":"<p>As a nice addon, you could extend your script with a shebang to start uv. This will also install and manage dependencies and python versions for you:</p> <p>The complete script looks like this:</p> <pre><code>#!/usr/bin/env -S uv run --script\n# /// script\n# dependencies = [\n#   \"cmem-cmemc\",\n# ]\n# ///\nimport json\nfrom os import environ\n\nfrom click.testing import CliRunner\n\nfrom cmem_cmemc.cli import cli\n\n\ndef cmemc(params: list[str]) -&gt; str | list[str] | list[dict]:\n    \"\"\"Run a cmemc command and provide the output.\n\n    When using the cmemc --raw option, output is a parsed list of dictionaries.\n    When using the cmemc --id-only option, output is a parsed list of strings.\n\n    Have a look at click API documentation for more options:\n    https://click.palletsprojects.com/en/stable/api/#click.testing.CliRunner\n    \"\"\"\n    result = CliRunner().invoke(cli, params)\n    if result.exit_code != 0:\n        raise RuntimeError(result.exception)\n    output: str = result.stdout\n    if \"--id-only\" in params:\n        return output.splitlines()\n    if \"--raw\" in params:\n        return json.loads(output)\n    return output\n\n\n# use the cmemc connection\nenviron[\"CMEMC_CONNECTION\"] = \"my-dev-cmem\"\n\n# specify the command as a list of arguments\n# in this case: list all existing worflows\ncommand = [\"workflow\", \"list\", \"--raw\"]\n\n# iterate and process the output\nfor workflow in cmemc(command):\n    project_id = workflow.get(\"projectId\")\n    workflow_id = workflow.get(\"id\")\n    print(f\"Workflow '{workflow_id}' in '{project_id}'\")\n</code></pre> <p>Executing this python script will look like this:</p> <pre><code>$ ./cmemc-script.py\nWorkflow 'input-output' in 'io'\nWorkflow 'input-output-replaceable' in 'io'\nWorkflow 'normal' in 'io'\nWorkflow 'only-input' in 'io'\nWorkflow 'only-input-included' in 'io'\nWorkflow 'only-input-replaceable' in 'io'\nWorkflow 'only-output' in 'io'\nWorkflow 'only-output-replaceable' in 'io'\n</code></pre>","tags":["cmemc","Python","Automate"]},{"location":"develop/cmempy-python-api/","title":"cmempy - Python API","text":"","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#introduction","title":"Introduction","text":"<p>cmempy is a Python API wrapper around the eccenca Corporate Memory HTTP APIs which can be used to rapidly script processes which interact with Corporate Memory. cmempy is also the underlying Python module which powers the cmemc - Command Line Interface.</p>","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#installation","title":"Installation","text":"<p>cmempy is published as an Apache 2 licensed open source python package at pypi.org, hence you are able to install it with a simple pip command:</p> <pre><code>pip install cmem-cmempy\n</code></pre>","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#configure-a-connection","title":"Configure a Connection","text":"<p>The used Corporate Memory connection is configured by providing environment variables similar to cmemc (Environment based Configuration).</p> <p>These environment variables can be created and changed in your code or used from the process which executes your python code (e.g. your shell). If you have a working cmemc file based configuration setup already you can export the environment to your shell using cmemc config eval.</p> <p>The following table lists all processed environment variables:</p> Variable Description Default Value CMEM_BASE_URI Base URL of your Corporate Memory http://docker.localhost DI_API_ENDPOINT Build (Data Integration) API endpoint CMEM_BASE_URI/dataintegration DP_API_ENDPOINT Explore backend API endpoint CMEM_BASE_URI/dataplatform OAUTH_TOKEN_URI OAuth 2.0 Token endpoint CMEM_BASE_URI/auth/realms/cmem/protocol/openid-connect/token OAUTH_GRANT_TYPE OAuth 2.0 grant type (password or client_credentials) client_credentials OAUTH_USER Username to retrieve the token admin OAUTH_PASSWORD Password to retrieve the token secret OAUTH_CLIENT_ID OAuth 2.0 client id cmem-service-account OAUTH_CLIENT_SECRET OAuth 2.0 client secret secret SSL_VERIFY Verify SSL certs for API requestsv True REQUESTS_CA_BUNDLE Path to the CA Bundle file (.pem) Internal path to included CA bundle","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#commented-example","title":"Commented Example","text":"<p>Here is a commented code example how to configure and use cmempy.</p> <p>The example demonstrates, how to execute SPARQL queries via Explore backend, as well as how to work with the Build (DataIntegration) workspace and retrieve workflow status information:</p> example_usage.py<pre><code>\"\"\"Basic example, how to use cmempy\"\"\"\n\nfrom os import environ\n\nfrom cmem.cmempy.workspace.projects.project import get_projects\nfrom cmem.cmempy.workflow import get_workflows\nfrom cmem.cmempy.workspace.activities.taskactivity import get_activity_status\nfrom cmem.cmempy.queries import SparqlQuery\n\n# setup the environment for the connection to Corporate Memory\nenviron[\"CMEM_BASE_URI\"] = \"http://docker.local\"\nenviron[\"OAUTH_GRANT_TYPE\"] = \"client_credentials\"\nenviron[\"OAUTH_CLIENT_ID\"] = \"cmem-service-account\"\nenviron[\"OAUTH_CLIENT_SECRET\"] = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n\n# this query simply lists 5 resource subjects from the triple store\nQUERY_TEXT = \"SELECT DISTINCT ?s WHERE {?s ?p ?o} LIMIT 5\"\n\n# the default result is a JSON structure according to the W3C standard\n# seeAlso: https://www.w3.org/TR/sparql11-results-json/\nresults = SparqlQuery(QUERY_TEXT).get_results()\nprint(results)\n\n# loop over project descriptions\nfor project in get_projects():\n    project_id = project[\"name\"]\n    print(\"Project: {}:\".format(project_id))\n\n    # loop over workflow ids for a project\n    for workflow_id in get_workflows(project_id):\n        # get the status object of a specific workflow\n        status = get_activity_status(project_id, workflow_id)\n        message = status[\"message\"]\n        print(\"- Workflow: {} ({}):\".format(workflow_id, message))\n</code></pre> <p>Starting this script should result in an output similar to this:</p> <pre><code>$ python example_usage.py\n{\n  \"head\": {\n    \"vars\": [ \"s\" ]\n  } ,\n  \"results\": {\n    \"bindings\": [\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"https://vocab.eccenca.com/dsm/\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"https://vocab.eccenca.com/dsm/ThesaurusProject\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"http://www.w3.org/2002/07/owl#\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"http://www.w3.org/2000/01/rdf-schema#\" }\n      }\n    ]\n  }\n}\n\nProject: cmem:\n- Workflow: my-workflow (Idle):\n</code></pre>","tags":["API","Python"]},{"location":"develop/dataintegration-apis/","title":"Build (DataIntegration) APIs","text":"<p>The latest OpenAPI specification is available at https://releases.eccenca.com/OpenAPI/.</p> <p>You can (re)view it with the redoc web UI or the petstore web UI.</p>","tags":["API"]},{"location":"develop/dataintegration-apis/#introduction","title":"Introduction","text":"<p>eccenca Build (DataIntegration) APIs can be used to control, initiate and setup all task and activities related to the \u2605 Build step (such as datasets, transformations, linking tasks etc.).</p>","tags":["API"]},{"location":"develop/dataintegration-apis/#media-types","title":"Media Types","text":"<p>The default media type of most responses is application/json. Other possible response media types can be reached by changing the Accept header of the request.</p> <p>Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method.</p> <p>Dependent on the specific API, eccenca Build (DataIntegration) works with the following application media types which correspond to the following specification documents:</p> Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/xml XML Media Types application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/problem+json Problem Details for HTTP APIs","tags":["API"]},{"location":"develop/dataplatform-apis/","title":"Explore backend APIs","text":"<p>The latest OpenAPI specification is available at https://releases.eccenca.com/OpenAPI/.</p> <p>You can (re)view it with the redoc web UI or the petstore web UI.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#introduction","title":"Introduction","text":"<p>eccenca Explore backend (DataPlatform) APIs can be used to import, export, query and extract information from graphs as well as to check access conditions.</p> <p>This section describes common characteristics and features of all provided APIs.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#media-types","title":"Media Types","text":"<p>The default media type of most responses is <code>application/json</code>. Other possible response media types can be reached by changing the Accept header of the request. Alternatively, the desired response media type can be expressed in the request URI.</p> <p>Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method.</p> <p>Dependent on the specific API, eccenca Explore backend works with the following application media types which correspond to the following specification documents:</p> Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/ld+json JSON-LD 1.0 text/turtle RDF 1.1 Turtle - Terse RDF Triple Language application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/rdf+xml RDF 1.1 XML Syntax application/n-quads RDF 1.1 N-Quads application/trig RDF 1.1 TriG application/sparql-query SPARQL 1.1 Query Language application/sparql-update SPARQL 1.1 Update application/sparql-results+json SPARQL 1.1 Query Results JSON Format application/sparql-results+xml SPARQL Query Results XML Format (Second Edition) text/csv SPARQL 1.1 Query Results CSV and TSV Formats text/tab-separated-values SPARQL 1.1 Query Results CSV and TSV Formats application/vnd.openxmlformats-officedocument.spreadsheetml.sheet Microsoft Office Excel (.xlsx) format application/problem+json Problem Details for HTTP APIs","tags":["API"]},{"location":"develop/dataplatform-apis/#media-type-request-by-uri","title":"Media type request by URI","text":"<p>The desired response media type can be requested by adding a format query parameter. The parameter value is interpreted as a media type abbreviation that is expanded to a proper media type string. eccenca Explore backend maps the following abbreviations to media types it supports:</p> Abbreviation Media Type rdf application/rdf+xml ttl text/turtle jsonld application/ld+json nt application/n-triples trig application/trig nq application/n-quads srj application/sparql-results+json srx application/sparql-results+xml csv text/csv tsv text/tab-separated-values <p>Thus, for example, a request to <code>/proxy/default/graph</code> with the Accept header value <code>text/turtle</code> and a request to <code>/proxy/default/graph?format=ttl</code> express the same intent for the media type of the response.</p> <p>If both this format query parameter and an Accept header is present in a request, the parameter value takes precedence.</p> <p>Usage of media type request by URI can be useful to create browser links that will express an intent for the media type of the response.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#security-schemes","title":"Security Schemes","text":"<p>The default security scheme is OAuth 2.0. However, this can be changed in the configuration.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#sparql-result-set-streaming","title":"SPARQL result set streaming","text":"<p>The SPARQL proxy pipes the results of SPARQL queries directly from the underlying data endpoint to the request client. This however does not always apply for CONSTRUCT queries.</p> <p>The result of a CONSTRUCT query is a set of statements. RDF graph serialization formats tend to group the information for compactness - e.g. in Turtle, all statements for a subject are written together - avoiding subject and subject-predicate repetition, for which it is necessary to have the complete result set at disposal.</p> <p>Therefore, before sending the result to the request client, the complete result is loaded and then serialized. This creates a potential danger whenever a large result set is build and could lead to overload of the server.</p> <p>There is however one serialization format (the N-Triples format) which is streaming friendly and that should always be used whenever large result sets are expected.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#sparql-default-graph-rdf-dataset","title":"SPARQL default graph &amp; RDF dataset","text":"","tags":["API"]},{"location":"develop/dataplatform-apis/#default-graph","title":"Default graph","text":"<p>The definition of the RDF dataset of a query in the SPARQL 1.1 specification leads to problems regarding the default graph of a SPARQL service. On one hand it is defined that:</p> <p>A SPARQL query is executed against an RDF dataset which represents a collection of graphs. An RDF dataset comprises one graph, the default graph, which does not have a name, and zero or more named graphs, where each named graph is identified by an IRI.</p> <p>Furthermore, it says:</p> <p>A SPARQL query may specify the dataset to be used for matching by using the FROM clause and the FROM NAMED clause to describe the RDF dataset. If a query provides such a dataset description, then it is used in place of any dataset that the query service would use if no dataset description is provided in a query. The RDF dataset may also be specified in a SPARQL protocol request, in which case the protocol description overrides any description in the query itself. A query service may refuse a query request if the dataset description is not acceptable to the service.</p> <p>The FROM and FROM NAMED keywords allow a query to specify an RDF dataset by reference; they indicate that the dataset should include graphs that are obtained from representations of the resources identified by the given IRIs (i.e. the absolute form of the given IRI references). The dataset resulting from a number of FROM and FROM NAMED clauses is:</p> <ul> <li>a default graph consisting of the RDF merge of the graphs referred to in the FROM clauses, and</li> <li>a set of (IRI, graph) pairs, one from each FROM NAMED clause.</li> </ul> <p>If there is no FROM clause, but there is one or more FROM NAMED clauses, then the dataset includes an empty graph for the default graph.</p> <p>That means the default graph of a SPARQL service cannot be explicitly referenced in the RDF dataset of a SPARQL query using FROM / FROM NAMED.</p> <p>For this reason, Explore backend does not allow the manipulation of the service\u2019s default graph.</p> <p>To enforce this policy, the following restriction applies to incoming SPARQL 1.1 Update queries:</p> <ul> <li>Update queries (INSERT DATA, DELETE DATA and DELETE/INSERT) targeted against the service\u2019s default graph will not be accepted by returning an HTTP 400 Bad Request status code.</li> </ul>","tags":["API"]},{"location":"develop/dataplatform-apis/#default-rdf-dataset","title":"Default RDF dataset","text":"<p>The interpretation of the RDF dataset of a query differs between various SPARQL service implementations. In the case a query declares no RDF dataset, Explore backend uses the following default RDF dataset declaration to provide a uniform behavior for all supported SPARQL services:</p> <ul> <li>The default graph is the union (RDF Merge graph) of all named graphs the user is allowed to access.</li> <li>The set of named graphs contains all named graphs the user is allowed to access.</li> </ul>","tags":["API"]},{"location":"develop/dataplatform-apis/#http-error-responses","title":"HTTP error responses","text":"<p>The default format for HTTP error responses is compliant with RFC 7807 Problem Details for HTTP APIs. An HTTP error response contains a JSON object that provides at least two fields:</p> <ul> <li><code>title</code>: A short, human-readable summary of the problem type.</li> <li><code>detail</code>: A human-readable explanation specific to this occurrence of the problem.</li> </ul> <p>The following optional non-standard fields may also be set:</p> <ul> <li><code>status</code>: The HTTP status code for this occurrence of the problem.</li> <li><code>cause</code>: The cause for this occurrence of the problem. It contains at least the same elements as specified previously, such as <code>title</code> and <code>detail</code>.</li> </ul> <p>The following example shows an HTTP response containing JSON problem details using the <code>application/problem+json</code> media type:</p> <pre><code>HTTP/1.1 500\nContent-Type: application/problem+json\n\n{\n  \"title\": \"Internal Server Error\",\n  \"status\": 500,\n  \"detail\": \"Database server 'Stardog' unavailable\",\n  \"cause\": {\n    \"title\": \"Internal Server Error\",\n    \"status\": 500,\n    \"detail\": \"Connection refused (Connection refused)\"\n  }\n}\n</code></pre>","tags":["API"]},{"location":"develop/python-plugins/","title":"Python Plugins","text":"<p>Beginning from version 22.1, we support the extension of Build (DataIntegration) with build plugins.</p> <p>The following pages give an overview about this feature:</p> <ul> <li> <p> Installation and Usage</p> <p>Intended for Linked Data Experts and Deployment Engineers, this page outlines how to install and use existing python plugins.</p> </li> <li> <p> Development</p> <p>Intended for Developers, this page gives an overview on the plugin concepts and how to start developing your own plugins.</p> </li> <li> <p> Setup and Configuration</p> <p>Intended for Deployment Engineers, this page discusses setup and configuration issues.</p> </li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/","title":"Python Plugin Development","text":"","tags":["Python"]},{"location":"develop/python-plugins/development/#introduction","title":"Introduction","text":"<p>Python plugins are small software projects which extend the functionality of eccenca Corporate Memory. They have its own release cycle and are not included in the main software. Python plugins can can be installed and uninstalled during runtime.</p> <p>In order to support the development of python plugins, we published a base package as well as a project template. Please have a look at these projects to get started.</p> <p>This page gives an overview of the concepts you need to understand in order to develop plugins.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#base-package","title":"Base Package","text":"<p><code>cmem-plugin-base</code> is a Python library that provides a set of base classes for developing plugins for the eccenca Corporate Memory (CMEM) platform. These base classes provide a consistent interface for defining new plugins, handling configuration, and communicating with the Build (DataIntegration) of CMEM.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin-types","title":"Plugin Types","text":"<p>The following plugin types are defined.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#workflow-plugins","title":"Workflow Plugins","text":"<p>A workflow plugin implements a new operator (task) that can be used within a workflow. A workflow plugin may accept an arbitrary list of inputs and optionally returns a single output.</p> <p>The lifecycle of a workflow plugin is as follows:</p> <ul> <li>The plugin will be instantiated once the workflow execution reaches the respective plugin.</li> <li>The <code>execute</code> function is called and gets the results of the ingoing operators as input.</li> <li>The output is forwarded to the next operator.</li> </ul> <p>The following depiction shows a task of the plugin My Workflow Plugin. The task has two connected incoming tasks and one connected outgoing task.</p> <p></p> <p>The corresponding source code of the plugin is listed below.</p> workflow.py<pre><code>from typing import Sequence\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext\nfrom cmem_plugin_base.dataintegration.description import PluginParameter, Plugin\nfrom cmem_plugin_base.dataintegration.entity import Entities\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\n\n@Plugin(label=\"My Workflow Plugin\")\nclass MyWorkflowPlugin(WorkflowPlugin):\n    \"\"\"My Workflow Plugin\"\"\"\n\n    def execute(\n        self, inputs: Sequence[Entities], context: ExecutionContext\n    ) -&gt; Entities:\n        return inputs[0]\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#transform-plugins","title":"Transform Plugins","text":"<p>A transform plugin can be used in transform and linking rules. It accepts an arbitrary number of inputs and returns an output. Each input as well as the output consists of a sequence of values.</p> <p>The image below shows a value transformation that uses the My Transform Plugin plugin. The plugin splits the input string into a list of words and forwards only the last one.</p> <p></p> <p>The corresponding source code of the plugin is listed below.</p> transform.py<pre><code>from typing import Sequence\nfrom cmem_plugin_base.dataintegration.description import PluginParameter, Plugin\nfrom cmem_plugin_base.dataintegration.plugins import TransformPlugin\n\n\n@Plugin(label=\"My Transform Plugin\")\nclass MyTransformPlugin(TransformPlugin):\n    \"\"\"My Transform Plugin\"\"\"\n\n    def transform(self, inputs: Sequence[Sequence[str]]) -&gt; Sequence[str]:\n        for item in inputs:\n            return item[0].split(\" \")[-1]\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin","title":"Plugin","text":"<p>The <code>@Plugin</code> decorator is used to mark a Python class as a Workflow/Transform Operator plugin. This decorator takes several parameters that provide information about the plugin, such as the <code>label</code>, <code>plugin_id</code>, and <code>description</code>. It also allows for defining a list of <code>parameters</code> that can be used to customize the plugin behavior. The <code>documentation</code> parameter can be used to provide additional information about the plugin that will be displayed in the Workflow Editor.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin-parameter","title":"Plugin Parameter","text":"<p>The <code>PluginParameter</code> represents a parameter that can be used to customize a Workflow/Transform Operator plugin\u2019s behavior.</p> <p>The <code>PluginParameter</code> class can be instantiated multiple times within a <code>@Plugin</code> decorator to create a list of <code>parameters</code> that can be used to customize the plugin\u2019s behavior. The <code>param_type</code> parameter can be set to a specific parameter type class that extends the <code>ParameterType</code> base class to validate user input and provide additional functionality.</p> <p>The <code>PluginParameter</code> has several parameters that can be specified when initializing an instance:</p> <ul> <li><code>name</code>: The name of the parameter. This is a required parameter and must be specified.</li> <li><code>label</code>: A visible label of the parameter. This is an optional parameter and can be left blank. If left blank, the name of the parameter will be used as the label.</li> <li><code>description</code>: A visible description of the parameter. This is an optional parameter and can be left blank.</li> <li><code>param_type</code>: Optionally overrides the parameter type. Usually, this does not have to be set manually as it will be inferred from the plugin automatically.</li> <li><code>default_value</code>: The parameter default value (optional). If not specified, it will be inferred from the plugin automatically.</li> <li><code>advanced</code>: A boolean flag indicating whether or not this is an advanced parameter that can only be changed in the advanced section. This is an optional parameter and defaults to False.</li> <li><code>visible</code>: A boolean flag indicating whether or not the parameter will be displayed to the user in the UI. This is an optional parameter and defaults to True.</li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/#parameter-type","title":"Parameter Type","text":"<p>The <code>ParameterType</code> class is a generic class that serves as the base for all other parameter types. It has methods for converting parameter values to and from strings, and for providing auto-completion suggestions.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#concrete-parameter-types","title":"Concrete Parameter Types","text":"<p>There are several concrete parameter types defined in the module, including <code>StringParameterType</code>, <code>IntParameterType</code>, <code>FloatParameterType</code>, <code>BoolParameterType</code>, <code>PluginContextParameterType</code>, and <code>EnumParameterType</code>. These correspond to different types of parameters that a plugin might use, such as strings, integers, and boolean values.</p> <p>Each concrete parameter type implements the <code>from_string</code> and <code>to_string</code> methods for converting parameter values to and from strings.</p> <p><code>EnumParameterType</code> also takes an additional argument in its constructor to specify the enumeration type it represents.</p> <p>Concrete Parameters Initialization</p> <pre><code>\"\"\"Concrete Parameters Example\"\"\"\nfrom enum import Enum\nfrom typing import Sequence\n\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext\nfrom cmem_plugin_base.dataintegration.description import Plugin, PluginParameter\nfrom cmem_plugin_base.dataintegration.entity import (\n    Entities,\n)\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\nfrom cmem_plugin_base.dataintegration.types import EnumParameterType\n\n\nclass Animal(Enum):\n    \"\"\"Animal Enum\"\"\"\n\n    CAT = 1\n    DOG = 2\n    HORSE = 3\n    LION = 4\n\n\n@Plugin(\n    label=\"Concrete Parameters Example\",\n    description=\"Use of concrete parameters and set default values\",\n    documentation=\"\"\"\n- `value_int`: A parameter of Integer Type\n- `value_float`: A parameter of Integer Type\n- `value_str`: A Parameter of String Type\n- `value_bool`: A Parameter of Boolean Type\n- `value_enum`: A Parameter of Enum Type\n\"\"\",\n    parameters=[\n        PluginParameter(\n            name=\"value_int\",\n            label=\"Integer\",\n            description=\"A parameter of Integer Type\",\n            default_value=10,\n        ),\n        PluginParameter(\n            name=\"value_float\",\n            label=\"Float\",\n            description=\"A parameter of Integer Type\",\n            default_value=5.0,\n        ),\n        PluginParameter(\n            name=\"value_str\",\n            label=\"String\",\n            description=\"A Parameter of String Type\",\n            default_value=\"eccenca Developer\",\n        ),\n        PluginParameter(\n            name=\"value_bool\",\n            label=\"Boolean\",\n            description=\"A Parameter of Boolean Type\",\n            default_value=True,\n        ),\n        PluginParameter(\n            name=\"value_enum\",\n            label=\"Enum\",\n            description=\"A Parameter of Enum Type \",\n            param_type=EnumParameterType(enum_type=Animal),\n            default_value=Animal.CAT,\n        ),\n    ],\n)\nclass ConcreteParameters(WorkflowPlugin):\n    \"\"\"Example Workflow Plugin: Random Values\"\"\"\n\n    def __init__(\n        self,\n        value_int: int = 10,\n        value_float: float = 5.0,\n        value_str: str = \"eccenca Developer\",\n        value_bool: bool = True,\n        value_enum: Animal = Animal.CAT,\n    ) -&gt; None:\n        self.value_int = value_int\n        self.value_float = value_float\n        self.value_str = value_str\n        self.value_bool = value_bool\n        self.value_enum = value_enum\n\n    def execute(self, inputs: Sequence[Entities], context: ExecutionContext) -&gt; None:\n        self.log.info(\"Concrete Parameters Example\")\n        self.log.info(f\"{self.value_int}\")\n        self.log.info(f\"{self.value_float}\")\n        self.log.info(f\"{self.value_str}\")\n        self.log.info(f\"{self.value_bool}\")\n        self.log.info(f\"{self.value_int}\")\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#build-dataintegration-parameter-types","title":"Build (DataIntegration) Parameter Types","text":"<p>In addition to concrete parameter types, the base package offers some special types that are derived from data integration. These special types include Password, Dataset, Multiline, Choice Type, and others. These types are provided to enhance the development of plugins and offer greater flexibility when creating custom parameters.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#choice-parametertype","title":"Choice ParameterType","text":"<p><code>ChoiceParameterType</code> that represents a parameter type with a pre-defined set of choices. It allows users to select from the available choices using autocompletion, and provides labels for each of the choices. Example</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#dataset-parametertype","title":"Dataset ParameterType","text":"<p><code>DatasetParameterType</code> can be used as a parameter type for plugins that require dataset input. It provides autocompletion suggestions based on the user\u2019s query terms and allows filtering datasets by dataset type (csv, json, etc.) in the current project. It also returns the label of the selected dataset as its value. Example</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#graph-parametertype","title":"Graph ParameterType","text":"<p><code>GraphParameterType</code> that represents a parameter type for selecting a knowledge graph. It provides autocompletion suggestions for available graphs, based on various filtering criteria, such as whether to show DI project graphs or system resource graphs, and which classes the graphs should belong to.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#multiline-parametertype","title":"Multiline ParameterType","text":"<p><code>MultilineStringParameterType</code> is used to represent a multiline string parameter type in a Build (DataIntegration), which allows multiline text entry from users. Example</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#password-parametertype","title":"Password ParameterType","text":"<p><code>PasswordParameterType</code> is a parameter type that can be used in plugins to handle password strings. When a <code>password</code> is entered by a user, this parameter type will encrypt and store the password so that it cannot be viewed by the user or stored in plain text. Example</p> <p>These are some examples of special type parameters, and you can find more for your plugin development here.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#code-parametertypes","title":"Code ParameterTypes","text":"<p><code>CodeParameterType</code> supports various different code languages.</p> <p>Currently, the following types are supported:</p> <ul> <li><code>JinjaCode</code>: Jinja 2 templates</li> <li><code>JsonCode</code>: JSON</li> <li><code>PythonCode</code>: Python</li> <li><code>SparqlCode</code>: SPARQL queries</li> <li><code>SqlCode</code>: SQL queries</li> <li><code>TurtleCode</code>: Turtle</li> <li><code>XmlCode</code>: XML</li> <li><code>YamlCode</code>: YAML configuration</li> </ul> <p>The simplest way to employ a code type is to annotate the respective parameter in the constructor:</p> <p>Example Jinja code parameter</p> <pre><code>@Plugin(label=\"Code test plugin\")\nclass TransformTestPlugin(TransformPlugin):\n\n    def __init__(self, jinja: JinjaCode = JinjaCode(\"default template\"):\n        self.jinja = jinja\n</code></pre> <p>If a plugin with a code parameter is created or edited in the UI, a code editor with syntax highlighting will be shown to the user.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#autocomplete-method","title":"AutoComplete Method","text":"<p>The <code>ParameterType</code> class also defines an <code>autocomplete()</code>, which can be used to provide auto-completion suggestions for a parameter value. The <code>EnumParameterType</code>is an example of a parameter type that uses auto-completion suggestions. This is a method that is designed to assist with autocompleting values when querying terms. It is a special method that requires attention as it plays an important role in providing suggestions for autocomplete functionality.</p> <p><code>autocomplete()</code> takes in three parameters: <code>query_terms</code>, <code>depend_on_parameter_values</code>, and <code>context</code>. It returns a list of <code>Autocompletion</code> objects, which represent the possible auto-completion results.</p> <ul> <li> <p>The <code>query_terms</code> parameter is a list of lower case conjunctive search terms. These are the search terms that the user has entered, and the <code>auto-completion()</code> will attempt to find results that match all of them.</p> </li> <li> <p>The <code>depend_on_parameter_values</code> parameter is a list of values for the parameters that the <code>auto-completion()</code> depends on. These values will be used to generate the auto-completion results. The type of each parameter value is the same as in the init method, which means that if a <code>password</code> parameter is specified, the type of the parameter value will be of <code>Password</code> Type.</p> </li> <li> <p>The <code>context</code> parameter represents the <code>PluginContext</code> in which the auto-completion is requested. This could be, for example, the context of a specific plugin, or the context of the entire system.</p> </li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/#autocompletion","title":"Autocompletion","text":"<p>The method returns a list of <code>Autocompletion</code> objects, which represent the possible auto-completion results. Each <code>Autocompletion</code> object has two attributes: value and label.</p> <ul> <li>The <code>value</code> attribute represents the value to which the parameter value should be set.</li> <li>The <code>label</code> attribute is an optional label that a human user would see instead.</li> </ul> <p>Note</p> <p><code>autocomplete()</code> should be modified to generate actual auto-completion results based on the input parameters.</p> <p>Example</p> <p><pre><code>class KaggleSearch(StringParameterType):\n\"\"\"Kaggle Search Type\"\"\"\n\nautocompletion_depends_on_parameters: list[str] = [\"username\", \"api_key\"]\n\n# auto complete for values\nallow_only_autocompleted_values: bool = True\n# auto complete for labels\nautocomplete_value_with_labels: bool = True\n\ndef autocomplete(\n    self,\n    query_terms: list[str],\n    depend_on_parameter_values: list[Any],\n    context: PluginContext,\n) -&gt; list[Autocompletion]:\n    auth(depend_on_parameter_values[0], depend_on_parameter_values[1].decrypt())\n    result = []\n    if len(query_terms) != 0:\n        datasets = search(query_terms=query_terms)\n        for dataset in datasets:\n            slug = get_slugs(str(dataset))\n            result.append(\n                Autocompletion(\n                    value=f\"{slug.owner}/{slug.name}\",\n                    label=f\"{slug.owner}/{slug.name}\",\n                )\n            )\n        result.sort(key=lambda x: x.label)  # type: ignore\n        return result\n    if len(query_terms) == 0:\n        label = \"Search for kaggle datasets\"\n        result.append(Autocompletion(value=\"\", label=f\"{label}\"))\n    result.sort(key=lambda x: x.label)  # type: ignore\n    return result\n</code></pre> The <code>KaggleSearch</code> class is a <code>StringParameterType</code> that allows users to search for Kaggle datasets. It inherits from the <code>StringParameterType</code> class and overrides its <code>autocomplete()</code> to provide autocompletion(of type <code>Autocompletion</code>) for search results.</p> <p>The <code>autocomplete()</code> method uses the search function to search for datasets on Kaggle and returns a list of <code>Autocompletion</code> objects representing the search results.</p> <p>The <code>autocompletion_depends_on_parameters</code> attribute of the KaggleSearch class is a list of strings that specifies which parameter values this autocomplete method depends on. In this case, it depends on the values of the <code>username</code> and <code>api_key</code> parameters in order to authenticate the Kaggle API.</p> <p>If the <code>query_terms</code> list is empty, it returns a single Autocompletion object with an empty value and a label prompting the user to search for Kaggle datasets.</p> <p>The <code>allow_only_autocompleted_values</code> attribute is set to <code>True</code>, which means that the user can only select values from the autocomplete suggestions. The autocomplete_value_with_labels attribute is set to <code>True</code>, which means that the autocomplete suggestions include both values and human-readable labels.</p> <p>This code block is derived from cmem-plugin-kaggle</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#context-objects","title":"Context Objects","text":"<p>The cmem-plugin-base package describes context objects, which are passed to the plugin depending on the executed method.</p> <p></p>","tags":["Python"]},{"location":"develop/python-plugins/development/#basic-understanding","title":"Basic Understanding","text":"<p>Context objects have been introduced to provide a way to access context-dependent functionalities during plugin creation, update, or execution.</p> <p>These context objects allow accessing various useful functionalities such as the current OAuth token, updating the execution report for workflows, DI version, and current project details.</p> <p>Note</p> <p>Having a basic understanding of context objects and their functionalities can help developers effectively use them to create and execute plugins in Build (DataIntegration).</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#system-context","title":"System Context","text":"<p>SystemContext can be used to obtain important system information. It has three methods: di_version, encrypt, and decrypt. The <code>di_version()</code> returns the version of the running Build (DataIntegration) instance. The encrypt and decrypt methods can be used to secure values using a secret key that is configured in the system. Overall, the SystemContext is useful when needing to obtain system information or encrypt/decrypt values in a secure manner.</p> <p>Example</p> <ul> <li> <p>Password Parameter Type: it is used to decrypt an encrypted password value.</p> </li> <li> <p>Example of decrypting the key.</p> </li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/#user-context","title":"User Context","text":"<p>UserContext can be used to obtain information about the user that is interacting with the system. It has three methods: <code>user_uri()</code>, <code>user_label()</code>, and <code>token()</code>. The <code>user_uri()</code> returns the URI of the user, which can be used to identify them uniquely. The <code>user_label()</code> returns the name of the user, which can be used for display purposes. The <code>token()</code> retrieves the OAuth token for the user, which can be used to authenticate requests made on behalf of the user.</p> <p>Example</p> <p>Here: The UserContext is used to set up the cmempy user access before accessing the resource from the dataset.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#task-context","title":"Task Context","text":"<p>TaskContext can be used to obtain information about the project and task that an object is part of. The <code>project_id()</code> returns the identifier of the project, which can be used to retrieve information about the project or to associate the object with the project. The <code>task_id()</code> returns the identifier of the task, which can be used to retrieve information about the task or to associate the object with the task. This information can be used for various purposes, such as retrieving additional metadata about the project or task, or associating the object with the project or task in order to perform specific operations.</p> <p>Example</p> <p>Here is an example of how the context object can be used to retrieve the <code>project_id</code> of the current <code>task</code>.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#execution-report","title":"Execution Report","text":"<p>ExecutionReport is used to provide insight into the execution of a workflow operator. It contains important information such as the number of <code>entities</code> that have been processed, a short label and <code>description</code> of the executed operation, a <code>summary</code> table representing the summary of the report, any warnings or user-friendly messages that occurred during execution, and an <code>error</code> message in case a fatal error occurred.</p> <p>ExecutionReport is used by workflow operators to generate execution reports. This information can be used for various purposes, such as providing insight into the performance of the operator, identifying any warnings or errors that occurred during execution, and stopping the workflow execution in case a fatal error occurred. The information contained in the ExecutionReport can also be displayed in real-time in the user interface.</p> <p>Example</p> <p>Here, the execution report with the number of successful messages sent by the producer and a summary of Kafka statistics. It also includes information about the operation performed (i.e., write) and a description of the operation (i.e., messages sent), making it easier to track the progress of the process.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#report-context","title":"Report Context","text":"<p>ReportContext is used to pass context information into workflow plugins that may generate a report during execution. It contains a single method called update that can be called repeatedly during operator execution to update the current execution report. <code>update()</code> takes an instance of the ExecutionReport as input and updates the current report with the information contained in the ExecutionReport. This allows plugins to generate reports that can be used for various purposes, such as providing insight into the performance of the plugin, identifying any warnings or errors that occurred during execution, and stopping the workflow execution in case a fatal error occurred.</p> <p>Example</p> <p>Here: While producing messages in Kafka, the message count is constantly updated.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin-context","title":"Plugin Context","text":"<p>PluginContext provides important context information during plugin creation or update. It has three attributes: system, user, and project_id.</p> <p>The <code>system</code> attribute is of type SystemContext and contains general system information. The <code>user</code> attribute is of type UserContext and contains information about the user. The <code>project_id</code> attribute contains the identifier of the project that contains or will contain the plugin.</p> <p>Note</p> <p>After creation, the plugin may be updated or executed by another user.</p> <p>Example</p> <p>All parameters here use the <code>PluginContext</code>, to get the context in which the param type is requested.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#execution-context","title":"Execution Context","text":"<p>ExecutionContext combines context objects that are available during plugin execution. It contains four attributes:</p> <ul> <li><code>system</code>: An instance of the SystemContext, which provides general system information.</li> <li><code>user</code>: An optional instance of the UserContext, which provides information about the user that issued the plugin execution.</li> <li><code>task</code>: An instance of the TaskContext, which provides metadata about the executed plugin.</li> <li><code>report</code>: An instance of the ReportContext, which allows to update the execution report.</li> </ul> <p>The ExecutionContext is used to provide context information to plugins during execution, enabling plugins to access information about the environment in which they are running, the user who initiated the execution, and the task being executed. The ReportContext attribute allows plugins to generate and update reports during execution.</p> <p>Note</p> <p>Here, The ExecutionContext is only available to the WorkflowPlugins and provides information and resources related to the execution environment. Plugins can use it to access/update information that may impact the report, such as logging and configuration data.</p> <p>Example</p> <p>Here is an example of how the context object can be used to retrieve the <code>project_id</code> of the current <code>task</code> and the <code>user</code> associated with the current execution.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#entities","title":"Entities","text":"<p>An <code>entity</code> is a structure to describe data objects which are passed around in workflows from one task to another task. An entity is identified by a <code>uri</code> and holds <code>values</code>, i.e., a sequence of string sequences (= a list of multi-value fields).</p> <p>Multiple entities are handled as <code>entities</code> objects, which have an attached <code>schema</code> to it.</p> <p>A <code>schema</code> contains of <code>path</code> descriptions and is identified by a <code>type_uri</code>.</p> <p>The following image shows these terms and their relationships. (1)</p> <ol> <li>The concrete implementation details of entities can be found in the entity module of the cmem-plugin-base package.</li> </ol> <p></p> Class Description <code>Entities</code> Holds a collection of entities and their schema <code>Entity</code> An Entity can represent an instance of any given concept. <code>EntitySchema</code> An entity schema that represents the type of uri and list of paths <code>EntityPath</code> A path in a schema","tags":["Python"]},{"location":"develop/python-plugins/development/#producing-entities","title":"Producing Entities","text":"<p>The following section shows the source code of a Produce Entities plugin, which is commented below.</p> entities-producer.py<pre><code>\"\"\"Entities Producer\"\"\"\nimport uuid\nfrom secrets import token_urlsafe\nfrom typing import Sequence\n\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext, ExecutionReport\nfrom cmem_plugin_base.dataintegration.description import Plugin, PluginParameter\nfrom cmem_plugin_base.dataintegration.entity import (\n    Entities,\n    Entity,\n    EntitySchema,\n    EntityPath,\n)\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\n\n\n@Plugin(\n    label=\"Produce Entities\",\n    description=\"Generates random values of X rows a Y values.\",\n    documentation=\"\"\"\nThis example workflow operator generates random values as Entities.\n\nThe values are generated in X rows a Y values. Both parameter can be specified:\n\n- 'number_of_entities': How many rows do you need.\n- 'number_of_values': How many values per row do you need.\n\"\"\",\n    parameters=[\n        PluginParameter(\n            name=\"number_of_entities\",\n            label=\"Entities (Rows)\",\n            description=\"How many rows will be created per run.\",\n            default_value=\"10\",\n        ),\n        PluginParameter(\n            name=\"number_of_values\",\n            label=\"Values (Columns)\",\n            description=\"How many values are created per entity / row.\",\n            default_value=\"5\",\n        ),\n    ],\n)\nclass EntitiesProducer(WorkflowPlugin):\n    \"\"\"Entities Producer Plugin\"\"\"\n\n    def __init__(self, number_of_entities: int = 2, number_of_values: int = 2) -&gt; None:\n        if number_of_entities &lt; 1:\n            raise ValueError(\"Entities (Rows) needs to be a positive integer.\")\n\n        if number_of_values &lt; 1:\n            raise ValueError(\"Values (Columns) needs to be a positive integer.\")\n\n        self.number_of_entities = number_of_entities\n        self.number_of_values = number_of_values\n\n    def execute(\n        self, inputs: Sequence[Entities], context: ExecutionContext\n    ) -&gt; Entities:\n        self.log.info(\"Start creating random values.\")\n        self.log.info(f\"Config length: {len(self.config.get())}\")\n        entities_counter = 0\n        value_counter = 0\n        entities = []\n        for _ in range(self.number_of_entities):\n            entity_uri = f\"urn:uuid:{str(uuid.uuid4())}\"\n            entities_counter += 1\n            values = []\n            for _ in range(self.number_of_values):\n                values.append([token_urlsafe(16)])\n                value_counter += 1\n                context.report.update(\n                    ExecutionReport(\n                        entity_count=entities_counter,\n                        operation=\"wait\",\n                        operation_desc=\"entities generated\",\n                    )\n                )\n            entities.append(Entity(uri=entity_uri, values=values))\n        paths = []\n        for path_no in range(self.number_of_values):\n            path_uri = f\"https://entities.org/vocab/RandomValuePath/{path_no}\"\n            paths.append(EntityPath(path=path_uri))\n        schema = EntitySchema(\n            type_uri=\"https://entities.org/vocab/RandomValueRow\",\n            paths=paths,\n        )\n        self.log.info(\n            f\"Happy to serve {entities_counter} entities with {value_counter} values.\"\n        )\n        context.report.update(\n            ExecutionReport(\n                entity_count=entities_counter,\n                operation=\"wait\",\n                operation_desc=\"entities generated\",\n                summary=[\n                    (\"No. of entities\", f\"{entities_counter}\"),\n                    (\"No. of values\", f\"{value_counter}\"),\n                ],\n            )\n        )\n        return Entities(entities=entities, schema=schema)\n</code></pre> <p>Code explanation:</p> <ol> <li>Provide a label, description and short documentation for the plugin. (#17-27)</li> <li>Define the parameters of the plugin. Here, two parameters are defined, where one specifies the number of <code>rows</code> and the other acthe number of <code>columns</code>. (#24-41)</li> <li>Intialise the parameters of the plugin. Additionally, you can validate and raise exceptions from <code>init()</code>. (#46-54)</li> <li>To return Entities we have to create a list of <code>entities</code> and its <code>schema</code>. As a first step, declare entities as an empty list. (#62)</li> <li>As previously mentioned, each <code>Entity</code> should have a <code>URI</code> and it can have sequence of <code>values</code>. Here, a list of entities is created with random UUIDs based on rows and values are created based on columns. After each entity is created it is appended to the entities list. (#64-78)</li> <li>To generate a <code>schema</code> (which is of type <code>EntitySchema</code>), which should have a <code>type_uri</code> and a sequence of <code>paths</code>, define an empty list of paths. (#79)</li> <li>Based on the columns, each unique path is appended to the paths list. Once all paths are added, the schema is updated with <code>type_uri</code> and <code>paths</code> respectively. (#80-86)</li> <li>Once the entities and the schema are generated you can return them. (#101)</li> <li>Update plugin logs using <code>PluginLogger</code> which is available as a default logger. (#87-91)</li> <li>To make your plugin more user-friendly you can use the Context API <code>report.update()</code> to update the workflow report. (#91-100)</li> </ol>","tags":["Python"]},{"location":"develop/python-plugins/development/#consuming-entities","title":"Consuming Entities","text":"<p>Consuming entities in a workflow plugin means that you process at least one <code>entities</code> object from the <code>inputs</code> list.</p> <p>The following code shows a plugin which loops through all inputs and counts all entities and its values.</p> entities-consumer.py<pre><code>\"\"\"Consume Entities\"\"\"\nfrom typing import Sequence\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext, ExecutionReport\nfrom cmem_plugin_base.dataintegration.description import Plugin\nfrom cmem_plugin_base.dataintegration.entity import Entities\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\n\n\n@Plugin(\n    label=\"Consume Entities\",\n    description=\"Reads random values of X rows a Y values.\",\n    documentation=\"\"\"\nThis example workflow operator reads random values.\n\"\"\",\n)\nclass EntitiesConsumer(WorkflowPlugin):\n    \"\"\"Entities Consumer\"\"\"\n\n    def execute(self, inputs: Sequence[Entities], context: ExecutionContext):\n        entities_counter = 0\n        value_counter = 0\n        for item in inputs:\n            for entity in item.entities:\n                entities_counter += 1\n                for _ in entity.values:\n                    value_counter += 1\n        context.report.update(\n            ExecutionReport(\n                entity_count=entities_counter,\n                operation=\"wait\",\n                operation_desc=\"entities received\",\n                summary=[\n                    (\"No. of entities\", f\"{entities_counter}\"),\n                    (\"No. of values\", f\"{value_counter}\"),\n                ],\n            )\n        )\n</code></pre> <p>Code explanation:</p> <ol> <li><code>inputs</code> from the workflow is a sequence of <code>Entities</code> with each item from the input having a list of entities and each entity having values. The entities and values are seperately counted. (#22-26)</li> <li>Once the counting is done, the workflow report is updated with the total number of entities and values as the summary. (#27-37)</li> </ol>","tags":["Python"]},{"location":"develop/python-plugins/development/#configuration","title":"Configuration","text":"<p>Plugins can have an application-wide configuration which cannot be changed on runtime and applies to all instances of this plugin.</p> <p>This plugin configuration is provided in the <code>self.config</code> PluginConfig object of the plugin. The <code>get</code> method of this object returns a JSON string of the configuration.</p> <p>Plugin configurations use the <code>plugin_id</code> as a config path in <code>dataintegration.conf</code>.</p> Example plugin configuration<pre><code>plugins.python.&lt;plugin_id&gt; = {\n    key1 = \"value1\"\n    key2 = \"value2\"\n}\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#logging","title":"Logging","text":"<p>Logging should be done with the PluginLogger, which is available as <code>self.log</code> in all plugins.</p> <pre><code>self.log.info(\"Successfully executed Workflow Plugin\")\n</code></pre> <p>On runtime, this logger will be replaced with a JVM based logging function feeding the plugin logs to the normal Build (DataIntegration) log stream. This JVM-based logger will prefix all plugin logs with <code>plugins.python.&lt;plugin id&gt;</code>.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#concurrency","title":"Concurrency","text":"<p>CMEM uses JEP to run Python plugins inside the JVM. Python\u2019s concurrent.futures.ProcessPoolExecutor relies on forking or spawning new operating system processes, which is not compatible with JEP for several reasons:</p> <ul> <li>Forking a process within the JVM environment is problematic and can lead to deadlocks or unstable behavior.</li> <li>A missing <code>__main__</code> context can also result in deadlocks.</li> <li>JEP shares memory between Python and Java, which conflicts with multiprocessing\u2019s requirement for isolated memory spaces.</li> </ul> <p>In contrast, Python\u2019s concurrent.futures.ThreadPoolExecutor does not encounter these issues. It uses threads that share the same memory space and operate within a single process, avoiding the need for subprocess creation.</p> <p>Recommendation: Always use <code>ThreadPoolExecutor</code> in CMEM Python plugins running under JEP, as <code>ProcessPoolExecutor</code> may cause deadlocks.</p>","tags":["Python"]},{"location":"develop/python-plugins/installation/","title":"Installation and Usage of Python Plugins","text":"<p>Plugins are a released as parts of Python packages. They can but do not need to be open-source and published on pypi.org (a widely used Python Package Index). One package can contain of multiple plugins.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/installation/#installation","title":"Installation","text":"<p>If you want to install a python plugin package, you need to use cmemc\u2019s admin workspace python command group.</p> <p>The following shell commands demonstrate the basic workflow:</p> Install a plugin package from pypi.org:<pre><code>$ cmemc admin workspace python install cmem-plugin-graphql\nInstall package cmem-plugin-graphql ... done\n</code></pre> List installed plugins:<pre><code>$ cmemc admin workspace python list-plugins\nID                                 Type            Label\n---------------------------------  --------------  -------------\ncmem_plugin_graphql-GraphQLPlugin  WorkflowPlugin  GraphQL query\n</code></pre> <p>You can get a list of all installed python packages: (1)</p> <ol> <li>This list contains all installed packages in the python environment, not just your plugin packages.</li> </ol> List all installed python packages:<pre><code>$ cmemc admin workspace python list\nName                Version\n------------------  -----------\ncertifi             2022.5.18.1\ncharset-normalizer  2.0.12\ncmem-cmempy         22.1.1\ncmem-plugin-base    1.2.0\nidna                3.3\nisodate             0.6.1\njep                 4.0.2\npip                 20.3.4\npyparsing           3.0.9\nrdflib              6.1.1\nrequests            2.27.1\nrequests-toolbelt   0.9.1\nsetuptools          52.0.0\nsix                 1.16.0\nurllib3             1.26.9\nwheel               0.34.2\n</code></pre> <p>You also can (un-)install packages in a specific version or from a source distribution file. Please have a look at the admin workspace python command group for a complete documentation of the package / plugin commands.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/installation/#usage","title":"Usage","text":"<p>Depending on the plugin type, an installed plugin can appear in different parts of the Build workbench:</p> <p></p> <p>Workflow Plugins are listed in the Create new item dialog in the Task category. From there, you can create a task in your project and use it in a workflow.</p> <p></p> <p>Transform Plugins are listed in the sidebar of the Value formula editor and the Linking editor in the  Transform tab. Drag and drop it on the canvas, and connect it with ingoing and / or outgoing links to other elements.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/installation/#known-issues","title":"Known Issues","text":"<p>Sharing the <code>PYTHONPATH</code> via NFS</p> <p>The <code>PYTHONPATH</code> is the location where the installed python plugins and their dependencies are stored for Build (DataIntegration). When you share this path via NFS, there are issues with open file locks which will sometimes break the plugin installation requests. A workaround for this is to install or upgrade plugins right after rebooting DataIntergration (and before you start a workflow that uses a python plugin). We currently advise against using NFS on this path directly. The problem is known by the python community but there is no fix or workaround available yet.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/setup/","title":"Setup and Configuration","text":"<p>This section describes which backend components are needed on the Build (DataIntegration) server, in order to use python plugins.</p>","tags":["Python"]},{"location":"develop/python-plugins/setup/#basic-configuration-and-dependencies","title":"Basic Configuration and Dependencies","text":"<p>Info</p> <p>When using the official eccenca docker images, setup and basic configuration is already done.</p> Build (DataIntegration) Configuration <p>The following Build (DataIntegration) configuration section describes how to setup and enable the Python Plugin system.</p> <pre><code>#################################################\n# Plugin Configuration\n#################################################\n\n# this (optional) file can be used to hold python plugin specific configuration\ninclude \"python-plugins.conf\"\n\ncom.eccenca.di.scripting = {\n  python = {\n    PythonPluginRegistry = {\n      # Python plugins will only be loaded if 'enabled' is set to true.\n      enabled = true\n\n      # Plugins will only be loaded below the following base package.\n      basePackage = \"cmem\"\n    }\n\n    PythonPackageManager = {\n      # Python package installer executable.\n      pipExecutable = \"cmem-pip-wrapper.sh\"\n    }\n  }\n}\n</code></pre> Python Interpreter <p>An installation of the CPython distribution (at least version 3.3) is required. Although other distributions, such as Anaconda, should work as well, only CPython is officially supported.</p> <p>The official image ships with a tested python interpreter (currently - 2024 - Python 3.11).</p> Java Embedded Python (Jep) <p>The Jep package needs to be installed.</p> <p>The libraries contained in the Jep module need to be accessible from the Java Virtual Machine running Build (DataIntegration). This can be achieved by setting an environment variable to the directory path where the Jep module is located:</p> <ul> <li> Linux: set <code>LD_LIBRARY_PATH</code>.</li> <li> OS X: set <code>DYLD_LIBRARY_PATH</code>.</li> <li>:simple-windows: Windows: set <code>PATH</code>.</li> </ul> <p>For alternative installation methods, visit </p> <p>The official image ships with a tested Jep module.</p>","tags":["Python"]},{"location":"develop/python-plugins/setup/#specific-changes-from-the-default","title":"Specific Changes from the default","text":"","tags":["Python"]},{"location":"develop/python-plugins/setup/#package-index-locations","title":"Package Index Locations","text":"<p>The basic setup allows for installation of packages from the pypi.org python package index, maintained by the Python Software Foundation. In order to change the remote index server, from where you can install python packages, you need to set the following environment variables in the data integration container:</p> <ul> <li><code>PIP_INDEX_URL</code> - Base URL of the default python package index Base URL. This should point to a repository which is compliant with PEP 503 (the simple repository API). If this variable is not set, the official Python Package Index is used.<ul> <li>Example Value: <code>https://pypi.eccenca.com/simple</code> (the eccenca Python Package Index holds only published Corporate Memory Python Plugins and respective dependencies)</li> <li>Changing this value means, that you can install packages only from this repository.</li> </ul> </li> <li><code>PIP_EXTRA_INDEX_URL</code> - Extra URLs of package indexes to use in addition to the default package index.<ul> <li>Example Value: <code>https://pypi.eccenca.com/simple https://example.org/simple</code></li> <li>Multiple index URLs have to be given space-separated.</li> <li>Changing this values means you can install packages from the given repositories in addition to the main index.</li> </ul> </li> </ul> <p>For individual needs, you can use additional environment variables known by <code>pip</code> (<code>PIP_TRUSTED_HOST</code>, <code>PIP_CERT</code>, \u2026). Please have a look at the pip documentation.</p>","tags":["Python"]},{"location":"develop/python-plugins/setup/#local-packages-only","title":"Local Packages only","text":"<p>In cases, where you have limited or disabled network capabilities to the internet, you can disable package retrieval and provide the packages in a local directory. To do so, you need to set the following environment variables in the data integration container:</p> <ul> <li><code>PIP_NO_INDEX</code> - set the value as <code>true</code> to disable the package retrieval completely.</li> <li><code>PIP_FIND_LINKS</code> - set to a container internal directory, where the packages and its dependencies will be provided.<ul> <li>Example Value: <code>/data/downloaded-packages</code></li> </ul> </li> </ul> <p>This setup will allow installation of packages and its dependencies ONLY from the given directory.</p> <p>As a next step, you need to provide the needed packages in this directory. To do so, use the <code>pip download</code> command and copy or mount the downloaded files in your container.</p> Example shell session showing the usage of <code>pip download</code> <pre><code>$ cat requirements.txt\ncmem-plugin-validation\ncmem-plugin-graphql\ncmem-plugin-kafka\ncmem-plugin-yaml\n\n$ pip download --only-binary=:all: -r requirements.txt --platform manylinux2014_x86_64\nCollecting cmem-plugin-validation (from -r requirements.txt (line 1))\n...\nSaved ./six-1.16.0-py2.py3-none-any.whl\nSuccessfully downloaded cmem-plugin-validation ...\n</code></pre> <p>Please note the <code>--platform</code> identifier. This is only needed in case you prepare the downloaded package directory on a machine with a different platform compared to you server (e.g. on a Mac).</p>","tags":["Python"]},{"location":"develop/python-plugins/setup/#package-path","title":"Package Path","text":"<p>The basic setup provides a <code>/data</code> directory inside of the Build (DataIntegration) container, where all changed files are managed in subdirectories. The environment variable <code>PYTHONPATH</code> defines the directory, where the user-managed python packages are saved. This directory shall be persisted between restarts of Build (DataIntegration). The default value of this variable is <code>/data/python-packages/</code>. DataIntegration won\u2019t start if the directory defined by <code>PYTHONPATH</code> is not present and can\u2019t be created. In addition Build (DataIntegration) needs write access to that folder. This is tested on Build (DataIntegration) startup.</p> <p>Info</p> <p>By setting environment variable <code>PYTHONPATH_FAILURE</code> (default: <code>true</code>) to other values than <code>true</code> this behavior can be skipped. However this might effect the usability of python plugins.</p>","tags":["Python"]},{"location":"explore-and-author/","title":"Explore and Author","text":""},{"location":"explore-and-author/#explore-and-author","title":"Explore and Author","text":"<p>In the Explore section you will learn how Corporate Memory allows you to interact with your Enterprise Knowledge Graph. All relevant modules and functionalities are described. You will also learn how we make use of\u00a0SHACL Shapes\u00a0in order to\u00a0customize the way how you can interact with your data.</p> <p> Intended audience: Linked Data Experts and Domain Experts</p> <ul> <li> <p> Graph Exploration</p> <p>This module provides a generic and extensible RDF data browser and editor.</p> </li> <li> <p> Vocabulary Catalog</p> <p>This module allows for managing vocabularies in Corporate Memory that are accessible for the user.</p> </li> <li> <p> Thesauri Management</p> <p>The Thesaurus module provides a user interface to create, browse and edit thesaurus resources and general taxonomical data modeled in SKOS.</p> </li> <li> <p> Query Module</p> <p>The\u00a0Query\u00a0module provides a user interface to store, describe, search and edit SPARQL queries.</p> </li> </ul>"},{"location":"explore-and-author/bke-module/","title":"Business Knowledge Editor Module","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#introduction","title":"Introduction","text":"<p>This feature allows for the visual exploration of Knowledge Graphs. It allows to save and share explorations. Furthermore, sophisticated individual search settings (filter presets) can be created and configured per workspace.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#usage","title":"Usage","text":"<p>If enabled, content of Knowledge Graphs can be explored in a visual way, rendering nodes and edges and allowing the user to expand along the relationships between the nodes.</p> <p>Start using <code>Business Knowledge Editor</code> by selecting the respective module entry in the main navigation.</p> <p></p> <p>At the module welcome screen the user can either load a saved visualization of start searching for an initial node / resource by providing a search term.</p> <p>Note</p> <p>The graph selection drop-down might or might not be visible depending the existence of an (optional) <code>Business Knowledge Editor Module</code> configuration. In case no specific module configuration exists or non has not has been set for the current workspace the graph selection will be shown. A <code>EasBusiness Knowledge EditoryNav Module</code> configuration pre-configures a graph. Thus, the dropdown will not be shown if such has been configured for the current workspace.</p> <p></p> <p>Enter a search term to populate the result list. Click a result to start the visual graph exploration.</p> <p></p> <p>The exploration starts with the selected node (or a saved exploration). The nodes can further be expanded along the relationships that exist to other resources. Therefore, click the node expansion button on the right side of a node (the point where the arrows originate in the screenshot below).</p> <p></p> <p>Any expanded resource / node can be added to the current exploration by double-clicking the node. Clicking anywhere on the empty canvas will close the relationship dialog and retain the added nodes and their relationships only.</p> <p></p> <p>Click  on a node to see literal values related to this resource  closes the details again.</p> <p><code>Save</code> allows to save an exploration,  will start a new exploration while  allows to open any previously saved exploration.</p> <p></p> <p>The <code>Visualization catalog</code> dialog shows the saved exploration and allows to  open,  delete or to  copy the link to the exploration.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#setup","title":"Setup","text":"<p>This feature is enabled by default. It can be customized or disabled in the respective workspace configuration section.</p> <p>Without further (workspace) specific configuration the feature can be used asking for the graph that shall be explored every time a new exploration is started.</p> <p>Optionally a <code>Business Knowledge Editor Module</code> configuration can be created to provide a fixed graph selection and search filter settings.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#create-a-business-knowledge-editor-module-configuration","title":"Create a Business Knowledge Editor Module Configuration","text":"<p>In the <code>Knowledge Graphs</code> module navigate to the <code>CMEM Configuration</code> graph.</p> <p>Select the class <code>Business Knowledge Editor Module</code> and <code>Create a new \"Business Knowledge Editor Module\"</code>.</p> <p></p> <p>Provide a <code>Name</code> for your configuration and select the <code>Default Graph</code> which contains the nodes you want to explore visually. This graph can of course be an integration graph.</p> <p><code>Search Configuration</code> is optional but a powerful feature to create predefined search filter/facets. If want to use this capability select existing <code>Search Configuration</code>s in the drop down or create stubs for the configurations you want to setup.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#set-the-business-knowledge-editor-module-in-the-workspace-configuration","title":"Set the Business Knowledge Editor Module in the Workspace configuration","text":"<p>After creating the <code>Business Knowledge Editor Module</code> configuration it need to be selected in workspace configuration(s) that shall be using it.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#create-a-search-configuration","title":"Create a Search Configuration","text":"<p>Follow the stub link from creating a new configuration in the <code>Module</code> dialog. Then click edit to provide the necessary details.</p> <p></p> <p>At least a <code>Name</code> and <code>Search Weight</code> need to be specified. The weight can be used to boost the results of one search configuration over another in case multiple <code>Search Configuration</code>s are used.</p> <p><code>Graph Resource Pattern</code> are a topic on its own and explained here.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#technical-background","title":"Technical Background","text":"<p><code>Search Configuration</code>s will be cumulatively executed when search terms are provided. Which means each additional <code>Search Configuration</code> increases the time to produce results.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/bke-module/#llm-ontology-assist","title":"(LLM) Ontology Assist","text":"<p>Ontology Assist is a preview feature powered by generative AI, LLM, and a Retrieval Augmented Generation (RAG) approach. It allows bootstrap and evolve an ontology using natural language and a chat like interaction.</p> <p>The query assist can be enabled in the dataplatform configuration.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/charts-catalog/","title":"Charts Catalog and Charts Integration","text":"<p>The Charts Catalog is a module suitable to visualize your data in a chart.</p>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#graph-selection","title":"Graph Selection","text":"<p>The charts module lists all the graphs containing queries - which will be the data foundation for your charts. Select the graphs that contains the queries you want to user to get started. You will see the charts that are already defined based select any to edit or add a new with Create chart.</p> <p></p>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#charts-management","title":"Charts Management","text":"<p>You can open the Charts Catalog from the main menu on the left. You will see a list of existing charts or the information Your charts catalog is empty. Here you can create a new chart or edit an existing one.</p> <p></p> <p>After selecting Create Chart or an existing Chart, you will see the Chart Editor, which is divided into four components:</p>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#metadata","title":"Metadata","text":"<p>You can give your charts a name and a description.</p>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#query-selection","title":"Query selection","text":"<p>Select a query from the  Query Catalog to retrieve the data you want to visualize.</p> <p>The following activities can be performed in this component:</p> <ul> <li>Select a query \u2014 Select a query to visualize by clicking on a  button.     The Assisted chart form (see below) supports a single query, while the Advanced chart form can use multiple queries.</li> <li>From the dropdown menu:<ul> <li>Parameters \u2014 Some queries have parameters that need to be filled with real values.</li> <li>Preview \u2014 View a preview of the retrieved data.</li> <li>View in query catalog \u2014 Opens the query in the query catalog.</li> </ul> </li> </ul>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#chart-forms","title":"Chart forms","text":"<p>There are two types of forms: Assisted and Advanced.</p> <p>The assisted form can be used to create simple chart types such as line, bar, bar-line and pie charts. It consists of fields that help you visualize your data.</p> <p>The advanced form is completely flexible when it comes to chart configuration. It consists of a JSON editor that allows you to configure the chart yourself.</p> <p>For more information about chart configuration and examples, visit the echarts.apache.org.</p>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#preview","title":"Preview","text":"<p>The main content on the right side is the preview, where you can see the visual results of your changes in the configuration.</p> <p>When using an advanced chart, you will also see a tab with the datasets that were created from the selected queries.</p>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/charts-catalog/#charts-integration","title":"Charts Integration","text":"<p>You can integrate your charts into existing Node or Property shapes in order to show a chart in the context of a resource. To do so, add  the Chart Visualization property to the shape and select the desired Chart.</p> <p>Note</p> <p>This means that your Chart must be accessible from the Shape Catalog. This can be achieved either by copying the chart and query resources into the Shape Catalog graph or by importing the query catalog graph in the Shape Catalog graph.</p> <p>To customize the chart several placeholders can be used in you queries:</p> <ul> <li><code>{{shuiResource}}</code>, the resource currently shown with the node shape of this property shape.</li> <li><code>{{shuiMainResource}}</code>, refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage).</li> <li><code>{{shuiGraph}}</code>, the currently used graph.</li> </ul>","tags":["KnowledgeGraph","Dashboards"]},{"location":"explore-and-author/embedding-services-via-the-integrations-module/","title":"Embedding Services via the Integrations Module","text":"<p>A Explore module is available that can be used to embed / integrate other web-services in Corporate Memory. The module can be used and configured visually through the  Workspace configuration module.</p>","tags":["Dashboards"]},{"location":"explore-and-author/graph-exploration/","title":"Graph Exploration","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#introduction","title":"Introduction","text":"<p>The\u00a0Explore\u00a0module provides a generic and extensible RDF data browser and editor. Use this\u00a0module to browse through your resources, to change between list and detail views and to edit resources.</p> <p>To open the\u00a0Explore\u00a0module, click\u00a0 Knowledge Graphs in the main menu.</p> <p>The user interface of the\u00a0Explore\u00a0module shows the following main areas:</p> <ul> <li>the header area, showing:<ul> <li>selected elements,</li> <li>possible actions (e.g.  create or  remove resource),</li> <li>a  Go to resource (2) input field,</li> <li>and a  user menu</li> </ul> </li> <li>the navigation area, showing the Graphs and the Navigation structures, (1)</li> <li>the main area, providing multiple views, depending on which resource has been selected.</li> </ul> <ol> <li> <p>If necessary, you can toggle the\u00a0navigation area by using the      (hide) and  (show) buttons.</p> </li> <li> <p>Go to resource is used with an IRI to open the resource details page directly.     It is not a search field.     You can try to search for a keyword, which might or might not be doing what you intended.</p> </li> </ol>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#graphs","title":"Graphs","text":"<p>The\u00a0Graphs\u00a0box shows lists of graphs you have access to. To select a graph click the graph name in the\u00a0Graphs\u00a0box. The navigation structure of the selected graph is displayed in the Navigation box below. In the main area, the\u00a0Metadata\u00a0view of the selected graph appears, showing several tabs with metadata information.</p> <p>Note</p> <p>This default categorization is just a suggestion and can be modified by changing the workspace configuration in the CMEM Configuration graph.</p> <p>The Graphs are categorized into groups as follows:</p> <ul> <li>User: All graphs which represent user data (created manually or by build processes).</li> <li>Vocabularies:\u00a0All graphs containing vocabularies.</li> <li>System: All graphs containing configuration data.</li> <li>All</li> </ul> <p>You can search for a specific graph with  Search.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#adding-a-new-graph","title":"Adding a new graph","text":"<p>To add a new graph to the\u00a0Graphs\u00a0list:</p> <ul> <li>Click\u00a0 Add new graph. A dialog appears.</li> <li>Select a graph type. (1)</li> <li>Provide a name and enter the graph URI (e.g.\u00a0<code>https://ns.eccenca.com</code>).</li> <li>Click Next and provide metadata (different types, require different metadata to enter).</li> <li>Click\u00a0Save\u00a0to create the new graph.</li> </ul> <ol> <li>More concrete, you select a shape here.     This can be configured in the workspace configuration as well.</li> </ol>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#downloading-a-graph","title":"Downloading a graph","text":"<p>To download a graph from the\u00a0Graphs\u00a0list:</p> <ul> <li>In the\u00a0Graphs\u00a0list, click  Download graph on the graph you want to download.</li> <li>A message box appears, stating that downloading can take a long time.</li> <li>Click\u00a0Download.</li> </ul>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#managing-a-graph","title":"Managing a graph","text":"<p>Use this function to add or replace data in the a graph.</p> <p>To update or replace data of a graph:</p> <ul> <li>In the\u00a0Graphs\u00a0box, select  Manage graph on the graph you want to update or replace.</li> <li>A dialog box appears.</li> <li>Click\u00a0Choose file\u00a0to upload a file containing the new or updated data. (1)</li> <li>Choose one of the following options:<ul> <li>Update: add uploaded data to Graph.</li> <li>Replace: clear Graph and add uploaded data.</li> </ul> </li> <li>Click\u00a0Update\u00a0to start the upload process.</li> </ul> <ol> <li>You can upload one of the following file formats: Turtle, N-Triples, RDF/XML, or JSON-LD.</li> </ol> <p>To delete a graph, select  Remove graph on the graph you want to remove and confirm deletion process.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#navigation","title":"Navigation","text":"<p>When a graph is selected in the\u00a0Graphs\u00a0box, the structure of the graph is displayed in the\u00a0Navigation\u00a0box. By default, only the top classes of the graph are listed. An arrow\u00a0( Open tree)\u00a0indicates that a class has subclasses. Click the arrow to show the subclasses.</p> <p>Use the\u00a0 Search\u00a0field of the\u00a0Navigation\u00a0box to search for an item in the navigation structure of the graph. Enter a keyword and press\u00a0Enter\u00a0to start the search. To reset the results delete the keyword and press\u00a0Enter.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#instance-list-of-a-class","title":"Instance List of a class","text":"<p>Select a class in the\u00a0Navigation\u00a0box to show all instances of this class in the main area. (1)</p> <ol> <li>The table uses a default query to list all resources with a given class.      This can be configured by adding a <code>shui:navigationListQuery</code> to the class shape.</li> </ol>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#instance-details","title":"Instance Details","text":"<p>To open the\u00a0Instance Details\u00a0of a resource click on that resource in the\u00a0Instance List. Resources are shown as grey chip buttons.</p> <p></p> <p>Warning</p> <p>When you remove the resource, all triples related to that resource are deleted as well.</p> <p>Select  Remove resource\u00a0on the upper right corner to remove the resource. A dialog box appears where you are asked to confirm the operation.</p> <p>In the Instance Details view, multiple horizontal tabs provide different views in the resource. The availability of these views depends on the context and the resource type.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#resource","title":"Resource","text":"<p>The\u00a0Resource\u00a0tab provides a view based on the shapes of the selected resource. The details of the shaped view depends on the configuration.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#properties","title":"Properties","text":"<p>The Properties tab shows all properties and objects of the selected resource independently from a shape selection.</p> <p>Use the icons on the right side to edit or delete properties. Use\u00a0SHOW IN LIST\u00a0to display objects in a list view. Select\u00a0ADD\u00a0to add a new value as an object to a property. In the dialog box, select the type from the drop-down list and enter the value. Click\u00a0SAVE\u00a0to save your changes.</p> <p>To add a new property select  Add Property . In the dialog box, enter a property, select the value type from the drop-down list and enter a value. Click\u00a0SAVE\u00a0to save your changes.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#statistics","title":"Statistics","text":"<p>Note</p> <p>This view is only available on graphs (not on resources).</p> <p>The\u00a0Statistics\u00a0tab indicates the number of classes, properties, entities and triples of the graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#graph","title":"Graph","text":"<p>Note</p> <p>This view is only available on graphs (not on resources).</p> <p>The\u00a0Graph\u00a0tab shows a visual graph representation of the graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#vocab","title":"Vocab","text":"<p>Note</p> <p>This view is only available on graphs (not on resources).</p> <p>This tab shows a graph visualization of an installed vocabulary. It displays all classes showing the class-subclass.\u00a0 You can open the class details and view the list of instances related to that class. It also allows you to\u00a0copy the resource IRI.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#references","title":"References","text":"<p>This tab shows all resources that link back to the selected resource.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#turtle","title":"Turtle","text":"<p>This tab shows the turtle RDF representation of the raw data representing the resource. You can use this tab to edit the selected resource:</p> <ul> <li>Enter your changes in turtle.</li> <li>Click\u00a0UPDATE\u00a0to save your changes.</li> </ul> <p>Deleting the entire turtle representation deletes the resource.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/","title":"Building a customized User Interface","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#introduction","title":"Introduction","text":"<p>Working with shapes allows for creation of a customized Linked Data user interface. In addition to the standard PROPERTIES tab that shows all properties of a data resource, you can create custom \u201cform\u201d-like data interfaces. These configurable forms allow for a cleaner interface to view and author data resources. In addition, they enable integration of data from other resources that are linked to the current resource, creating a more concise view on your data.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#defining-forms","title":"Defining forms","text":"<p>You can define forms using SHACL rules. The rules state:</p> <ol> <li>What types of resources the form definition applies to. This is based on the\u00a0<code>rdf:type</code>\u00a0of a resource.</li> <li>What fields are shown in the form in which order. Field contents are retrieved from properties connected to the resource.</li> <li>Which other, linked resources are shown in the form. Linked resources can either be shown as links or as their full form.</li> <li>Which texts are used to name and describe fields, as well as the tab in the user interface.</li> </ol> <p>Forms are defined in the CMEM Shapes Catalog graph. The graph URI is\u00a0<code>https://vocab.eccenca.com/shacl/</code>.</p> <p>Form definitions are twofold:</p> <ol> <li>The form itself is defined as so called\u00a0<code>NodeShape</code>. NodeShapes define which types of resources the form applies to (the target class), and which fields are shown in the form (the Properties).</li> <li>The individual fields are defined as so called\u00a0<code>PropertyShape</code>. PropertyShapes define which property is used to retrieve data for the field (the path), the name of the field, a description, its cardinality (min and max count), its position in the form (the order), and if it should always be shown. In case of object properties, it also defines the type of the linked resource (the class). The full list of features is described in\u00a0PropertyShapes.</li> </ol> <p>To define a new form, for example for\u00a0<code>foaf:Person</code>\u00a0resources, navigate to the CMEM Shapes Catalog graph and select\u00a0<code>NodeShape</code>\u00a0in Navigation. The list of existing NodeShapes is shown. Click \u201cCreate a new SHACL Node shape\u201d\u00a0in the upper right to create a new NodeShape. Enter a name of the resource. An empty NodeShape resource is created and shown.</p> <p></p> <p>To create the initial definition, click\u00a0\u00a0(Edit). A form is shown to you with input fields\u00a0Name,\u00a0Property Shapes,\u00a0Vocabulary,\u00a0Target class\u00a0and\u00a0Statement Annotation. The initial definition requires the name, and the target class. Fields are attached to the form later.\u00a0Target class\u00a0in particular binds the form to the resources it should cover. The\u00a0Target class\u00a0field features an auto-complete that displays all classes stored in Corporate Memory. The example form should cover resources of the type\u00a0<code>foaf:Person</code>, so enter\u00a0<code>foaf:Person</code>\u00a0in the\u00a0Target class\u00a0field. Click\u00a0SAVE\u00a0to save the NodeShape.</p> <p></p> <p>You have now created an \u201cempty\u201d form that covers\u00a0<code>foaf:Person</code>\u00a0resources with tab name \u201cPerson\u201d. Navigating to a\u00a0<code>foaf:Person</code>\u00a0resource, you see a new tab as defined. You can still see all properties of the resource in the\u00a0PROPERTIES\u00a0tab.</p> <p></p> <p>To define new fields, for example showing the email address of the person (defined as\u00a0<code>foaf:mbox</code>), navigate to the CMEM Shapes Catalog graph and select\u00a0<code>PropertyShape</code>\u00a0in Navigation. The list of existing PropertyShapes is shown. Click\u00a0CREATE NEW PROPERTYSHAPE\u00a0in the upper right to create a new PropertyShape. Enter a name of the resource. An empty PropertyShape resource is created and shown.</p> <p>Edit the form using\u00a0. A form is shown with all relevant properties of a field definition. Required in this step are:</p> <ol> <li>The name of the field, which will be displayed left of the data content or input field in the form.</li> <li>The description, which will be displayed as tooltip on the question mark to the right of the name.</li> <li>The path, which states which property the field represents. In this example, it is\u00a0<code>foaf:mbox</code>.</li> <li>The form the field should be shown in (Property of). The field provides an auto-complete, so just enter \u201cPerson\u201d and select the NodeShape resource you defined in the previous step.</li> </ol> <p>Click\u00a0SAVE\u00a0after filling out the required fields.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#using-forms","title":"Using forms","text":"<p>Once a Node Shape is created for a specific class, you are able to use the specified entry form in the Explore component of Corporate Memory.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#editing-existing-resources","title":"Editing existing resources","text":"<p>While browsing your knowledge graph, you will always see your shape in action, when you click on a resource which is an instance of the class which is linked with\u00a0<code>shacl:targetClass</code>\u00a0from your Node Shape.</p> <p>The next images demonstrate this behavior :</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#creating-new-resources","title":"Creating new resources","text":"<p>You can also create new resources by using a shaped form. One way to achieve this, is to select the class in the navigation tree on the lower left part in the Explore component and then click the Floating Action Button at the bottom or use the context menu on upper right side.</p> <p>The next images demonstrate this\u00a0behaviour:</p> <p></p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/","title":"Datatypes","text":"<p>This is a list of supported data types in shapes.</p> <p>Warning</p> <p>Not all datatypes result in specific widgets.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#anyuri","title":"anyURI","text":"<p>The \u00b7lexical space\u00b7 of anyURI is finite-length character sequences which, when the algorithm defined in Section 5.4 of [XML Linking Language] is applied to them, result in strings which are legal URIs according to [RFC 2396], as amended by [RFC 2732]. Note:  Spaces are, in principle, allowed in the \u00b7lexical space\u00b7 of anyURI, however, their use is highly discouraged (unless they are encoded by %20).</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#anyURI</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#base64binary","title":"base64Binary","text":"<p>The lexical forms of base64Binary values are limited to the 65 characters of the Base64 Alphabet defined in [RFC 2045], i.e., a-z, A-Z, 0-9, the plus sign (+), the forward slash (/) and the equal sign (=), together with the characters defined in [XML 1.0 (Second Edition)] as white space. No other characters are allowed.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#base64Binary</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#boolean","title":"boolean","text":"<p>An instance of a datatype that is defined as \u00b7boolean\u00b7 can have the following legal literals {true, false, 1, 0}.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#boolean</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#byte","title":"byte","text":"<p>byte is \u00b7derived\u00b7 from short by setting the value of \u00b7maxInclusive\u00b7 to be 127 and \u00b7minInclusive\u00b7 to be -128. byte has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126, +100.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#byte</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#date","title":"date","text":"<p>The lexical space of date consists of finite-length sequences of characters of the form: <code>'-'? yyyy '-' mm '-' dd zzzzzz?</code> where the date and optional timezone are represented exactly the same way as they are for dateTime. The first moment of the interval is that represented by: <code>'-' yyyy '-' mm '-' dd 'T00:00:00' zzzzzz?</code> and the least upper bound of the interval is the timeline point represented (noncanonically) by: <code>'-' yyyy '-' mm '-' dd 'T24:00:00' zzzzzz?</code>.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#date</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#datetime","title":"dateTime","text":"<p>The \u00b7lexical space\u00b7 of dateTime consists of finite-length sequences of characters of the form: <code>'-'? yyyy '-' mm '-' dd 'T' hh ':' mm ':' ss ('.' s+)? (zzzzzz)?</code> For example, <code>2002-10-10T12:00:00-05:00</code> (noon on 10 October 2002, Central Daylight Savings Time as well as Eastern Standard Time in the U.S.) is <code>2002-10-10T17:00:00Z</code>, five hours later than <code>2002-10-10T12:00:00Z</code>.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#dateTime</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#datetimestamp","title":"dateTimeStamp","text":"<p>The lexical space of dateTimeStamp consists of strings which are in the \u00b7lexical space\u00b7 of dateTime and which also match the regular expression \u2018.*(Z|(+|-)[0-9][0-9]:[0-9][0-9])\u2019</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#dateTimeStamp</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#decimal","title":"decimal","text":"<p>decimal has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) separated by a period as a decimal indicator. An optional leading sign is allowed. If the sign is omitted, \u2018+\u2019 is assumed. Leading and trailing zeroes are optional. If the fractional part is zero, the period and following zeroes can be omitted. For example: -1.23, 12678967.543233, +100000.00, 210.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#decimal</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#double","title":"double","text":"<p>double values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent \u00b7must\u00b7 be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for double.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#double</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#duration","title":"duration","text":"<p>The lexical representation for duration is the ISO 8601 extended format <code>PnYnMnDTnHnMnS</code>, where <code>nY</code> represents the number of years, <code>nM</code> the number of months, <code>nD</code> the number of days, <code>T</code> is the date/time separator, <code>nH</code> the number of hours, <code>nM</code> the number of minutes and <code>nS</code> the number of seconds. The number of seconds can include decimal digits to arbitrary precision.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#duration</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#float","title":"float","text":"<p>float values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent \u00b7must\u00b7 be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for float.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#float</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gday","title":"gDay","text":"<p>The lexical representation for gDay is the left truncated lexical representation for date: <code>---DD</code> . An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gDay</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gmonth","title":"gMonth","text":"<p>The lexical representation for gMonth is the left and right truncated lexical representation for date: <code>--MM</code>. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gMonth</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gmonthday","title":"gMonthDay","text":"<p>The lexical representation for gMonthDay is the left truncated lexical representation for date: <code>--MM-DD</code>. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats. This datatype can be used to represent a specific day in a month. To say, for example, that my birthday occurs on the 14th of September ever year.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gMonthDay</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gyear","title":"gYear","text":"<p>The lexical representation for gYear is the reduced (right truncated) lexical representation for dateTime: <code>CCYY</code>. No left truncation is allowed. An optional following time zone qualifier is allowed as for dateTime. To accommodate year values outside the range from <code>0001</code> to <code>9999</code>, additional digits can be added to the left of this representation and a preceding <code>-</code> sign is allowed. For example, to indicate 1999, one would write: <code>1999</code>. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gYear</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gyearmonth","title":"gYearMonth","text":"<p>The lexical representation for gYearMonth is the reduced (right truncated) lexical representation for dateTime: CCYY-MM. No left truncation is allowed. An optional following time zone qualifier is allowed. To accommodate year values outside the range from 0001 to 9999, additional digits can be added to the left of this representation and a preceding \u2018-\u2019 sign is allowed. For example, to indicate the month of May 1999, one would write: 1999-05. See also ISO 8601 Date and Time Formats (\u00b7D).</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gYearMonth</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#hexbinary","title":"hexBinary","text":"<p>hexBinary has a lexical representation where each binary octet is encoded as a character tuple, consisting of two hexadecimal digits ([0-9a-fA-F]) representing the octet code. For example, \u20180FB7\u2019 is a hex encoding for the 16-bit integer 4023 (whose binary representation is 111110110111).</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#hexBinary</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#html","title":"HTML","text":"<p>The datatype of RDF literals storing fragments of HTML content</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#int","title":"int","text":"<p>int is \u00b7derived\u00b7 from long by setting the value of \u00b7maxInclusive\u00b7 to be 2147483647 and \u00b7minInclusive\u00b7 to be -2147483648. int has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126789675, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#int</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#integer","title":"integer","text":"<p>integer has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) with an optional leading sign. If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#integer</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#jinja-template-string","title":"Jinja Template String","text":"<p>Jinja is a modern and designer-friendly templating language for Python and other languages.</p> <p>IRI: <code>https://vocab.eccenca.com/shui/jinja</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#langstring","title":"langString","text":"<p>The datatype of language-tagged string values</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#langString</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#language","title":"language","text":"<p>language represents natural language identifiers as defined by by [RFC 3066] . The \u00b7value space\u00b7 of language is the set of all strings that are valid language identifiers as defined [RFC 3066] . The \u00b7lexical space\u00b7 of language is the set of all strings that conform to the pattern [a-zA-Z]{1,8}(-[a-zA-Z0-9]{1,8})* . The \u00b7base type\u00b7 of language is token.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#language</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#long","title":"long","text":"<p>long is \u00b7derived\u00b7 from integer by setting the value of \u00b7maxInclusive\u00b7 to be 9223372036854775807 and \u00b7minInclusive\u00b7 to be -9223372036854775808. long has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#long</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#markdown","title":"Markdown","text":"<p>In addition to rdf:HTML, this is the datatype of RDF literals storing fragments of markdown content. eccenca Corporate Memory user interfaces support the rendering of all basic Markdown syntax features as well as the extensions for tables, code blocks, strikethrough, task lists and footnotes.</p> <p>IRI: <code>http://ns.ontowiki.net/SysOnt/Markdown</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#name","title":"Name","text":"<p>Name represents XML Names. The \u00b7value space\u00b7 of Name is the set of all strings which \u00b7match\u00b7 the Name production of [XML 1.0 (Second Edition)]. The \u00b7lexical space\u00b7 of Name is the set of all strings which \u00b7match\u00b7 the Name production of [XML 1.0 (Second Edition)]. The \u00b7base type\u00b7 of Name is token.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#Name</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#ncname","title":"NCName","text":"<p>NCName represents XML \u2018non-colonized\u2019 Names. The \u00b7value space\u00b7 of NCName is the set of all strings which \u00b7match\u00b7 the NCName production of [Namespaces in XML]. The \u00b7lexical space\u00b7 of NCName is the set of all strings which \u00b7match\u00b7 the NCName production of [Namespaces in XML]. The \u00b7base type\u00b7 of NCName is Name.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#NCName</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#negativeinteger","title":"negativeInteger","text":"<p>negativeInteger has a lexical representation consisting of a negative sign (\u2018-\u2018) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: -1, -12678967543233, -100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#negativeInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#nmtoken","title":"NMTOKEN","text":"<p>NMTOKEN represents the NMTOKEN attribute type from [XML 1.0 (Second Edition)]. The \u00b7value space\u00b7 of NMTOKEN is the set of tokens that \u00b7match\u00b7 the Nmtoken production in [XML 1.0 (Second Edition)]. The \u00b7lexical space\u00b7 of NMTOKEN is the set of strings that \u00b7match\u00b7 the Nmtoken production in [XML 1.0 (Second Edition)]. The \u00b7base type\u00b7 of NMTOKEN is token.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#NMTOKEN</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#nonnegativeinteger","title":"nonNegativeInteger","text":"<p>nonNegativeInteger has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, the positive sign (\u2018+\u2019) is assumed. If the sign is present, it must be \u2018+\u2019 except for lexical forms denoting zero, which may be preceded by a positive (\u2018+\u2019) or a negative (\u2018-\u2018) sign. For example: 1, 0, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#nonNegativeInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#nonpositiveinteger","title":"nonPositiveInteger","text":"<p>nonPositiveInteger has a lexical representation consisting of an optional preceding sign followed by a finite-length sequence of decimal digits (#x30-#x39). The sign may be \u2018+\u2019 or may be omitted only for lexical forms denoting zero, in all other lexical forms, the negative sign (\u2018-\u2018) must be present. For example: -1, 0, -12678967543233, -100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#nonPositiveInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#normalizedstring","title":"normalizedString","text":"<p>normalizedString represents white space normalized strings. The \u00b7value space\u00b7 of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The \u00b7lexical space\u00b7 of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The \u00b7base type\u00b7 of normalizedString is string.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#normalizedString</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#positiveinteger","title":"positiveInteger","text":"<p>positiveInteger has a lexical representation consisting of an optional positive sign (\u2018+\u2019) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: 1, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#positiveInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#short","title":"short","text":"<p>short is \u00b7derived\u00b7 from int by setting the value of \u00b7maxInclusive\u00b7 to be 32767 and \u00b7minInclusive\u00b7 to be -32768. short has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678, +10000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#short</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#string","title":"string","text":"<p>The string datatype represents character strings in XML. The \u00b7value space\u00b7 of string is the set of finite-length sequences of characters (as defined in [XML 1.0 (Second Edition)]) that \u00b7match\u00b7 the Char production from [XML 1.0 (Second Edition)]. A character is an atomic unit of communication, it is not further specified except to note that every character has a corresponding Universal Character Set code point, which is an integer.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#string</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#time","title":"time","text":"<p>The lexical representation for time is the left truncated lexical representation for dateTime: <code>hh:mm:ss.sss</code> with optional following time zone indicator. For example, to indicate 1:20 pm for Eastern Standard Time which is 5 hours behind Coordinated Universal Time (UTC), one would write: <code>13:20:00-05:00</code>. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#time</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#token","title":"token","text":"<p>token represents tokenized strings. The \u00b7value space\u00b7 of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The \u00b7lexical space\u00b7 of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The \u00b7base type\u00b7 of token is normalizedString.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#token</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedbyte","title":"unsignedByte","text":"<p>unsignedByte is \u00b7derived\u00b7 from unsignedShort by setting the value of \u00b7maxInclusive\u00b7 to be 255. unsignedByte has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 126, 100.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedByte</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedint","title":"unsignedInt","text":"<p>unsignedInt is \u00b7derived\u00b7 from unsignedLong by setting the value of \u00b7maxInclusive\u00b7 to be 4294967295. unsignedInt has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 1267896754, 100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedInt</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedlong","title":"unsignedLong","text":"<p>unsignedLong is \u00b7derived\u00b7 from nonNegativeInteger by setting the value of \u00b7maxInclusive\u00b7 to be 18446744073709551615. unsignedLong has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678967543233, 100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedLong</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedshort","title":"unsignedShort","text":"<p>unsignedShort is \u00b7derived\u00b7 from unsignedInt by setting the value of \u00b7maxInclusive\u00b7 to be 65535. unsignedShort has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678, 10000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedShort</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#xmlliteral","title":"XMLLiteral","text":"<p>The datatype of XML literal values.</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/","title":"Node Shapes","text":"<p>Node Shapes are resources of type\u00a0<code>shacl:NodeShape</code>. They can be used to validate resources as well as to define custom forms for presenting and editing resources of a specific type.</p> <p>This page lists all supported properties to describe node shapes.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#naming-and-presentation","title":"Naming and Presentation","text":"<p>Info</p> <p>In this group, presentation and naming properties are collected. Most of the properties are straight forward to use.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#name","title":"Name","text":"<p>The name of the node is presented to the user only when he needs to distinguish between different shapes for the same resource.</p> <p>Used Path: <code>shacl:name</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#description","title":"Description","text":"<p>The node description should provide context information for the user when creating a new resource based on this node.</p> <p>Used Path: <code>rdfs:comment</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#navigation-list-query","title":"Navigation list query","text":"<p>This property links the node shape to a SPARQL 1.1 Query in order to provide a sophisticated user navigation list query e.g. to add specific additional columns. The query should use {{FROM}} as a placeholder for the FROM section.</p> <p>Used Path: <code>shui:navigationListQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#depiction-image","title":"Depiction Image","text":"<p>This property links a node shape to an image in order to use this image when showing resources based on this node shape somewhere.</p> <p>Used Path: <code>http://xmlns.com/foaf/0.1/depiction</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#order","title":"Order","text":"<p>Specifies the order of this node shape.</p> <p>This property is used for the drop-down list in the shaped resource view as well as for priorising depictions and update queries. It is only relevant in case multiple node shapes are on the same level in the shape hierarchy (which is based on the rdfs:subClassOf relationship).</p> <p>Used Path: <code>shacl:order</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#chart-visualization","title":"Chart Visualization","text":"<p>Integrates a chart visualization in the node shape area. This Property is deprecated - charts on node shape level are not supported anymore.</p> <p>Used Path: <code>shui:provideChartVisualization</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#widgets","title":"Widgets","text":"<p>Integrate non-validating visualization widget in the node shape area.</p> <p>Used Path: <code>shui:WidgetIntegration_integrate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#vocabulary","title":"Vocabulary","text":"<p>Info</p> <p>In this group, the affected vocabulary classes as well as the used property shapes are managed.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#property-shapes","title":"Property Shapes","text":"<p>The used property shapes on this node. Please note that this is NOT a link to a datatype or object property but to a SHACL property shape.</p> <p>Used Path: <code>shacl:property</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#target-class","title":"Target class","text":"<p>Class this NodeShape applies to. This is a direct link to a class resource from a vocabulary.</p> <p>Used Path: <code>shacl:targetClass</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#processing-user-interaction","title":"Processing / User Interaction","text":"<p>Info</p> <p>In this group all shape properties are managed, which have an effect on how new or existing resources are processed or created.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#severity","title":"Severity","text":"<p>Categorize validation results (:Info, :Warning, :Violation). Defaults to :Violation.</p> <p>Used Path: <code>shacl:severity</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#sparql-constraints","title":"SPARQL Constraints","text":"<p>Add additional SPARQL based validation to your Node Shape.</p> <p>Used Path: <code>shacl:sparql</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#uri-template","title":"URI template","text":"<p>The URI template which is used, when a user manually creates new resources with this Node Shape.</p> <p>Used Path: <code>shui:uriTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#closed-node","title":"Closed Node","text":"<p>Enabling this will result in failing validation if the resource / node has properties which are NOT described with attached property shapes.</p> <p>Used Path: <code>shacl:closed</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#query-on-delete-update","title":"Query: On delete update","text":"<p>A query which is executed when the resource the node shape applies to gets deleted.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiGraph}}</code> - the currently used graph</li> <li><code>{{shuiAccount}}</code> - the account IRI of the active user, this includes the username (use a SUBSTR() function if you need the name only)</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> </ul> <p>Used Path: <code>shui:onDeleteUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#query-on-update-update","title":"Query: On update update","text":"<p>A query which is executed when this node shape is submitted. The query should be saved in the same graph as the shape (or imported).</p> <p>The query can use these placeholders:</p> <ul> <li><code>{{shuiGraph}}</code> - the currently used graph</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiAccount}}</code> - the account IRI of the active user, this includes the username (use a SUBSTR() function if you need the name only)</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> </ul> <p>Used Path: <code>shui:onUpdateUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#target-graph-template","title":"Target Graph Template","text":"<p>Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect).</p> <p>Used Path: <code>shui:targetGraphTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#query-is-creatable-resource","title":"Query: Is Creatable Resource","text":"<p>This query is executed to check if users get the controls to create resources (described with the node shape).</p> <p>The query needs to be an ASK query and can include the following placeholders, which will be substituted before execution:</p> <ul> <li><code>{{shuiGraph}}</code> - the IRI of the current working graph</li> <li><code>{{shuiAccount}}</code> - the IRI of the active user account</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> </ul> <p>Used Path: <code>shui:askIfCreatableQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#query-is-removable-resource","title":"Query: Is Removable Resource","text":"<p>This query is executed to check if users get the controls to remove resources (described with the node shape).</p> <p>The query needs to be an ASK query and can include the following placeholders, which will be substituted before execution:</p> <ul> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiGraph}}</code> - the IRI of the current working graph</li> <li><code>{{shuiAccount}}</code> - the IRI of the active user account</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> </ul> <p>Used Path: <code>shui:askIfRemovableQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#query-is-cloneable-resource","title":"Query: Is Cloneable Resource","text":"<p>This query is executed to check if users get the controls to remove resources (described with the node shape).</p> <p>The query needs to be an ASK query and can include the following placeholders, which will be substituted before execution:</p> <ul> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiGraph}}</code> - the IRI of the current working graph</li> <li><code>{{shuiAccount}}</code> - the IRI of the active user account</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> </ul> <p>Used Path: <code>shui:askIfCloneableQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#on-update-trigger-workflow","title":"On update trigger workflow","text":"<p>A workflow trigger which is executed when this nodeshape is submitted. The workflow(s) run instantaneously upon submitting the form.</p> <p>Used Path: <code>shui:onUpdateTriggerWorkflow</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#statement-annotation","title":"Statement Annotation","text":"<p>Info</p> <p>Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#enable","title":"Enable","text":"<p>A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape.</p> <p>Used Path: <code>shui:enableStatementLevelMetadata</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#provide-as-shape","title":"Provide as Shape","text":"<p>A value of true enables this node shape to be applied as statement annotation (reification).</p> <p>Used Path: <code>shui:isApplicableAsStatementLevelMetadata</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/","title":"Property Shapes","text":"<p>Property Shapes are resources of type\u00a0<code>shacl:PropertyShape</code>. They are used to\u00a0specify constraints and UI options that need to be met in the context of a Node Shape.</p> <p>The following Property Shape properties are supported:</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#naming-and-presentation","title":"Naming and Presentation","text":"<p>Info</p> <p>In this group, presentation and naming properties are collected. Most of the properties are straight forward to use, other properties provide more complex features, such as table reports.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#name","title":"Name","text":"<p>This name will be shown to the user.</p> <p>Used Path: <code>shacl:name</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#description","title":"Description","text":"<p>This text will be shown to the user in a tooltip. You can use new and blank lines for basic text structuring.</p> <p>Used Path: <code>shacl:description</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#order","title":"Order","text":"<p>Specifies the order of the property in the UI. Ordering is separate for each group.</p> <p>Used Path: <code>shacl:order</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#group","title":"Group","text":"<p>Group to which the property belongs to.</p> <p>Used Path: <code>shacl:group</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#show-always","title":"Show always","text":"<p>Default is false. A value of true let optional properties (min count = 0) show up by default.</p> <p>Used Path: <code>shui:showAlways</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#read-only","title":"Read only","text":"<p>Default is false. A value of true means the properties are not editable by the user. Useful for displaying system properties.</p> <p>Used Path: <code>shui:readOnly</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#chart-visualization-deprecated","title":"Chart Visualization (deprecated)","text":"<p>Integrates a chart visualization in the property shape area. Shapes with an integrated chart are ALWAYS shown in read mode and NEVER shown in edit mode. This Property is deprecated - please use a Widget Integration instead.</p> <p>Used Path: <code>shui:provideChartVisualization</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#provide-workflow-trigger-deprecated","title":"Provide Workflow Trigger (deprecated)","text":"<p>Integrates a workflow trigger button in order to execute workflows from or with this resource. Shapes with an integrated workflow trigger are ALWAYS shown in read mode and NEVER shown in edit mode.</p> <p>This property is deprecated - use a Widget Integration instead.</p> <p>Used Path: <code>shui:provideWorkflowTrigger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#vocabulary","title":"Vocabulary","text":"<p>Info</p> <p>In this group, property paths as well cardinality restrictions are managed.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#property-of","title":"Property of","text":"<p>The node shape this property shape belongs to.</p> <p>Used Path: <code>shacl:property</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#path","title":"Path","text":"<p>The datatype or object property used in this shape. This path will be ignored if there is a table report defined for the property shape. However, in the Business Knowledge Editor, this path can always be used for exploration.</p> <p>Used Path: <code>shacl:path</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-path-builder","title":"Query: Path Builder","text":"<p>Use this property to define a dynamic set of target resources or literals using a custom SPARQL query. This allows for advanced selection logic, including filtering, transitive relationships, or specific path sequences as well as the definition of a result set order.</p> <p>The values bound to the first variable in the query\u2019s projection will be treated as directly connected via the <code>sh:path</code> specified in this property shape. This enables simple backward-chaining inference. Note that the infererred connections are not (yet) included in SHACL validation.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiGraph}}</code> - the currently used graph</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiAccount}}</code> - the account IRI of the active user, this includes the username (use a SUBSTR() function if you need the name only)</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> </ul> <p>Note that for a proper usage of this feature, you additionally need to have the projection variables <code>?graph</code> or <code>?_graph</code> (the graph IRI where the relation statement is saved resp. in the context of which it was inferred) in your query. </p> <p>If the connected property value is a resource, the variables in the projection of this query will be used to populate the columns in the complex widget and the advanced editor.</p> <p>Used Path: <code>shui:valueQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#node-kind","title":"Node kind","text":"<p>The type of the linked nodes. In the Business Knowledge Editor, if these nodes are literals, they cannot be explored, but will be shown as metadata.</p> <p>Used Path: <code>shacl:nodeKind</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#min-count","title":"Min count","text":"<p>Min cardinality, 0 will show this property under optionals unless \u2018Show always = true\u2019</p> <p>Used Path: <code>shacl:minCount</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#max-count","title":"Max count","text":"<p>Max cardinality</p> <p>Used Path: <code>shacl:maxCount</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#datatype-property-specific","title":"Datatype Property Specific","text":"<p>Info</p> <p>In this group, all shape properties are managed, which only have effects on datatype properties.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#datatype","title":"Datatype","text":"<p>The datatype of the property.</p> <p>Used Path: <code>shacl:datatype</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#use-textarea","title":"Use textarea","text":"<p>Default is false. A value of true enables multiline editing capabilities for Literals via a <code>textarea</code> widget.</p> <p>Used Path: <code>shui:textarea</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#regex-pattern","title":"Regex Pattern","text":"<p>A XPath regular expression (Perl like) that all literal strings need to match.</p> <p>Used Path: <code>shacl:pattern</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#regex-flags","title":"Regex Flags","text":"<p>An optional string of flags for the regular expression pattern (e.g. \u2018i\u2019 for case-insensitive mode)</p> <p>Used Path: <code>shacl:flags</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#languages-allowed","title":"Languages allowed","text":"<p>This limits the given Literals to a list of languages. This property works only in combination with the datatype <code>rdf:langString</code>. Note that the expression for this property only allows for \u20182 Char ISO-639-1-Codes\u2019 only (no sub-tags).</p> <p>Used Path: <code>shui:languageIn</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#languages-unique","title":"Languages Unique","text":"<p>Default is false. A value of true enforces that no pair of Literals may use the same language tag.</p> <p>Used Path: <code>shacl:uniqueLang</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#object-property-specific","title":"Object Property Specific","text":"<p>Info</p> <p>In this group, all shape properties are managed, which only have effects on object properties.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#class","title":"Class","text":"<p>Class of the connected IRI if its nodeKind is sh:IRI. In the Business Knowledge Editor, any new node that a user creates by means of this property shape, will be an instance this class.</p> <p>Used Path: <code>shacl:class</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#used-class-for-resource-creation","title":"Used Class for Resource Creation","text":"<p>Use this property to overrule which class is used when a user creates a new resource inside of this property shape on-the-fly.</p> <p>Used Path: <code>shui:usedClassForResourceCreation</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#disable-default-sorting-of-values","title":"Disable default sorting of values","text":"<p>Per default, related resources are shown ordered by its IRI. This ordering can have a performance impact with large lists. Setting this property to true will disable this default behaviour. In case a path builder query with an order statement is used, this property has no effect.</p> <p>Used Path: <code>shui:disableDefaultValueSorting</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-selectable-resources","title":"Query: Selectable Resources","text":"<p>This query allows for listing selectable resources in the dropdown list for this property shape.</p> <p>You need to provide the projection variable <code>resource</code> in your query.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiGraph}}</code> - the currently used graph</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiAccount}}</code> - the account IRI of the active user, this includes the username (use a SUBSTR() function if you need the name only)</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> </ul> <p>Beta Feature: This query will be used as well to populate the selectable resources for the advanced editor and the candidate resources in the Business Knowledge Editor.</p> <p>Used Path: <code>shui:uiQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#inverse-path","title":"Inverse Path","text":"<p>Default is false. A value of true inverts the expected / created direction of a relation.</p> <p>Used Path: <code>shui:inversePath</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#deny-new-resources","title":"Deny new resources","text":"<p>A value of true disables the option to create new resources.</p> <p>Used Path: <code>shui:denyNewResources</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#resource-viewer-widget","title":"Resource viewer widget","text":"<p>Selects default object relation resource viewer widget. (NOTE: shacl2 only)</p> <p>Used Path: <code>shui:viewResourcesWithWidget</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#node-shape","title":"Node shape","text":"<p>This shape will be used to create an embedded view of the linked resource.</p> <p>Used Path: <code>shacl:node</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#processing-user-interaction","title":"Processing / User Interaction","text":"<p>Info</p> <p>In this group all shape properties are managed, which have an effect on how new or existing resources are processed or created.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#message","title":"Message","text":"<p>If there is a message value, then all validation results produced as a result of this shape will have exactly this message.</p> <p>Used Path: <code>shacl:message</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#severity","title":"Severity","text":"<p>Categorize validation results (:Info, :Warning, :Violation). Defaults to :Violation.</p> <p>Used Path: <code>shacl:severity</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#ignore-on-clone","title":"Ignore on clone","text":"<p>Disables reusing the value(s) when creating a clone of the resource. Note: This feature was named \u2018copy\u2019 before.</p> <p>Used Path: <code>shui:ignoreOnCopy</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-on-insert-update","title":"Query: On insert update","text":"<p>This query is executed when a property value is added or changed.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiGraph}}</code> - the currently used graph</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiAccount}}</code> - the account IRI of the active user, this includes the username (use a SUBSTR() function if you need the name only)</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> <li><code>{{shuiObject}}</code> - the object value of the statement matched by the property shape</li> <li><code>{{shuiProperty}}</code> - the IRI of the property of the statement matched by the property shape</li> </ul> <p>Used Path: <code>shui:onInsertUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-on-delete-update","title":"Query: On delete update","text":"<p>A query which is executed when the statement the property shape applies to gets deleted.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiGraph}}</code> - the currently used graph</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape)</li> <li><code>{{shuiAccount}}</code> - the account IRI of the active user, this includes the username (use a SUBSTR() function if you need the name only)</li> <li><code>{{shuiAccountName}}</code> - the user name/ID of the active user account</li> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage)</li> <li><code>{{shuiObject}}</code> - the object value of the statement matched by the property shape</li> <li><code>{{shuiProperty}}</code> - the IRI of the property of the statement matched by the property shape</li> </ul> <p>Used Path: <code>shui:onDeleteUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#target-graph-template","title":"Target Graph Template","text":"<p>Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect).</p> <p>Used Path: <code>shui:targetGraphTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#statement-annotation","title":"Statement Annotation","text":"<p>Info</p> <p>Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#enable","title":"Enable","text":"<p>A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape.</p> <p>Used Path: <code>shui:enableStatementLevelMetadata</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#provided-shapes","title":"Provided Shapes","text":"<p>Instead of providing all possible statement annotation node shapes for the creation of new statement annotations, this property will limit the list to the selected shapes only.</p> <p>Used Path: <code>shui:provideStatementLevelMetadataShapes</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/","title":"Workflow Trigger","text":"","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#introduction","title":"Introduction","text":"<p>Workflow Trigger allow for manual execution of data integration workflows inside of the exploration interface.</p> <p>Optionally, a reference to the resource in view on workflow execution can be sent, allowing an executed workflow to act specifically on this resource (or a specific portion of the Knowledge Graph related to it). Workflow trigger are associated to Node Shapes by defining special-purpose non-validating Property Shape resources.</p>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#setup","title":"Setup","text":"<p>To specify the Build (DataIntegration) workflow which should be executed via the shape element, a Workflow Trigger resource has to be created first. Workflow Trigger can be defined and used in any active\u00a0Shape Catalog\u00a0(active means, it is imported from the main Shape Catalog). A workflow trigger resource references a data integration workflow by URI.</p> <p></p> <p>To define a workflow trigger the following information is needed:</p> <ul> <li>Label: The trigger resource needs a label (can be given in different languages), which is used for the button presentation.</li> <li>Description: The trigger resource needs a description, which is used as text that is sitting left of the button for further documentation of the activity to the user.</li> <li>Workflow: the workflow parameter defines the workflow that shall be executed upon clicking the button. The workflow can be selected from a dropdown list.</li> <li>Refresh View: can be either\u00a0<code>true</code>or\u00a0<code>false</code>.\u00a0If this value is set to\u00a0<code>true</code>, the view that contains the workflow trigger will be reloaded upon workflow completion</li> <li>Send Resource Reference: can be either\u00a0<code>true</code>or\u00a0<code>false</code>. If this value is set to\u00a0<code>true</code>, a payload that consists of the\u00a0resource IRI\u00a0that is represented in the view as well as the\u00a0graph IRI\u00a0of the graph that is currently selected.</li> </ul>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#integration","title":"Integration","text":"<p>Once a trigger resources is defined, it can be attached to a Node Shape by using a special-purpose non-validating Property Shape resources. Such\u00a0property shapes use a\u00a0<code>shui:provideWorkflowTrigger</code>\u00a0statement to define, which workflow trigger are to be represented. SHACL path statements on such Property Shape resources are meaningless and ignored, but may be provided.</p>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#payload-structure","title":"Payload Structure","text":"<p>When\u00a0Send Resource Reference\u00a0is set to\u00a0true,\u00a0a payload is added to the call of the workflow. The payload consists of a JSON document with two attributes:</p> <p>Workflow Payload</p> <pre><code>{\n   \"graphIRI\": \"http://example.org/example-graph\",\n   \"resourceIRI\": \"http://example.org/example-graph/examle-resource\"\n}\n</code></pre> <ul> <li><code>graphIRI</code>\u00a0is the IRI of the graph that is currently viewed, and</li> <li><code>resourceIRI</code>is the IRI of the resource that is viewed.</li> </ul>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/statement-annotations/","title":"Statement Annotations","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#introduction","title":"Introduction","text":"<p>Statement Annotations provide a way to express knowledge about statements. Typical use cases for Statement Annotations include:</p> <ul> <li>the temporal validity of information,</li> <li>the origin of information, or</li> <li>just a way to annotate a specific statement with a human readable comment.</li> </ul>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#usage","title":"Usage","text":"<p>If enabled on a specific type of statement or type of resource, you see a Statement Annotation text bubble beside every annotatable statement:</p> <p></p> <p>This bubble has different status:</p> <ul> <li>A\u00a0empty text bubble\u00a0indicates, that there is no annotation on the statement, but the annotation feature is enabled for this statement.</li> <li>A\u00a0filled text bubble\u00a0indicates, that there is at least one annotation on the statement.</li> <li>No bubble\u00a0indicates, that the annotation feature is NOT enabled on this type of statement.</li> </ul> <p>Clicking on one of the text bubbles opens the Statement Annotation dialog for this specific statement:</p> <p></p> <p>In the Statement Annotation dialog, you can select the Statement Annotation Template and click Create.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#setup","title":"Setup","text":"<p>In order to have a working Statement Annotation setup, the following steps need to be done in the component  Knowledge Graphs:</p> 1. Create a Statement Annotation Graph <p>Open the graph selection and create a new graph . Choose New Statement Annotation Graph and provide the needed meta data.</p> <p></p> 2. Setup and import the Statement Annotation Graph in your data graph <p>Open the graph selection and switch to your data graph where the resources exist which you want to annotate.  Edit the graph description and  add the property Statement Annotation Graph as well as Imports, where your select your newly created annotation graph in both fields.</p> <p></p> 3. Create a shape which will be used to annotate statements <p>Open the graph selection and switch to your Shape Catalog where your project shapes are managed. Create a node shape which can be used as an Annotation and configure Provide as Shape\u00a0in the Group Statement Annotation to\u00a0true for this node shape.</p> <p></p> 4. Allow statement annotations for specific Classes or Properties <p>Open the graph selection and switch to your Shape Catalog where your project shapes are managed. Select the Node or Property Shape from your Shape Catalog and enable annotations by setting the\u00a0Enable\u00a0option in the\u00a0Statement Annotations\u00a0group to\u00a0true.</p> <p></p> <p>This will enable the feature on the statements of all resources shown with this Node Shape or on all statements shown with this Property Shape.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#technical-background","title":"Technical Background","text":"<p>From the technical point of view, the Statement Annotation feature uses RDF Reification to annotate Statements (Triples) with additional background information. Statement resources can be annotated with custom Annotation Resources. These Annotation Resources are based on specific Shapes which are enabled as Statement Annotation shapes. Reification Resources as well as Annotation Resources are managed in a Statement Annotation Graph, which need to be configured on a Graph as well as imported to this Graph. The following illustration depicts this schema with boxes and arrows:</p> <p></p> <p>Some notes on this:</p> <ul> <li>There is one\u00a0Statement Reification Resource per Statement Annotation.</li> <li>Removing the\u00a0Statement Annotation also removes the\u00a0Statement Reification Resource.</li> <li>All annotation triples (8 triples in the image) are created in the Statement Annotation Graph, so Step 2 of the setup procedure is important.</li> </ul>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#querying-statement-annotations","title":"Querying Statement Annotations","text":"<p>In order to automate access to Statement Annotations, you can query them with SPARQL e.g. via\u00a0cmemc\u00a0or the API endpoint.</p> <p>Here is a query example to start with:</p> <pre><code># Select resources with statement annotations and statement annotation graph\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nSELECT DISTINCT ?AnnotatedResource ?StatementAnnotationGraph ?AnnotationResource\nWHERE {\n  GRAPH ?StatementAnnotationGraph {\n    [] a rdf:Statement ;\n        rdf:subject ?AnnotatedResource ;\n        rdf:value ?AnnotationResource .\n  }\n}\n</code></pre>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/","title":"Versioning of Graph Changes","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#introduction","title":"Introduction","text":"<p>This feature keeps track of changes to your Knowledge Graphs by creating change set data based on the user\u2019s editing activities.</p> <p>Info</p> <p>Versioning of graph changes is currently supported for (tracked) graphs of type <code>void:Dataset</code> only.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#usage","title":"Usage","text":"<p>If enabled on a graph, all changes using shaped user interfaces will be tracked in the configured Versioning Graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#setup","title":"Setup","text":"<p>To enable this feature on a specific graph you need to setup the following steps.</p> 1. Create a Versioning Graph <p>In Exploration, create a new graph and define it as a Versioning Graph.</p> <p></p> 2. Configure a graph to use this Versioning Graph <p>In Exploration, edit this graph and add the Versioning Graph property to select the newly created Versioning Graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#technical-background","title":"Technical Background","text":"<p>For each editing activity (\u2192 Save a Form), a ChangeSet resource will be created. This resource has some metadata (user, timestamp, label) as well as links to added and deleted Statements (using RDF Reification).</p> <p>The details of the used vocabulary are available at\u00a0the Changeset Vocabulary\u00a0page.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/query-module/","title":"Query Module","text":"","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#introduction","title":"Introduction","text":"<p>The\u00a0Query\u00a0module provides a user interface to store, describe, search and edit SPARQL queries. The queries are evaluated on the Knowledge Graph and provide a way to granularly aggregate semantic data as tables. These tables can then be exported as CSV, Excel or JSON documents.</p> <p>The Query module features two areas, the catalog and the editor.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#catalog-graph-selection","title":"Catalog graph selection","text":"<p>The Query module lists all the graphs containing queries. Use the Queries catalogue graph selector in the top left to select the graph whose queries should be listed and edited.</p> <p></p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#query-catalog","title":"Query catalog","text":"<p>The catalog lists all existing SPARQL queries including name, type and description.</p> <p>Use the\u00a0 Search\u00a0bar in order to look for a specific query.</p> <p></p> <p>Select the query from the Queries catalog, to open and load the query.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#query-editor","title":"Query editor","text":"<p>Use the Query editor to edit and execute SPARQL queries. In this view you can also browse query result previews and download full query results as CSV files.</p> <p>The Query editor provides an interface where you can write and edit your SPARQL queries. The query editor features SPARQL syntax highlighting and SPARQL validation, allowing only syntactically correct SPARQL queries to be executed.</p> <p>The Query editor allows to Run query, Download Results, Delete, Save and Save as Queries.</p> <p></p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#run-a-query","title":"Run a query","text":"<p>Click\u00a0 Run to execute the query and to display a preview of the query results under the query editor. The results are presented as a table with pagination.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#export-results","title":"Export results","text":"<p>To export the full set of results without any limits in form of a CSV file click\u00a0 Download result on the top right.</p> <p></p> <p>Info</p> <p>The preview result ordering has no impact on the result ordering in the exported file. If you want to export some ordered query results, you need to use the\u00a0<code>ORDER BY</code>\u00a0construct in the SPARQL query itself.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#save-a-query","title":"Save a query","text":"<p>To save a query in the Query catalog click\u00a0 Save. This opens a dialog that allows you to overwrite the existing query.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#placeholders","title":"Placeholders","text":"<p>In addition to the standard SPARQL syntax, placeholders can be used to parametrize a query. Placeholders are indicated in the query using a string of the form\u00a0<code>{{placeholdername}}</code>. Multiple placeholders can be defined by changing the name inside the brackets.</p> <p>When a query contains a placeholder, the placeholder list to the right of the query editor shows a field with its name.</p> <p></p> <p>When running a query that contains placeholders, the query editor replaces the\u00a0<code>{{placeholdername}}</code>\u00a0string in the query with the respective string entered into the placeholder list. This is a direct string replacement, so placeholders can contain simple strings and literal values, URIs, variables or even sub queries.</p> <p>Running a query with a placeholder is only possible when all placeholder fields in the placeholder list have been filled.</p> <p>A typical use case is restricting a query to a specific class of objects stated by a placeholder:</p> <pre><code>SELECT * WHERE { ?classInstance a &lt;http://dbpedia.org/ontology/{{class}}&gt; .}\n</code></pre> <p>This query selects all instances of a specific DBpedia Ontology class. When you enter\u00a0<code>Person</code>\u00a0into the\u00a0<code>class</code>\u00a0placeholder field in the placeholder list the following query is executed:</p> <pre><code>SELECT * WHERE { ?classInstance a &lt;http://dbpedia.org/ontology/Person&gt; .}\n</code></pre>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#llm-query-assist","title":"(LLM) Query Assist","text":"<p>Query Assist is a preview feature powered by generative AI, LLM, and a Retrieval Augmented Generation (RAG) approach. It allows you to interact with your data using natural language. Describe the insights you need in plain English sentences and have a SPARQL query generated for you.</p> <p>The query assist can be enabled in the dataplatform configuration.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/thesauri-management/","title":"Thesauri Management","text":"","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#introduction","title":"Introduction","text":"<p>The Thesaurus module provides a user interface to create, browse and edit thesaurus resources and general taxonomical data modeled in the\u00a0Simple Knowledge Organization System (SKOS).</p> <p>A thesaurus is a reference work that lists concepts with similar meaning, containing for example synonyms, often including taxonomical relations between these concepts. Taxonomies describe classifications of concepts into categories sharing particular features and their relations to broader (parent) and narrower (child) concepts.</p> <p>An example for a taxonomy or classification is how companies can be categorized into industries, industry groups and sectors. An airport belongs to the sector Airport Services, the broader category Transportation Infrastructure, the broader category Transportation and the broader category Industrials. You can think of these relations as a hierarchical tree representing the relations as individual branches like shown in the navigation tree on the left side of the Thesaurus view. In a concept scheme Industries, a top branch in this tree, as for example the sub-industry Industrials or Health Care, is called a top concept. All branches together belong to the concept scheme Industries.</p> <p></p> <p>Info</p> <p>In order to build thesauri or taxonomies with the Thesaurus module you should be familiar with SKOS structures.</p> <p>SKOS is a convenient way to model taxonomical data. The\u00a0SKOS Reference\u00a0provides detailed documentation on the usage of SKOS. The Thesaurus module allows to create, browse and edit such structures, providing a way to structure your hierarchical data in a simple interface and make it accessible for use cases like documentation and master data management.</p> <p>Info</p> <p>Before you start working with the Thesaurus module ensure that the vocabulary\u00a0Simple Knowledge Organization System\u00a0is installed in the Vocabulary catalog (see section\u00a0Vocabulary Catalog).</p> <p>Click\u00a0 Thesauri\u00a0in the main menu, to open the Thesaurus project catalog.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#thesaurus-project-catalog","title":"Thesaurus project catalog","text":"<p>The Thesaurus project catalog lists thesaurus projects with relevant metadata in a searchable and sortable table.</p> <p>In order to get more information on a thesaurus project and edit its metadata, click\u00a0 Show more\u00a0in the table row. The view expands showing the project metadata. Click\u00a0 Edit on the right side of the row to open the edit mode, enter your changes and click\u00a0SAVE.</p> <p></p> <p>To open the detail view of a thesaurus project, click the project name in the catalog.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#creating-a-new-thesaurus-project","title":"Creating a new thesaurus project","text":"<p>In order to create a new thesaurus project, select Create new thesaurus project\u00a0on the upper right of the thesaurus project catalog. Enter a name for your thesaurus project and more metadata if required and click\u00a0SAVE. The thesaurus detail view is shown, displaying an empty thesaurus project.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#importing-an-existing-thesaurus","title":"Importing an existing thesaurus","text":"<p>You can import existing thesaurus data in a thesaurus project. To import a thesaurus make sure that the data is in Turtle format and you are on project level in the thesaurus detail view. Click the context menu\u00a0 Show more options\u00a0in the upper right of the thesaurus detail view, then select\u00a0Import data. Upload the file containing the thesaurus. Click\u00a0SAVE\u00a0to import the data.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#exporting-a-thesaurus-project","title":"Exporting a thesaurus project","text":"<p>To export a thesaurus project, go to the project level, click\u00a0the context menu\u00a0 Show more options\u00a0in the upper right of the thesaurus detail view, then select\u00a0Export project. Confirm the dialog and click\u00a0DOWNLOAD\u00a0to download the thesaurus project.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#removing-a-thesaurus-project","title":"Removing a thesaurus project","text":"<p>To remove a thesaurus project, go to the project level in the navigation tree, click\u00a0the context menu\u00a0 Show more options\u00a0in the upper right of the thesaurus detail view, then select\u00a0Remove project. Confirm the removal and click\u00a0REMOVE\u00a0to delete the thesaurus project.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#thesaurus-detail-view","title":"Thesaurus detail view","text":"<p>After selecting a thesaurus project in the Thesaurus project catalog, the thesaurus detail view is shown. This view consists of three components. The navigation tree component is displayed to the left. It displays the hierarchical structure of the thesaurus. The upmost element is the project level. Clicking\u00a0\u00a0brings you back to the Thesaurus project catalog.</p> <p>Below the project level, the concept scheme(s) and concepts are displayed. A\u00a0Concept scheme\u00a0serves as a meaningful aggregation of a number of concepts. A concept itself constitutes a unit of thought, a conceptual class or category of objects you want to describe. For example, all concepts on a particular topic or domain could belong to the same concept scheme.</p> <p>If a concept in the navigation tree has narrower (child) concepts, you can expand this branch of the tree by clicking the arrow displayed in front of the concept. Arrows are only shown for concepts for which narrower concepts exist. Clicking the name of a concept scheme or concept updates both the detail view as well as the concept list to the right of the navigation tree.</p> <p>The tab\u00a0Statistics\u00a0shows statistical information about the content of the thesaurus project.</p> <p>A list of concept schemes of the thesaurus project is shown in the lower part as a searchable table.</p> <p>To edit project metadata in the thesaurus detail view click\u00a0 Edit.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#concept-detail-view","title":"Concept detail view","text":"<p>Clicking on a\u00a0Concept scheme\u00a0or a\u00a0Concept\u00a0in the navigation tree displays information on that resource in the concept detail view to the right.</p> <p>The concept detail view displays the concept\u2019s preferred and alternative labels in the\u00a0Overview\u00a0tab, as well as definitions and other metadata. A list of top concepts of a concept scheme or narrower (child) concepts of a concept is shown below these metadata as a searchable table called concept list.</p> <p>To find specific sub concepts of a concept, you can use the search bar in the concept list. Click any result row to display more information on the concept, like its labels, definition and notation. Click the concept name in the row itself to open the concept detail view for this concept. Selecting a concept in the concept list updates the navigation tree to highlight the concept\u2019s position in the tree.</p> <p>In case of concept schemes, the\u00a0Statistics\u00a0tab can be used to learn more about the number of concepts in this concept scheme, as well as their relations.</p> <p>The\u00a0Triples\u00a0tab shows the RDF source code of the concept as\u00a0Turtle\u00a0serialization. The triples in this view are editable, but be aware that changes in the triple editor can cause major errors in the Thesaurus module. When you are not an expert in working with triples do not use the triple view as an editor.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#adding-new-concepts-and-concept-schemes","title":"Adding new concepts and concept schemes","text":"<p>Concept schemes can be added on project level in an opened thesaurus project. Click on the project name in the navigation tree, then click the context menu\u00a0 Show more options\u00a0and select\u00a0Create new concept scheme. You can return to this level by clicking the name of the thesaurus project above the navigation tree.</p> <p>Enter at least one preferred label naming the concept scheme. Select the language of the preferred label to the right of the label field. Click  under the label field to set multiple labels. Create the concept scheme by clicking\u00a0SAVE. A new concept scheme will appear in the concept list as well as the navigation tree.</p> <p>Top concepts can be added in the same way. Instead of starting on project level, select an existing concept scheme first. Click the context menu\u00a0 Show more options\u00a0and a new option\u00a0Create new top concept\u00a0is available. Use it to create a new top concept.</p> <p>Selecting a normal concept brings up the option\u00a0Create new concept\u00a0in the same menu. Using this option creates a new concept and automatically add it as a narrower concept to the concept you created it on. It also automatically adds the\u00a0<code>broader</code>\u00a0back link on the newly created concept.</p> <p>To edit a concept or concept scheme, click\u00a0 Edit\u00a0to get a form displaying all available fields, like labels and definition.</p> <p>Info</p> <p>For each concept, you have to enter one preferred label per language. Be aware that SKOS allows only one preferred label per language. If you want to enter more labels use alternative or hidden labels as specified by SKOS.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#creating-relations-between-concepts","title":"Creating relations between concepts","text":"<p>Besides the narrower relation automatically generated when a new concept is created, you can add further relations between concepts. You can add, for example, a second broader concept for an existing concept or a related concept to indicate associative relations.</p> <p>To add relations, select the concept in the navigation tree. In the detail view, click\u00a0 Edit to open the edit mode.</p> <ul> <li>To add an associative relation to another concept, enter the concept name in the field\u00a0Related concept.</li> <li>To add a further broader relation, enter the name of the broader concept in the field\u00a0Broader concepts.</li> </ul> <p>You can only choose from existing concepts. Click\u00a0SAVE\u00a0to confirm your changes.</p> <p>In the same way you can also add a top concept to a second concept scheme. Use therefore the field\u00a0Top concept of\u00a0in the editing mode of a top concept.</p> <p>When adding relations the inverse relation is automatically added, too.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#removing-concepts-and-concept-schemes","title":"Removing concepts and concept schemes","text":"<p>Warning</p> <p>Be aware that removing a concept or concept scheme with child elements (top concepts or narrower concepts) means that the complete substructure, i.e.\u00a0all childs, are also deleted regardless whether they are used in another concept scheme.</p> <p>To remove concepts or concept schemes, select the resource in the navigation tree, click the context menu\u00a0 Show more options\u00a0and select the Remove option. Confirm the dialog and click\u00a0REMOVE.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/","title":"Vocabulary Catalog","text":"","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/#introduction","title":"Introduction","text":"<p>Vocabularies are the foundation for semantic data lifting activities.This module shows the list of all managed vocabularies in Corporate Memory that are accessible for the user. The table represents the list of known vocabularies. Installed vocabularies are indicated by the orange switch in the column\u00a0<code>Installed</code>.</p> <p></p>","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/#add-new-vocabulary","title":"Add new vocabulary","text":"<p>Click\u00a0\u00a0to register a new vocabulary.</p> <p>A new form will be shown, fill it an add the file to import the vocabulary to Corporate Memory.</p> <p>Use the\u00a0Search\u00a0bar to find vocabularies based on name or other metadata.</p>","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/#extended-information-and-options","title":"Extended information and options","text":"<p>Each table row provides a menu\u00a0with more options clicking on\u00a0\u00a0or in the\u00a0<code>Vocabulary</code>\u00a0column.</p> <p>A vocabulary which is known and available but not installed, looks like this:</p> <p></p> <p>Example of extended information of uninstalled Vocabulary Catalog</p> <ul> <li>Use\u00a0Install\u00a0or the switch in the column\u00a0<code>Installed</code>\u00a0to install the Catalog.</li> <li>Use\u00a0View\u00a0to access the Vocabulary.</li> </ul> <p>A vocabulary which is installed looks like this</p> <p></p> <p>Example of extended information of installed Vocabulary Catalog</p> <ul> <li>Use\u00a0Uninstall\u00a0to remove an installed vocabulary or\u00a0Install\u00a0to install a vocabulary.</li> <li>Use\u00a0View\u00a0to access the Vocabulary.</li> <li>Use\u00a0Upload\u00a0to install or overwrite the vocabulary from a file.</li> </ul>","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/workspace-configuration/","title":"Workspaces","text":"<p>The specific configuration of the application defines which options are available here, i.e. whether you can select one of several workspaces, access only a default workspace or are allowed to create own workspaces.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/workspace-configuration/#select-a-workspace","title":"Select a workspace","text":"<p>To select a workspace click on the user icon on the right side of the page</p> <p></p> <p>Click the drop-down list and click the workspace you want to open.</p> <p></p> <p>Step Result</p> <p>The workspace opens and now you can enable or disabled the modules and change modeule configuration as per your requirement.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/workspace-configuration/#configure-a-workspace","title":"Configure a workspace","text":"<p>Click on the user icon on the right side of the page then click on Configuration.</p> <p></p> <p>Click on Workspace then select the workspace you want to see the details.</p> <p></p> <p>Click on down arrow to expand the Workspace and DI Workspace Configuration to see the configuration details as shown below.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/workspace-configuration/#add-a-workspace","title":"Add a Workspace","text":"<p>Click on the user icon on the right side of the page then click on Configuration.</p> <p></p> <p>Click on Workspace on the left side of the page then click on Create New Workspace</p> <p></p> <p>Type the Id and Label name then click on Add</p> <p></p> <p>Step Result</p> <p>The workspace created sucessfully and now you can enable or disabled the modules and change modeule configuration as per your requirement.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/workspace-configuration/#delete-a-workspace","title":"Delete a workspace","text":"<p>Click on User Icon on the right side of the page then click on Configuration then click on Workspace</p> <p></p> <p>Select the Workspace from the drop-down you want to delete</p> <p></p> <p>Click on the Delete Icon on the right side of the page.</p> <p></p> <p>Click on Delete</p> <p></p> <p>Step Result</p> <p>The workspace has been deleted.</p> <p></p> <p>Note</p> <p>When you delete a workspace, no graphs or Build projects are deleted.</p>","tags":["KnowledgeGraph"]},{"location":"getting-started/","title":"Getting Started","text":"","tags":["BeginnersTutorial"]},{"location":"getting-started/#introduction","title":"Introduction","text":"<p>This page describes how to work with Corporate Memory and shortly outlines all functionalities of the user interface. For the installation and configuration of Corporate Memory refer to the\u00a0\u2606 Deploy and Configure\u00a0section.</p> <p>Info</p> <p>Functions described in this manual for adding, editing or deleting data depend on the write permissions that are assigned to you. In case you only have read permission these functions are not available for you.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#about","title":"About","text":"<p>eccenca Corporate Memory is a semantic data management software that accelerates analytics and reporting projects by transforming the way enterprises understand, align, prepare and access their data.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#main-features","title":"Main features","text":"<p>The main features of Corporate Memory include:</p> <ul> <li>Flexible metadata and schema layer based on knowledge graphs</li> <li>Data virtualization and analytics</li> <li>Data integration and indexing</li> <li>Dataset and vocabulary management</li> <li>Thesaurus and taxonomy management</li> <li>Big data scalability</li> <li>Access control</li> </ul>","tags":["BeginnersTutorial"]},{"location":"getting-started/#minimal-requirements","title":"Minimal requirements","text":"<p>For the best user experience, we recommend to use the newest version of Google Chrome or Mozilla Firefox. Corporate Memory is tested with the following browsers:</p> <ul> <li>Google Chrome 83 or later</li> <li>Mozilla Firefox 78 or later</li> <li>Microsoft Edge 83 (on Windows) or later</li> </ul>","tags":["BeginnersTutorial"]},{"location":"getting-started/#login-and-logout","title":"Login and Logout","text":"<p>To start eccenca Corporate Memory:</p> <ol> <li>Enter the URL in your web browser.</li> <li>Enter your credentials and click\u00a0LOG IN.</li> </ol> <p>After you logged in to your Corporate Memory instance, the main application view appears.</p> <p>To log out, open the menu\u00a0 in the Module bar and click\u00a0Logout.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#workspaces","title":"Workspaces","text":"<p>The specific configuration of the application defines which options are available here, i.e. whether you can select one of several workspaces, access only a default workspace or are allowed to create own workspaces.</p> <p>See the workspaces section for more details.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#user-interface-and-modules","title":"User interface and modules","text":"<p>The user interface of Corporate Memory usually consists of two sections:</p> <ol> <li>The module bar providing access to the various modules of Corporate Memory and to a menu with further options</li> <li>The main section for operating the software functions</li> </ol> <p></p> <p>Each module provides a set of functionalities and views for specific use cases. To access a module, click the module name. The active module is highlighted.</p> <p>By default, Corporate Memory provides the following modules:</p> <ul> <li>EXPLORE - for Knowledge Graph browsing and exploration, specifically<ul> <li>Knowledge Graphs\u00a0- a generic and extensible RDF data browser and editor</li> <li>Vocabularies\u00a0- for vocabulary management</li> <li>Thesauri\u00a0- for managing thesauri and taxonomies based on SKOS</li> <li>Queries\u00a0- a SPARQL query interface</li> </ul> </li> <li>BUILD\u00a0- for creating and integrating Knowledge Graphs, with specific links to<ul> <li>Projects - the BUILD Projects level</li> <li>Datasets - the Datasets across all BUILD Projects</li> <li>Workflows - the Workflows across all BUILD Projects</li> <li>Activities - activities overview and monitoring</li> </ul> </li> </ul> <p>Note</p> <p>Depending on your specific product package, more or fewer modules can be available.</p> <p>Use the provided search field(s) in each module to search for specific keywords or strings in names and labels of resources.</p> <p>The\u00a0Knowledge Graphs\u00a0module provides more search fields (e.g. in the Graph box, Navigation box, etc.) where you can limit your search to specific graphs or resources.</p> <p></p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#settings-menu-for-table-views","title":"Settings menu for table views","text":"<p>Each table view of Corporate Memory provides a setting menu (3) to adjust the table according to your requirements. You can change the sorting of columns, show or hide specific columns or set filters. To open the settings menu click\u00a0.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#context-help","title":"Context help","text":"<p>Corporate Memory provides a context help (4) for the main functions of modules. To open the context help click\u00a0\u00a0in the upper right corner. To close the context help, click\u00a0.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#used-icons","title":"Used Icons","text":"<p>This section provides an overview of icons and their functionality in Corporate Memory.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#icons-of-main-interaction","title":"Icons of main interaction","text":"Icon Description Add a new element. Edit an element. Remove data or an element. Note that there is no \u2018Trash\u2019 or \u2018Recycle Bin\u2019. Open the context specific user help.","tags":["BeginnersTutorial"]},{"location":"getting-started/#icons-for-navigation","title":"Icons for navigation","text":"Icon Description Navigate back. Navigate one page backwards. Navigate one page forward. Go to the first page. Go to the last page.","tags":["BeginnersTutorial"]},{"location":"getting-started/#icons-for-table-views","title":"Icons for table views","text":"Icon Description Adjust settings for a table view. Change filter of table columns. Indicates no specific ordering. Order this column ascending. Order this column descending.","tags":["BeginnersTutorial"]},{"location":"getting-started/#editor-specific-icons","title":"Editor specific icons","text":"Icon Description Object mappings. Value mappings.","tags":["BeginnersTutorial"]},{"location":"getting-started/with-your-sandbox/","title":"Welcome to your eccenca Corporate Memory Sandbox","text":"<p>Thank you for registering your eccenca Corporate Memory Sandbox! We\u2019re excited to have you on board and look forward to showing you how Corporate Memory can help you with your data management needs. We hope that you find this experience valuable and informative. If you have any questions or feedback, please don\u2019t hesitate to reach out to us. Thanks again for joining us!</p> <p>eccenca\u2019s Corporate Memory is a platform for creating and managing Enterprise Knowledge Graphs. It has three main stages: Build, Explore, and Consume. In the Build stage, you can convert legacy data points from existing datasets into a Knowledge Graph structure. The Explore stage allows you to interact with your Knowledge Graph, while the Consume stage is used to retrieve information from the graph and integrate it programmatically with your IT infrastructure.</p> <ul> <li> <p> Get Started and Get Help</p> <p>Learn how to get started with Corporate Memory in our Getting Started Guide. Understand the user interface, application structure, and basic concepts.</p> <p> Getting Started</p> <p>Community support for the sandbox is provided in this  forum, use it seek for help, report issues or suggestions, or discuss solution ideas.</p> <p>Find and contact us at:  \u2022  \u2022  \u2022 </p> </li> <li> <p> Sandbox Resources</p> <p>The sandbox includes a sample build project named \u201cProduct Data Integration Demo\u201d and the graphs generated by that project, as well as an integration graph as an entry point: \u201cProducts - Integration\u201d. Shacl shapes are provided for the product vocabulary. These are used in the Business Knowledge Editor module for visual exploration as well as in a custom workspace configuration called Product Data Integration to demonstrate how the user interface can be customized.</p> </li> <li> <p> Masterclass Material</p> <p>A list of materials and resources to reproduce and follow the masterclass session: From Zero to KG Hero: Boosting Your KG Creation Productivity with eccenca Corporate Memory. Originally presented at The Knowledge Graph Conference 2023. Watch the recording on .</p> <p> materials and resources</p> </li> <li> <p> Tutorials and Examples</p> <p>Our tutorials help you to create Knowledge Graphs and to use the exploration and consumption features. To get started, we recommend:</p> <ul> <li>Lift tabular sources (CSV, XSLX, JDBC)</li> <li>Active Learning of Linking Rules</li> <li>Building a customized User Interface</li> <li>Populate Data to Neo4j</li> <li>Data in any Format via Custom API</li> </ul> </li> <li> <p> BUILD</p> <p>BUILD data product pipelines that turn you existing data points into Enterprise Knowledge Graphs. Use them to create data product assembly lines in a visual, intuitive, and business-user friendly way.</p> <p> Learn more about Build</p> </li> <li> <p> EXPLORE</p> <p>With EXPLORE, you can interact with and visualize your Knowledge Graph data in both list and drill-down views. Configure custom front-end configurations to meet domain-specific needs without duplicating data.</p> <p> Learn more about Explore</p> </li> <li> <p> CONSUME</p> <p>The CONSUME tier provides several standard APIs for retrieving data from your Knowledge Graph. We also provide native integrations with PowerBI and redash.Custom APIs can be configured to provide data in any format.</p> <p> Learn more about Consume</p> </li> <li> <p> AUTOMATE</p> <p>With AUTOMATE, you can easily set up and automate processes in your Knowledge Graph. Our <code>cmemc</code> command line tool simplifies the management and migration of data and configurations in Corporate Memory. Learn about our vision of a DataOps process based on Corporate Memory, how to schedule workflows, and how to use variable data inputs.</p> <p> Learn more about Automate</p> </li> <li> <p> Training and Certification</p> <p>Our Learning Management System includes courses and certifications for different audiences: e.g., business, (linked data) consultants, or DevOps.</p> <p> register at eccenca LMS</p> </li> <li> <p> About Corporate Memory</p> <p>eccenca\u2019s Corporate Memory solution transforms background knowledge about products, processes, partners, people, policies, and data into understandable and executable containers. By automating decisions across hundreds of individual processes, Corporate Memory helps scale the use and reuse of knowledge, increasing the productivity and effectiveness of knowledge workers. This provides a sustainable approach to scaling decision automation and AI governance.</p> </li> </ul>"},{"location":"getting-started/with-your-sandbox/material/","title":"Masterclass - Material and Namespace Suggestions","text":"<p>A list of materials and resources to reproduce and follow the masterclass (MC).</p> <p>About Session</p> <p>This masterclass provides the foundation of KG solutions based on the eccenca Corporate Memory platform. The platform covers the full lifecycle of KG applications. Our partners and experts love it for its productivity, ease of use and level of automation in KG creation and evolution. It boosts your abilities to capture, access and re-use knowledge from your organization in a whole new way. Join our tutors to learn about the platform and what it can do in your KG projects.</p>"},{"location":"getting-started/with-your-sandbox/material/#file-resources","title":"File resources","text":"Type Name Resource Dataset (XLSX) Hardware Products hardware.xlsx Dataset (CSV) Service Products services.csv Dataset (JSON) Supplier supplier.json Dataset (XML) Organizational Information orgmap.xml Vocabulary* Products Vocabulary pv.ttl <p>*) vocabulary already installed, attached for information purposes only.</p>"},{"location":"getting-started/with-your-sandbox/material/#namespace-suggestions","title":"Name(space) suggestions","text":"Type Name IRI Dataset (KG) MC Prod - Integration <code>http://mc.eccenca.com/prod-int/</code> Dataset (KG) MC Prod - Hardware <code>http://mc.eccenca.com/prod-hw/</code> Dataset (KG) MC Prod - Services <code>http://mc.eccenca.com/prod-srv/</code> Dataset (KG) MC Prod - Supplier <code>http://mc.eccenca.com/prod-suppl/</code> Dataset (KG) MC Prod - Organization <code>http://mc.eccenca.com/prod-org/</code> Dataset (KG) MC Prod - Links <code>http://mc.eccenca.com/prod-links/</code> Build Project MC Product Build Demo"},{"location":"getting-started/with-your-sandbox/material/#resource-iri-suggestions","title":"Resource IRI suggestions","text":"Type IRI Department <code>http://mc.eccenca.com/prod-data/dept-{id}</code> Employee <code>http://mc.eccenca.com/prod-data/empl-{email}</code> Hardware <code>http://mc.eccenca.com/prod-data/hw-{id}</code> Price <code>http://mc.eccenca.com/prod-data/price-{parent-id}-{currency}</code> Product Category <code>http://mc.eccenca.com/prod-data/prod-cat-{name|uuid}</code> Service <code>http://mc.eccenca.com/prod-data/srv-{id}</code> Supplier <code>http://mc.eccenca.com/prod-data/suppl-{id}</code>"},{"location":"release-notes/corporate-memory-19-10/","title":"Corporate Memory 19.10","text":"<p>Corporate Memory 19.10 is the third release in 2019.</p> <p>The highlights of this release are:</p> <ul> <li>Switched from an own OAuth 2.0 authorization server implementation to a more capable and supported solution based on Keycloak.</li> <li>Largely enhanced JDBC / SQL support:</li> <li>Overall performance improvements.</li> <li>Allowing hierarchical mappings to write to JDBC/SQL datasets.</li> <li>Support for Oracle SQL databases.</li> <li>Input validation of mandatory fields in shaceline resource details views.</li> <li>Resource Table results can be directly downloaded in Excel and other formats.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataPlatform configuration must be adapted according to the migration notes below.</p> <p>Consequently this release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v19.10</li> <li>eccenca DataIntegration v19.10</li> <li>eccenca DataManager v19.10</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#eccenca-dataintegration-v1910","title":"eccenca DataIntegration v19.10","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Write support for hierarchical data via JDBC.</li> <li>Allow arbitrary column names on <code>SqlEndpoint</code> datasets.</li> <li>Support for Oracle SQL.</li> <li>JSON Rest endpoint that allows to evaluate portions of a linking rules.</li> <li>SPARQL 1.1 endpoint each <code>RdfDataset</code> thereby allowing instantaneous SPARQL access via REST and SPARQL SERVICE keyword.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Add parameter that decides how empty values are handled by the concat transformer.</li> <li>Add \u2018ZIP file regex\u2019 parameter to all bulk resource datasets that allows to filter resources inside the bulk resource container (currently ZIP files only).</li> <li>Simplify Hive Serialization, refactor SQL utility methods.</li> <li>SPARQL Update operator</li> <li>Do not read from input data source when the SPARQL Update template is static, i.e. when it always runs the same static SPARQL Update query exactly once.</li> <li>Add SPARQL Update execution report containing various statistics, e.g. number of queries, query throughput etc.</li> <li>Support Apache Velocity Engine based templates. This adds logic like conditional branching and loops to the templates. For more information visit https://velocity.apache.org</li> <li>SPARQL Select operator generates an execution report with various statistics, e.g. rows processed, runtime etc.</li> <li>SPARQL dataset generates execution report when executing SPARQL Update queries e.g. from the SPARQL Update operator with statistics like remaining queries, time estimation etc.</li> <li>Extended SQL Endpoint documentation.</li> <li>Added local execution to JDBC dataset. The execution is more efficient and pushes limits and group-by columns into the database.</li> <li>While workspace is initializing, subsequent requests will timeout.<ul> <li>Config parameter <code>workspace.timeouts.waitForWorkspaceInitialization</code> (in milliseconds, default: 5000ms).</li> </ul> </li> <li>Scheduler: Added \u2018Stop On Error\u2019 parameter. If enabled this will stop a scheduler on the first execution error. Default: <code>false</code>.</li> <li>Added <code>addMarkdownDocumentation</code> parameter to <code>/plugins</code> and <code>/plugins/:pluginType</code> REST endpoints in order to request the optional Markdown documentation for plugins.</li> <li>Added SPARQL query timeout parameter in order to limit query execution times to \u2018Knowledge Graph\u2019 and \u2018SPARQL endpoint\u2019 datasets and to the SPARQL Select operator.</li> <li>RDF serialization: Tasks referenced in RDF serialization with <code>di:output</code> and <code>di:task</code> use the correct project task URIs instead of artificial URIs or literals.</li> <li>Complete Zip Stream support (replacing reliability on zip files only).</li> <li>MultiCsvZip are now BulkResourceDatasets.</li> <li>JDBC dataset: Removed database parameter. If required, the database needs to be specified as part of the JDBC URL, e.g., <code>jdbc:mysql://localhost:port/databaseName</code>. Appending connection parameters to the URL is supported now.</li> <li>Safer defaults:<ul> <li>SPARQL Endpoint and Knowledge Graph dataset: Do not clear graphs before execution to prevent accidental deletion of graphs.</li> <li>Knowledge Graph dataset: Increase page size from 1000 to 100,000, because small page sizes lead to suboptimal execution performance.</li> <li>SPARQL Update operator: Decrease batch size from 10 to 1.</li> </ul> </li> <li>Upgraded to Apache Spark 2.3.3</li> <li>SQL datasets: If a URI attribute is specified, the URI will be added as a new column of that name.</li> <li>All projects are loaded first, before any cache or any other autorun activity is started. This improves loading when both the workspace provider and some caches load from the same database.</li> <li>Improved JDBC dataset performance for MySQL and MariaDB by using the \u2018Load Data\u2019 command to load a local CSV file into the database.</li> <li>Shipping with MariaDB JDBC driver.</li> <li>Redirect to original request URL instead of the start page after authenticating a user and logging in.</li> <li>Revised generation of table names in JDBC dataset (see documentation). Keep letter case of table names.</li> <li>Schedulers are always started by the super user, if the super user is configured.</li> <li>On start-up if DataPlatform is configured, DI will wait for DP to become healthy for a configurable amount of time. Parameters:<ul> <li><code>eccencaDataPlatform.health.waitingTimeInSeconds</code>: Overall time in seconds DI should wait for DP to be up and healthy. Setting this to 0 will disable this check. Default: 60.</li> <li><code>eccencaDataPlatform.health.delayBetweenRetriesInSeconds</code>: Amount of time in seconds to wait between retries. Default: 5.</li> </ul> </li> <li>Enhanced <code>JdbcDataset</code> and <code>SqlEndpoint</code> parameters and improve their descriptions.</li> <li>Suppress case changes in <code>SqlEndpoint</code> table names.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#eccenca-datamanager-v1910","title":"eccenca DataManager v19.10","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>New module <code>task</code><ul> <li>Offers a direct resource actions. Interfaces only available by URL. See documentation for more details.</li> <li>Path <code>/task/resource/create</code> allows to create a new resource by given graph and type.</li> </ul> </li> <li>General<ul> <li>Config parameter <code>js.config.api.defaultTimeout</code> for default UI queries timeout.</li> <li>Config parameter <code>js.config.resourceTable.timeoutDownload</code> for Resource Table timeout on download requests on Explore and Query modules.</li> <li>Validation of mandatory fields in <code>shacline</code> view.</li> <li>Add new property <code>shui:onUpdateUpdate</code> for <code>sh:NodeShape</code>.</li> </ul> </li> <li>Module Explore<ul> <li>Config parameter <code>js.config.modules.explore.graphlist.whiteList</code> to filter specific graphs.</li> <li>Config parameter <code>js.config.modules.explore.graphlist.internalGraphs</code> to hide specific graphs.</li> <li>Config parameter <code>js.config.modules.explore.navigation.itemsPerPage</code> show items per page in navigation box.</li> <li>Support for inverse property relations.</li> </ul> </li> <li>Module Query<ul> <li>Config parameter <code>js.config.modules.query.timeout</code> for manual queries.</li> <li>Config parameter <code>js.config.modules.query.graph</code> to define the graph were data is saved and requested.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Default pagination size of 20 elements for all Resource Tables.</li> <li>Allow datatype <code>xsd:anyURI</code> for literals.</li> <li>Upgraded to react 16.</li> </ul> </li> <li>Module Explore<ul> <li>Merged graph view <code>RDFDoc</code> into \u2018resource details view\u2019.</li> <li>Renamed global search label.</li> <li>Graph creation will add the type <code>void:Dataset</code> instead of <code>owl:Ontology</code>.</li> <li>Use the label of the type of the instances for the name of the CSV file downloaded from the Resource Table.</li> <li>Display the context graph in <code>properties</code> and <code>references</code> tables.</li> </ul> </li> <li>Module Dataset<ul> <li>Adjusted position and tooltip of parameter <code>uriProperty</code> in \u2018Add data stepper\u2019.</li> </ul> </li> <li>Module Query<ul> <li>Use the dataset label for the name of the CSV file downloaded from the Resource Table.</li> </ul> </li> <li>Module Login<ul> <li>Renew tokens when they expire.</li> </ul> </li> <li>Module Administration<ul> <li>Allow to search in IRIs for list of readable and writeable graphs.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Module Explore<ul> <li>Config parameter <code>js.config.modules.explore.graphlist.listQuery</code> which is now obsolete.</li> <li>Config parameter <code>js.config.modules.explore.details.history</code> which is now obsolete as the feature is no longer supported.</li> <li>\u2018History\u2019 tab.</li> </ul> </li> <li>Module Sync also known as <code>SubscriptionManagement</code>.</li> </ul> <p>In addition to that, multiple stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#eccenca-dataplatform-v1910","title":"eccenca DataPlatform v19.10","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>SPARQL 1.1 Query endpoint<ul> <li>An <code>in-iris</code> property to the JSON <code>search</code> parameter to enable search over IRIs.</li> <li>A <code>timeout</code> parameter which allows to configure the maximal amount of milliseconds that a query execution can run.</li> <li>Support for Microsoft Excel (<code>.xlsx</code>) file download for <code>SELECT</code> queries.</li> </ul> </li> <li>SPARQL 1.1 Update endpoint<ul> <li>A <code>timeout</code> parameter which allows to configure the maximal amount of milliseconds that an update execution can run (Stardog only).</li> </ul> </li> <li>SPARQL 1.1 Graph Store Protocol<ul> <li><code>multipart/form-data</code> support for HTTP PUT.</li> <li>Added the <code>timeout</code> parameter, which allows to configure the maximal amount of milliseconds that a request execution should run.</li> <li>Documentation for content negotiation by <code>format</code> query parameter.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Data Sharing: A WebSub based Publish-Subscribe service for RDF named graphs.</li> <li>IoT Permissions Plugin: A plugin which enables the usage of the IoT Permissions Service API 2.</li> <li>OAuth 2.0 authorization server: Issues access tokens to a client after successfully authenticating a user.</li> <li>Authentication: User management via authentication providers as it was only needed by the OAuth 2.0 authorization server.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Stardog<ul> <li>Upgraded support to version 7.0.2.</li> <li>Versioning does no longer work with Stardog 7.</li> <li>Legacy versioning support for Stardog 6 (deprecated).</li> </ul> </li> <li>OAuth 2.0: Resource protection is now mandatory (can no longer be disabled, use anonymous access instead).</li> <li>SPARQL 1.1 Query endpoint<ul> <li>The value of the <code>string</code> property of the JSON <code>search</code> parameter is now tokenized which means that each token will be searched separately. Only results matching all tokens will be returned.</li> <li>Updated Spring Boot version from 1.5.21 to 1.5.22.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#migration-notes","title":"Migration Notes","text":"<p>With the removal of the OAuth 2.0 authorization server capability, many configuration properties have been changed.</p> <ul> <li>Removed<ul> <li>The properties\u00a0<code>oauth2.clients.*</code>\u00a0have been removed.</li> <li>The properties\u00a0<code>authentication.*</code>\u00a0have been removed.</li> </ul> </li> <li>Moved<ul> <li>The property\u00a0<code>oauth2.jwt.signing.verificationKey</code>\u00a0has been moved to\u00a0<code>security.oauth2.resource.jwt.keyValue</code>\u00a0.</li> <li>The property\u00a0<code>oauth2.anonymous</code>\u00a0has been moved to\u00a0<code>security.oauth2.resource.anonymous</code>\u00a0.</li> <li>The claims mapping properties under\u00a0<code>oauth2.resourceServer.claimsMapping.*</code>\u00a0have been moved to\u00a0<code>security.oauth2.resource.jwt.claims.*</code>\u00a0.</li> <li>The properties\u00a0<code>oauth2.authorizeRequests.*</code>\u00a0to configure the resources to be protected by the resource server have been moved to\u00a0<code>security.oauth2.resource.authorizeRequests.*</code>\u00a0.</li> </ul> </li> <li>Added<ul> <li>The value of the property\u00a0<code>security.oauth2.resource.id</code>\u00a0\u00a0(defaults to\u00a0<code>dataplatform</code>) must be part of the\u00a0<code>aud</code>\u00a0(audience) claim in the JWT used to access a protected resource.</li> </ul> </li> </ul> <p>Don\u2019t forget to update your configuration accordingly. For instance, assuming you have the following old configuration:</p> <pre><code>oauth2:\n  anonymous: true\n  clients:\n    - id: client\n      secret: secret\n      grantTypes:\n        - authorization_code\n      redirectUris:\n        - http://example.org/oauth/client\n  jwt:\n    enabled: true\n      signing:\n        verificationKey: |\n          -----BEGIN PUBLIC KEY-----\n          ...\n          -----END PUBLIC KEY-----\n  resourceServer:\n    claimsMapping:\n      username: 'preferred_username'\n      clientId: 'azp'\n      groups:\n        key: 'groups'\n</code></pre> <p>The migrated properties should look like this:</p> <pre><code>security:\n  oauth2:\n    resource:\n      anonymous: true # optional, defaults to `false`\n      jwt:\n        keyValue: |\n          -----BEGIN PUBLIC KEY-----\n          ...\n          -----END PUBLIC KEY-----\n     claims:\n       username: preferred_username # optional, defaults to `preferred_username`\n       groups: groups # optional, defaults to `groups`\n       clientId: azp # optional, defaults to `azp`\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/","title":"Corporate Memory 20.03","text":"<p>Corporate Memory 20.03 is the first release in 2020.</p> <p>The highlights of this release are:</p> <ul> <li>DataIntegration supports resources to be stored in an AWS S3 buckets.</li> <li>Rich SHACL forms can be used for the creation of new resources.</li> <li>New BUILD module is introduced in DataManager to provide an experts shortcut to DataIntegration.</li> <li>SPARQL queries can now be used to define arbitrary result tables directly in SHACL views.</li> <li>Object properties can be switched between chips and resource table view in SHACL views.</li> <li>cmemc, our Corporate Memory Command Line Interface is now generally available</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.03</li> <li>eccenca DataIntegration v20.03</li> <li>eccenca DataManager v20.03</li> <li>eccenca Corporate Memory Control (cmemc) v20.03</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-dataintegration-v2003","title":"eccenca DataIntegration v20.03","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Support for additional value types for mapping targets (XML Schema date/time types, duration, etc.).</li> <li>More date types to <code>DateTypeParser</code>.</li> <li>Script operator can also be used in local execution mode.</li> <li>Operator search in mapping rule editor.</li> <li>Safe-mode that prevents access to external data systems, e.g. JDBC, SPARQL dataset:<ul> <li>Data access in executed workflows is not affected by the safe-mode.</li> <li>Safe-mode can be toggled on and off at runtime in the UI.</li> <li>To enable safe-mode, set following parameter in the config: <code>config.production.safeMode = true</code>.</li> </ul> </li> <li>Config parameter <code>caches.config.enableAutoRun</code>, to enable/disable automatic execution of caches (default: <code>true</code>).</li> <li>Knowledge Graph File Upload Operator: Lets the user upload N-Triples files from the file repository into a DataPlatform graph.</li> <li>Support for file resource repositories on S3.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Improved password encryption<ul> <li>Using AES-256 instead of AES-128.</li> <li>If no valid key has been configured in production mode, application does not start.</li> <li>Better error messages, if key is invalid.</li> <li>Secret AES-256 key is generated from the configured key using SHA256 hashing, allowing for arbitrarily long keys.</li> </ul> </li> <li>Improved SQL writing performance for MariaDB and MySQL.</li> <li>Rework of the dataset view:<ul> <li>If a dataset is opened, the SPARQL (for RDF datasets) or table view (other datasets) is directly opened.</li> <li>Added Material Design formatting.</li> <li>Added scrollbars to tables with many columns.</li> </ul> </li> <li>Active learning UI uses Material Design cards.</li> <li>the config endpoint <code>/core/config</code> is no longer available when running in production mode.</li> <li>If a mapping reads from a CSV column that does not exist, the mapping still executes successfully, but a warning is displayed in the execution report.</li> <li>With XML dataset in streaming mode default URIs are now created by using the row and column numbers of the XML element instead of a hash value.</li> <li>Reduce memory foot print of linking evaluation and execution.</li> <li>We are now sorting tasks in workspace by label.</li> <li>Now displaying the modification date in resource dialog.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-datamanager-v2003","title":"eccenca DataManager v20.03","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General<ul> <li>Blank nodes are filtered in shacline views.</li> <li>Open external links in a new browser window.</li> <li><code>shui:valueQuery</code> for tabular representation<ul> <li>load of pre-defined queries as a <code>shui:valueQuery</code>.</li> </ul> </li> <li><code>ResourceTable</code> now allow to resolve labels on download results.</li> </ul> </li> <li>Access Control<ul> <li>Allow to create user and groups providing just a label.</li> </ul> </li> <li>Module Explore<ul> <li>Shacl views now allow to switch object property links between chip and <code>ResourceTable</code> view</li> <li>allow to add additional columns, search and filter using a <code>ResourceTable</code>.</li> <li><code>sh:path</code> is no longer mandatory on Shacl. One of both <code>sh:path</code> or <code>shui:valueQuery</code> is now mandatory.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Layout make better use of widescreen estate.</li> <li>Show existing resources linked by an object property in a <code>ResourceTable</code> in edit mode.</li> </ul> </li> <li>Module Explore<ul> <li>Navigation box uses search query only when a search term is present.</li> <li>Creation of new resources can now make use of rich shacline forms.</li> <li>Add new config parameter <code>modules.explore.navigation.defaultClass</code> that selects a default class EXPLORE should start with when <code>modules.explore.graphlist.defaultGraph</code> is defined.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Datasets management<ul> <li>Config parameter <code>includeOAuthToken</code> is no longer used. DataIntegration authentication will be done in an iFrame instead.</li> </ul> </li> <li>Access Control<ul> <li>Support for parameter <code>Requires client</code> has been removed from Access Control module.</li> </ul> </li> </ul> <p>In addition to that, multiple stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-dataplatform-v2003","title":"eccenca DataPlatform v20.03","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>SPARQL 1.1 Query endpoint<ul> <li>Support for non-string literals when using the <code>contains</code>, <code>startsWith</code> and <code>endsWith</code> filter functions.</li> <li>Server side label resolution by using <code>resolveLabels</code>, which allows <code>NONE</code> and <code>LABEL</code> for resolving IRIs to literals.</li> <li>The search parameter utilizes Stardog\u2019s built-in text match instead of <code>SPARQL CONTAINS</code> if a Stardog database is used. The search string is cleaned from special characters and english stop words and conjuncts all search terms.</li> </ul> </li> <li>SPARQL 1.1 Update endpoint<ul> <li><code>owl:imports</code> resolution on <code>USING</code>/<code>USING NAMED</code> clauses.</li> </ul> </li> <li><code>/info</code> and <code>/health</code> in addition to defaults <code>/actuator/info</code> and <code>/actuator/health</code> for backward compatibility.</li> <li>Show Redis status in application health if used as cache.</li> <li>The property <code>spring.security.oauth2.resourceserver.jwt.issuerUri</code> or <code>spring.security.oauth2.resourceserver.jwt.jwk-set-uri</code> must now be set in order to allow for JWT (signature) validation (see migration notes below).</li> <li>Access Conditions<ul> <li>Allow embedded creation of elements of type <code>eccauth:Account</code> or <code>eccauth:Group</code>.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Upgraded Stardog support to version 7.1.1.</li> <li>The default value of the property <code>spring.security.oauth2.resourceserver.jwt.claims.clientId</code> has been changed from <code>azp</code> to <code>clientId</code>.</li> <li>The properties under <code>security.oauth2.resource.jwt.claims.*</code> have been moved to <code>spring.security.oauth2.resourceserver.jwt.claims.*</code>.</li> <li>The property <code>security.oauth2.resource.anonymous</code> has been moved to <code>spring.security.oauth2.resourceserver.anonymous</code>.</li> <li>The property <code>http.cors.allowOriginRegex</code> has been moved to <code>http.cors.allowedOrigins</code>.</li> <li>The property <code>http.cors.allowMethods</code> has been moved to <code>http.cors.allowedMethods</code>.</li> <li>The property <code>http.cors.allowHeaders</code> has been moved to <code>http.cors.allowedHeaders</code>.</li> <li>The property <code>http.cors.exposeHeaders</code> has been moved to <code>http.cors.exposedHeaders</code>.</li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Versioning support has been removed.</li> <li>Access Conditions<ul> <li>Support for <code>eccauth:requiresProtocol</code> and <code>eccauth:requiresClient</code> has been removed.</li> </ul> </li> <li>The properties <code>security.oauth2</code> have been removed.</li> <li>The property <code>http.cors.enabled</code> has been removed.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-corporate-memory-control-cmemc-v2003","title":"eccenca Corporate Memory Control (cmemc) v20.03","text":"<p>This version of eccenca Corporate Memory Control (cmemc) adds the following new features:</p> <ul> <li><code>config</code> command group, to <code>list</code>, <code>edit</code> and <code>check</code> configurations</li> <li><code>graph</code> command group, to <code>list</code>, <code>import</code>, <code>export</code>, <code>delete</code> and <code>open</code> graphs</li> <li><code>project</code> command group, to <code>list</code>, <code>import</code>, <code>export</code>, <code>create</code> and <code>delete</code> projects</li> <li><code>query</code> command group, to <code>list</code> and <code>execute</code> local and remote SPARQL queries</li> <li><code>workflow</code> command group, to <code>list</code>, <code>execute</code>, <code>open</code> or <code>inspect</code> workflows</li> <li><code>workspace</code> command group, to <code>import</code> and <code>export</code> the workspace</li> <li>ability to work with SSL enabled deployments (add CA certs)</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#dataintegration","title":"DataIntegration","text":"<p>With v20.03 the following changes need to be made in your dataintegration.conf file when upgrading from v19.10:</p> <ul> <li>Remove the <code>play.crypto.secret</code> property, it has been deprecated with v20.03.</li> <li>Two properties need to be added: <code>play.http.secret.key</code> and <code>plugin.parameters.password.crypt.key</code><ul> <li>both take an arbitrary alpha numerical string of minimum 16 characters length</li> <li>depending on your deployment set them in your <code>production.conf</code> or <code>application.conf</code> DataIntegration configuration file</li> </ul> </li> </ul> <pre><code>...\nplay.http.secret.key = \"uiodshfoun78qwg8asd7gfasdasddfgn87gsn8fdsngasdfsngf8ds\"\n...\nplugin.parameters.password.crypt.key = \"uiodshfoun78qwg8\"\n...\n</code></pre> <p>Note</p> <p>In case you are deploying based on the DataIntegration docker images eccenca provides a <code>production.conf</code> configuration file needs to be used, the <code>dataintegration.conf</code> cannot be used to set the <code>play.http.secret.key</code> parameter.</p> <p>Warning</p> <p>The property <code>plugin.parameters.password.crypt.key</code> is used to encrypt / decrypt the passwords stored with you project configuration (e.g. JDBC passwords). When you set or change this property, all passwords in your DataIntegration projects need to be re-entered.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#datamanager","title":"DataManager","text":"<p>With v20.03 a the new BUILD module is introduced. In order to enable and configure it add the following section to you <code>application.yml</code>:</p> DataManager application.yml BUILD module configuration<pre><code>js.config.modules.build:\n  enable: true\n  url: \"&lt;DI-BASE_URI&gt;/workspace\"\n</code></pre> <p>Where <code>&lt;DI-BASE-URI&gt;</code> need to point to the DataIntegration URI (e.g. <code>https://host.domain.com/dataintegration</code>).</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#dataplatform","title":"DataPlatform","text":"<p>With v20.03 the following changes need to be made in your <code>application.yml</code> file when upgrading from v19.10:</p> <ul> <li>the key <code>http.cors.enabled</code> has been removed</li> <li>the key <code>http.cors.allowOriginRegex</code> has been renamed to <code>http.cors.allowedOrigins</code> and takes now a list of origins:</li> </ul> DataPlatform application.yml http.cors configuration<pre><code>http:\n  cors:\n    allowedOrigins: # optional, defaults to allow all: \"*\"\n      - \"http://docker.local\"\n      - \"https://docker.local\"\n</code></pre> <ul> <li>the key <code>security.oauth2.resource.jwt.keyValue</code> has been removed</li> <li>the key <code>spring.security.oauth2.resourceserver.jwt.jwk-set-uri</code> need to be specified.<ul> <li>Refer to your keycloaks Corporate Memory (cmem) realm \u201cOpenID Endpoint Configuration\u201d details where the relevant uri is listed as <code>jwks_uri</code>:</li> </ul> </li> </ul> DataPlatform application.yml spring.security configuration<pre><code>spring:\n## OAuth2Properties\n  security:\n    oauth2:\n      resourceserver:\n        jwt:\n          jwk-set-uri: http://keycloak:8080/auth/realms/cmem/protocol/openid-connect/certs\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/","title":"Corporate Memory 20.06","text":"<p>Corporate Memory 20.06 is the second release in 2020.</p> <p>The highlights of this release are:</p> <ul> <li>Jinja template support in DataIntegration workflows</li> <li>Physical unit normalization and distance measure operators</li> <li>Versioning of user edits in SHACL based forms</li> <li>Named query API to get data without SPARQL know-how</li> <li>OpenAPI compliant DataPlatform API specification and UI</li> <li>Preview/Beta release of the upcoming DataIntegration Workspace</li> <li>Preview/Beta support for GraphDB as triple store backend \u2192 Vendor Homepage</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below. In addition to that, cmemc has some changed default outputs.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.06</li> <li>eccenca DataIntegration v20.06</li> <li>eccenca DataManager v20.06</li> <li>eccenca Corporate Memory Control (cmemc) v20.06</li> <li>eccenca Corporate Memory PowerBI Connector v20.06</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-dataintegration-v2006","title":"eccenca DataIntegration v20.06","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Workflow operator that evaluates a user-defined template on entities.<ul> <li>Jinja templating language is supported.</li> <li>Can be used after a transformation or directly after datasets that output a single table, such as CSV or Excel.</li> <li>For each input entity, an output entity is generated that provides a single output attribute, which contains the evaluated template.</li> <li><code>DataIntegration</code> transformation tasks can be used as Jinja filters.</li> </ul> </li> <li>Operators to normalize and compare physical quantities.<ul> <li>Transform rule operator to normalize physical quantities to a base unit.</li> <li>Distance measure to compare two physical quantities.</li> </ul> </li> <li>The transform evaluate tab allows the selection of the mapping rule to be evaluated.</li> <li>Rule operator for generating UUIDs.</li> <li>Complete rework of the workspace UI</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Removed deprecated SQL query strategies.</li> <li>Updated eccenca logo and favicon.</li> <li>Consistent resource deletion behavior for resource repositories:<ul> <li>For resource repositories that do not share resources between projects, resources are removed on project deletion.</li> <li>For resource repositories that do share resources between projects, resources are NOT removed on project deletion.</li> <li>In both cases, the user is informed in the UI about the behavior of the configured resource repository.</li> </ul> </li> <li>RDF Workspace Provider: Improved reading of project data if Graph Store protocol is supported by RDF endpoint.</li> <li>RDF Workspace Provider: Improved import of projects if Graph Store protocol is supported by RDF endpoint.</li> <li>More consistent labels for tasks, operators and their parameters.</li> <li>If active learning is started with an existing linkage rule, it\u2019s also used to generate the unlabeled pool.</li> <li><code>ExcelMapTransformer</code> reloads the referenced resource if it\u2019s modification time changed. For performance reasons the check may be deferred by some seconds.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-datamanager-v2006","title":"eccenca DataManager v20.06","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Query Module<ul> <li>The results of CONSTRUCT queries can now be downloaded.</li> </ul> </li> <li>General<ul> <li>Support to send base64 encoded queries in SPARQL and framed requests (configure with <code>js.config.api.sparqlQueryBase64Enconded</code>).</li> <li>Add new helper for unify datatype info (as regex)</li> </ul> </li> <li>Build module<ul> <li>Show/hide build module based on user (ACL) action <code>urn:eccenca:di</code>.</li> </ul> </li> <li>Shacline<ul> <li>Allow sh:name and sh:description to used multiple times - in different languages - in the SHAPE definitions.</li> <li>Support for the <code>xsd</code> formats <code>gYearMonth</code>, <code>gYear</code>, <code>gMonthDay</code>, <code>gDay</code>, <code>gMonth</code> and <code>duration</code>.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Upgrade node, npm and yarn.</li> <li>Allow <code>sh:pattern</code> and <code>sh:flags</code> for shapes definition of literals of type string, numeric and dates.</li> </ul> </li> <li>Shacline</li> <li>Shacl-groups without content are now hidden.</li> <li>ReadOnly properties are not available for adding on edit mode.</li> <li>Use languages from <code>js.config.titleHelper.languages</code> from config file as default languages.</li> </ul> <p>In addition to that, multiple stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-dataplatform-v2006","title":"eccenca DataPlatform v20.06","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>SPARQL 1.1 Query &amp; Update endpoint<ul> <li>Support for base64 encoded query strings.</li> </ul> </li> <li>Manual Edit Endpoint<ul> <li>Fine grained access control schemes for edits performed on SHACL shapes.</li> <li>Basic versioning of those edits.</li> <li>Shapes that support the configuration of those features directly in DataManager</li> </ul> </li> <li>OpenAPI 3 compliant API documentation<ul> <li>OpenAPI compliant documentation of all DataPlatform APIs</li> <li>Swagger UI based browser interface for interactive learning and experiments with the APIs</li> </ul> </li> <li>Backend Support<ul> <li>Support for GraphDB was added.</li> </ul> </li> <li>Additional APIs<ul> <li>Title Helper (<code>/api/explore/title</code>): Finds short labels for Resources.</li> <li>Named Query (<code>/api/queries</code>): Allows passing the identifier of the query and a parameterization to generate a CSV report</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>OpenAPI 3 compliant API documentation<ul> <li>All REST controllers have been annotated with OpenAPI metadata annotations</li> <li><code>SecurityConfiguration</code> has been modified to allow access to the <code>/v3/api-docs</code>(<code>.yaml</code>) and <code>/swagger-ui</code> endpoints.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>OpenAPI 3 compliant API documentation<ul> <li>RAML documentation</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-corporate-memory-control-cmemc-v2006","title":"eccenca Corporate Memory Control (cmemc) v20.06","text":"<p>This version of eccenca Corporate Memory Control (cmemc) adds the following new features:</p> <ul> <li>Shipped with a MacOSX binary (installable as copy deployment or via the homebrew package manager).</li> <li>Support for base64 encoded SPARQL queries and updates (<code>--base64</code>) - this needs eccenca DataPlatform v20.06.</li> <li>Extension of the query list command to list labels and parameters of queries in the query catalog (also the <code>--id-only</code> parameter to get an ID only list).</li> <li>Support for parameterized SPARQL queries: the query execute command now has the option <code>-p</code> / <code>--parameter</code> to provide parameter/value pairs for the query execution.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>query execute<ul> <li>default result format is now <code>text/csv</code> for tables and text/turtle for graphs (was <code>*</code> before)</li> <li>Migration Note: use <code>--accept</code> in case you need the old defaults</li> </ul> </li> <li>query list<ul> <li>now has a tabularized output</li> <li>Migration Note: use <code>--id-only</code> option to get the old URI only list</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved:</p> <ul> <li>cmemc now supports all SPARQL query types when executing a query from a file (there were errors on <code>ASK</code>, <code>DESCRIBE</code> and <code>CONSTRUCT</code> before)</li> <li>all tab completion results are now correctly sorted as well as filtered case-insensitively</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-corporate-memory-powerbi-connector-v2006","title":"eccenca Corporate Memory PowerBI Connector (v20.06)","text":"<p>This is the first release of our PowerBI Connector which enables PowerBI users to retrieve data into PowerBI, based on collected queries in the Corporate Memory Query Catalog.</p> <p>The feature of this release are:</p> <ul> <li>add and delete eccenca Corporate Memory data sources</li> <li>get data out of SELECT queries from the Query Catalog</li> </ul> <p>We provided a tutorial for this new component: Consuming Graphs in Power BI</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#dataintegration","title":"DataIntegration","text":"<p>With v20.06 the API has been improved:</p> <ul> <li>The JSON format of transform, linking and workflow tasks has changed and is now consistent with dataset and custom tasks, i.e. all config parameters are now under property <code>parameters</code>.</li> <li>The JSON format of transform tasks has in addition following changes:<ul> <li><code>output</code> instead of <code>outputs</code> property that is a single string value instead of an array of strings.</li> <li><code>mappingRule</code> instead of <code>root</code> for the property of the mapping rule.</li> </ul> </li> <li>The JSON format of linking tasks had in addition following change:<ul> <li><code>output</code> instead of <code>outputs</code> property that is a single string value instead of an array of strings.</li> </ul> </li> <li>The JSON format for the <code>/plugins</code> endpoint has changed. The <code>type</code> attribute is now correctly specifying the JSON schema data type, e.g. <code>string</code> or <code>object</code>.</li> <li>The actual, more specific, parameter type has been renamed to <code>parameterType</code>.</li> <li>JSON format of resources have no relative and absolute path anymore.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#datamanager","title":"DataManager","text":"<p>With v20.06 a new title helper configuration section is introduced. It is used to define you language preferences in the data:</p> DataManager application.yml BUILD module configuration<pre><code>js.config.titleHelper:\n  languages:\n    - en\n    - ''\n</code></pre> <p>In addition to that, we introduced a new Access Condition Action which represents the right to use the EXPLORE tab. Please have a look at the access condition documentation and add <code>urn:eccenca:ExploreUserInterface</code> to already existing conditions if needed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#cmemc","title":"cmemc","text":"<p>With v20.06 the following changed need to be made:</p> <ul> <li><code>query execute</code> default result format has changed, to keep the previous behavior change your cmemc <code>query execute</code> calls to:<ul> <li><code>cmemc query execute --accept '*'</code></li> </ul> </li> <li><code>query list</code> has a different default output, to return to the previous behavior change your cmemc <code>query list</code> calls to:<ul> <li><code>cmemc query list --id-only</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/","title":"Corporate Memory 20.10","text":"<p>Corporate Memory 20.10 is the third release in 2020.</p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Release of the new DataIntegration workspace.</li> <li>Support for statement annotations, in order to express knowledge about specific statements.</li> <li>Support for tracking change sets for all shape based editing activities.</li> <li>Support for automation of vocabulary and dataset management with cmemc.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc has a change default behaviour.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.10.1</li> <li>eccenca DataIntegration v20.10</li> <li>eccenca DataManager v20.10.1</li> <li>eccenca Corporate Memory Control (cmemc) v20.10</li> <li>eccenca Corporate Memory PowerBI Connector v20.10</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-dataintegration-v20101","title":"eccenca DataIntegration v20.10.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Improvements to new Workspace UI:<ul> <li>New Workspace UI allows to export projects with and without file resources.</li> <li>Basic support for multiple languages in the New Workspace UI. Initially English and German are supported and plugins are not translated yet.</li> <li>Multi-step Project import in new workspace UI.</li> <li>Multi-step, asynchronous project import REST API.</li> <li>Profiling UI component to start dataset profiling and show profiling information in the dataset preview.</li> <li>Navigation menu in new workspace UI.</li> <li>In link tables, clicking on an entity redirects to the corresponding resource in DataManager, if the entity is coming from an RDF dataset.</li> </ul> </li> <li>New/improved operators:<ul> <li>New transform operator to retrieve lat/long of a location from a specified API in order to normalize location data.</li> <li>New operator to scale similarity values in linking rules by a specified factor.</li> <li>Email operator improvements:<ul> <li>multiple recipients in TO, CC and BCC</li> <li>CC and BCC recipients</li> <li>Timeout parameter</li> <li>SSL support</li> </ul> </li> </ul> </li> <li>Improvements to datasets<ul> <li>CSV Dataset supports UTF-8-BOM encoding for writing CSVs that open correctly in Excel.</li> <li>Support for #id and #text paths in JSON sources.</li> </ul> </li> <li>API improvements<ul> <li>Task activities API that allows to fetch a list of task activities with optional project and status filter.</li> <li>Profiling data is available via the API.</li> </ul> </li> <li>Global vocabulary cache that holds all installed vocabularies from the DataPlatform.<ul> <li>REST endpoint to trigger cache updates.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Vocabulary caches are not persisted between reboots and workspace reloads</li> <li>Disable geo location data type detector by default via plugin.blacklist parameter</li> <li>Item search API returns plugin IDs where available</li> <li>Expose some Amazon S3 client configuration. Can be changed in the Dataintegration configuration now</li> <li>Improvements to Spark execution engine<ul> <li>Entities are stored in DataFrames instead of RDDs</li> <li>Performance improvements</li> <li>Bugfixes</li> </ul> </li> <li>Check for usages of resources in all tasks, before deleting them. This was checked only in datasets before</li> <li>File management improvements<ul> <li>Allow multi file uploads</li> <li>Ask to replace existing files</li> <li>Allow to delete uploaded files in upload dialog</li> <li>When deleting files check for usages of resources in all items, before deleting them, e.g. transform tasks. This was checked only in datasets before</li> <li>When deleting files that are in use, link the dependent items</li> <li>Upload modal does not close when clicking outside of the modal</li> <li>If the limit parameter of the itemSearch API is set to 0, it will now return all search results instead of none</li> <li>Frontend initialisation endpoint returns initial language preference and configured DM base URL</li> </ul> </li> </ul> <p>Finally, the following performance and stability issues were solved:</p> <ul> <li>Regression: the output of a transformation is lost after reloading</li> <li>Added warning to the CSV datasets \u2018maxCharsPerColumn\u2019 parameter to make it clear that it affects the heap size</li> <li>Fixed reading of JSON files that contain Unicode byte order marks (BOMs)</li> <li>Workflow not interrupted on invalid XML from Triple-store</li> <li>Fixed generating paths for JSON files that contain keys with special characters, such as spaces. Those will be encoded now</li> <li>Project\u2019s rdfs:label uses project ID instead of label</li> <li>Generate consistent URIs for object mappings on JSON files</li> <li>Caches have not been written if the XML workspace provider was used</li> <li>Do not recreate caches on every run</li> <li>In link tables, the header shows the task labels instead of the task ids</li> <li>Fixed search field in link tables (did not work with characters that need to be URL encoded)</li> <li>Meta data description does not maintain whitespace formatting in XML serialisation</li> <li>New workspace UI has invalid favicon</li> <li>Creating a new project with description does not store the description in the new workspace UI</li> <li>XML Dataset: Values that include HTML entities are not retrieved</li> <li>Support for MS Internet Explorer 11 in new workspace</li> <li>Logout action not working. Should perform a global logout</li> <li>Deleting S3 backed resources broken due to a slash added to filenames</li> <li>Update PostgreSQL driver to v42.2.14 because of security vulnerability</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-datamanager-v20101","title":"eccenca DataManager v20.10.1","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General<ul> <li>Add translations and i18n language selection (and ship english and german translations)</li> <li>Allow for Annotation of Statements with additional meta data</li> <li>Integrate with the new DataIntegration workspace (Data Integration Tab)</li> </ul> </li> <li>Shacline<ul> <li>Add support for \u2018sh:languageIn\u2019 (as multiple values) in literal properties</li> </ul> </li> <li>Resource Tables<ul> <li>Allow Lucene syntax in the search field of any resource table (Query Syntax)<ul> <li>This search will be applied to the label(s) configured in <code>proxy.labelProperties</code>; by default the search will only be applied to the first column, the labels of the selected resource</li> </ul> </li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Shacline<ul> <li>Use the new resource/shaped API to generate / save shacl forms.</li> <li>Rendering empty fields on every change</li> <li>Add class triple to save only if class is a string.</li> <li>Prevent labels to be cloned on adding a new block.</li> <li>Nested Table query now defines default graph</li> <li><code>{graph}</code> can now be used as a placeholder in RFC6570 URI Template string</li> </ul> </li> <li>ResourceTable<ul> <li>Download data does not retain column order</li> <li>Add pagination/limit on config file</li> <li>Lock Drag and Drop while adding columns to prevent collision</li> <li>Update default pagination limit to 25 and default pagination interval to 5, 10, 25, 100, 500, 1000</li> </ul> </li> <li>General<ul> <li>use new backend API to retrieve labels.</li> <li>use new backend API to retrieve facets (possible columns)</li> <li>DEPRECATE titleHelper configuration parameters</li> <li>BREAKING remove support for Internet Explorer 11.</li> <li>Disable Datasets module, moved to Data Integration</li> <li>Disable Build module, moved to Data Integration</li> </ul> </li> <li>ResourceSelect<ul> <li>Wait until click on it to load values.</li> </ul> </li> <li>Explore<ul> <li>Cyclic references on Tabs content crash the app</li> <li>modules.explore.navigation.topQuery changed in order to list configured graph classes (<code>shui:managedClasses</code>)</li> <li>Update Navigation pagination limit to 15</li> <li>Load ResourceTable pagination limit from config file</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-dataplatform-v2010","title":"eccenca DataPlatform v20.10","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>Custom endpoint<ul> <li>Create custom json endpoints by defining a query for retrieving the data and a template for transforming the result.</li> </ul> </li> <li>Concise Boundary Description retrieval depth is adjustable.</li> <li>New submodule <code>:src:it</code> for integration tests</li> <li>Statement Annotations/Metadata<ul> <li>APIs for providing access and managing existing relations</li> </ul> </li> <li>Additional APIs<ul> <li>Explore Facets (<code>/api/explore/facets</code>): Lists the properties of a class or query.</li> <li>Graph List (<code>/api/graphs/list</code>): Returns a list of graphs readable by the current user, optionally including OWL imports.</li> <li>Graph List Detailed (<code>/api/graphs/list-detailed</code>): Like the previous one, but adding details of triples, classes and instances counts.</li> <li>Added openapi.server.urls env variable in order to define custom baseUrl to be used in</li> <li>Added resource shaping to the backend, this includes<ul> <li>Resource (<code>/api/resources</code>) api for getting information about individual resources</li> <li>Shape (<code>/api/shapes</code>) api for applying shape information onto the graph</li> <li>Statement Level Metadata (<code>/api/statementmetadata/</code>) management for adding statement annotations.</li> </ul> </li> <li>Added Caching to internal handling of prefixes, vocabularies and shapes lists. Caches are invalidated by updates.</li> <li>Added Showcase (<code>/api/admin/showcase</code>) endpoint, which inserts a scalable test dataset into the configured endpoint.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-corporate-memory-control-cmemc-v2010","title":"eccenca Corporate Memory Control (cmemc) v20.10","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>A <code>dataset</code> command group, enabling users to <code>create</code>, <code>delete</code> and <code>update</code> datasets as well as <code>upload</code> and <code>download</code> dataset file resources.</li> <li>A <code>vocabulary</code> command group, enabling users to manage vocabularies similar to the vocabulary catalog.</li> <li>The <code>query execute</code> command has some new options for limit, offset distinct and timeout settings.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Added:<ul> <li>The <code>workflow status</code> command has a <code>--project</code> option</li> </ul> </li> <li>Changed:<ul> <li>The <code>graph import</code> command outputs a replace/add status message per graph.</li> <li>Much faster <code>workflow status</code> retrieval by using a new activity API</li> <li>The <code>dataset export</code> command default file template changed to <code>{{date}}-{{connection}}-{{id}}.project</code></li> <li>The <code>query execute</code> command now uses POST instead of GET requests for SPARQL queries</li> </ul> </li> <li>Fixed:<ul> <li>The <code>graph import --replace</code> command  does not re-replace a same graph with a different file anymore.</li> <li>The completion of <code>--filename-template</code> resulted in files with wrong chars.</li> <li>The python version is disabled in completion mode.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-corporate-memory-powerbi-connector-v2010","title":"eccenca Corporate Memory PowerBI Connector (v20.10)","text":"<p>This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#dataintegration","title":"DataIntegration","text":"<ul> <li>XML serialization for meta data elements is not forward compatible, i.e. projects exported with this version cannot be imported in older DataIntegration versions.</li> <li>The logout URL needs to be set to make sure that DataIntegration also triggers a logout inside the Keycloak instance:     <pre><code>oauth.logoutRedirectUrl = ${DEPLOY_BASE_URL}\"/auth/realms/cmem/protocol/openid-connect/logout?redirect_uri=\"${DEPLOY_BASE_URL}\n</code></pre></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#datamanager","title":"DataManager","text":"<ul> <li>The <code>graphInfo</code> flag in the explore module is now enabled by default.</li> <li>Due to the introduction of the new DataIntegration workspace these changes need to be applied:<ul> <li>The modules <code>build</code> as well as <code>datasets</code> are disabled now by default.</li> <li>The module <code>explore</code> is the default first entry point (<code>startsWith</code>).</li> <li>This section needs to be added to each workspace configuration: <code>yaml     DIWorkspace:       enable: true       url: /dataintegration/workbench</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#cmemc","title":"cmemc","text":"<ul> <li>If your automation scripts rely on the created file name of the project export command, you need to change your scripts and set the old export name explicitly with <code>-t {{id}}</code>.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/","title":"Corporate Memory 20.12","text":"<p>Corporate Memory 20.12 is the fourth release in 2020.</p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: With the integration of all main views of the Data Integration build workbench, building Knowledge Graphs was never so smooth and streamlined.</li> <li>Automate: With cmemc\u2019s workflow io command, execution of workflows with variable file payload (input) as well as receiving data from a workflows (output) was never so easy before. Please refer to the corresponding tutorial.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc deprecates a command which will be removed in the next release.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.12</li> <li>eccenca DataIntegration v20.12</li> <li>eccenca DataManager v20.12</li> <li>eccenca Corporate Memory Control (cmemc) v20.12</li> <li>eccenca Corporate Memory PowerBI Connector v20.12</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-dataintegration-v2012","title":"eccenca DataIntegration v20.12","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Improvements to new Workspace UI<ul> <li>Existing views (e.g. editors and reporting interfaces) are now integrated.<ul> <li>Knowledge graph datasets show an embedded query and explore view.</li> </ul> </li> <li>Quick search dialog: The hotkey / allows to quickly switch between projects and tasks.</li> </ul> </li> <li>Optimized DataPlatform entity retrieval strategy for stability and performance.</li> <li>RDF workspace backend does write additional convenience properties for transform tasks and linking tasks<ul> <li><code>di:usedSourceClass</code>: lists all classes for which entities are read by this task.</li> <li><code>di:usedSourceProperty</code>: lists all properties that are read by this task.</li> <li><code>di:usedTargetClass</code>: lists all classes for which entities are written by this task.</li> <li><code>di:usedTargetProperty</code>: lists all properties that are written by this task.</li> </ul> </li> <li>Project resource download button.</li> <li>Persisted execution reports:<ul> <li>Added a configurable report manager that persists execution reports and allows to retrieve previous reports.</li> <li>Configurable retention time. Reports older than that will be deleted.</li> </ul> </li> <li>Error output for transformations:<ul> <li>Transformations have a new parameter \u201cerror output\u201d. When executing the workflow, all erroneous entities together with the error description are written to the configured dataset.</li> </ul> </li> <li>Add JSON sink</li> <li>Simple variable workflow execution REST endpoint<ul> <li>Allows to execute simple variables workflow (at most one variable input and/or output dataset)</li> <li>Input is provided via query parameters or directly in the request body</li> <li>Output is defined by the <code>ACCEPT</code> header and is output in the corresponding MIME type in the response body</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>ExcelMap operator:<ul> <li>If there are multiple values for a given key, it now returns all values instead of just the last one</li> </ul> </li> <li>Project file multi-upload:<ul> <li>Upload one file after the other instead of all at once</li> </ul> </li> <li>The configuration parameter <code>workbench.showHeader</code> is no longer supported. Instead, a URL parameter is used to decide whether the header should be shown.</li> <li>Data preview for XML, CSV and JSON now automatically loaded and shown on the dataset details page.</li> <li>After deleting an item the user is now redirected to the project page (previously to the workbench page).</li> <li>In addition to that, multiple performance and stability issues were solved.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-datamanager-v2012","title":"eccenca DataManager v20.12","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General:<ul> <li>Support for datatype <code>rdf:HTML</code> in object view.</li> </ul> </li> <li>Data Integration:<ul> <li>Add config parameter <code>DIWorkspace.baseURL</code> for logout.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped</p> <ul> <li>General:<ul> <li>Add error message if browser is not compatible: Safari, IE.</li> <li>Override default config language with workspace language if user did not select a language before.</li> </ul> </li> <li>Shacline:<ul> <li>Redirect to new resource after use the clone resource feature.</li> <li>Load graph permissions directly from the data request.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-dataplatform-v2012","title":"eccenca DataPlatform v20.12","text":"<p>This version of eccenca DataPlatform ships the following changes:</p> <ul> <li>Spring libraries:<ul> <li>Spring Boot version upgraded to <code>2.2.10.RELEASE</code>, Spring Cloud version upgraded to <code>Hoxton.RELEASE</code>.</li> </ul> </li> <li>Bootstrap &amp; Vocabularies:<ul> <li>cmem ontologies and graphs can now be updated using the bootstrap endpoint.</li> </ul> </li> <li>Statement Annotation:<ul> <li>additional endpoints for browsing and bulk editing</li> </ul> </li> <li>Graph List:<ul> <li>enriched endpoint with additional information</li> </ul> </li> <li>Label Resolution:<ul> <li>per default, in case no language matches, the precedence is ignored an any one value of the defined properties is taken as fallback.</li> <li>in case of multiple matches, the alphabetically first entry is chosen.</li> </ul> </li> <li>Localization / i18n (20.10.1):<ul> <li>language selection in titlehelper, shapes and facets API</li> <li>uses <code>shui:languageIn</code> instead of <code>sh:languageIn</code></li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-corporate-memory-control-cmemc-v2012","title":"eccenca Corporate Memory Control (cmemc) v20.12","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>The <code>workflow io</code> command was added to allow for executing workflows with variable file payload (input) and receive data from a workflow (output).<ul> <li>This feature is described in the advanced tutorial Processing data with variable input workflows.</li> <li>In addition to that, the concepts of io workflows is described in Workflow execution and orchestration.</li> </ul> </li> <li>The <code>admin</code> command group was added and includes the following commands:<ul> <li><code>bootstrap</code> - Update/Import bootstrap data.</li> <li><code>showcase</code> - Create showcase data.</li> <li><code>status</code> - Output health and version information.</li> </ul> </li> <li>The <code>template</code> option of the <code>graph export</code> command allows for using the <code>{{iriname}}</code> placeholder now.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>The <code>config check</code> command outputs a deprecation warning now (use the <code>admin status</code> command instead).</li> <li>cmemc now sends a User-Agent header with every call, currently: <code>cmemc/20.12 (Python 3.7.7)</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-corporate-memory-powerbi-connector-v2012","title":"eccenca Corporate Memory PowerBI Connector v20.12","text":"<p>This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#dataintegration","title":"DataIntegration","text":"<ul> <li>No Forward Compatibility of Dataintegration Projects Exports<ul> <li>Due to a change in the internal XML serialization DI projects from this version can not be imported into instances running older version of DataIntegration.</li> <li>Please try the following workaround if this is something you need to perform. Contact our support team in case this procedure does not work in your case:</li> </ul> </li> </ul> <p>Info</p> <ol> <li>download your project resources (if needed)</li> <li>export the project using cmemc:<ul> <li>cmemc project export \u2013type rdfTurtle  <li>import the project at the back-level instance using cmemc:<ul> <li>cmemc project import .project.ttl  <li>upload your file resources to the back-level instance (if needed)</li> <ul> <li>Configuration<ul> <li>Remove the following configuration parameter: <code>workbench.showHeader</code></li> <li>In order to enable the new Execution Report Manager you need to configure the respective plugin, see \u201cExecution Report Manager\u201d in DataIntegration for Details.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#datamanager","title":"DataManager","text":"<ul> <li>In your workspaces configuration add <code>DIWorkspace.baseUrl</code> (mostly this will be <code>\"/dataintegration\"</code>): <pre><code>js.config.workspaces:\n  default:\n    ...\n    DIWorkspace:\n      ...\n      baseUrl: /dataintegration\n</code></pre></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#cmemc","title":"cmemc","text":"<ul> <li>The <code>config check</code> command has been deprecated, please use the <code>admin status</code> command instead.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/","title":"Corporate Memory 21.02","text":"<p>Corporate Memory 21.02 is the first release in 2021.</p> <p></p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: Our re-designed mapping suggestion wizard make bootstrapping of your transformations a quick and easy exercise.</li> <li>Explore: interactively and visually browse through your graph with our integrated GRAPH visualization.</li> <li>Automate: The command line has never been more colorful, enjoyable and helpful (guessing your wishes as typos might happen). Experience the UX centric redesign of our command line client cmemc.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration, DataManager, DataPlatform configuration and cmemc command behavior has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.02.1</li> <li>eccenca DataIntegration v21.02.1</li> <li>eccenca DataManager v21.02</li> <li>eccenca Corporate Memory Control (cmemc) v21.02</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-dataintegration-v21021","title":"eccenca DataIntegration v21.02.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>REST request operator:<ul> <li>added \u201cAccept all SSL certificates\u201d parameter in advanced section. Default: <code>false</code>.</li> </ul> </li> <li>If the option <code>eccencaDataPlatform.writeGraphType</code> is set to true the a graph in CMEM generated by Dataintegration will contain a triple (<code>&lt;graph&gt; rdf:type di:Dataset</code>) indicating that this graph was written by DataIntegration.</li> <li>Improved mapping suggestion:<ul> <li>Allows to select from multiple matching candidates.</li> <li>The user can switch between data source and target vocabulary view.</li> <li>A sub-set of the vocabularies can be selected.</li> <li>The user can pick any (non-matched) property from the target vocabularies via search.</li> <li>Improved filtering and sorting.</li> <li>Added multi word text search filter.</li> <li>Allows to choose am existing or custom URI prefix for the auto-generated target properties.</li> <li>Added tooltip for source paths with e.g. example values.</li> <li>Added tooltip for target property selection with meta data and a link to DataManager for that property resource.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>The \u201cTimestamp to date\u201d and \u201cDate to timestamp\u201d transformers support configurable time units and full <code>xsd:dateTime</code> values.</li> <li>Updated build to Spark 2.4, Scala 2.12 and sbt 1.x</li> <li>If a transformer inside a linkage rule throws a validation error, it will no longer fail the entire linking task, but will not generate a value for that transformer.</li> <li>In the linking evaluation view and the reference links view, validation errors are shown in the evaluation tree.</li> <li>Allow to persist caches between restarts in order to reduce application start-up time.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-datamanager-v2102","title":"eccenca DataManager v21.02","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Explore<ul> <li>Add a tab with the Ontodia tool to explore graph detail view.</li> <li>Make the Ontodia tab configurable: js.modules.explore.details.ontodia.enable</li> </ul> </li> <li>Shacline<ul> <li>Display rdfs:comment of the selected node shape in the shacl form view.</li> <li>Add group:comment to the header of groups.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Explore<ul> <li>Remove js.modules.explore.graphlist.internalGraphs from DataManager config since DataPlatform is now providing this information.</li> <li>Rename tab: ontodia (Data@en, Daten@de).</li> <li>Rename tab: visualization (Vocab@en, Vocab@de).</li> <li>Move ontodia tab next to visualization tab.</li> </ul> </li> <li>Vocabulary<ul> <li>Fix format to new endpoint with Titles already loaded.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-dataplatform-v21021","title":"eccenca DataPlatform v21.02.1","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>GraphDB datastore implementation<ul> <li>Added <code>spring.servlet.multipart.max-file-size: \"20GB\"</code> in properties files and dist file.</li> <li>Added <code>useDirectTransfer</code> to use native Graph Store operation without shared folder</li> </ul> </li> <li>Admin Endpoint for listing currently running queries.</li> <li>Graph API<ul> <li>New endpoints for owl:imports resolution.</li> </ul> </li> <li>Bootstrap Data<ul> <li>Shapes for managing prefix declarations</li> <li>Shape catalogs have prefix declarations as managed classes.</li> </ul> </li> <li>Imports resolution<ul> <li><code>owl:imports</code> self references ignored.</li> <li><code>owl:imports</code> are now resolved for graph uris in request parameters.</li> </ul> </li> <li>Vocabulary List<ul> <li>Title resolve according to standard i18n settings.</li> </ul> </li> <li>Shapes Endpoint<ul> <li>Comments of shapes included.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Dist Config<ul> <li><code>skos:prefLabel</code> now preferred over <code>rdfs:label</code>.</li> </ul> </li> <li>Query Logging<ul> <li>Query logging uses console appender only.</li> <li>Query logging level set to <code>DEBUG</code>.</li> <li>Environment variables no longer needed:<ul> <li><code>QUERY_LOGGING_DIR</code></li> <li><code>QUERY_LOGGING_MAX_FILE_SIZE</code></li> <li><code>QUERY_LOGGING_TOTAL_SIZE_CAP</code></li> <li><code>QUERY_LOGGING_MAX_HISTORY</code></li> </ul> </li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-corporate-memory-control-cmemc-v2102","title":"eccenca Corporate Memory Control (cmemc) v21.02","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>New configuration option <code>OAUTH_GRANT_TYPE=prefetched_token</code></li> <li>New <code>admin token</code> command, fetch and output an access token</li> <li>New <code>project open</code> command, open projects in the browser</li> <li><code>graph list</code> command<ul> <li>Added table output with graph type and label.</li> <li><code>--id-only</code> option added: get only graph IRIs.</li> </ul> </li> <li><code>project list</code> command<ul> <li>Added table output with project ID and label.</li> <li><code>--id-only</code> option added: get only project IDs.</li> <li><code>--raw</code> option added: get raw JSON.</li> </ul> </li> <li><code>workflow list</code> command<ul> <li>Added table output with workflow ID and label.</li> <li><code>--id-only</code> option added: get only graph IRIs.</li> <li><code>--raw</code> option added: get raw JSON.</li> </ul> </li> <li><code>graph list</code> command<ul> <li><code>--filter imported-by IRI</code> added: filter to all graphs imported recursively by a graph.</li> </ul> </li> <li><code>graph tree</code> command added, output <code>owl:imports</code> tree for each selected graph.</li> <li><code>graph export</code> and <code>graph delete</code> command<ul> <li><code>--include-imports</code> option added: work with selected graph(s) and all graphs which are imported from the selected graph(s).</li> </ul> </li> <li><code>workflow list</code> command<ul> <li><code>--filter</code> option added: filter by project or io command capability (input, output, both, any).</li> </ul> </li> <li><code>graph export</code> command<ul> <li><code>--create-catalog</code> added: create a Protege XML catalog for import resolution.</li> </ul> </li> <li><code>query status</code> command, list and view still running and executed queries.</li> <li>output coloring<ul> <li>json output is highlighted</li> <li>help texts are colored (red terms indicate writing commands, possible dangerous to your data)</li> <li>table headers are colored</li> </ul> </li> <li>git-like did-you-mean command suggestion for misspelled commands</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>docker image: now based on <code>debian:stable-20201209-slim</code>.</li> <li><code>graph list</code> command<ul> <li>default output changed.</li> <li>use the <code>--id-only</code> or <code>CMEMC_GRAPH_LIST_ID_ONLY=true</code> to get the IRI list.</li> </ul> </li> <li><code>project list</code> command<ul> <li>default output changed.</li> <li>use the <code>--id-only</code> or <code>CMEMC_PROJECT_LIST_ID_ONLY=true</code> to get the ID list.</li> </ul> </li> <li><code>workflow list</code> command<ul> <li>default output changed.</li> <li>use the <code>--id-only</code> or <code>CMEMC_WORKFLOW_LIST_ID_ONLY=true</code> to get the ID list.</li> </ul> </li> <li><code>graph list</code> command<ul> <li>filter option changed.</li> <li>use <code>--filter access readonly</code>|<code>writeable</code> instead of <code>--filter readonly</code>|<code>writeable</code></li> </ul> </li> <li><code>workflow open</code> command<ul> <li>URL changed for new workbench</li> </ul> </li> <li><code>admin status</code> command, now warns you if cmemc is too new for the current backend</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#dataintegration","title":"DataIntegration","text":"<ul> <li>Timestamp to date operator changed default behavior<ul> <li>The \u201cTimestamp to date\u201d now assumes milliseconds instead of seconds by default. In addition, it generates full xsd:dateTime values instead of simple dates.<ul> <li>To makes sure that existing usages don\u2019t break, please open {DataIntegration}/api/core/usages/plugins/timeToDate and check all usages.</li> <li>In order to revert to the previous behavior, the following changes have to be made to each usage:<ul> <li>Change the unit to \u201cseconds\u201d.</li> <li>Change the format to \u201cyyyy-MM-dd\u201d</li> </ul> </li> </ul> </li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#datamanager","title":"DataManager","text":"<ul> <li>In your application.yml the following config property can be removed, if existing. This information is now internally provided by Datalatform: <code>js.modules.explore.graphlist.internalGraphs</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#dataplatform","title":"DataPlatform","text":"<ul> <li>in case you used Query Logging of DataPlatform you can remove the following environment variables as they are no longer needed: <code>QUERY_LOGGING_DIR</code>, <code>QUERY_LOGGING_MAX_FILE_SIZE</code>, <code>QUERY_LOGGING_TOTAL_SIZE_CAP</code>, <code>QUERY_LOGGING_MAX_HISTORY</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#cmemc","title":"cmemc","text":"<ul> <li><code>workspace import</code>|<code>export</code>|<code>reload</code> commands are deprecated now<ul> <li>use <code>admin workspace import</code>|<code>export</code>|<code>reload</code> commands instead</li> </ul> </li> <li>Many commands have new default output:<ul> <li><code>graph list</code>  command, use the <code>--id-only</code> or <code>CMEMC_GRAPH_LIST_ID_ONLY=true</code> to get the IRI list.</li> <li><code>project list</code> command, use the <code>--id-only</code> or <code>CMEMC_PROJECT_LIST_ID_ONLY=true</code> to get the ID list.</li> <li><code>workflow list</code> command, use the <code>--id-only</code> or <code>CMEMC_WORKFLOW_LIST_ID_ONLY=true</code> to get the ID list.</li> <li><code>graph list</code> command, use <code>--filter access readonly</code>|<code>writeable</code> instead of <code>--filter</code> <code>readonly</code>|<code>writeable</code></li> </ul> </li> <li>The command config check  was removed (was deprecated in v20.12)</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/","title":"Corporate Memory 21.04","text":"<p>Corporate Memory 21.04 is the second release in 2021.</p> <p></p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: The mapping editor now allows for auto-completion of paths on any level in multi-hop paths, including source type specific paths with special semantics, e.g. <code>#idx</code> for CSV datasets. This feature lowers the barrier for new Corporate Memory users and allows for much master mapping creation.</li> <li>Explore: Manual authoring of resources via SHACL-shape based customized user interfaces is now supported with client-side datatype validation (in addition to store-based validation). This feature provides instant user feedback while typing Literals and therefor allows faster data entry.</li> <li>Automate: The new vocabulary import command of cmemc adds a turtle file as a vocabulary to Corporate Memory (upload and create catalog entry). This allows for automation of CI/CD pipeline which depend on vocabularies managed in a Git Repository.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataManager configuration as well as cmemc command behavior has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.04</li> <li>eccenca DataIntegration v21.04</li> <li>eccenca DataManager v21.04</li> <li>eccenca Corporate Memory Control (cmemc) v21.04</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-dataintegration-v2104","title":"eccenca DataIntegration v21.04","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>A new mapping parameter enables the user to specify whether single or multiple values are written by a particular mapping:<ul> <li>Supported by both value and object mappings.</li> <li>Replaces the \u201cis Attribute\u201d parameter on value mappings, which has been specific to XML.</li> <li>Generates a validation error if multiple values are written if single values are configured.</li> <li>Datasets may adapt the written schema based on the chosen option (see help text in mapping editor).</li> </ul> </li> <li>Improved auto-completion value path mapping field:<ul> <li>Allows auto-completion of paths on any level in multi-hop paths.</li> <li>Supports auto-completion of properties inside of property filters.</li> <li>Proposes data source specific paths with special semantics, e.g. <code>#idx</code> for CSV.</li> <li>Validates the syntax of the value path and highlights errors to the user.</li> </ul> </li> <li>Tasks can be copied between projects.</li> <li>A new transform task parameter (<code>abortIfErrorsOccur</code>) specifies whether the execution will fail if a validation error occurs.</li> <li>Added URI literal type for writing <code>xsd:anyURI</code> values to Knowledge Graphs.</li> <li>Added option for \u201cRegex extract\u201d transformer for extracting all matches.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Support editing of auto-completed values in mapping editor.</li> <li>Mapping editor auto-complete element:<ul> <li>Support editing of auto-completed values.</li> <li>added support for multi-word search highlighting.</li> <li>Show label and value of the selected value, e.g. label and URI for target property.</li> </ul> </li> <li>Improvements to execution reports<ul> <li>Added an overview section, which displays general information about the execution:<ul> <li>Final workflow status</li> <li>Start, finish and cancellation times</li> <li>Users account which started and canceled (if any) the execution</li> </ul> </li> <li>All operator executions are persisted. For instance, workflow reports will contain separate task reports for writing and reading a dataset.</li> <li>For each operator execution, an optional operation label can be shown (e.g., read, write, generate queries).</li> <li>The order of the task reports is stable now and reflects the order of execution.</li> <li>Added a scrollbar to the task list, if it is too large to be displayed.</li> </ul> </li> <li>Improved JSON writing support:<ul> <li>A rewritten implementation supports arbitrarily large JSON files by using a memory-mapped key-value store.</li> <li>An optional template may be specified to customize the written JSON.</li> <li>Values are only wrapped in JSON arrays if the multiple value option is set in the mapping.</li> </ul> </li> <li>Transform Evaluation now shows evaluation of the URI rule.</li> <li>Improved workflow saving:<ul> <li>On loading, the save button is only enabled after the workflow has been loaded to prevent the user from saving an empty workflow.</li> <li>A spinner is shown while the workflow is saved.</li> <li>The save button is disabled while the workflow is saved.</li> <li>A confirm dialog is shown, if the user tries to leave while a save is in progress.</li> </ul> </li> <li>The behavior of linking rules in case of missing values has been improved:<ul> <li>The <code>required</code> attribute has been removed, because it has lead to unexpected and sometimes inconsistent behavior.</li> <li>Boolean aggregators have been reworked to interpret missing values as false.</li> <li>The \u201cHandle missing values\u201d aggregator has been added to handle cases in which missing values should default to a user specified score (For instance, if missing value should be interpreted as true).</li> <li>The \u201cDefault value\u201d transformer has been added to generate default values for missing values.</li> </ul> </li> <li>Improved inline documentation using markdown rather than text description.</li> <li>Workflow progressbars now show the task labels instead of their internal identifiers.</li> <li>Page header contents are now created directly in the artifact view templates instead being an independent component pulling all information via holistic approach.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-datamanager-v2104","title":"eccenca DataManager v21.04","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Shacl<ul> <li>Add datatype validation for all supported datatypes</li> </ul> </li> <li>Configuration<ul> <li>Add a configurable link to account settings in keycloak:<ul> <li><code>js.config.modules.accountSettings.enable: true</code></li> <li><code>js.config.modules.accountSettings.url: http://docker.local/auth/realms/cmem/account/?referrer={{REFERRER}}&amp;referrer_uri={{REFERRER_URI}}</code></li> </ul> </li> <li>General<ul> <li>Global error handling to display errors preventing most grey screens</li> </ul> </li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General         - Use redux store to manage notifications in DataManager (MessageHandler) and improve error parse / handle         - Use redux store to manage main application state.         - Change value of <code>js.config.modules.explore.overallSearchQuery</code> and <code>js.config.modules.explore.navigation.searchQuery</code> to use the <code>\"\"\"\"</code> SPARQL string separator.         -   BREAK please use <code>\"\"\"</code> if you use custom queries for that values</li> <li>Development<ul> <li>Switch to GUI elements repository from Github</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-dataplatform-v2104","title":"eccenca DataPlatform v21.04","text":"<p>These followin changes are shipped:</p> <ul> <li>General<ul> <li>Virtuoso is now using the custom build-in function to list graphs faster.</li> </ul> </li> <li>Bootstrap Data<ul> <li>ucum removed as default vocab in the vocabulary catatog</li> <li>qudt added as a default vocab in the vocabulary catalog</li> <li>all vocabularies are provided via download.eccenca.com now</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p> <ul> <li>Health Endpoint<ul> <li>race condition in the health condition that leads to rare cases of wrongly reporting DOWN</li> </ul> </li> <li>HTTP Connection Pool<ul> <li>size increased to increase parallelism and resilience</li> </ul> </li> <li>Statement-Level Metadata<ul> <li>works now on inverse properties</li> </ul> </li> <li>Graph List<ul> <li>incorrect type statements are ignored</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-corporate-memory-control-cmemc-v2102","title":"eccenca Corporate Memory Control (cmemc) v21.02","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>new <code>config get</code> command<ul> <li>get the value of certain configuration key (such as <code>DP_API_ENDPOINT</code>)</li> </ul> </li> <li>new <code>dataset open</code> command<ul> <li>similar to the other open commands, opens a dataset in the browser</li> </ul> </li> <li><code>graph export</code> command<ul> <li><code>--mime-type</code> option added to specify requested mime type</li> <li>the default mime type is still <code>application/n-triples</code></li> </ul> </li> <li>new <code>vocabular import</code> command<ul> <li>Import a turtle file as a vocabulary (upload and create catalog entry)</li> </ul> </li> <li><code>project export</code> command<ul> <li><code>--extract</code> option added in order to export projects to directories</li> <li><code>--help-types</code> option added to get a list of export formats</li> </ul> </li> <li><code>project import</code> command<ul> <li>add support for importing projects from extracted directories</li> <li>add <code>--overwrite</code> option to import files/directory to an existing project</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>default values for <code>OAUTH_USER</code> and <code>OAUTH_PASSWORD</code> is <code>None</code> except for grant type password</li> <li>docker base image forwarded to <code>debian:stable-20210408-slim</code></li> <li><code>graph export</code> command<ul> <li>does not load the result in memory anymore but stream the result to the file</li> </ul> </li> <li><code>graph import</code> command<ul> <li>does not load the payload in memory anymore but stream it to the endpoint</li> </ul> </li> <li><code>project export</code> command<ul> <li>command now fails early if a non-existing project is requested</li> </ul> </li> <li><code>project create</code> command<ul> <li>command now fails early if a project is already there</li> </ul> </li> <li><code>project delete</code> command<ul> <li>command now fails early if a non-existing project is requested</li> </ul> </li> <li><code>project import</code> command<ul> <li>now uses the new project import API</li> </ul> </li> <li>removed reference to config keys<ul> <li><code>CMEM_BASE_PROTOCOL</code> and <code>CMEM_BASE_DOMAIN</code> were never used</li> </ul> </li> <li>remove <code>workspace</code> command group from root<ul> <li>was in 21.02 deprecated</li> <li>now removed from root and available as <code>admin workspace</code> command group</li> </ul> </li> <li>fix: <code>project export</code> command<ul> <li>command now fails correctly with exit code 1 if a non-existing project is requested</li> </ul> </li> <li>fix: <code>project import</code> command<ul> <li>command now fails correctly with exit code 1 in case of an import to an existing project</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#dataintegration","title":"DataIntegration","text":"<ul> <li>The behavior of linking rules in case of missing values has been changed:<ul> <li>Now, boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d.</li> <li>Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing.</li> <li>If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#datamanager","title":"DataManager","text":"<ul> <li>To allow special characters to be search in <code>js.config.modules.explore.overallSearchQuery</code> and <code>js.config.modules.explore.navigation.searchQuery</code>, use <code>\"\"\"\"</code> instead of just <code>\"</code> to delimitate strings with search placeholders. e.g: <code>regex(str(?resource),\"{{QUERY}}\",\"i\")</code> should be written as <code>regex(str(?resource),\"\"\"{{QUERY}}\"\"\",\"i\")</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#dataplatform","title":"DataPlatform","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#cmemc","title":"cmemc","text":"<ul> <li>The exit code values of <code>project import</code> and <code>export</code> commands are fixed (in case of failure) so you may have to change these calls in your scripts.</li> <li>The deprecated <code>workspace</code> command group is now only available as <code>admin workspace</code> command group so you have to change these calls in scripts.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/","title":"Corporate Memory 21.06","text":"<p>Corporate Memory 21.06 is the third release in 2021.</p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: The Data Integration workflow editor got a complete remake based on a more flexible and better extensible drawing engine. Workflow tasks use the same icons and tags from the workspace now and are better integrated in the build user interface.</li> <li>Explore: The new Data Manager vocabulary viewer visualises classes and its relations (subclasses, domain/range relations) from an installed vocabulary.</li> <li>Automate: cmemc is now able to fetch credentials from external processes in order to integrate with company wide or personal password infrastructure.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and cmemc configuration and behaviour has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.06</li> <li>eccenca DataIntegration v21.06.1</li> <li>eccenca DataManager v21.06.3</li> <li>eccenca Corporate Memory Control (cmemc) v21.06</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#eccenca-dataintegration-v21061","title":"eccenca DataIntegration v21.06.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>New workflow editor:<ul> <li>Completely rewritten workflow editor that replaces the old editor.</li> <li>The old workflow editor can be re-enabled by setting the following configuration value: <code>workbench.tabs.legacyWorkflowEditor = true</code>.</li> <li>Added API endpoint to fetch workflow node (input) port configurations.</li> </ul> </li> <li>Enable the creation and execution of nested workflows.<ul> <li>Workflows can be nested within other workflows.</li> <li>Nested workflow reports can be viewed in the execution report.</li> <li>Already nested workflows cannot be used in other nested workflows to protect from too complex projects.</li> </ul> </li> <li>Added script transform operators<ul> <li>Python and Scala are supported as scripting languages.</li> <li>Need to be enabled in the configuration.</li> </ul> </li> <li>Template transform operator.</li> <li>Synonym-based mapping suggestion<ul> <li>Added global vocabulary synonym cache that extracts synonyms for vocabulary properties from existing mapping rules.</li> <li>Use synonyms in mapping suggestion so more properties can be suggested to the user based on existing mapping rules that map similarly named attributes/properties.</li> <li>Config parameters:</li> <li><code>mapping.suggestion.features.extractSynonymsFromExistingMappingRules.enabled</code>: Enables the synonym based mapping suggestion. Default: <code>true</code></li> <li><code>mapping.suggestion.features.extractSynonymsFromExistingMappingRules.timeBetweenRefreshes</code>: The minimum time in milliseconds between synonym cache refreshes. Default: 10 seconds</li> <li><code>mapping.suggestion.features.extractSynonymsFromExistingMappingRules.waitForCacheToFinish</code>: The max. time to wait for a new cache value during a mapping suggestion request if the current value has gotten stale. Default: 50ms</li> </ul> </li> <li>New OpenAPI based documentation of HTTP API:<ul> <li>Replaces the previous RAML-based documentation.</li> <li>Can be viewed live in the UI at <code>{DI_URL}/doc/api</code>.</li> </ul> </li> <li>New Neo4j dataset, which supports writing into Neo4j graphs and reading them back.</li> <li>Coalesce transform operator that forwards the first input that has any value/s.</li> <li>Add concrete item type, e.g. \u2018CSV\u2019 or \u2018Transform\u2019, to search result and recently viewed items and make it searchable.</li> <li>Add tooltip to search item if item description is too long to show in a single line.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Mapping suggestion improvements:<ul> <li>Support source path column filters: show only already mapped source paths, show only unmapped source paths.</li> <li>Do not show filters in from-vocabulary view that cannot be applied, e.g. show auto-generated only.</li> <li>Shortened source paths are shown as tooltip in full length on hover.</li> <li>Show source path type (data type or object) in source info box.</li> <li>For object source paths show their direct sub-paths.</li> <li>Improve error reporting in mapping suggestion and mapping rule example view.</li> </ul> </li> <li>Prefix management improvements:<ul> <li>Validate prefix name and value in the UI</li> <li>Ask before updating existing prefix names. Also change button to \u2018Update\u2019 when prefix name matches an existing prefix.</li> </ul> </li> <li>Allow object rule mappings with empty target property and non-empty source path in order to change the source resource, but stay on the target resource.</li> <li>The configuration of plugins (in particular the blacklist) has been improved.<ul> <li>Plugin configuration has been grouped under a common root</li> <li>Plugins can be enabled/disabled individually</li> <li>See breaking changes for details</li> </ul> </li> <li>E-mail operator extensions and improvements:<ul> <li>Allow to send multiple e-mails with different configurations (from, to, subject, content, cc, bcc).</li> <li>Add e-mail execution report</li> <li>Add retry mechanism</li> </ul> </li> <li>XML dataset (streaming mode):<ul> <li>Allow property filters on attributes in object paths</li> </ul> </li> <li>The RDF file dataset now autocompletes formats.</li> <li>Centralized error handling.</li> <li>Upgrade to Play 2.8.</li> <li>(21.06.1) Added a retry mechanism if connections on S3 are interrupted (CMEM-3675)<ul> <li>Per default, at most 10 retries are attempted.</li> <li>Number of retries can be changed by setting the configuration parameter <code>retryCount</code> (available on <code>workspace.repository.s3</code> and <code>workspace.repository.projectS3</code>).</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#eccenca-datamanager-v21065","title":"eccenca DataManager v21.06.5","text":"<p>This version of eccenca DataManager fixes the following issues:</p> <ul> <li>Linkrules<ul> <li>several stability improvements for GraphDB backends</li> </ul> </li> <li>Explore<ul> <li>New Vocabulary Visualization Component to the graph explore view.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Vocabs<ul> <li>Show spinner during installing/uninstalling vocab</li> <li>Install new vocabularies failed if preferredNamespace is not defined.</li> </ul> </li> <li>ObjectView<ul> <li>Render images without knowing the relation by parsing the value. Images are provided as URI: <code>&lt;data:image_svg+xml......&gt;</code></li> </ul> </li> <li>Linkrules<ul> <li>Fix of missing dcterms prefix in a query</li> <li>Fix of the publish/unpublish query</li> <li>Restore Joint.js common styles</li> <li>A warning about saving published rule should be shown once</li> <li>Layout position is preserved for newly added operators</li> <li>Evaluation is presented inline without conflicting with the manual placement</li> <li>Empty text fields are no longer reset to the placeholder value</li> <li>wrong order of selection items in templates</li> <li>ID generation for operators, in case multiple prototypical pipelines are presents</li> <li>Link Rules Type Error</li> <li>Failed DI calls not catched in DM</li> <li>Broken visual connection after evaluation in LinkRuleEditor</li> </ul> </li> <li>General<ul> <li>prevent errors in login</li> <li>Bearer Token not persistent</li> <li>DM Query fails because endpoint id and token are not set</li> </ul> </li> <li>Query<ul> <li>Search in labels does not work</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#eccenca-corporate-memory-control-cmemc-v2106","title":"eccenca Corporate Memory Control (cmemc) v21.06","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>graph import</code> command</li> <li>new option <code>--skip-existing</code> will not touch graphs which are already there</li> <li><code>admin token</code> command<ul> <li>new option <code>--decode</code> shows content of the decoded auth token</li> <li>in combination with <code>--raw</code> it outputs the decoded token as json</li> </ul> </li> <li>configuration<ul> <li>new configuration keys to fetch credentials from external processes</li> <li>use the parameter <code>OAUTH_PASSWORD_PROCESS</code>, <code>OAUTH_CLIENT_SECRET_PROCESS</code> and <code>OAUTH_ACCESS_TOKEN_PROCESS</code> to setup an external executable</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>docker base image is now <code>debian:stable-20210721-slim</code></li> <li>support for <code>OAUTH_PASSWORD_ENTRY</code> and <code>OAUTH_CLIENT_SECRET_ENTRY</code> removed</li> <li>fix: freeze click dependency to 7.1.2</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#dataintegration","title":"DataIntegration","text":"<ul> <li>All script operators are disabled by default now and need to be re-enabled by configuration.</li> <li>The (not-working) spotlight transform operator is disabled by default now.</li> <li>Optional: When loading existing workflows in the new workflow editor, the operators might overlap and may need to be re-arranged manually.<ul> <li>This does not influence the actual execution of the workflows in any way.</li> <li>An auto-layouting feature will be added in the future</li> </ul> </li> <li>Plugin configuration has been changed. The \u2018plugin.blacklist\u2019 has been deprecated and will be removed in future versions. See example below for new format: <pre><code>pluginRegistry {\n  # External plugins are loaded from this folder\n  pluginFolder = ${elds.home}\"/etc/dataintegration/plugins/\"\n\n  # Configuration of individual plugins.\n  plugins {\n    pluginId1.enabled = false\n    ...\n  }\n}\n</code></pre></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#datamanager","title":"DataManager","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#dataplatform","title":"DataPlatform","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#cmemc","title":"cmemc","text":"<ul> <li>The configuration keys <code>*_ENTRY</code> are not supported anymore. In case you used them, switch to <code>*_PROCESS</code> configuration</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/","title":"Corporate Memory 21.11","text":"<p>Corporate Memory 21.11 is the fourth release in 2021.</p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: The workflow editor user interface allows for undo/redo of you activities now, as well as shows inline (live) progress and statistics of a running workflow.</li> <li>Explore: The Explore interface is now adapted to our new look and feel + the Knowledge Graph list component can be configured to show multiple lists of named graphs e.g. to distinguish between user, vocabulary and system graphs.</li> <li>Automate: cmemc can now interact with workflow scheduler (disable, enable, inspect, list, open) as well as dataset resources (delete, inspect, usage, list).</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataPlatform configuration and behaviour has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.11.1</li> <li>eccenca DataIntegration v21.11</li> <li>eccenca DataManager v21.11.5</li> <li>eccenca Corporate Memory Control (cmemc) v21.11.4</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataintegration-v2111","title":"eccenca DataIntegration v21.11","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Remote Excel Dataset using Google Drive Spreadsheets as resources.</li> <li>Remote Office 365 Spreadsheets Support.</li> <li>Workflow editor improvements:<ul> <li>View navigation via clicking and dragging the mouse on the mini-map.</li> <li>Config input port is hidden by default and can be enabled via menu entry.</li> <li>Inline (live) progress and statistics.</li> </ul> </li> <li>Support mapping of RDF literals in object mappings.<ul> <li>Literals are handled as entities and can be mapped in object mappings.</li> <li>Special path #text that allows to access the lexical value of the mapped resource. This allows to access the value of a mapped literal in an object mapping.</li> </ul> </li> <li>RDF datasets:<ul> <li>Special path #lang that allows to access the language tag of a language tagged RDF literal.</li> </ul> </li> <li>Extensions to the Excel dataset:<ul> <li>Excel columns may be addressed by their letter code (<code>#A</code>, <code>#B</code>, etc.) as well.</li> <li>A new parameter \u2018hasHeader\u2019 allows handling of pure data sheets with no table header.</li> </ul> </li> <li>Support selecting multiple vocabularies with auto-completion support.</li> <li>Transform URI pattern improvements:<ul> <li>Validation of URI patterns in the UI and backend, i.e. it checks that URI templates generate valid URIs.</li> <li>Auto-completion support in URI pattern input component.</li> <li>URI pattern validation endpoint</li> <li>URI pattern auto-completion endpoint</li> <li>Change initial URI pattern of complex URI rules from <code>/</code> to <code>{}/&lt;OBJECT_RULE_ID&gt;</code></li> </ul> </li> <li>REST endpoint to fetch an activity execution error report as JSON or Markdown.</li> <li>Activity integration into task detail pages:<ul> <li>Primary, running, failed and all related caching activities are shown on the task detail pages.</li> <li>Caching activities are grouped and have additional information and controls like \u2018refresh all caches\u2019, last update etc.</li> <li>For failed activities it is possible to see and download an execution error report.</li> </ul> </li> <li>Support for requesting task parameter values in the item search API.</li> <li>New workflow operator to stop the current workflow execution (without failing) if a specified condition has been met.<ul> <li>Add global cache for URI patterns that stores all URI patterns extracted from transform tasks.</li> <li>Add API endpoint to fetch all URI patterns used for given target class URIs.</li> <li>Allow to select from existing URI patterns (related to the same target classes) in object mapping rule form.</li> </ul> </li> <li>Support setting the URI pattern during creation of an object mapping rule.</li> <li>Application version info in user menu (sidebar on the right side).</li> <li>JDBC dataset does support token-based authentication for MS SQL server now.</li> <li>Undo/redo support in workflow editor.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Improved task search with highlighting for nodes in the canvas.</li> <li>Added \u201cDefault Value\u201d Transformation to recommended category.</li> <li>OLE2-based Excel documents (e.g., .xls) are now supported in non-streaming mode.</li> <li>Enabled the Excel \u2018LEFT\u2019 transform function.</li> <li>Mapping Rule Editor will show the rule label (if any) and the mapping target.</li> <li>The JSON dataset supports streaming.<ul> <li>The change applies to reading JSON, writing was already streamed.</li> <li>If streaming is enabled, files won\u2019t be loaded into memory, allowing to read large JSON files without running into OutOfMemory errors.</li> </ul> </li> <li>Allow to open the value mapping rule formula editor from the create/edit value mapping rule form.</li> <li>Improvements to Template operators:<ul> <li>Added option to forward input attributes.</li> <li>Allow tests in conditions (e.g., <code>if input1 is sequence</code>).</li> <li>Updated Jinja library to latest bugfix release.</li> </ul> </li> <li>Render markdown (links only) in meta data preview and search item description instead of markdown markup text.</li> <li>Moved <code>Endpoint</code> parameter of Knowledge Graph dataset into advanced section</li> <li>JDBC dataset will retry failed queries due to interrupted connections. Retries will start at the offset of the previously read row.</li> <li>Include query URLs in result from REST operator (multi input).</li> <li>Workflow editor: Adapt node height based on number of inputs.</li> <li>Project import does not show the error message.</li> <li>Added rail navigation bar, replacing the old navigation.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-datamanager-v21115","title":"eccenca DataManager v21.11.5","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General<ul> <li>Main navigation and application header<ul> <li>New header was enabled</li> <li>showing the title of the current view or actually shown resource</li> <li>includes main actions for the page</li> </ul> </li> <li>Main navigation was moved to the right sidebar<ul> <li>can get expanded permanently</li> <li>offers option to get expanded in a reduced form by hovering it with the cursor</li> <li>we now have various section in the main nav, modules with main navigation items can be configured via <code>subSection</code> parameter (order of sections need to be defined in <code>Navigationbar</code> component, currently we have <code>timetracker</code>, <code>explore</code>, <code>build</code> and <code>other</code> as options, if not set it is automatically organized into <code>explore</code> or <code>other</code>)</li> <li>Deprecation notice: configuration variables <code>windowTitle</code> and <code>headerName</code> are now deprecated, please use <code>companyName</code>, <code>productName</code> and <code>applicationName</code> from <code>appPresentation</code></li> </ul> </li> </ul> </li> <li>Query Module<ul> <li>New Query Module v2</li> <li>Activated per default</li> <li>Improved catalog functionality</li> <li>Richer editor based on yasqe</li> <li>Integrated prefix handling</li> </ul> </li> <li>Vocabs Module<ul> <li>Allow create new empty ontology without uploading a file.</li> <li>Check if graph exist and show an error while creating a new vocab.</li> </ul> </li> <li>Explore<ul> <li>Allow hide / show the vocab viz module via configuration <code>details.visualization.enable</code></li> <li>Center automatically load vocab viz on load</li> <li>Show precise tooltips for controls of vocab viz</li> <li><code>NavigationListQuery</code> now accepts <code>{{GRAPH}}</code> as placeholder</li> <li>Selectbox load values on focus rather than on click to allow using keyboard.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Explore<ul> <li>bug producing runtime errors in vocab viz</li> <li>Extended existing <code>externalTools</code> mechanism for custom iframes</li> </ul> </li> <li>Build<ul> <li>simplify configuration object DIWorkspace removing unused <code>url</code> value, letting only <code>enable: BOOLEAN</code> and <code>baseUrl: URL_TO_DATAINTEGRATION</code></li> </ul> </li> <li>Shacl<ul> <li>Reload form if language changes</li> <li>Load form data using the current selected language</li> <li>Show group header if there are visible elements with no value in shacline</li> <li>Display selected value in options in select boxes when it is no multi select</li> <li>Don\u2019t send empty datatypes to saveShaped as are interpreted as <code>&lt;file:///data/&gt;</code>.</li> </ul> </li> <li>Query Module<ul> <li>Improve search on query module</li> <li>Order prefixes alphabetically</li> <li>URL parameters: use <code>query</code> to open a query by IRI or <code>queryString</code> to open a query by query string</li> </ul> </li> <li>General<ul> <li>Reorder modules. Move Explore to position 1, query to position 4 and manage to the last</li> </ul> </li> <li>Explore<ul> <li>Group values if only origin graph differs in shacl view.</li> <li>replace <code>markdown-it</code> with <code>react-markdown</code> library and refactor usage</li> <li>update navigation queries <code>navigation.topQuery</code> and <code>navigation.subQuery</code></li> </ul> </li> <li>Vocab<ul> <li>Use relative paths properly when requesting DI login prefixes or cache endpoints.</li> </ul> </li> <li>Thesaurus<ul> <li>Fix links in detail view to point to the selected concept</li> <li>Fix button \u201cSee more in list\u201d to point to explore instance view</li> <li>Prevent infinite loop displaying tabs in detail view</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataplatform-v21112","title":"eccenca DataPlatform v21.11.2","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>Prometheus and Spring metrics endpoints are now exposed per default, i.e. <code>./actuator/prometheus</code> or <code>actuator/metrics</code> for list and, exemplarily, <code>./actuator/metrics/cache.size</code> for the metric of interest, see the spring doc for more information.<ul> <li>you can deactivate them using the configuration properties in <code>application.yml</code> (or any other spring config) application.yml<pre><code>endpoint:\n    prometheus:\n        enabled: false\n    metrics:\n        enabled: false\n</code></pre></li> <li>Users roles need to match values of <code>authorization.abox.adminGroup</code> or <code>authorization.abox.metricsGroup</code> role definition for accessing those endpoints. <code>authorization.abox.metricsGroup</code> defaults to <code>metrics</code>, therefore in keycloak a user needs to <code>metrics</code> added as role, for example via a group and groupmapping.</li> </ul> </li> <li>graphdb lucene index support<ul> <li>the index is used for example in the explore section to allow fast and userfriendly access</li> </ul> </li> <li>Graph List<ul> <li>The graph list query is now configurable, using the parameter <code>proxy.graphListQuery</code> with a default value of <code>SELECT distinct ?g {graph ?g {?s ?p ?o}}</code></li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>Middleware Upgrades<ul> <li>Upgraded Stardog support to version 7.8.3.</li> <li>Upgraded GraphDB support to version 9.10.2</li> </ul> </li> <li>Change of proxied graph store get endpoint <code>/proxy/{id}/graph</code><ul> <li>Removal of support for timeout and ETags</li> <li>Usage of underlying store graph store endpoints (if available) for performance</li> </ul> </li> <li>Upgraded Stardog support to version 7.8.3.</li> <li>Upgraded GraphDB support to version 9.10.2</li> <li>stop words in search expression are no longer removed</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataplatform-v21111","title":"eccenca DataPlatform v21.11.1","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>Prefixes are now used in TURTLE serializations<ul> <li>Prefixes defined in the Vocaulary catalog are used.</li> </ul> </li> <li>Added support for all shacl:path expressions<ul> <li><code>shui:inversePath</code> is still supported, however please use <code>sh:inversePath</code> wherever possible.</li> </ul> </li> <li>Property usage analytics endpoints <code>api/vocabusage/*</code> for both explicit Vocab definitions and usage information. Please refer to the OpenAPI definitions for more information.</li> <li>Explicitly defined supported-submit-methods property to enable / disable \u201cTry I Out\u201d button in Swagger UI.</li> <li>Server side UI configuration Support<ul> <li>Shapes for <code>WorkspaceConfiguration</code> added</li> <li>Configuration endpoint <code>api/conf/workspace</code> exposes workspace specific information about graph lists.</li> <li><code>graphs/list</code> endpoint includes graph list association for customization of the navigation list.</li> </ul> </li> <li>Shape-intgrated Workflow triggering<ul> <li>Shapes extended for linking a Shape to DI workdlows</li> <li>extension of the <code>/api/resource/shaped</code> endpoint to include this information</li> </ul> </li> <li>Bootstrap Data:<ul> <li>example configuration the default workspace\u2019s navlist</li> </ul> </li> <li>Admins can now manually flush all caches per api call.</li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>Apache Jena 3.17 is now used.<ul> <li>SPARQL requests to virtuoso are now executed over HTTP (see migration notes)</li> </ul> </li> <li>Graph exports now sorted by subject resource</li> <li>IT tests:<ul> <li>Updated IT test GraphDB Docker image to v9.9.0-1-se</li> <li>parametrization</li> <li>DefaultGraphIT</li> <li>Resources refactoring</li> </ul> </li> <li>Bootstrap Data:<ul> <li>upgrade some vocabulary references to new versions (fibo, org, qudt, schema, gist, time)</li> <li>Updates Spring Security to mitigate potential security issue: CVE-2021-22119</li> <li>Improved API documentation for APIs offering multiple HTTP methods (i.e. <code>GET</code> and <code>POST</code>). Improves OpenAPI client generation</li> </ul> </li> <li>GSP Endpoint:<ul> <li>read content type from multipart files and use only file extension only as backup</li> </ul> </li> <li>Jinja templates:<ul> <li>templates no fail on unknown tokens, allowing easier useage of SPARQL <code>OPTIONAL</code> values in templates.</li> </ul> </li> <li>Removed:<ul> <li>Virtuoso Provisioned Authorization is no longer supported. Use <code>FROM</code> authorization instead.</li> </ul> </li> <li>Errors in the configuration graph gracefully fall back to the default config</li> <li>GraphDB FROM Authorization is correctly initialized for the SPARQL proxy</li> <li>Type fetching on stardog could lead to NullPointerException</li> <li>Facets on graphs without imports on Stardog</li> <li>Graph type query was optimized to only fetch types of interest</li> <li>Errors in the configuration graph gracefully fall back to the default config</li> <li>GraphDB FROM Authorization is correctly initialized for the SPARQL proxy</li> <li>Type fetching on stardog could lead to NullPointerException</li> <li>Facets on graphs without imports on Stardog</li> <li>Graph type query was optimized to only fetch types of interest</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-corporate-memory-control-cmemc-v21114","title":"eccenca Corporate Memory Control (cmemc) v21.11.4","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>dataset resource</code> command group with the following resource commands:<ul> <li><code>delete</code> - Delete file resources.</li> <li><code>inspect</code> -Display all meta data of a file resource.</li> <li><code>list</code> - List available file resources.</li> <li><code>usage</code> - Display all usage data of a file resource.</li> </ul> </li> <li>scheduler command group with the following workflow scheduler commands:<ul> <li><code>disable</code> - Disable a scheduler.</li> <li><code>enable</code> - Enable a scheduler.</li> <li><code>inspect</code> - Display meta data of a scheduler.</li> <li><code>list</code> - List available schedulers.</li> <li><code>open</code> - Open scheduler in the browser.</li> </ul> </li> <li><code>config eval</code> command<ul> <li>shell environment preparation for configuration</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>docker base image is now <code>debian:stable-20211011-slim</code></li> <li>workflow execute commands now uses <code>ExecuteDefaultWorkflow</code> as activity (instead of <code>ExecuteLocalWorkflow</code>)</li> <li><code>admin token --decode</code> table now sorted by Key column</li> <li><code>dataset inspect</code> table now sorted by Key column</li> <li><code>query status</code> table now sorted by Key column</li> <li><code>vocabulary cache list</code> table now sorted by IRI column</li> <li><code>dataset create --type</code> completes now plugin IDs as well</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#dataintegration","title":"DataIntegration","text":"<ul> <li>The fix of \u2018XML dataset ignores base path when type URI is set.\u2019, could break existing projects that are relying on the previously broken behaviour.<ul> <li>This may affect all use cases with XML datasets that have the \u2018base path\u2019 parameter set to a non-empty value AND where a transformation or linking task overwrites this path by setting a type path/URI.</li> <li>Removing the \u2018base path\u2019 value from the affected XML datasets should solve the issue.</li> </ul> </li> <li>Generating URIs for entities failed if XML tags contained dots</li> <li>The JSON dataset uses streaming by default now, which does not support backward paths.</li> <li>Using backward paths will fail and the error message will contain a suggestion to change the streaming parameter to false.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#datamanager","title":"DataManager","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#dataplatform","title":"DataPlatform","text":"<ul> <li>Jinja templates will no longer fail on unknown tokens. If this was used for signaling errors or fail-fast evaluation, this has to be implemented in regular conditional checks.</li> <li>Virtuoso config requires adjustments, its HTTP port needs to be configured.<ul> <li>Please ensure, that the configured user has the same access rights in virtuoso via ODBC and HTTP application.yml (old)<pre><code>sparqlEndpoints:\n  virtuoso:\n  - id: \"default\"\n    authorization: NONE\n    host: \"store\"\n    port: \"1111\"\n    username: \"dba\"\n    password: \"dba\"\n</code></pre> becomes application.yml (new)<pre><code>sparqlEndpoints:\n  virtuoso:\n  - id: \"default\"\n      authorization: NONE\n      host: \"store\"\n      port: \"1111\"\n      httpPort: \"80\"\n      username: \"dba\"\n      password: \"dba\"\n</code></pre></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#cmemc","title":"cmemc","text":"<p>No migration notes.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/","title":"Corporate Memory 22.1","text":"<p>Corporate Memory 22.1 is the first release in 2022.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>The all new linking editor offering a new level of user experience in the linking process supercharged with inline preview and inline validation, improved operator search and much more</li> <li>Python plugin SDK (workflow and transformation plugins)</li> </ul> </li> <li>Explore:<ul> <li>Shacl: Customizable workflow execute button in Property Shapes allows for declarative embedding of</li> <li>Backend: Support for Amazon Neptune as primary Knowledge Graph Store incl. bulk loading of large files via Amazon S3</li> </ul> </li> <li>Automate:<ul> <li>new commands and command groups making the Corporate Memory swiss-command-line-army-knife - cmemc - even more useful</li> <li>Python plugin command group adds capabilities for managing python plugins in your build workspace (admin workspace python)</li> <li>Store command group adds managing commands on quad store level (admin store)</li> <li>Metrics command groups allows for inspecting of server metrics (admin metrics, DataPlatform metrics only at the moment)</li> </ul> </li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataPlatform configuration and behavior has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v22.1</li> <li>eccenca DataIntegration v22.1</li> <li>eccenca DataManager v22.1.1</li> <li>eccenca Corporate Memory Control (cmemc) v22.1.1</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-dataintegration-v221","title":"eccenca DataIntegration v22.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Artifact creation dialogs now allows to set the parent project</li> <li>Added a parameter to JDBC datasets to allow clearing the table before workflow execution.</li> <li>Support for custom project and task identifiers at item creation time.</li> <li>Menu option to copy item ID to clipboard.</li> <li>Value type to represent geometry as WKT literals.</li> <li>Python plugin support<ul> <li>Initial support for workflow and transform plugins.</li> <li>Plugins are executed in a Python 3 environment.</li> <li>Check documentation for details: Python Plugins</li> </ul> </li> <li>User-defined tags on projects and tasks.</li> <li>REST endpoint to fetch rule operator plugins (<code>/api/core/ruleOperatorPlugins</code>).</li> <li><code>convertToComplex</code> query parameter to GET transform rule REST endpoint to always request a complex value transform rule.</li> <li>Add new route to display a task view plugin all by itself without headers, side bar etc.<ul> <li>Route: <code>/workbench/projects/:projectId/item/:pluginId/:taskId/view/:viewId</code></li> </ul> </li> <li>REST endpoint to evaluate a linking rule against the reference links only.</li> <li>New linking and transform editors:<ul> <li>User created layout and auto-layouting support.</li> <li>Multi-word rule operator search with highlighting.</li> <li>New node actions: select nodes, move selection</li> <li>New edge actions: Connect edge to first free input port via dragging over node, Swap edges</li> <li>Delete selected node(s) or edge via Backspace</li> <li>Select nodes via select box (press Shift + left mouse button &amp; draw rectangle) or multi select (press Alt or Cmd + left clicks): Delete, move clone selection</li> <li>Filter out \u2018Excel\u2019 category of operators when <code>hideGreyListedParameters</code> query parameter is set to true.</li> <li>Supports read-only mode. Query parameter <code>readOnly</code> sets the editor in permanent read-only mode.</li> <li>Allow to edit rule node parameters in a larger modal, so complex parameter values can be more easily edited.<ul> <li>All changes done in that modal can be updated in a single transaction (wrt. UNDO/REDO) or cancelled.</li> </ul> </li> <li>Inline evaluation<ul> <li>Show operator output values and evaluation scores directly inside the linking rule editor nodes</li> <li>Support to show link to external reference links UI via <code>referenceLinksUrl</code> query parameter.</li> <li>Support for real-time evaluation when reference links are available</li> </ul> </li> </ul> </li> <li>Simple path operator auto-completion</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Support writing large XML files by using a memory-mapped key-value store internally.</li> <li>Request N-Triples instead of Turtle for SPARQL Construct queries in all RDF datasets, because some RDF stores run into memory problems when requesting Turtle.</li> <li>Improved performance of listing project resources on S3.</li> <li>The maximum size of the internal key value store can be configured now.<ul> <li>Default value is: <code>caches.persistence.maxSize=10GB</code></li> </ul> </li> <li>Improved writing to PostgresQL by using CSV import.</li> <li>If an operator writes multiple tables into a dataset that cannot hold multiple tables, an error is thrown now.<ul> <li>Example: A hierarchical transformation writes into a CSV file.</li> <li>Previously, the last table has been written.</li> </ul> </li> <li>Workflows can also be executed asynchronously using the \u201csimple\u201d workflow execution endpoints.</li> <li>If a sub-workflow is running, the workflow editor will display the full sub-workflow report.</li> <li>Keep sort config and page size when switching filters in faceted search views.</li> <li>In generated transform object rules for nested data sources (XML, JSON and RDF) keep the source path even for paths pointing at literal values, e.g. strings.</li> <li>Use DataPlatform\u2019s <code>facets</code> and <code>vocabusage</code> endpoints to fetch available properties for Knowledge Graph datasets in order to improve performance and load.</li> <li>Generated project &amp; task identifiers: Put label-part as prefix and shorten and simplify generated random string.</li> <li>Show labels and links for dependent tasks in delete modal.</li> <li>Added parameter to \u201clower than\u201d and \u201cgreater than\u201d metrics to choose order.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-datamanager-v2211","title":"eccenca DataManager v22.1.1","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Shacl: Customizable workflow execute button in Property Shapes.</li> <li>Explore: New Graph List component with configurable lists.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Updated <code>typescript</code> to <code>^4.5.2</code> and <code>@reduxjs/toolkit</code> to <code>^1.6.2</code>.</li> <li>Use session cookie to authenticate in DI requests.</li> <li>Removed Save graph void stats from the Statistics tab.</li> </ul> </li> <li>Shacl<ul> <li>Allow HTML in Markdown of Shacl descriptions and ObjectView</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-dataplatform-v221","title":"eccenca DataPlatform v22.1","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>Admin endpoints for zip backup / restore of all graphs</li> <li>New endpoints for storage, analysis and upload of files (rdf files and zip/tgz)<ul> <li><code>/api/upload/</code> for storing and analyzing stored files</li> <li><code>/api/upload/transfer</code> for transferring stored files to rdf triple store</li> <li>stored files on system are removed in housekeeping maintenance job</li> </ul> </li> <li>Integrated Neptune as a triple store including bulk loading of large files via Amazon Simple Storage Service (Amazon S3)</li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>Changed proxied graph store to get endpoint <code>/proxy/{id}/graph</code><ul> <li>Removed support for timeout and ETags.</li> <li>Used underlying store graph store endpoints (if available) for performance.</li> </ul> </li> <li>Middleware Upgrades<ul> <li>Upgraded Stardog support to version 7.9.0</li> <li>Upgraded GraphDB support to version 9.10.2</li> </ul> </li> <li>Library Upgrades<ul> <li>Upgrade to Java 11</li> <li>Upgrades of several libraries including Spring Boot 2.6.6 has been done.</li> <li>Upgrade of Apache Jena 4.4 implies usage of JDK11 http client library instead of apache http client.</li> </ul> </li> <li>Query Monitor<ul> <li>New fields added to output of <code>/api/admin/currentQueries</code> endpoint: <code>user</code> (Executing user in form of IRI), <code>type</code> (Query type - one of <code>ASK</code>, <code>SELECT</code>, <code>CONSTRUCT</code>, <code>DESCRIBE</code>, <code>UNKNOWN</code>), <code>traceId</code> (Trace id of call to Dataplatform - this id bundles 1-n child ids for each query call to backend store)</li> <li>Introduced Spring Sleuth tracing for generation of query IDs and tracing IDs of requests</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-corporate-memory-control-cmemc-v2211","title":"eccenca Corporate Memory Control (cmemc) v22.1.1","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>admin workspace python</code> command group<ul> <li><code>install</code> - Install a python package to the workspace</li> <li><code>list</code> - List installed python packages</li> <li><code>list-plugins</code> - List installed workspace plugins</li> <li><code>uninstall</code> - Uninstall a python package from the workspace</li> </ul> </li> <li><code>project import</code> command<ul> <li>output warnings in case there are failed tasks errors</li> </ul> </li> <li><code>graph export</code> command<ul> <li>the <code>--filename-template</code> / <code>-t</code> option has now a completion of common examples</li> </ul> </li> <li><code>query replay</code> command<ul> <li>replay query logs from the query status command</li> </ul> </li> <li><code>admin status</code> command<ul> <li>Output of DataManager version and status</li> <li>Output of ShapesCatalog version and status</li> </ul> </li> <li><code>query status</code> command<ul> <li>Type filter allows for filtering by query type</li> <li>status filter accepts value <code>error</code> to filter for non-successful queries</li> </ul> </li> <li><code>admin store</code> command group<ul> <li><code>export</code> - backup all knowledge graphs to a ZIP archive</li> <li><code>import</code> - restore graphs from a ZIP archive</li> <li><code>bootstrap</code> - was <code>admin bootstap</code></li> <li><code>showcase</code> - was <code>admin showcase</code></li> </ul> </li> <li><code>admin metrics</code> command group<ul> <li><code>get</code> - Get sample data of a metric</li> <li><code>inspect</code> - Inspect a metric</li> <li><code>list</code> - List metrics for a specific job</li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>docker base image is now <code>python:3.9-slim</code></li> <li>graph tree <code>--id-only</code> option<ul> <li>this option now outputs a flat, de-duplicated list of existing graphs</li> <li>the old output was similar to the default tree output and not useful for piping</li> </ul> </li> <li>tested and build python version is now 3.9</li> <li>cmempy tests for python 2.7 are now disabled</li> <li><code>graph list</code> command</li> <li>SPARQL 1.1 Service Description namespace now recognised as <code>sd:</code> (e.g. in virtuoso used)</li> <li><code>query list</code> command<ul> <li>newly introduced query types are treated correctly now</li> <li>additional types: <code>DELETE</code>, <code>DROP</code> and <code>INSERT</code></li> </ul> </li> <li>docker image has now an empty <code>config.ini</code> in order to avoid warnings when using cmemc with environment variables only</li> </ul> <p>The following commands are deprecated:</p> <ul> <li><code>admin bootstap</code> command<ul> <li>is now in admin store command group, will be removed with the next release</li> </ul> </li> <li><code>admin showcase</code> command     is now in admin store command group, will be removed with the next release</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#dataintegration","title":"DataIntegration","text":"<ul> <li>Writing hierarchical transformations into a CSV dataset or any other datasets that are single tables will lead to an error.</li> <li>The following plugin IDs have been renamed. Old projects can still be loaded with this DI version, but reading projects written with this version using DI releases older than 22.1 can result in project loading errors:<ul> <li>Substring comparison: ID <code>substring</code> to <code>substringDistance</code></li> <li>Constant distance measure: ID <code>constant</code> to <code>constantDistance</code></li> <li>Negate transformer: ID <code>negate</code> to <code>negateTransformer</code></li> </ul> </li> <li>DI uses new DataPlatform endpoints for schema extraction from knowledge graphs by default, i.e. some functionality will break when run with an older DataPlatform.<ul> <li>Set <code>eccencaDataPlatform.sparqlSource.retrievePathsViaDpEndpoints</code> to false in order to use the old approach.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#datamanager","title":"DataManager","text":"<ul> <li>New Graph List component with configurable lists.<ul> <li>The lists can be configured in the cmem config graph.</li> <li>Configurations in the <code>&lt;datamanager&gt;/application.yml</code> are ignored any customizations need to be migrated.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#dataplatform","title":"DataPlatform","text":"<ul> <li>While updating, property <code>spring.profiles=PROFILE</code> needs to be replaced by <code>spring.config.activate.on-profile</code>. For further information, please see this blog post.</li> <li>Removed custom redirect for Swagger UI under <code>/swagger-ui</code>. Swagger UI only accessible under <code>/swagger-ui.html</code> (Spring Boot Default)</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#cmemc","title":"cmemc","text":"<ul> <li>docker image usage<ul> <li>the cmemc docker image is now built to run as user <code>cmem</code> (id: <code>999</code>)</li> <li>the container internally used config file has changed<ul> <li>old: <code>/root/.config/cmemc/config.ini</code></li> <li>new: <code>/config/cmemc.ini</code></li> <li>This means, mounted config volumes need to be changed!</li> </ul> </li> </ul> </li> <li>deprecated commands<ul> <li><code>admin bootstrap</code>|<code>showcase</code> are deprecated</li> <li>use <code>admin store bootstrap</code>|<code>showcase</code> instead</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/","title":"Corporate Memory 22.2.3","text":"<p>Corporate Memory 22.2.3 is the third patch release in the 22.2 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>The all new Active (Link) Learning UI</li> <li>Extended Python Plugin SDK</li> </ul> </li> <li>Explore:<ul> <li>New graph exploration module EasyNav</li> </ul> </li> <li>Automate:<ul> <li>Tag filter, better status monitoring and complete query management</li> </ul> </li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataPlatform configuration and behavior has changed and needs to be adapted according to the migration notes below.</p> <p>Warning</p> <p>With this release of Corporate Memory the (DataIntegration) Python plugin SDK contains the <code>ExecutionContext</code> class. This results in a changed signature of the SDK API functions and causes a breaking change to your exisitng code. Your python SDK based plugins need to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v22.2.2</li> <li>eccenca DataIntegration v22.2.1</li> <li>eccenca DataManager v22.2.3</li> <li>eccenca Corporate Memory Control (cmemc) v22.2</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-dataintegration-v2221","title":"eccenca DataIntegration v22.2.1","text":"<p>v22.2.1 of eccenca DataIntegration adds the following new features:</p> <ul> <li>Rule and workflow editors:<ul> <li>Support automatic scrolling when moving beyond the editor canvas borders on a all drag and edge connect/update operations.</li> </ul> </li> <li>Added \u201csort words\u201d transform operator, which sorts words in each value.</li> </ul> <p>In addition to that, these changes are included in v22.2.1 of eccenca DataIntegration:</p> <ul> <li>Rule editors (linking, transform):<ul> <li>On tab change do not remove the search text, instead select the text to easily overwrite it.</li> <li>Allow to search for input paths in the <code>All</code> tab.</li> </ul> </li> <li>If a long-running workflow is executed manually, the same workflow can be started by a scheduler in the background.</li> <li>Executing workflows did not occupy a slot in the thread pool (i.e., unlimited workflows could be executed concurrently).</li> <li>Generating links could lead to a deadlock, if no slot in the thread pool is available.</li> <li>Entering an invalid URI as path input in the linking editor with a knowledge graph as input results in the rule being broken in the editor.</li> <li>Linking editor: Show the same property labels in the input path auto-completion as in the tab auto-completion.</li> </ul> <p>v22.2 of eccenca DataIntegration adds the following new features:</p> <ul> <li>New active learning UI</li> <li>Python plugins: Added context objects that allow accessing context dependent functionalities, such as:<ul> <li>The current OAuth token</li> <li>Updating the execution report (for workflows)</li> <li>DI version</li> <li>Current project and task identifiers</li> <li>Requires <code>cmem-plugin-base &gt;=2.0.0</code></li> </ul> </li> <li>Workflows search link in main navigation</li> <li>Linking rule editor<ul> <li>Advanced parameter toggle that shows/hides advanced parameters like <code>weight</code> and advanced section in rule parameter modal</li> </ul> </li> <li>Support for sticky notes in both linking and workflow editors</li> <li>Parameter <code>profiling.defaults.noEntities</code> to configure the default entity limit for profiling operations</li> <li>Parameter <code>org.silkframework.runtime.activity.concurrentExecutions</code> to set the max. concurrent activity instances</li> <li>Support for the <code>URI attribute</code> parameter of datasets</li> <li>Support for auto-configuration in create/update dialog</li> <li>Config parameters:<ul> <li><code>profiling.defaults.noEntities</code> to configure the default entity limit for profiling operations</li> <li><code>org.silkframework.runtime.activity.concurrentExecutions</code> to set the max. concurrent activity instances</li> <li><code>cors.enabled</code>, <code>cors.config.allowOrigins</code> and <code>cors.config.allowCredentials</code> to configure CORS settings</li> </ul> </li> </ul> <p>In addition to that, these changes are included in v22.2:</p> <ul> <li>Move <code>outputTemplate</code> parameter to advanced section of XML dataset plugin</li> <li>Improved performance of conversions to floating point numbers</li> <li>Improved linking performance</li> <li>Show report on linking execution tab</li> <li>When the evaluation fails because of missing paths in the cache give specific error message with node highlighting instead of generic error notification</li> <li>Errors in invalid Python packages are recorded and returned, instead of failing</li> <li>Size of the activity thread pool can be configured</li> <li>Linking rule editor<ul> <li>Show linking rule label above toolbar when in integrated mode</li> <li>Handle \u201creversible\u201d comparators, e.g. \u201cGreater than\u201d, by allowing to switch source/target inputs instead of setting the \u2018reverse\u2019 parameter</li> </ul> </li> <li>DataPlatform API timeout is configurable now</li> <li>Workflow progress information was moved to node footer that is displayed empty when no information is available</li> <li>Docker image base: <code>debian:bullseye-20220912-slim</code></li> <li>Return 503 error before exceeding the concurrent activity execution limit instead of discarding a running activity instance</li> <li>Do not execute empty object mapping rules to improve performance</li> <li>Remove root (start) page:<ul> <li>Redirect to workbench project search page</li> <li>Remove legacy workspace link from user menu</li> <li>Add \u201cload example project\u201d action to user menu</li> </ul> </li> <li>Show activity labels instead of IDs in task activity overview</li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-datamanager-v2223","title":"eccenca DataManager v22.2.3","text":"<p>v22.2.3 of eccenca DataManager has the following fixes:</p> <ul> <li>LinkRules<ul> <li>Rule Setup: Fix display of filter</li> </ul> </li> </ul> <p>v22.2.2 of eccenca DataManager has the following fixes:</p> <ul> <li>General<ul> <li>Logout in DM also triggers logout in DI</li> </ul> </li> <li>LinkRules<ul> <li>Rule Setup: Rule filter correctly displays OneOf and NoneOf</li> <li>Rule is correctly serialized after editing, preventing the rule contents to be deleted</li> </ul> </li> </ul> <p>v22.2.1 of eccenca DataManager has the following fixes:</p> <ul> <li>LinkRules<ul> <li>Fixed trigger of refetching data after an update</li> <li>Display of negative Reference Links</li> </ul> </li> </ul> <p>v22.2 of eccenca DataManager adds the following new features:</p> <ul> <li>Navigation<ul> <li>Add DataIntegration workflows link to main navigation</li> </ul> </li> <li>Vocabulary Catalog<ul> <li>Inline vocabulary metadata via (editable) shape</li> <li>Ability to activate git synchronization of changes<ul> <li>Change history with diff view and ability to revert to a specific commit</li> </ul> </li> </ul> </li> <li>Explore<ul> <li>New (Shacl) Template based graph creation wizard<ul> <li>Supporting different methods to define / select graph IRIs</li> <li>Support for bulk add via <code>.zip</code> archives containing multiple RDF files</li> </ul> </li> </ul> </li> <li>i18n<ul> <li>French translation</li> </ul> </li> <li>EasyNav<ul> <li>New graph visualization module</li> <li>With search filter configuration</li> <li>Bulk node search and bulk add</li> <li>Ability to save, load and share explorations</li> </ul> </li> </ul> <p>In addition to that, these changes are included in v22.2 of eccenca DataManager:</p> <ul> <li>Increase height of Turtle editor in the resource details view.</li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-dataplatform-v2222","title":"eccenca DataPlatform v22.2.2","text":"<p>v22.2.2 of eccenca DataPlatform has the following changes:</p> <ul> <li>Fixed<ul> <li>reintroduced support for IRI templates in node shapes, with only the uuid placeholder.</li> <li>Prevent buffer overflow for large query results streaming to client</li> </ul> </li> <li>Changed<ul> <li>Maintenance: Updated Spring Boot to 2.7.8</li> </ul> </li> </ul> <p>v22.2.1 of eccenca DataPlatform has the following fixes:</p> <ul> <li>Update of dependencies because of vulnerabilities i.e. Spring Boot.</li> <li>Addition of logstash runtime dependency as to enable json logging.</li> <li>GraphDb indices are created without facet option causing problems.</li> <li>Fix of memory leak in query monitor causing high heap usage.</li> <li>Refactoring of spring integration tests (IT) and inclusion of most tests in the cucumber subproject.</li> </ul> <p>v22.2 of eccenca DataPlatform ships the following new features:</p> <ul> <li>Added support for manual query/update cancellation:<ul> <li>active for graphdb, stardog, neptune</li> <li>DELETE <code>/api/admin/currentQueries/{queryId}</code></li> <li>Neptune updates cannot be cancelled because queryId header not processed</li> </ul> </li> <li>Added support for creation of configured graphdb repository on DP startup<ul> <li><code>store.graphdb.createRepositoryOnStartup</code>: Flag if repository shall be created on startup (default: false)</li> </ul> </li> <li>Added support for selective invalidation of caches (graph list, shapes) via Update parsing / GraphDb Change Tracking<ul> <li><code>proxy.cache-selective-invalidation</code>: true if activated, false otherwise full flush on every write (default: true)</li> <li><code>store.graphdb.graphDbChangeTrackingActive</code>: Whether change tracking for updates is active - better results for cache invalidation (default: true)</li> <li><code>store.graphdb.graphDbChangeTrackingMaxQuadMemory</code>: Amount of quads as a result of an update which are loaded into memory for analyzing consequences for caches (default: 1000)</li> </ul> </li> <li>Automatic creation of default <code>application.yml</code> and gradle tasks for generation of markdown documentation</li> <li>Added endpoints for supporting easynav graph visualizations<ul> <li>search and resource listing via <code>/api/search</code></li> <li>managing of persisted visualisations via <code>/api/navigate</code> endpoints</li> </ul> </li> <li>Added provisioning of jinja templates with provided substitution map for endpoint <code>/api/custom/{slug}</code></li> <li>Added property <code>proxy.descriptionProperties</code> (analogous to <code>proxy.labelProperties</code>) for defining search relevant description properties</li> <li>Extend query monitor<ul> <li>Added fields per entry<ul> <li><code>timeout</code>: value in ms of the query/update timeout</li> <li><code>timedOut</code>: boolean value on whether the query timed out or not</li> <li><code>cancelled</code>: boolean value on whether the query has been cancelled manually</li> <li><code>running</code>: boolean value on whether the query is currently still being executed</li> <li><code>affectedGraphs</code>: on successfully finished query/update the affected graphs are shown (if possible to determine)</li> </ul> </li> <li>Added property for memory bound consumption in MB for query monitor list<ul> <li><code>proxy.queryMonitorMaxMemoryInMb</code> (Default: 30)</li> </ul> </li> <li>Added fields to prometheus metrics endpoint<ul> <li><code>querymonitor_memoryusage_total</code>: memory usage of query queue in MB</li> <li><code>querymonitor_queuesize_total</code>: query queue size</li> </ul> </li> </ul> </li> <li>Extend actuator info endpoint with store backend properties, <code>/actuator/info</code>:<ul> <li>fields under store:<ul> <li><code>type</code>: same as <code>store.type</code> property (MEMORY, HTTP, GRAPHDB, STARDOG, VIRTUOSO, NEPTUNE)</li> <li><code>version</code>: if possible / otherwise UNKNOWN</li> <li><code>host</code>: if applicable otherwise N/A</li> <li><code>repository</code>: if applicable otherwise N/A</li> <li><code>user</code>: if applicable otherwise N/A</li> </ul> </li> </ul> </li> <li>Add non-transactional git sync of graph changes<ul> <li>graphs can be configured via graph configuration for bi-directional git sync</li> <li>cf. config properties under <code>gitSync.*</code></li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are included in v22.2 of eccence DataPlatform:</p> <ul> <li>New store configuration properties, see below for migration notes</li> <li>Changed property for defining select query for graphList<ul> <li>setting is store dependant and not valid for some stores</li> <li>property <code>proxy.graphListQuery</code> (<code>proxy.graph_list_query</code>) moved to store settings:<ul> <li><code>store.stardog.graphListQuery</code></li> <li><code>store.neptune.graphListQuery</code></li> </ul> </li> </ul> </li> <li>Changed property for scheduled cache invalidation<ul> <li><code>proxy.cacheInvalidationCron</code>: Spring boot cron entry cf. (default: <code>* */30 * * * *</code>)</li> <li>https://docs.spring.io/spring-framework/docs/current/reference/html/integration.html#scheduling-cron-expression</li> </ul> </li> <li>Library updates including Spring Boot / Stardog</li> <li>Changed property for DP query system timeout<ul> <li><code>proxy.queryTimeoutGeneral</code> -&gt; <code>store.queryTimeoutGeneral</code> in ISO 8601 duration format (default: <code>PT1H</code>)</li> </ul> </li> <li>Changed loading of model entities i.e. shapes cache<ul> <li>load model entities using GSP requests instead of construct queries</li> <li>Changed property for base IRI: <code>files.defaultBaseIri</code> to <code>proxy.defaultBaseIri</code> (default: <code>http://localhost/</code>)</li> </ul> </li> </ul> <p>The following functionalities have been discontinued:</p> <ul> <li>Support for provisioned store authorization</li> <li>Command line options create-config, update-war</li> <li>WAR build target and support for WAR servlet deployment</li> <li>Property for DP query system timeout check interval<ul> <li><code>proxy.queryTimeoutCheckCron</code> not necessary anymore</li> </ul> </li> <li>Support for multiple endpoints</li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-corporate-memory-control-cmemc-v222","title":"eccenca Corporate Memory Control (cmemc) v22.2","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>project reload</code> command<ul> <li>Reload all tasks of a project from the workspace provider</li> </ul> </li> <li><code>admin workspace python list-plugins</code> command<ul> <li>New option <code>--package-id-only</code> to output only package IDs</li> </ul> </li> <li><code>admin workspace python install</code> command completion<ul> <li>now also provides plugin packages published on pypi.org</li> </ul> </li> <li><code>query status</code> command<ul> <li>New filter <code>query</code>:<ul> <li><code>graph</code> - List only queries which affected a certain graph (URL)</li> <li><code>regex</code> - List only queries which query text matches a regular expression</li> <li><code>trace-id</code> - List only queries which have the specified trace ID</li> <li><code>user</code> - List only queries executed by the specified account (URL)</li> </ul> </li> <li>New values for filter <code>status</code>:<ul> <li><code>cancelled</code>: List only queries which were cancelled</li> <li><code>timeout</code>: List only queries which ran into a timeout</li> </ul> </li> </ul> </li> <li><code>query cancel</code> command<ul> <li>cancel a running query - this stops the execution in the backend</li> <li>Depending on the backend store, this will result in a broken result stream (stardog, neptune and virtuoso) or a valid result stream with incomplete results (graphdb)</li> </ul> </li> <li><code>dataset list</code>|<code>delete</code> commands<ul> <li>New option <code>--filter</code> with the following concrete filter<ul> <li><code>project</code> - filter by project ID</li> <li><code>regex</code> - filter by regular expression on the dataset label</li> <li><code>tag</code> - filter by tag label</li> <li><code>type</code> - filter by dataset type</li> </ul> </li> </ul> </li> <li><code>workflow list</code> command<ul> <li>New option <code>--filter</code> with the following concrete filter<ul> <li><code>project</code> - filter by project ID</li> <li><code>regex</code> - filter by regular expression on the dataset label</li> <li><code>tag</code> - filter by tag label</li> <li><code>io</code> - filter by io type</li> </ul> </li> </ul> </li> <li><code>admin status</code> command<ul> <li>overall rewrite</li> <li>new table output</li> <li>new option <code>--raw</code> to output collected status / info values</li> <li>new option <code>--key</code> to output only specific values</li> <li>new option <code>--enforce-table</code> to enforce table output of <code>--key</code></li> </ul> </li> <li><code>vocabular import</code> command<ul> <li>new option <code>--namespace</code>: In case the imported vocabulary file does not include a preferred namespace prefix, you can manually add a namespace prefix</li> </ul> </li> <li><code>workflow io</code> command<ul> <li>new flag <code>--autoconfig</code> / <code>--no-autoconfig</code> for input dataset auto configuration</li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are included:</p> <ul> <li><code>admin workspace python list-plugins</code> command<ul> <li>Additionally outputs the Package ID</li> </ul> </li> <li><code>project import</code> command<ul> <li>The project id is now optional when importing project files</li> </ul> </li> <li><code>admin status</code> command<ul> <li>new table output (similar to the other tables)</li> <li><code>status</code> filter with <code>error</code> value<ul> <li>only execution errors are listed</li> <li>this specifically means no cancelled and timeouted queries (they have there own status now)</li> </ul> </li> </ul> </li> <li>Add pysocks dependency to cmempy<ul> <li>This allows for using the <code>all_proxy</code> evironment variable</li> </ul> </li> <li><code>dataset list --raw</code> output<ul> <li>output was not a JSON array and not filtered correctly</li> </ul> </li> <li>cmempy get graph streams<ul> <li>stream enabled</li> </ul> </li> <li><code>admin status</code> command<ul> <li>command will now always return, even if a component is down</li> </ul> </li> </ul> <p>The following commands are discontinued:</p> <ul> <li><code>admin bootstap</code> command<ul> <li>was deprecated in 22.1, use <code>admin store bootstrap</code> command instead</li> </ul> </li> <li><code>admin showcase</code> command<ul> <li>was deprecated in 22.1, use <code>admin store showcase</code> command instead</li> </ul> </li> <li><code>dataset list</code>|<code>delete</code> command<ul> <li><code>--project</code> option, use <code>--filter projext XXX</code> instead</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v22.2 into DataIntegration v22.1 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v22.1 can be imported into DataIntegration v22.2.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#dataintegration","title":"DataIntegration","text":"<ul> <li>CSV attributes specified via the <code>properties</code> parameter had inconsistent encoding rules. For CSV datasets where the <code>properties</code> parameter is used this can lead to changed source paths.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#python-plugins","title":"Python plugins","text":"<p>Due to the added context classes, the signature of a number of functions has been changed. The following changes need to be made for the implementation of these classes:</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#workflowplugin","title":"WorkflowPlugin","text":"<ul> <li>The execute function has a new parameter <code>context</code>:<ul> <li><code>def execute(self, inputs: Sequence[Entities], context: ExecutionContext)</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#parametertype","title":"ParameterType","text":"<ul> <li>The <code>project_id</code> parameters of the label and the autocompletion functions have been replaced by the PluginContext:<ul> <li><code>def autocomplete(self, query_terms: list[str], context: PluginContext) -&gt; list[Autocompletion]</code></li> <li><code>def label(self, value: str, context: PluginContext) -&gt; Optional[str]</code></li> <li>The project identifier can still be accessed via <code>context.project_id</code></li> </ul> </li> <li>The <code>fromString</code> function has a new parameter <code>context</code>:<ul> <li><code>def from_string(self, value: str, context: PluginContext) -&gt; T</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#dataplatform","title":"DataPlatform","text":"<p>Due to the removed multiple endpoint support the store configuration properties have changed. Please revise your store configuration section(s) in your DataPlatform <code>application.yml</code>. The new configuration properties are:</p> <ul> <li>Type of store (general settings)<ul> <li><code>store.type</code>: MEMORY, HTTP, GRAPHDB, STARDOG, VIRTUOSO, NEPTUNE</li> <li><code>store.authorization</code>: NONE, REWRITE_FROM</li> </ul> </li> <li>MEMORY:<ul> <li><code>store.memory.files</code>: List of files loaded on startup</li> </ul> </li> <li>HTTP:<ul> <li><code>store.http.queryEndpointUrl</code>: SPARQL Query endpoint (mandatory)</li> <li><code>store.http.updateEndpointUrl</code>: SPARQL Update endpoint (mandatory)</li> <li><code>store.http.graphStoreEndpointUrl</code>: SPARQL GSP endpoint (optional but highly recommended)</li> <li><code>store.http.username</code>: Username (optional)</li> <li><code>store.http.password</code>: Password (optional)</li> </ul> </li> <li>GRAPHDB:<ul> <li><code>store.graphdb.host</code>: host of graphdb backend (i.e. localhost)</li> <li><code>store.graphdb.port</code>: port of graphdb backend (i.e. 7200)</li> <li><code>store.graphdb.ssl-enabled</code>: flag if ssl (https) is enabled (default: false)</li> <li><code>store.graphdb.repository</code>: name of repository (i.e. cmem)</li> <li><code>store.graphdb.username</code>: Username (optional)</li> <li><code>store.graphdb.password</code>: Password (optional)</li> <li><code>store.graphdb.useDirectTransfer</code>: flag if direct GSP endpoints of graphdb shall be used instead of workbench upload (default: true)</li> <li><code>store.graphdb.importDirectory</code>: Import directory to be utilized in the \u201cworkbench import with shared folder\u201d approach.</li> <li><code>store.graphdb.graphDbChangeTrackingActive</code>: Whether change tracking for updates is active - better results for cache invalidation (default: true)</li> <li><code>store.graphdb.graphDbChangeTrackingMaxQuadMemory</code>: Amount of quads as a result of an update which are loaded into memory for analyzing consequences for caches (default: 1000)</li> </ul> </li> <li>STARDOG:<ul> <li><code>store.stardog.host</code>: host of stardog backend (i.e. localhost)</li> <li><code>store.stardog.port</code>: port of stardog backend (i.e. 5820)</li> <li><code>store.stardog.ssl-enabled</code>: flag if ssl (https) is enabled (default: false)</li> <li><code>store.stardog.repository</code>: name of repository (i.e. cmem)</li> <li><code>store.stardog.username</code>: Username (optional)</li> <li><code>store.stardog.password</code>: Password (optional)</li> <li><code>store.stardog.userPasswordSalt</code>: salt for generated user password (optional)</li> <li><code>store.stardog.updateTimeoutInMilliseconds</code>: Timeout in ms for updates (default: 0 = deactivated)</li> <li><code>store.stardog.graphListQuery</code>: Query for graph list - graph must be bound to variable ?g</li> </ul> </li> <li>NEPTUNE:<ul> <li><code>store.neptune.host</code>: host of neptune backend (i.e. neptune-cluster123.eu-central-1.neptune.amazonaws.com)</li> <li><code>store.neptune.port</code>: port of neptune backend (i.e. 8182)</li> <li><code>store.neptune.graphListQuery</code>: Query for graph list - graph must be bound to variable ?g</li> <li>Settings under store.neptune.aws (mandatory):<ul> <li><code>store.neptune.aws.region</code>: AWS region where the configured neptune cluster is located (e.g. eu-central-1)</li> <li><code>store.neptune.aws.authEnabled</code>: Flag on whether authentication is enabled on neptune cluster (default: true)</li> </ul> </li> <li>Settings under <code>store.neptune.s3</code> for upload of large files (&gt;150MB uncompressed) (optional):<ul> <li><code>store.neptune.s3.bucketNameOrAPAlias</code>: Name of bucket or access point for S3 bulk load</li> <li><code>store.neptune.s3.iamRoleArn</code>: ARN of role under which neptune cluster loads from S3</li> <li><code>store.neptune.s3.bulkLoadThresholdInMb</code>: Load threshold in MB for GSP access, if graph data greater than S3 upload is used (default: 150)</li> <li><code>store.neptune.s3.bulkLoadParallelism</code>: Degree of parallelism for neptune S3 bulk loader (LOW (default), MEDIUM, HIGH, OVERSUBSCRIBE)</li> </ul> </li> </ul> </li> <li>VIRTUOSO:<ul> <li><code>store.virtuoso.host</code>: host of virtuoso backend (i.e. localhost)</li> <li><code>store.virtuoso.port</code>: http port of virtuoso backend (i.e. 8080)</li> <li><code>store.virtuoso.databasePort</code>: database port of virtuoso backend (i.e. 1111)</li> <li><code>store.virtuoso.ssl-enabled</code>: flag if ssl (https) is enabled (default: false)</li> <li><code>store.virtuoso.username</code>: Username (optional)</li> <li><code>store.virtuoso.password</code>: Password (optional)</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#cmemc","title":"cmemc","text":"<ul> <li><code>dataset list</code>|<code>delete command</code><ul> <li>option <code>--project</code> is removed</li> <li>Please use <code>--filter project XXX</code> instead</li> </ul> </li> <li><code>admin status</code> command<ul> <li>in case you piped the normal output of this command and reacted on that, you need to use the <code>--key</code> command now</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/","title":"Corporate Memory 23.1.3","text":"<p>Corporate Memory 23.1.3 is the second patch release in the 23.1 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>Support for global variables in dataset and task parameters.</li> <li>Extensions to the Python Plugin API, including autocompleted parameter type and password parameter type.</li> </ul> </li> <li>Explore:<ul> <li>Workspaces are now selectable at runtime.</li> <li>Enhanced editing capabilities in the EasyNav editor.</li> </ul> </li> <li>Automate:<ul> <li>New <code>admin user</code> command group for managing user accounts in the Keycloak CMEM realm.</li> </ul> </li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataManager configuration has changed and needs to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v23.1.3</li> <li>eccenca DataIntegration v23.1.2</li> <li>eccenca DataIntegration Python Plugins v3.0.0</li> <li>eccenca DataManager v23.1.5</li> <li>eccenca Corporate Memory Control (cmemc) v23.1.3</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration-v2312","title":"eccenca DataIntegration v23.1.2","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v23.1, featuring numerous enhancements, bug fixes, and deprecations. This release introduces global variables support, Python Plugin API extensions, improved handling of replaceable datasets, and much more.</p> <p>v23.1.2 of eccenca DataIntegration ships following fixes:</p> <ul> <li>Saving a transform or linking rule with an operator that references a project resource fails.</li> <li>Cannot read large Excel files from S3.</li> </ul> <p>v23.1.1 of eccenca DataIntegration ships following fixes:</p> <ul> <li>Fixed various vulnerabilities by upgrading affected libraries.</li> <li>Workflows using the \u201cSPARQL Update query\u201d operator fail with \u201cNeed non-empty resource manager\u201d errors.</li> <li>use cmem-plugin-base 3.1.0 instead of RC1</li> <li>remove some unused base image packages</li> </ul> <p>v23.1 of eccenca DataIntegration adds the following new features:</p> <ul> <li>Support for global variables:<ul> <li>Dataset and task parameters can be set to Jinja templates.</li> <li>Templates may access configured global variables. User-defined variables will be added later.</li> <li>Global variable resolution is supported by the \u2018Evaluate template\u2019 transform operator.</li> <li>Disabled by default.</li> </ul> </li> <li>Extensions to the Python Plugin API:<ul> <li>Autocompleted parameter types may declare dependent parameters.</li> <li>Password plugin parameter type.</li> <li>Custom parameter types can be registered</li> <li>For details, see changelog of the cmem-plugin-base module.</li> </ul> </li> <li>REST endpoint to search for properties in the global vocabulary cache:<ul> <li>GET /api/workspace/vocabularies/property/search</li> <li>Warn of invisible characters in input fields and offer action to remove them from the input string.</li> </ul> </li> <li>Autocompletion of graph parameters.</li> <li>Auto-completion support to linking rule \u2018link type\u2019 parameter.</li> <li>Improve handling of replaceable datasets:<ul> <li>Datasets that can be replaced/configured in a workflow at API request time can be set in the workflow editor.</li> <li>This allows for the execution of workflows with mock data, which has not been possible with \u2018Variable dataset\u2019 tasks.</li> </ul> </li> <li>Allow to config datasets as read-only to prevent accidentally writing into them.</li> <li>New resource endpoints to replace the deprecated resource endpoints. See deprecation section for more details.</li> <li>Allow to force start activity.</li> <li>Rewritten linking evaluation view.</li> </ul> <p>v23.1 of eccenca DataIntegration introduces the following changes:</p> <ul> <li>Check token expiration (&gt; 5s left) before sending a request to prevent unnecessary request retries.</li> <li>\u2018Concatenate\u2019 and \u2018Concatenate multiple values\u2019 transformer:<ul> <li>In \u2018glue\u2019 parameter value support <code>\\t</code>, <code>\\n</code> and <code>\\\\</code> as escaped characters.</li> </ul> </li> <li>Indexing of levenshtein comparisons can be configured now.</li> <li>Rename \u2018Constant\u2019 comparison operator to \u2018Constant similarity value\u2019.</li> <li>Neo4j improvements:<ul> <li>Support for paths when reading entities (forward and backward operators).</li> <li>Using a relation at the end of a path will return the URI of the node.</li> <li>The <code>#id</code> special path will return the internal node id.</li> </ul> </li> <li>CSV dataset auto-configuration now supports detecting more encodings for the Charset parameter.</li> <li>Changed search behavior in most places to search after typing stops instead of needing to hit the ENTER key:<ul> <li>In the \u2018Create new item\u2019 dialog hitting the Enter key now has the same effect as clicking the \u2018Add\u2019 button.</li> </ul> </li> <li>Show value type label primarily instead of ID.</li> <li>Show default URI pattern example in a object rule mapping form when the source path is non-empty.</li> <li>Response body of a failed REST operator request is also added to the workflow report in addition to being logged.</li> <li>Linking execution report has a warning message when the link limit was reduced because of the config of <code>linking.execution.linkLimit.max</code>.</li> <li>Disable streaming in \u2018Parse JSON\u2019 operator, so backward paths can be used against it.</li> <li>Improved online documentation of many rule operators:<ul> <li>Distance measures: Added information if a measure is either boolean, normalized or unbounded.</li> <li>Distance measures: Clarified what happens with multiple values for single value measures.</li> <li>Transformers, Distance measures and Aggregators: Added examples</li> </ul> </li> </ul> <p>v23.1 of eccenca DataIntegration ships following fixes:</p> <ul> <li>Layout breaks on small screens on detail pages of the workspace.</li> <li>Mapping suggestion list is empty when there is no matching response even though source paths exist.</li> <li>Active Learning shows incorrect entity values.</li> <li>Add notes dialog keeps focus when workflow is executed and running.</li> <li>Race condition in project/task tag selection.</li> <li>Dataset auto-configure parameter changes not set for parameters that support auto-completion.</li> <li>Label and description of existing root/object rules cannot be changed.</li> <li>DI writes invalid XML, if the last segment of a URI starts with a number.</li> <li>Optimize peak endpoint if only one path is requested.</li> <li>Python Plugin Environment: package dependencies can not update the base requirements anymore.</li> <li>Spinner is being shown eternally when no comparison pairs have been found in the link learning.</li> <li>Value path auto-completion can suggest wrong paths if backward paths exist in the paths cache.</li> <li>Show spinner while transform examples are requested from the backend.</li> <li>Abort a not fully consumed S3 input stream instead of closing it which leads to warnings.</li> <li>Date parser fails when no input/output pattern is selected even though an alternative input/output pattern is given.</li> <li>Dependent parameter auto-completion using default values of other parameters.</li> <li>Support replaceable/variable datasets in nested workflows.</li> <li>Display info message when a parameter is disabled because it depends on other parameters to be set.</li> <li>\u2018Fix URI\u2019 operator trims the URI before fixing it and tries better to maintain the original URI with only the invalid characters encoded.</li> <li>Task completion message is shown without executing the transformation.</li> <li>Evaluation in mapping rule editor does not work when inside object mappings.</li> <li>Show error message when project import fails because of errors detected in the backend instead of closing the project import modal.</li> <li>Linking editor evaluation toolbar component issues.</li> <li>Levensthein indexing slow if combined conjunctively.</li> <li>Transform execution tab layout issues.</li> </ul> <p>v23.1 of eccenca DataIntegration introduced the following deprecations:</p> <ul> <li>Resource endpoints:<ul> <li>All resources endpoints that have the file path (<code>workspace/projects/:project/resources/:name</code>) encoded in the URL path are now deprecated. The files endpoints using a query parameter for the path should be used now.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration-python-plugins-v300","title":"eccenca DataIntegration Python Plugins v3.0.0","text":"<p>Corporate Memory v23.1 includes the DataIntegration Python Plugins support in version 3.0.0.</p> <p>v3.0.0 of eccenca DataIntegration Python Plugins adds the following new features:</p> <ul> <li> <p>Autocompleted parameter types may declare dependent parameters. For instance, a parameter <code>city</code> may declare that its completed values depend on another parameter \u2018country\u2019:</p> <pre><code>class CityParameterType(StringParameterType):\n    autocompletion_depends_on_parameters: list[str] = [\"country\"]\n    def autocomplete(self,\n                     query_terms: list[str],\n                     depend_on_parameter_values: list[Any],\n                     context: PluginContext) -&gt; list[Autocompletion]:\n       # 'depend_on_parameter_values' contains the value of the country parameter\n       return ...\n</code></pre> </li> <li> <p>Password plugin parameter type. Passwords will be encrypted in the backend and not shown to users:</p> <pre><code>@Plugin(label=\"My Plugin\")\nclass MyTestPlugin(TransformPlugin):\ndef __init__(self, password: Password):\n    self.password = password\n\n# The decrypted password can be accessed using:\nself.password.decrypt()\n</code></pre> </li> <li> <p>Custom parameter types can be registered. See implementation of <code>PasswordParameterType</code> for an example.</p> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-datamanager-v2315","title":"eccenca DataManager v23.1.5","text":"<p>We are excited to announce the latest update to DataManager v23.1, which introduces new features, improvements and bug fixes. This release brings enhancements to workspaces, editing capabilities in the EasyNav editor, and updates to the authentication system.</p> <p>v23.1.5 of eccenca DataManager ships following fixes:</p> <ul> <li>Fixed download of query result in query editor.</li> <li>Setting the defaultGraph of a explore workspace configuration no longer prevents the Navigation box from loading.</li> <li>Fixes in the LinkRules modules: Result-Details, Rule-Deletions, Property-Search</li> </ul> <p>v23.1.4 of eccenca DataManager ships following changes:</p> <ul> <li>Switch from iframe to redirect based login view.<ul> <li>Known issues: Interactions after the timeout do not always trigger a reload and simply shows error messages or empty results. Using the navigation bar triggers a reload.</li> </ul> </li> </ul> <p>v23.1.3 of eccenca DataManager ships following fixes:</p> <ul> <li>use latest debian:bullseye-20230411-slim base image</li> <li>use wget instead of curl</li> </ul> <p>v23.1.2 of eccenca DataManager was a redacted build due to incomplete merge.</p> <p>v23.1.1 of eccenca DataManager ships following fixes:</p> <ul> <li>Fixes link rules creation dialogue setting a target property.</li> </ul> <p>v23.1 of eccenca DataManager adds the following new features:</p> <ul> <li>Workspaces are selectable at runtime.</li> <li>Routes can include a workspace selection.</li> <li>Added Editing capabilities to the EasyNav editor.</li> </ul> <p>v23.1 of eccenca DataManager introduces the following changes:</p> <ul> <li>Configuration is now fully retrieved from DataPlatform, the included Spring Boot based backend is solely delivering the javascript frontend.</li> <li>The configuration can be changed at runtime using a frontend in the <code>/admin</code> Module. Changes are visible with the next full browser reload.</li> <li>Authentication is now based on the OAuth2 Code Flow.</li> </ul> <p>v23.1 of eccenca DataManager ships following fixes:</p> <ul> <li>Removed session token from URL.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataplatform-v2313","title":"eccenca DataPlatform v23.1.3","text":"<p>We\u2019re excited to announce the latest update to DataPlatform v23.1, featuring significant improvements in caching, user rights management, and workspace configuration. This update also includes various bug fixes and the removal of deprecated properties. Here\u2019s an overview of the changes:</p> <p>v23.1.3 of eccenca DataPlatform ships following fixes:</p> <ul> <li>Fix wrong calculation of write graph access under certain conditions.</li> </ul> <p>v23.1.2 of eccenca DataPlatform ships following changes:</p> <ul> <li>DP/Infinispan: session timeout increased to 10h</li> <li>Login: switch from iframe to redirect based flow</li> </ul> <p>v23.1.1 of eccenca DataPlatform ships following fixes:</p> <ul> <li>docker image: use latest debian:bullseye-20230411-slim base image</li> <li>docker image: wget instead of curl</li> </ul> <p>v23.1 of eccenca DataPlatform adds the following new features:</p> <ul> <li>Added ability to use dynamic access conditions</li> <li>Added graph for infos about logged in users (iri, login):<ul> <li>Can be (de)activated using property <code>authorization.userInfoGraph.active</code> (default: true)</li> </ul> </li> <li>Workspace Selection and Configuration:<ul> <li>Activate OAuth 2.0 client role permanently</li> <li>Redirect login page to (exactly) one configured resource provider</li> <li>REST endpoints for workspace configuration</li> </ul> </li> </ul> <p>v23.1 of eccenca DataPlatform introduces the following changes:</p> <ul> <li>Integrate infinispan as sole cache provider:<ul> <li>Enables clustering of DataPlatform instances<ul> <li>clustering can be activated by <code>spring.cache.infinispan.mode=CLUSTER</code></li> </ul> </li> <li>Removed property <code>files.maintenanceCron</code> (housekeeping done by infinispan)</li> <li>Added property <code>files.storageDirectory</code> for configuring shared directory between multiple DataPlatform instances</li> <li>Replaced property <code>proxy.cacheInvalidationCron</code> with <code>proxy.cacheExpiration</code> (no scheduled flush anymore but cache expiration as default)</li> </ul> </li> <li>Changed logic of resolving user rights through access conditions - performance optimized</li> </ul> <p>v23.1 of eccenca DataPlatform ships following fixes:</p> <ul> <li>Prevent injection of formulas in Excel/CSV exports</li> <li>Diagnostic store operations / query rewrite log on logging topic <code>com.eccenca.elds.backend.sparql.query.diagnostic</code> - must be set to TRACE:<ul> <li>Activated update result statistics in existing query result logger</li> </ul> </li> <li>Missing access condition action resource for EasyNav added</li> </ul> <p>v23.1 of eccenca DataPlatform removed the following features and configurations:</p> <ul> <li>Deprecated properties under authorization.accessConditions<ul> <li><code>authorization.accessConditions.graph</code>: used graph is always the default graph from bootstrap</li> <li><code>authorization.accessConditions.url</code>: url as source for access condition not supported anymore</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-corporate-memory-control-cmemc-v2313","title":"eccenca Corporate Memory Control (cmemc) v23.1.3","text":"<p>We are excited to announce the latest updates to eccenca Corporate Memory Control v23.1, which brings new features and improvements. This release introduces new command functionalities, configuration options, and a change in the project structure.</p> <p>v23.1.3 of eccenca Corporate Memory Control introduces the following security updates:</p> <ul> <li>upgrade base image to python:3.11.4-slim-bullseye</li> <li>upgrade dependencies (esp. certifi)</li> </ul> <p>v23.1.2 of eccenca Corporate Memory Control introduces the following fixes:</p> <ul> <li>broken installation due to <code>urllib3</code> dependency<ul> <li><code>urllib3&gt;=2</code> was released 2023-04-26 but is broken with this error: <code>ImportError: cannot import name 'appengine' from 'urllib3.contrib'</code></li> <li>cmemc requested any version and not <code>^1.26.15</code> of this library, which resulted in broken installations with pip beginning from 2023-04-26</li> <li>quick fix to solve this without updating cmemc: <code>pip install urllib3==1.26.15</code> in the cmemc virtual env</li> </ul> </li> </ul> <p>v23.1.1 of eccenca Corporate Memory Control introduces the following changes:</p> <ul> <li>remove some unneeded packages from docker image</li> <li>switch to python 3.11.3 base image and tests</li> </ul> <p>v23.1 of eccenca Corporate Memory Control adds the following new features:</p> <ul> <li><code>admin status</code> command:<ul> <li>option <code>--exit-1</code> to specify, when to return non-zero exit code</li> <li>currently set to <code>never</code>, this will be changed to <code>always</code> in the future</li> </ul> </li> <li><code>admin user</code> command group:<ul> <li><code>create</code> command - add a user account to the keycloak CMEM realm</li> <li><code>delete</code> command - remove a user account from the keycloak CMEM realm</li> <li><code>list</code> command - list user accounts in the keycloak CMEM realm</li> <li><code>password</code> command - change the accounts password</li> <li><code>update</code> command - change a user account in the keycloak CMEM realm</li> </ul> </li> <li>optional <code>KEYCLOAK_BASE_URI</code> config environment</li> <li>optional <code>KEYCLOAK_REALM_ID</code> config environment</li> </ul> <p>v23.1 of eccenca Corporate Memory Control introduced the following deprecations:</p> <ul> <li><code>admin status</code> command <code>--exit-1</code> option default<ul> <li>currently set to <code>never</code>, this will be changed to <code>always</code> in a future release</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v23.1 into DataIntegration v22.2 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v22.2 can be imported into DataIntegration v23.1.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration","title":"eccenca DataIntegration","text":"<ul> <li>Resource endpoints:<ul> <li>All resources endpoints that have the file path (<code>workspace/projects/:project/resources/:name</code>) encoded in the URL path are now deprecated.</li> <li>Use corresponding endpoints starting with <code>workspace/projects/:project/files</code> instead, using a query parameter for the file path.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration-python-plugins","title":"eccenca DataIntegration Python Plugins","text":"<p>The signature of the autocomplete function has been changed. All autocomplete implementations need to be updated to the following signature:</p> <pre><code>def autocomplete(self, query_terms: list[str], depend_on_parameter_values: list[Any], context: PluginContext) -&gt; list[Autocompletion]\n</code></pre> <p>Parameters using the old signature will continue to work for one release, but a warning will be printed in the log.</p> <p>The same applies to the label function that has been updated to the following signature:</p> <pre><code>def label(self, value: str, depend_on_parameter_values: list[Any], context: PluginContext) -&gt; Optional[str]\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-datamanager","title":"eccenca DataManager","text":"<ul> <li>A manual migration for the graph based configuration of the EasyNav configuration and the graph list configuration of the explore module is necessary.</li> <li>A manual migration for the <code>.yml</code> based DataManager configuration is necessary.</li> <li>The new web based configuration tool can be used to migrate, create and manage your DataManager (workspace) configuration</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataplatform","title":"eccenca DataPlatform","text":"<ul> <li>Deprecated properties under <code>authorization.accessConditions</code> have been removed. The used graph is always the default graph from bootstrap, and URL as a source for access conditions is not supported anymore.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/","title":"Corporate Memory 23.2.1","text":"<p>Corporate Memory 23.2.1 is the first patch release in the 23.2 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>Support for user managed project variables in dataset and task parameters.</li> <li>All new UIs for transformation evaluation and reference links.</li> </ul> </li> <li>Explore:<ul> <li>New feature in Easynav like<ul> <li>nodes context menu,</li> <li>long label support,</li> <li>advanced graph selection dialog and</li> <li>automatic node layout.</li> </ul> </li> </ul> </li> <li>Automate:<ul> <li>New <code>admin client</code> command group for managing client accounts in the Keycloak CMEM realm.</li> </ul> </li> </ul> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v23.2.1</li> <li>eccenca DataIntegration v23.2.1</li> <li>eccenca DataIntegration Python Plugins v4.1.0</li> <li>eccenca DataManager v23.2</li> <li>eccenca Corporate Memory Control (cmemc) v23.2</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/#eccenca-dataintegration-v2321","title":"eccenca DataIntegration v23.2.1","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v23.2, featuring numerous enhancements, bug fixes, and deprecations.</p> <p>v23.2.1 of eccenca DataIntegration ships the following improvements and fixes:</p> <ul> <li>Added overview listing all available keyboard shortcuts, available by <code>?</code> key, or from the user menu.</li> <li>Improved vocabulary (needs to be installed separately).</li> <li>The file API will set the content type based on the file extension:<ul> <li>For instance, for a file ending in <code>.json</code> the <code>Content-Type</code> header will be set to <code>application/json</code>.</li> </ul> </li> <li>Upgraded several libraries to fix vulnerabilities.</li> </ul> <p>v23.2 of eccenca DataIntegration adds the following new features:</p> <ul> <li>User-defined project variables:<ul> <li>Can be used in dataset and task parameters and in the template transform operator.</li> <li>Variables may use templates that access other preceding project variables or globally configured variables.</li> </ul> </li> <li>All new transform evaluation UI.</li> <li>All new reference links view.</li> <li>Extensions to transform rule and linking rule editors:<ul> <li>Support for setting a language filter for a path operator conveniently without having to use the language filter syntax.</li> <li>Partial linking and transform rule (tree) evaluation.</li> </ul> </li> <li>Support fixing tasks that have failed loading:<ul> <li>Allow a user to reload a task. The user may change the original parameters of the task.</li> <li><code>POST /api/workspace/projects/{projectId}/reloadFailedTask</code> endpoint that reloads a task with optionally updated parameter values.</li> <li><code>GET /api/workspace/projects/{projectId}/failedTaskParameters/{taskId}</code> endpoint that fetches the original parameter values of a failed task.</li> </ul> </li> <li>Added \u201cConcatenate pairwise\u201d transform operator.</li> <li>Make transform suggestion matching link spec / workflow customizable.</li> <li>API extensions:<ul> <li>Transform evaluation endpoint: <code>/transform/tasks/{projectId}/{transformTaskId}/rule/{ruleId}/evaluated</code></li> <li>Added endpoints for uploading and downloading files of datasets.</li> <li>REST endpoint to fetch dataset characteristics.<ul> <li><code>GET /api/workspace/projects/(projectId)/datasets/{datasetId}/characteristics</code></li> </ul> </li> <li>API endpoint to fetch IDs of file based datasets.<ul> <li><code>/api/core/datasets/resourceBased</code></li> </ul> </li> <li>REST endpoint to copy an arbitrary linking task to the matching linking task that will be used in the mapping suggestion.<ul> <li><code>POST /ontologyMatching/replaceOntologyMatchingLinkSpec</code></li> <li>Both <code>matching.external.projectId</code> and <code>matching.external.linkSpecId</code> must be configured</li> </ul> </li> <li>REST endpoint that generates an ontology matching project and linking tasks based on a specific transformation task.<ul> <li><code>POST /ontologyMatching/generateMatchingLinkRule</code></li> </ul> </li> </ul> </li> </ul> <p>v23.2 of eccenca DataIntegration introduces the following changes:</p> <ul> <li>Close user menu automatically.</li> <li>Linking rule config:<ul> <li>Add \u2018Inverse link type\u2019 parameter that defined a URI that is generated from the target to the source resource, i.e. the inverse of the \u2018link type\u2019 parameter.</li> <li>Add \u2018Is reflexive\u2019 parameter that when enabled does not link resources with themselves.</li> </ul> </li> <li>In a workflow datasets with schema-less inputs, e.g. workflows, other datasets, are not considered to be outputs datasets anymore.</li> <li>Variable workflow API:<ul> <li>Support uploading large input files via multipart/form-data request</li> <li>Support custom mime type \u201capplication/x-plugin-\u201d in CONTENT-TYPE or ACCEPT in order to support all file based dataset plugins. <li>Support query parameters \u2018config-dataSourceConfig-\u2019 and \u2018config-dataSinkConfig-\u2019 to configure dataset parameters of the data source and sink. <li>Added read-only / uriProperty dataset attributes to endpoint responses.</li> <li>forward cmem-plugin-base to v4.1.0, via base image v2.2.0</li> <li>The handling of errors in transform rules has been aligned with what is already shown in the evaluation:<ul> <li>If a nested operator throws a validation error (e.g., if the input value is not a number for numeric operators), this no longer leads to a failure of the entire rule.</li> <li>The error will be added to the execution report.</li> <li>Failed operators will return no value.</li> </ul> </li> <p>v23.2 of eccenca DataIntegration ships the following fixes:</p> <ul> <li>Excel plugins are not available (CMEM-5088).</li> <li>Transform/linking tasks with operators that use projects resources, e.g. Excel transform, cannot be copied/cloned to other projects (CMEM-5065).</li> <li>Using a target property ending in <code>/valueOf</code> for a value mapping rule breaks the mapping editor (CMEM-5059).</li> <li>Re-configured pure input dataset in a workflow should not be seen as an output dataset (CMEM-5058).</li> <li>Prioritized and blocking activities should be run in a fork join pool as well (CMEM-4856).</li> <li>Workflow(s) info endpoints return error (500) when a workflow is invalid (CMEM-5099):<ul> <li>There is a \u2018warnings\u2019 property in the returned JSON that describes which information is missing and why.</li> </ul> </li> <li>Remove broken legacy navigation menu from client error template that is shown e.g. when a user is not authorized to use DI (CMEM-4977).</li> <li>Mapping editor: the path suggestion that exactly matches the search query is not shown in the list of suggestions (CMEM-5084).</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/#eccenca-dataintegration-python-plugins-v410","title":"eccenca DataIntegration Python Plugins v4.1.0","text":"<p>Corporate Memory v23.2 includes the DataIntegration Python Plugins support in version 4.1.0.</p> <p>v4.1.0 of eccenca DataIntegration Python Plugins adds the following new features:</p> <ul> <li>use <code>post_resource</code> api in <code>write_to_dataset</code> function to update dataset file resource</li> <li>use cmempy 23.2</li> <li>upgrade dependencies</li> <li>enforce usage of Python 3.11</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/#eccenca-datamanager-v232","title":"eccenca DataManager v23.2","text":"<p>We are excited to announce the latest update to DataManager v23.2, which introduces new features, improvements and bug fixes.</p> <p>v23.2 of eccenca DataManager adds the following new features:</p> <ul> <li>Added advanced options (\u201cInverse Linking Property\u201d and \u201cIrreflexive Linking\u201d) in a Link rule setup Dialog.</li> <li>Automatic layouts in Easynav.</li> <li>Configurable defaultTimeout for queries.</li> </ul> <p>v23.2 of eccenca DataManager ships the following changes:</p> <ul> <li>Auto-close user menu.</li> <li>Delete Resource Dialog was overhauled, and can now trigger a <code>shui:onDeleteUpdate</code> query.</li> <li>Adjusted the payload when saving a custom or the default workspace config so that it has only the modified values.</li> <li>Better error message for empty string after stopwords remove in the Easynav module.</li> <li>Allow stale facets initially to improve loading timing. After stale facets are fetched, actual ones will be requested in background.</li> <li>Easynav:<ul> <li>Long labels are shown in two lines.</li> <li>Save option for saving only the ontology changes.</li> <li>Context menu on resource nodes is re-enables.</li> <li>Query based entry into Easynav via the \u201cgraph\u201d tab in explore.</li> <li>Context graph name is shown in breadcrumbs.</li> <li>Detailed filtering in the visualization catalog.</li> <li>Search bar centers on the node.</li> <li>All possible connections are shown for a node, not just the ones with data.</li> <li>Rich graph selection widget.</li> </ul> </li> <li>Pathbuilder:<ul> <li>Added subpaths option if <code>hierarchyEnable</code> is true.</li> <li>Added server search for subject and predicate.</li> <li>Added support for longer labels in resource selector.</li> <li>Added helper texts for subject selection and subpaths selection.</li> <li>Added depictions for subjects and predicates.</li> <li>Added different colors for subpaths.</li> </ul> </li> <li>Explore/Properties View:<ul> <li>Geo-coordinates are only shown, when a map server is configured.</li> <li>The wording of the error message when no selected graph found is changed.</li> </ul> </li> </ul> <p>v23.2 of eccenca DataIntegration ships the following fixes:</p> <ul> <li>Replaced an obsolete <code>LabelResolutionApi</code> by RTK-Query to catch 401 errors (lost session/authorization) and bring cache (CMEM-4979).</li> <li>Restored notifications toast (CMEM-4979).</li> <li>Enabled deletion of large tables (CMEM-4898).</li> <li>Added <code>mapServer</code> options in explore config (CMEM-4926).</li> <li>Error message shows when trying to upload broken file to the graph (CMEM-4704).</li> <li>Fixed error with lost graph list tabs on workspace switch (CMEM-5087).</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/#eccenca-dataplatform-v2321","title":"eccenca DataPlatform v23.2.1","text":"<p>We\u2019re excited to bring you the latest update to DataPlatform v23.2, featuring numerous enhancements, bug fixes, and deprecations.</p> <p>v23.2.1 of eccenca DataPlatform ships following fixes:</p> <ul> <li>Bootstrap Data: Removed obsolete DataIntegration vocabulary from shape catalog</li> </ul> <p>v23.2 of eccenca DataPlatform adds the following new features:</p> <ul> <li>Bootstrap Data: allow <code>sh:order</code> for <code>sh:NodesShape</code>.</li> <li>Dynamic access conditions backend functionality.</li> <li>Add optional facets request param for getting (possibly) stale cached values.</li> <li>Provenance metadata for Easynav visualizations.</li> <li>Added property <code>hierarchyEnable</code> to Link Rule Modul.</li> <li>Added properties for <code>mapServer</code> to Explore Modul.</li> <li>Add node shape property shape for <code>onDeleteUpdate</code> of resource.</li> <li>Added DI vocabulary.</li> <li>Added provenance metadata to visualization catalogue entries.</li> </ul> <p>v23.2 of eccenca DataPlatform ships the following changes:</p> <ul> <li>PropertyUsage endpoint delivers language tags.</li> <li>Make order of node shapes editable.</li> <li>Resolving of depictions for resource along node shape order.</li> <li>Viewing labels of NodeShapes as <code>SHACL.Name</code> - changed from <code>RDFS.Label</code>.</li> <li>Resource deletion applies Symmetric Concise Bound Description including incoming links.</li> <li>Show full list of access conditions for users with access condition management action.</li> <li>Add configuration option to switch between ontodia / graph tab in explore view.</li> </ul> <p>v23.2 of eccenca DataPlatform ships following fixes:</p> <ul> <li>Prevent upload of incorrect file URIs (CMEM-4360).</li> <li>Reintroduce <code>defaultTimeout</code> for UI queries (CMEM-5100).</li> <li>Make property <code>defaultGraph</code> able to be overwritten in custom workspace (CMEM-4902).</li> <li>Fix labels of node shapes from <code>RDFS.Label</code> to <code>SHACL.Name</code> (CMEM-4743).</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/#eccenca-corporate-memory-control-cmemc-v232","title":"eccenca Corporate Memory Control (cmemc) v23.2","text":"<p>v23.2 of eccenca Corporate Memory Control adds the following new features:</p> <ul> <li><code>admin user password</code> command:<ul> <li>option <code>--request-change</code> added, to send a email to user to reset the password</li> </ul> </li> <li><code>dataset create</code> command:<ul> <li>add <code>readOnly</code> and <code>uriProperty</code> keys for the <code>-p/--parameter</code> option</li> </ul> </li> <li><code>admin client</code> command group:<ul> <li><code>list</code> command - list client accounts</li> <li><code>open</code> command - Open clients in the browser</li> <li><code>secret</code> command - Get or generate a new secret for a client account</li> </ul> </li> <li><code>project create</code> command:<ul> <li>new option <code>--from-transformation</code> to create a mapping suggestion project</li> </ul> </li> </ul> <p>v23.2 of eccenca Corporate Memory Control introduces the following changes:</p> <ul> <li><code>dataset upload</code> command:<ul> <li>use new endpoint which is aware of read-only datasets</li> </ul> </li> <li><code>workflow io</code> command:<ul> <li>use of extended io endpoint</li> <li>allows for uploading bigger files</li> <li>allows for more input and output mimetypes</li> <li>change default output to JSON</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-2/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v23.2 into DataIntegration v23.1 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v23.1 can be imported into DataIntegration v23.2.</p> <p>No component specific migrations are required going from 23.1 to 23.2.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/","title":"Corporate Memory 23.3.2","text":"<p>Corporate Memory 23.3.2 is the second patch release in the 23.3 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Explore and Author:<ul> <li>new charts catalog module added, which allows for defining BI widgets / charts which can be integrated into shapes</li> <li>preview release of our generative AI / LLM based Ontology and Query Assistant</li> </ul> </li> <li>Build:<ul> <li>operate BUILD like never before by using the new keyboard shortcuts (press \u201c?\u201d in the build module to learn the details)</li> <li>several improvements to the workflows view: create new datasets and other workflow-operators in place, dependencies and execution order is now explicitly modeled, show schema or ports</li> </ul> </li> <li>Automate:<ul> <li>new <code>project variable</code> command group plus several addition to existing commands</li> </ul> </li> </ul> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataIntegration v23.3.2</li> <li>eccenca DataManager v23.3.1</li> <li>eccenca DataPlatform v23.3.1</li> <li>eccenca Corporate Memory Control (cmemc) v23.3.0</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#eccenca-dataintegration-v2332","title":"eccenca DataIntegration v23.3.2","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v23.3, featuring numerous enhancements, bug fixes, and deprecations.</p> <p>v23.3.2 of DataIntegration ships the following fixes:</p> <ul> <li>Entities with values larger than 65k cannot be serialized.</li> <li>JSON property path evaluation fails for missing key.</li> </ul> <p>v23.3.1 of DataIntegration ships the following improvements:</p> <ul> <li>Workflow operator validates XML datasets against a provided XML Schema.</li> <li>Support entering a custom ID when cloning a project or project task.</li> <li>\u201cEvaluate Template\u201d operator has a new option for evaluating the template on the entire input set at once.</li> </ul> <p>v23.3.1 of DataIntegration ships the following fixes:</p> <ul> <li>Long task parameter values cannot be fully seen in task config preview.</li> <li>Project export does not fail if project files cannot be read.</li> <li>Unexpected inputs of a node are not executed anymore.</li> <li>Transform report does not count entities in child mappings.</li> <li>Rule operator parameter auto-complete default values do not have a label when creating a new operator.</li> </ul> <p>v23.3.0 of DataIntegration adds the following new features:</p> <ul> <li>plugin base library updated to v4.3.0 (changelog)</li> <li>Support for custom plugin icon.</li> <li>New <code>distinct by</code> Workflow operator that removes duplicated entities based on a user-defined path.</li> <li>Endpoint to download the DataIntegration vocabulary.</li> <li>Mappings allow custom target types<ul> <li>If the new custom data type is selected, a new input field type allows the user to enter a type URI.</li> </ul> </li> <li>Added variables widget to Workflow view.</li> <li>Added hotkey access to actions in DI workspace.</li> <li>Workflow editor:<ul> <li>Validate ports and connections and show warnings for found issues.</li> <li>Have menu option to show input/output schema for ports that either expect or output a fixed schema.</li> <li>Support dependency connections between workflow nodes to specify non-data execution dependencies.</li> </ul> </li> </ul> <p>v23.3.0 of DataIntegration introduces the following changes:</p> <ul> <li>The threshold field for distance measures has been improved:<ul> <li>For boolean distance measures, the threshold is not shown as it has no effect.</li> <li>For normalized and unbound measures, the range of allowed values as well as an improved tooltip has been added.</li> <li>A error message is shown if the entered threshold is not valid for the given distance measure.</li> </ul> </li> <li>Parse JSON operator now works with multiple entities.</li> <li>Updating or deleting project variables will update all affected tasks transactionally:<ul> <li>The user is not allowed to delete a variable that is used in a task parameter template.</li> <li>The user is not allowed to change a variable to a value that violates restrictions in a task that uses it<ul> <li>If the task parameter has a specific type (such as integer) and the template now evaluates to an incompatible type.</li> <li>If the task imposes other restrictions on a parameter (for instance, does not allow values below 0).</li> </ul> </li> </ul> </li> <li>base image switch to bookworm (python:3.11-slim-bookworm)</li> <li>If a path uses a property filter, an error will be thrown on writing data to a Knowledge Graph, if the property is not a valid URI.</li> <li>Support expanding all rule trees after expanding all rows in the transform evaluation.</li> <li>Change input ports definition of <code>Upload File to Knowledge Graph</code> operator from an empty fixed schema to variable inputs to make it better compatible.</li> <li>Transform page: When switching between tabs, e.g. from mapping editor to evaluation tab, the currently active rule stays active.</li> <li>JSON <code>#text</code> path now returns the formatted JSON as documented.</li> </ul> <p>v23.2 of DataIntegration ships the following fixes:</p> <ul> <li>Project variables widget showing the variables of the wrong project.</li> <li>Python package uninstall is not able to remove crucial packages anymore.</li> <li>Reload failed tasks after project/workspace reload.</li> <li>Reloaded failed tasks are missing the original label and description.</li> <li>Fix property pair alignment.</li> <li>Hotkey and quick search modal are not always shown on top of other modals.</li> <li>Testing for invalid documents downloaded from GDocs fixed and adapted to new behavior of wrong requests.</li> <li>Error message in \u2018SPARQL endpoint\u2019 plugin to mention prohibited URL redirect to a different protocol.</li> <li><code>JDBC endpoint</code> dataset: Setting the user via JDBC URL while leaving the user parameter blank does not work.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#eccenca-datamanager-v2331","title":"eccenca DataManager v23.3.1","text":"<p>We are excited to announce the latest update to DataManager v23.3, which introduces new features, improvements and bug fixes.</p> <p>v23.3.1 of DataManager ships the following fixes:</p> <ul> <li>Workspace configuration: explore <code>defaultGraph</code> is now correctly evaluated.</li> </ul> <p>v23.3.0 of DataManager adds the following new features:</p> <ul> <li>Implemented Charts module with Shacl integration.</li> <li>Added option to show edges without the shapes on the EasyNav canvas and in the sidebar, i.e. the node expansion is still shaped.</li> <li>Query module allows simple query creation with an form assisted dialogue.</li> </ul> <p>v23.3.0 of DataManager ships the following changes:</p> <ul> <li>Internal:<ul> <li>Query module is migrated from Redux to a Context storage.</li> <li>Query module is extracted to a separate common component.</li> </ul> </li> <li>ResourceSelect doesn\u2019t request options anymore if they have already been requested earlier.</li> <li><code>shui:listQuery</code> allows usage of the <code>{{username}}</code> placeholder, which is replaced by the name (i.e.not the IRI) of the logged in user.</li> </ul> <p>v23.3.0 of DataManager ships the following fixes:</p> <ul> <li>Fixed broken navigation (workspace part of URL was lost).</li> <li>CMEM Manual Testing 23.2 e2e - Don\u2019t do redundant redirects in the Module context.</li> <li>Navigation tree in the Thesaurus module was collapsed after a subitem select.</li> <li>Use more space for visualization catalogue if available.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#eccenca-dataplatform-v2331","title":"eccenca DataPlatform v23.3.1","text":"<p>We\u2019re excited to bring you the latest update to DataPlatform v23.3, featuring numerous enhancements, bug fixes, and deprecations.</p> <p>v23.3.1 of DataPlatform ships following fixes:</p> <ul> <li>Backport of fix for URI Template ordering, when storing shaped resource + sub-resource with uriTemplate for each</li> </ul> <p>v23.3.0 of DataPlatform ships following fixes:</p> <ul> <li>Fixed non-working query cancelling in GraphDb 10.3</li> <li>Wrong caching on facet query calls</li> </ul> <p>v23.3.0 of DataPlatform adds the following new features:</p> <ul> <li>Added endpoint for removal of system resources i.e. bootstrap data</li> <li>GraphDb embedded development build</li> </ul> <p>v23.3.0 of DataPlatform ships the following changes:</p> <ul> <li>Dataplatform health check update:<ul> <li>activation of spring boot kubernetes health groups <code>liveness/readiness</code>.</li> <li>creation of health group <code>sparql</code> which can be (de)activated:<ul> <li><code>management.health.sparql.enabled</code>: (De)activates check from DP to store backend (default: true).</li> <li><code>management.health.sparql.fixedDelayInMilliseconds</code>: delay in ms between store checks (default: 5000).</li> <li><code>management.health.sparql.timeoutInMilliseconds</code>: timeout on how long to wait for store to answer check request (default: 5000).</li> </ul> </li> <li>health group <code>sparql</code> contributes to readiness state / overall health endpoint.</li> <li>GraphDB health check uses Gdb endpoint for repository.</li> </ul> </li> <li>Charts configuration API and Shacl integration.</li> <li>Breaking change: remove property <code>authorization.abox.prefix</code> (fixed default: http://eccenca.com/).</li> <li>Workspace configuration adjustments:<ul> <li><code>Application presentation</code> added properties companyName, applicationName, bannerBackgroundColor.</li> <li><code>EasyNav module</code> added property shapePropertyView.</li> <li>Chart configuration module.</li> <li>Change default system workspace values.</li> </ul> </li> <li>Spring Boot v3.1.x</li> <li>Support for the <code>{label}</code> parameter in uri templates.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#eccenca-corporate-memory-control-cmemc-v2330","title":"eccenca Corporate Memory Control (cmemc) v23.3.0","text":"<p>We\u2019re excited to bring you the latest update to Corporate Memory Control (cmemc) v23.3, featuring numerous enhancements, bug fixes, and deprecations.</p> <p>v23.3.0 of Corporate Memory Control adds the following new features:</p> <ul> <li><code>project variable</code> command group<ul> <li><code>create</code> command - create a new project variable</li> <li><code>delete</code> command - delete a project variable</li> <li><code>get</code> command - get the value or other data of a project variable</li> <li><code>list</code> command - list available project variables</li> <li><code>update</code> command - update data of an existing project variable</li> </ul> </li> <li><code>admin workspace python</code> command group<ul> <li><code>open</code> command - open a package pypi.org page in the browser</li> <li><code>list --available</code> option - list published packages</li> <li><code>uninstall --all</code> option - reset the whole python environment</li> </ul> </li> <li><code>project</code> command group<ul> <li><code>create --label</code> option - give a label for the created project</li> <li><code>create --description</code> option - give a description for the created project</li> </ul> </li> <li><code>dataset</code> command group<ul> <li><code>update</code> command - update the configuration of an existing dataset</li> </ul> </li> <li><code>workflow</code> command group<ul> <li><code>execute --progress</code> option - show a progress bar</li> </ul> </li> <li><code>admin store</code> command group<ul> <li><code>bootstrap --remove</code> option - delete the bootstrap data</li> </ul> </li> </ul> <p>v23.3.0 of Corporate Memory Control introduces the following changes:</p> <ul> <li><code>workflow execute</code> command - more debug info when polling for workflow info</li> <li>Upgrade to <code>click</code> v8 (see Migration Notes).</li> <li>Upgrade to debian 12 based image: <code>3.11.6-slim-bookworm</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v23.2 into DataIntegration v23.1 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v23.1 can be imported into DataIntegration v23.2.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#dataintegration","title":"DataIntegration","text":"<p>There is a known issue and existing workaround with the new dependency port feature: you may receive a message like this when running your workflows:</p> <p><code>Workflow Execution Error: Not all workflow nodes were executed! Executed 2 of 7 nodes.</code></p> <p>In this case, open the affected workflow in DataIntegration and click the save button once (no changes are required). After saving it will work again.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#dataplatform","title":"DataPlatform","text":"<p>Due to the removal of the <code>authorization.abox.prefix</code> configuration option, a change in your setup may be required.</p> <p>Warning</p> <p>Even if you have not changed this value (to anything other than <code>http://eccenca.com/</code>), please remove the <code>authorization.abox.prefix</code> configuration property from your DataPlatform <code>application.yml</code>. This property must be absent or DataPlatform will not start.</p> <p>From v23.3 <code>AccessCondition</code>s are only regarded if their IRIs use the prefix <code>http://eccenca.com/</code> (e.g. have an IRI like <code>http://eccenca.com/170f25c2-3b92-40d7-b247-5bba42dbe22a</code>). Required action:</p> <ul> <li>If you have been using a different prefix for your <code>AccessCondition</code>s, change the prefix of these resources. E.g. by:<ul> <li>search / replace the old prefix with the new one in your RDF graph backup</li> <li> <p>using a <code>SPARQL query</code> like:</p> <pre><code>PREFIX eccauth: &lt;https://vocab.eccenca.com/auth/&gt;\n\nINSERT {\n    GRAPH &lt;urn:elds-backend-access-conditions-graph&gt; {\n        ?new_acl a eccauth:AccessCondition .\n        ?new_acl ?p ?o .\n    }\n}\nWHERE {\n    # this Graph IRI corresponds to your `authorization.abox.accessConditions.graph` configuration\n    # default &lt;urn:elds-backend-access-conditions-graph&gt;\n    GRAPH &lt;urn:your-custom-namespace&gt; {\n        ?acl a eccauth:AccessCondition .\n        ?acl ?p ?o .\n        BIND(IRI(REPLACE(STR(?acl), \"urn:your-custom-prefix\", \"http://eccenca.com/\")) AS ?new_acl)\n    }\n} ;\n</code></pre> </li> </ul> </li> </ul> <p>Update of <code>client-authentication-method</code> property needed</p> <p>Due to an upgrade of the spring boot library the <code>client-authentication-method</code> property needs to be chaged to <code>client_secret_basic</code> in your DataPlatform <code>application.yml</code>:</p> <pre><code>spring.security.oauth2.client.registration.keycloak.client-authentication-method: client_secret_basic\n</code></pre> <p>(was <code>basic</code> which is not working anymore)</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-3/#cmemc","title":"cmemc","text":"<ul> <li>The upgrade to <code>click</code> v8 involves new completion functions (see completion manual)<ul> <li>Old: <code>_CMEMC_COMPLETE=source_zsh cmemc</code></li> <li>New: <code>_CMEMC_COMPLETE=zsh_source cmemc</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/","title":"Corporate Memory 24.1.3","text":"<p>Corporate Memory 24.1.3 is the third patch release in the 24.1 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>New improved REST operator (v2) with lots of additional features</li> <li>Extendend Keyboard Shortcuts in workflow editor</li> </ul> </li> <li>Automate:<ul> <li>New <code>admin acl</code> command group to automate management of access conditions</li> <li>New <code>graph validation</code> command group to automate batch validation of graph resources against SHACL shapes</li> </ul> </li> <li>Explore and Author:<ul> <li>Preview of our new SHACL Authoring Engine (enable with feature flag <code>shacl2</code> on your workspace configuration: <code>Basics</code>&gt;<code>Workspace</code>&gt;<code>featureFlags</code>)</li> </ul> </li> </ul> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataIntegration v24.1.1</li> <li>eccenca DataManager v24.1.3</li> <li>eccenca DataPlatform v24.1.2</li> <li>eccenca Corporate Memory Control (cmemc) v24.1.4</li> </ul> <p>We tested this release with the following dependency components:</p> <ul> <li>Ontotext GraphDB v10.6.2</li> <li>Keycloak v24.0.3</li> </ul> <p>More detailed information for this release is provided in the next sections.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#eccenca-dataintegration-v2412","title":"eccenca DataIntegration v24.1.2","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v24.1, which introduces new features, improvements and bug fixes:</p> <p>v24.1.2 of DataIntegration introduces the following changes:</p> <ul> <li>Description fields provide a Markdown-editor now.</li> <li>Removed Dependency ports menu item from transform and linking nodes.</li> </ul> <p>v24.1.2 of DataIntegration ships the following fixes:</p> <ul> <li>Removed bloated Amazon AWS bundle from dependencies.</li> <li>Include Snowflake JDBC driver.</li> <li>docker image: bump zlibg to mitigate CVE-2023-45853.</li> <li>docker image: remove libaom to mitigate CVE-2023-6879.</li> <li>Cannot create XML with defined DTD in output template.</li> <li>Fixed upload process in DI project files widget.</li> </ul> <p>v24.1.1 of DataIntegration adds the following new features:</p> <ul> <li>A new facet has been added to the workspace search that allows to filter for read-only dataset</li> <li>The \u201cEvaluate template\u201d operator now supports hierarchical input entities if full evaluation is set</li> <li>Better preview of hierarchical formats, such as XML and JSON</li> </ul> <p>v24.1.1 of DataIntegration introduces the following changes:</p> <ul> <li>Icon of notification menu was aligned to DM, it\u2019s now a bell.</li> </ul> <p>v24.1.1 of DataIntegration ships the following fixes:</p> <ul> <li>Fixed various vulnerabilities</li> <li>AWS S3 workspace: IO Error Attempted read on closed stream</li> <li>Secret values (passwords) in DI task configurations not shown to users once entered</li> <li>The create project endpoint returns a custom error format instead of HTTP problem details</li> <li>Notification menu was fixed regarding its opening and closing behavior.</li> <li>XML Dataset produces wrong tags if the target property is a full URI</li> <li>Macro support for Jinja templates</li> </ul> <p>v24.1.0 of DataIntegration adds the following new features:</p> <ul> <li>Multiline editing of template values</li> <li>Added loose connection of workflow nodes similar to linking editor</li> <li>XML, JSON, Excel and CSV datasets support retrieving the line and column numbers</li> <li>Error report for (validation) errors in transform and linking rule editors and transform execution report<ul> <li>Shows additional details like a stacktrace and input values</li> </ul> </li> <li>Added hotkey integration for creating new items in the workflow editor</li> <li>Improved REST operator (v2)<ul> <li>With support for multiple REST requests, one per input entity</li> <li>Paging support: If the API does not return all results in a single request, this features allows to page via multiple requests and merge the results of all requests</li> <li>Better error handling and retry mechanism: Retries requests and collects errors for execution report</li> <li>Rate limiting of requests by setting a delay between subsequent requests</li> <li>Limit and offset: Only executes a specific \u201cwindow\u201d of the input entities/requests</li> <li>URL property: Allows to define a property that is injected into the result JSON that contains the original request URL</li> <li>Support dataset file output, i.e. a file based dataset can be connected to the operator output, which overwrites the dataset file with the results from the REST requests</li> <li>This allows to handle REST results as any dataset content</li> <li>Supports zip files. If a dataset (currently JSON, XML, RDF file, CSV) specifies a zip file (ending in .zip) a zip archive is written that contains one file per request result</li> </ul> </li> <li>JSON dataset<ul> <li>Support bulk resources, i.e. JSON files in a zip file</li> <li>Support reading JSON Lines files</li> </ul> </li> <li>Python workflow plugins can now consume and produce hierarchical entities</li> <li>Additions to the workflow configuration ports:<ul> <li>Allow to reconfigure transform and linking tasks in workflows</li> <li>Datasets can be connected directly to the configuration port</li> </ul> </li> <li>Extended auto-completion support when opening the mapping (rule) editor in a workflow context:<ul> <li>Support auto-completion of target properties for fixed target schema and config port schema (transformation connected to config port)</li> <li>Support auto-completion of values paths for fixed input schema</li> </ul> </li> <li>Added timer for workflow execution and in activity view</li> <li>Error notification<ul> <li>Add badge to error notification menu icon with error count.</li> </ul> </li> </ul> <p>v24.1.0 of DataIntegration introduces the following changes:</p> <ul> <li>Show project variables re-ordering errors (with details) directly in project variables widget</li> <li>Support PATCH and DELETE requests in REST operators</li> <li>Upgraded libraries, in particular Play to v2.9.1 and Spark to v3.5.0</li> <li>Support of custom tasks as input for transform and linking tasks</li> <li>Create/update dialogue:<ul> <li>When a parameter value is changed that other parameters are depending on, those parameter values are reset because they might not be valid anymore</li> </ul> </li> <li>Shortened workflow execution failure message shown in activity widget</li> <li>Added <code>Fail workflow</code> flag to <code>Cancel workflow</code> operator</li> </ul> <p>v24.1.0 of DataIntegration ships the following fixes:</p> <ul> <li>Many errors occurring in a form/modal, e.g. from requests, are hidden because they are shown in the global error notification which cannot be accessed while the form is open</li> <li>Missing or problematic error handling in several forms and other places</li> <li>Transform editor should show plugin labels instead of ids</li> <li>Transform execution report validation icons in mapping tree do not update after running the execution</li> <li>When upgrading a plugin, new parameters are not shown in transform editor</li> <li>Workflow editor: Creating a new connected task that has no input port connects to the output port</li> <li>Copying a project with custom prefixes into a project that misses these prefixes fails</li> <li>Workflow report always states <code>...has not finished execution yet.</code></li> <li>Cannot add a new project variable after having tried to add it with an empty value</li> <li>Support for ARM64 architecture</li> <li>View completely crashes when error is not caught in any tab view (plugin) - there should be an error boundary</li> <li>Mapping editor shows spinner when no network is available when switching to it</li> <li>Linking editor does not load when network unavailable instead of showing error</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#eccenca-datamanager-v2413","title":"eccenca DataManager v24.1.3","text":"<p>We are excited to announce the latest update to DataManager v24.1, which introduces new features, improvements and bug fixes:</p> <p>v24.1.3 of DataManager ships the following fixes:</p> <ul> <li>docker image: bump zlib package to mitigate CVE-2023-45853</li> </ul> <p>v24.1.2 of DataManager ships the following fixes:</p> <ul> <li>Version string is no longer suffixed by the dirty flag due to re-generated clients</li> </ul> <p>v24.1.1 of DataManager ships the following fixes:</p> <ul> <li>Inline View is used when opening a Knowledge Graph Dataset in DataIntegration</li> <li>Delete Thesaurus dialog is now working as expected</li> <li>Order of graph lists is respected, when determining the first graph to explore</li> <li>Fixed several issues with the unshaped-properties view mode of easynav, new visualizations and creating new, inverted, edges</li> <li>Hide license info for store, if no expiration date is available</li> </ul> <p>v24.1.0 of DataManager adds the following new features:</p> <ul> <li>License warnings for Corporate Memory and GraphDB license</li> <li>Added validation for invalid URI format in vocabulary registration form</li> <li>SHACL2 (beta feature, disable per default)<ul> <li>support for literals</li> <li>support for object properties</li> <li>validation</li> <li>context graph</li> </ul> </li> <li>SVG support for the object view</li> <li>A link to the DataPlatform API documentation</li> </ul> <p>v24.1.0 of DataManager ships the following changes:</p> <ul> <li>Explore Navigation Component, now supports depictions and pre-loading of the concepts list</li> <li>I18N<ul> <li>Increased coverage</li> <li>Enabled nesting of the keys in translations</li> <li>Improvements in the application header in explore</li> </ul> </li> </ul> <p>v24.1.0 of DataManager ships the following fixes:</p> <ul> <li>Security Update of Java wrapper</li> <li>Workspace selection resets module selection</li> <li>Considering <code>exploreModuleConfiguration.defaultGraph</code> during the Explore module mount</li> <li>Added navigation blocker for the EasyNav module</li> <li>Keeping EasyNav viewport parameters during visualization save</li> <li>Installing a vocabulary now fully refreshes the application state</li> <li>Workspaces, which are prefix of an other workspace, are now correctly handled</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#eccenca-dataplatform-v2412","title":"eccenca DataPlatform v24.1.2","text":"<p>We\u2019re excited to bring you the latest update to DataPlatform v24.1, which introduces new features, improvements and bug fixes:</p> <p>v24.1.2 of DataPlatform ships the following fixes:</p> <ul> <li>docker image: bump zlib package to mitigate CVE-2023-45853</li> </ul> <p>v24.1.1 of DataPlatform ships the following fixes:</p> <ul> <li>GraphDB license endpoints returns an empty value, if the GraphDB free is configured.</li> </ul> <p>v24.1.0 of DataPlatform adds the following new features:</p> <ul> <li>Add license information to DataPlatform actuator info endpoint response</li> <li>Added endpoints for SHACL validation / Resource shaping<ul> <li>SHACL validation and resource shaping<ul> <li>endpoints for validation, node shape structure views and data retrieval</li> </ul> </li> <li>SHACL batch validation<ul> <li>added application property <code>scheduler.backgroundQueryPoolSize</code> (Default: 4)<ul> <li>maximum numbers of threads for background jobs (i.e. SHACL batch validation)</li> </ul> </li> <li>added application property <code>proxy.shaclBatchResultsMemoryBoundaryInMb</code> (Default: 100)<ul> <li>amount in Megabytes (Mb) for SHACL batch validation results kept in memory for status retrieval</li> </ul> </li> </ul> </li> </ul> </li> <li>Access condition review endpoint<ul> <li>ability to check user rights (access conditions) for a set of groups</li> </ul> </li> </ul> <p>v24.1.0 of DataPlatform ships the following changes:</p> <ul> <li>Static access condition prefix split for newly created access conditions<ul> <li><code>&lt;http://eccenca.com/&gt;</code> \u2013 prefix for Access Condition Groups / Users</li> <li><code>&lt;http://eccenca.com/ac/&gt;</code> \u2013 prefix for Access Conditions</li> </ul> </li> <li>Added tracing id to audit logs</li> <li>Add feature flag field to workspace configuration</li> <li>Add support for GraphDB 10.5</li> </ul> <p>v24.1.0 of DataPlatform ships the following fixes:</p> <ul> <li>Allow blank nodes in update queries</li> <li>API endpoints do not return <code>null</code> values for unset fields anymore</li> <li>Correct documentation of API endpoints for named query execution</li> <li>Default language order is changed to: <code>[\"en\", \"\", \"de\"]</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#eccenca-corporate-memory-control-cmemc-v2414","title":"eccenca Corporate Memory Control (cmemc) v24.1.4","text":"<p>We\u2019re excited to bring you the latest update to Corporate Memory Control (cmemc) v24.1, which introduces new features, improvements and bug fixes:</p> <p>v24.1.4 of Corporate Memory Control (cmemc) ships the following fixes:</p> <ul> <li>restore python 3.10 compatibility</li> </ul> <p>v24.1.3 of Corporate Memory Control (cmemc) was a redacted build.</p> <p>v24.1.2 of Corporate Memory Control (cmemc) ships the following security patches:</p> <ul> <li>docker image: bump zlib1g to 1.3.dfsg+really1.3.1-1 to mitigate CVE-2023-45853</li> </ul> <p>v24.1.1 of Corporate Memory Control (cmemc) ships the following fixes:</p> <ul> <li>In case of using env-only configuration + SSL_VERIFY=false<ul> <li>InsecureRequestWarning output from urllib3 is now suppressed</li> <li>Normal user warning is given to stderr</li> </ul> </li> <li><code>admin workspace python install</code> command<ul> <li>completion of plugin packages does not list non-plugin packages anymore</li> </ul> </li> </ul> <p>v24.1.1 of Corporate Memory Control (cmemc) ships the following security updates:</p> <ul> <li>docker image: upgrade zlib package to 1:1.3.dfsg-3 in order to mitigate CVE-2023-45853</li> </ul> <p>v24.1.0 of Corporate Memory Control (cmemc) adds the following new features:</p> <ul> <li>Added support for importing vocabulary from standard input (<code>stdin</code>)</li> <li><code>admin acl</code> command group<ul> <li><code>create</code> command - Create an access condition</li> <li><code>delete</code> command - Delete access conditions</li> <li><code>inspect</code> command - Inspects the access condition</li> <li><code>list</code> command - List all access conditions</li> <li><code>review</code> command - Reviews the graph rights for a given access condition</li> <li><code>update</code> command - Updates an access condition</li> </ul> </li> <li><code>graph validation</code> command group<ul> <li><code>execute</code> command - Start a validation process</li> <li><code>inspect</code> command - Inspect validation process results</li> <li><code>list</code> command - List validation processes</li> <li><code>cancel</code> command - Cancel a running validation process</li> </ul> </li> <li><code>admin user list</code> command<ul> <li><code>--filter</code> option - filter user list</li> </ul> </li> <li><code>admin status</code> command<ul> <li>raises an error if the Corporate Memory license is expired (grace period)</li> <li>raises a warning if the GraphDB license expires in less than one month</li> </ul> </li> <li><code>dataset create</code> command<ul> <li>support to use JSON Lines files as JSON datasets</li> <li>support to use YAML files as TEXT datasets</li> </ul> </li> </ul> <p>v24.1.0 of Corporate Memory Control (cmemc) ships the following changes:</p> <ul> <li><code>graph import</code> command<ul> <li>importing a directory to a single graph no longer raises an error but imports all turtle files to this graph</li> </ul> </li> <li>docker image: python 3.11.8</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v24.1.0 into DataIntegration v23.3.0 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v23.3.0 can be imported into DataIntegration v24.1.0.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#dataintegration","title":"DataIntegration","text":"<p>There is a known issue and existing workaround with the new dependency port feature: you may receive a message like this when running your workflows:</p> <pre><code>Workflow Execution Error:\nNot all workflow nodes were executed! Executed 2 of 7 nodes.\n</code></pre> <p>In this case, open the affected workflow in DataIntegration and click the save button once (no changes are required). After saving it will work again.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#datamanager","title":"DataManager","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#migrate-default-graph-override","title":"Migrate Default Graph Override","text":"<p>If the default workspace sets a <code>exploreModuleConfiguration.defaultGraph</code> but an additional workspace is configured to show graph lists, you might see the following error message:</p> <pre><code>Missing Graph configuration for Context. Please check that the graph\n\"\"\nactually exists.\n</code></pre> <p>Previously, un-setting the <code>exploreModuleConfiguration.defaultGraph</code> of the default workspace required setting its value to the empty string <code>\"\"</code>. The configuration now follows the standard pattern and expects an explicitly blocked value (i.e. setting it to <code>null</code>). All non-default workspaces that previously set the <code>exploreModuleConfiguration.defaultGraph</code> to the empty string <code>\"\"</code> need to be migrated by:</p> <ol> <li>Open the workspace in the (workspace)  Configuration.</li> <li>Open Modules &gt; Explore.</li> <li>Delete the empty <code>\"\"</code> value of defaultGraph by clicking on the  icon.</li> <li>Click on the block button to set it to <code>null</code>.</li> <li>Save the workspace.</li> </ol>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#dataplatform","title":"DataPlatform","text":"<p>DP APIs do not return null values for unset fields anymore.</p> <p>Warning</p> <p>Check if null values of non mandatory fields are expected if you are using the DataPlatform APIs with a custom client.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-1/#cmemc","title":"cmemc","text":"<p>No migrations are required going from cmemc 23.3.x to 24.1.x.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/","title":"Corporate Memory 24.2.1","text":"<p>Corporate Memory 24.2.1 is the first patch release in the 24.2 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Explore and Author:<ul> <li>All-new, re-written shacl custom UI rendering engine (shacl2) is now generally available and the system default</li> <li>Support for Sankey chart type.</li> </ul> </li> <li>Build:<ul> <li>Quick creation of file based datasets in the workflow editor - dropping files into the workflow editor will automatically create a new dataset.</li> </ul> </li> <li>Automate:<ul> <li>Extension to many import commands to allow for importing graphs, projects, datasets and vocabularies from the web</li> <li>Extension to the graph validation export command to produce JUnit XML reports for better integration into CI/CD pipelines</li> </ul> </li> </ul> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataIntegration v24.2.1</li> <li>eccenca DataManager v24.2.2</li> <li>eccenca DataPlatform v24.2.1</li> <li>eccenca Corporate Memory Control (cmemc) v24.2.0</li> </ul> <p>We tested this release with the following dependency components:</p> <ul> <li>Ontotext GraphDB v10.7.2</li> <li>Keycloak v25.0.6</li> </ul> <p>More detailed information for this release is provided in the next sections.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#eccenca-dataintegration-v2421","title":"eccenca DataIntegration v24.2.1","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v24.2, which introduces new features, improvements and bug fixes:</p> <p>v24.2.1 of DataIntegration ships the following fixes:</p> <ul> <li>Drag and drop in react flow editors used in Linking and Transform tasks work again.</li> <li>Dragging operators in the react flow based editors when text is selected leads to large artifacts.</li> </ul> <p>v24.2.0 of DataIntegration adds the following new features:</p> <ul> <li>Quick creation of file based datasets in the workflow editor.<ul> <li>Dropping files into the workflow editor will automatically create a new dataset.</li> </ul> </li> <li>Button to reload all cache activities at once.</li> <li>The JDBC dataset can now be configured how it will write multiple values for a single property. A new strategy allows to write multiple rows in this case.</li> <li>Python:<ul> <li>Added <code>WorkflowContext</code> that allows plugins to access the workflow identifier as well as the current execution status.</li> <li>Added <code>packageName</code> attribute to the plugin JSON.</li> </ul> </li> </ul> <p>v24.2.0 of DataIntegration introduces the following changes:</p> <ul> <li>Re-use original parameters of a replaceable dataset if the dataset type matches with the requested one in a variable workflow request.</li> <li>Support Turtle files for the graph file upload operator (No chunking supported)</li> <li>Transform object header has been separated from its properties to improve visual appearance.</li> <li>Rule editors: Add generic path operator to input path tabs.</li> <li>When trying to execute an unsaved workflow notify the user that the workflow will be saved with the option to not show the dialog again.</li> <li>Re-added reload button to data preview to get updated content on config and data changes.</li> <li>Removed URL resource manager. URLs as file names won\u2019t be resolved anymore, e.g. for dataset inputs.</li> <li>The RDF datasets will always write the schema type for each entity.</li> </ul> <p>v24.2.0 of DataIntegration ships the following fixes:</p> <ul> <li>Workflow Task: exception message prefixed with wrong name.</li> <li>Workflow editor:<ul> <li>Workflow nodes with only a single dependency output might be executed twice.</li> <li>Loose connections from dependency ports not working anymore.</li> <li>Allow dependency connection from/to replaceable datasets.</li> <li>Drawing dependency connections from input dependency port results in unexpected connections, e.g. connections to data output ports.</li> </ul> </li> <li>Project page breaks if file resources are missing meta data like size or modified.</li> <li>Workflow operator: became invisible after workflow error.</li> <li>Always return a 401 (not authorized) instead of a 500 response when refreshing a token has failed with an <code>invalid_grant</code> error from Keycloak.</li> <li>Rule endpoint does not return new parameters after plugin upgrade.</li> <li>Zip file created with macOS Archive Utility containing files with macOS-specific metadata not working in bulk datasets.</li> <li>Fixed <code>overlayEditors</code> that close even with unsaved changes.</li> <li>Consistent navigation behavior or indicate links/buttons that open a new tab/window.</li> <li>Replace <code>highlightedState</code> properties in workflow editor.</li> <li>Rule endpoint does not return new parameters after plugin upgrade.</li> <li>Python: Changes in plugin submodule not recognized without restarting DI.</li> <li>Dataset API is not fenced against misuse of file parameter.</li> <li>Workflow Task: exception message prefixed with wrong name.</li> <li>Improve upload icon in DI file upload widget.</li> <li>JDBC: H2 driver not found.</li> <li>JDBC: Dataset should not write an extra <code>rdf_type</code> column.</li> <li>JDBC: Cannot rerun Data preview without reopening JDBC dataset update dialog.</li> <li>JDBC: naming the URI column <code>uri</code> causes <code>is ambiguous</code> error.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#eccenca-datamanager-v2422","title":"eccenca DataManager v24.2.2","text":"<p>We are excited to announce the latest update to DataManager v24.2, which introduces new features, improvements and bug fixes.</p> <p>v24.2.2 of DataManager ships the following fixes:</p> <ul> <li>Fixed the statistics display for link rules with inverted properties</li> </ul> <p>v24.2.1 of DataManager ships the following fixes:</p> <ul> <li>Fixed Create <code>&lt;type&gt;</code>-Button in explore</li> <li>Select NodeShapes according to their <code>sh:order</code></li> <li>Resolve NodeShapes sequentially, instead of parallel</li> <li>Node selection clears its state when deleting nodes</li> <li>Easynav: non-saved values disappear after search value change</li> <li>Resource list is updated after the creation of a new resource</li> <li>Newly created relations based on inverted shapes are pointing in the right direction</li> <li>Setting language for text areas</li> </ul> <p>v24.2.0 of DataManager adds the following new features:</p> <ul> <li>Charts module<ul> <li>Support for grouping chart series.</li> <li>Support for Sankey chart type.</li> </ul> </li> <li>Business Knowledge Editor - EasyNav<ul> <li>Easynav allows to create new, directly connected resources with the via the browse dialogue.</li> <li>Improved undo/redo functionality.</li> <li>Improved internal structure and robustness.</li> <li>Improved selection and searching.</li> <li>Adding new connections via shacl shape queries.</li> <li>Adding new connections on unsaved nodes.</li> <li>Unique Visualization names are enforced.</li> </ul> </li> <li>Other<ul> <li>Added a global notifications queue with the dropdown menu besides the user menu.</li> <li>Update Spring Wrapper to Spring Boot 3.2.</li> <li>Added icons for read-only graphs.</li> <li>Added \u201calgorithm\u201d param in workspace configuration and <code>/proxy/:id/resource</code> queries.</li> <li>Added a notification feature for retrieving the query catalog data when the backend response contains validation errors with the code \u201cDOUBLE_TRIPLE\u201d.</li> <li>Added the multi-source turtle component.</li> </ul> </li> </ul> <p>v24.2.0 of DataManager ships the following changes:</p> <ul> <li>SHACL Component - SHACL2 replaces now our default SHACL viewer/editor and brings in lot of new features and enhancements.     Note: this component was already introduced in v24.1, so this list is not exhaustive.<ul> <li>Improved Validation of inputs.</li> <li>Added the possibility to add properties that are not visible to shacl2.</li> <li>Migrated annotations to shacl2.</li> <li>Value and UI Query are used in relation manager, if provided.</li> <li>Added check for simple widget using <code>defaultResourceViewerIri</code>.</li> <li>Improved editor for highly connected resources.</li> <li>Improved access condition handling when creating new resources.</li> <li>Improved default language handling.</li> <li>Improved Layout for long labels.</li> <li>Partial support for qualified value shapes.</li> <li>Sticky toolbar for better usability.</li> </ul> </li> <li>Access Conditions - New Access conditions management interface replaces the former component.<ul> <li>Validation for the creation items, added links to grid items.</li> <li>AC review page.</li> </ul> </li> </ul> <p>v24.2.0 of DataManager ships the following fixes:</p> <ul> <li>Error messages<ul> <li>Improved error messages format, now they are more informative and user-friendly with title and details sections.</li> <li>Warnings are shown locally where triggered and errors are added to the global notifications queue.</li> <li>Warnings are shown properly without breaking the UI, closer to the place that triggered it.</li> </ul> </li> <li>Workflow triggers reload the page after the workflow is finished.</li> <li>Resource tags are links.</li> <li>Explore<ul> <li>Navigation component honors the module settings for navigationItemsPerPage.</li> <li>Navigation component shows correct pagination for search.</li> <li>Turtle tab is visible, even if the user has no write access.</li> </ul> </li> <li>Business Knowledge Editor - EasyNav<ul> <li>Inverse properties are shown in the correct direction.</li> <li>Labels with more than approx. 24 characters and no white space are now correctly split into two lines.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#eccenca-dataplatform-v2421","title":"eccenca DataPlatform v24.2.1","text":"<p>We\u2019re excited to bring you the latest update to DataPlatform v24.2, which introduces new features, improvements and bug fixes:</p> <p>v24.2.1 of DataPlatform ships the following fixes:</p> <ul> <li>Backup archives are zipped with ZIP64 option to allow &gt;=4G archives</li> <li>Prevent issues with long-running shacl batch jobs on single node deployments</li> <li>Include subclasses in validation of <code>sh:class</code></li> <li>Improved validation of file uploads</li> </ul> <p>v24.2.0 of DataPlatform adds the following new features:</p> <ul> <li>Added migration of workspace configuration<ul> <li>workspace/module configurations are stored in JSON content literals in the CMEM Config Graph since v23.1.</li> <li>New field https://vocab.eccenca.com/configuration/json.ver in workspace configuration graph.</li> <li>Endpoint for migrating workspace configurations to current version.</li> <li>Extension of actuator info endpoint: Shows current version and number of items to migrate.</li> </ul> </li> <li>Added SHACL RDF validation view for SHACL batch validations, <code>POST /api/shacl/validation/batches</code><ul> <li><code>validationResultsTargetGraph</code>: Graph to write rdf validation model into after batch finishes</li> <li><code>replace</code>: boolean value on whether to replace the graph (default: false)</li> </ul> </li> <li>Added option for SHACL Batch run to query target resources with a ignore list for OWL imports, <code>POST /api/shacl/validation/batches</code>         -   <code>owlImportsIgnoreList</code>: A set of graph IRIs which are not queried in the resource selection (i.e. owl imports ignored)</li> <li>Added module Access-Control to workspace configuration<ul> <li>Existing module Administration split into workspace configuration and access control.</li> <li>Existing administration module used for workspace configuration (as to avoid migration steps).</li> </ul> </li> <li>Add alternative endpoint to <code>/api/shapes/list</code> called <code>/api/shapes/listWithValidation</code><ul> <li>return object contains a field for errors in the data which prevent mapping.</li> <li>asked behavior results in shape not found responses on shapes which cannot be mapped</li> </ul> </li> <li>Add <code>owlImportsResolution</code> to resource store endpoints<ul> <li>optional parameter overriding dataplatform setting</li> </ul> </li> </ul> <p>v24.2.0 of DataPlatform ships the following changes:</p> <ul> <li>Update to Apache Jena 5<ul> <li>All JSON-LD output in JSON-LD 1.1, JSON-LD 1.0 support dropped</li> </ul> </li> <li>Changed to Access Condition endpoints<ul> <li><code>GET /api/authorization/groups</code> return IRIs instead of names (including public/admin group)</li> <li><code>GET /api/authorization splits</code> pageable parameter into single parameters page, size, sort</li> </ul> </li> <li>Deactivated graph db change tracking as default</li> <li>Extension of ACL review endpoint, response updated with matching access conditions</li> <li>Additional SHUI-Property for defining object relation default view<ul> <li><code>shui:viewResourcesWithWidget</code> with values (<code>shui:ComplexResourceViewerWidget</code>, <code>shui:SimpleResourceViewerWidget</code>)</li> </ul> </li> <li>Resource API endpoints <code>/proxy/{id}/resource</code> changed<ul> <li>Additional application parameter proxy.maxCBDStatements (default: 1000000) for limiting amount of statements in memory when loading (S)CBD</li> <li>CBD calculation does not include reifications anymore</li> <li>Additional one query based algorithm for CBD calculation (algorithm can be selected with optional query parameter algorithm)<ul> <li>workspace configuration: <code>apiConfiguration.conciseBoundLoadAlgorithm</code> (<code>ITERATIVE</code>, <code>QUERY</code>)</li> </ul> </li> </ul> </li> <li>Endpoint for workspace configuration set <code>/api/conf/workspaces</code> falls back on system default</li> <li>New default icons for resource, class and properties</li> <li>Removal of native stardog integration</li> </ul> <p>v24.2.0 of DataPlatform ships the following fixes:</p> <ul> <li>Broken workspace configurations fall back to system default workspace<ul> <li>actuator info endpoint contains field <code>workspaceConfigurationError</code> on error</li> </ul> </li> <li>SHACL controller validation endpoint fixed<ul> <li>only validates constraints which do not need other data if change-set is given as in memory</li> </ul> </li> <li>Fixed query rewriting of (named) graphs for users with limited read rights<ul> <li>From Graphs Rewriting: Prevent rewrite of where clause if no graph variable found</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#eccenca-corporate-memory-control-cmemc-v2420","title":"eccenca Corporate Memory Control (cmemc) v24.2.0","text":"<p>We\u2019re excited to bring you the latest update to Corporate Memory Control (cmemc) v24.2, which introduces new features, improvements and bug fixes:</p> <p>v24.2.0 of Corporate Memory Control (cmemc) adds the following new features:</p> <ul> <li><code>admin store migrate</code> command<ul> <li>Migrate configuration resources to the current version.</li> </ul> </li> <li><code>admin status</code> command</li> <li> <ul> <li>will warn in case there a workspace configurations, which can be migrated</li> </ul> </li> <li> <ul> <li>will exit with exit code 1 in case option <code>--exit-1 always</code> is given and migratable workspaces are found</li> </ul> </li> <li><code>graph validation export</code> command<ul> <li>export validation reports as JSON or jUnit XML</li> </ul> </li> <li><code>graph import</code> command<ul> <li>support for importing graphs from remote HTTP/HTTPS locations</li> </ul> </li> <li><code>project import</code> command<ul> <li>support for importing project zip files from remote HTTP/HTTPS locations</li> </ul> </li> <li><code>dataset create</code> command<ul> <li>support for creation of resource file from remote HTTP/HTTPS locations</li> </ul> </li> <li><code>dataset upload</code> command<ul> <li>support for uploading of resource file from remote HTTP/HTTPS locations</li> </ul> </li> <li><code>vocabulary import</code> command<ul> <li>support for importing vocabulary from remote HTTP/HTTPS locations</li> </ul> </li> <li><code>smart_path</code> package as a replacement for <code>pathlib.Path</code> and expanded functionality to support both local file paths and remote file paths</li> <li><code>ClickSmartPath</code> parameter type, extending <code>click.path</code> to accommodate remote files</li> <li><code>graph validation execute</code> command group<ul> <li>option <code>--query</code> to allow specifying a select query for resource selection.</li> <li>option <code>--ignore-graph</code> to provide multiple graph IRIs to be excluded from the resource selection.</li> <li>option <code>--result-graph</code> to specifies the graph where the validation results will be written.</li> <li>option <code>--replace</code> to replace the result graph with new validation results</li> </ul> </li> </ul> <p>v24.2.0 of Corporate Memory Control (cmemc) ships the following fixes:</p> <ul> <li><code>graph import</code> command<ul> <li>importing a directory to a single graph no longer raises an error but imports all turtle files to this graph</li> </ul> </li> <li><code>admin workspace python install</code> command<ul> <li>report errors from update_plugins API</li> </ul> </li> <li>using not existing configurations (<code>-c</code> / <code>--configuration</code>) now results in a proper error message</li> <li><code>workflow io</code> command<ul> <li>can now generate ttl output files</li> </ul> </li> <li><code>admin workspace python list</code> command<ul> <li>listing of published packages with the <code>--available</code> option now works for more than 19 packages</li> </ul> </li> <li><code>graph export</code> command<ul> <li>newly created directories have correct access conditions now</li> </ul> </li> <li><code>vocabulary install</code> command<ul> <li>raise proper usage error messages</li> </ul> </li> <li><code>vocabulary uninstall</code> command<ul> <li>raise proper usage error messages</li> </ul> </li> <li><code>admin store export</code> command<ul> <li>validates the exported zip and raises an error in case of a corrupted ZIP export</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v24.2.0 into DataIntegration v24.1.0 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v24.1.0 can be imported into DataIntegration v24.2.0.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#dataintegration","title":"DataIntegration","text":"<p>If during writing to a MySQL/MariaDB a <code>[\u2026] You have an error in your SQL syntax [\u2026]</code> error is encountered make sure <code>ANSIquotes</code> are used.</p> <p><code>sql_mode=ANSI_QUOTES</code> can be set via a URL parameter to the JDBC connection string like:</p> <pre><code># MySQL\njdbc:mysql://&lt;host&gt;:&lt;port, eg. 3306&gt;/&lt;database&gt;?sessionVariables=sql_mode=ANSI_QUOTES\n\n# MariaDB\njdbc:mariadb://&lt;host&gt;:&lt;port, eg. 3306&gt;/&lt;database&gt;?sessionVariables=sql_mode=ANSI_QUOTES\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#datamanager","title":"DataManager","text":"<p><code>shacl2</code> feature flag was removed, the Shacl2 engine is now the system default. In case you had the <code>shacl2</code> feature flag set in the workspace configuration, it can now be removed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#dataplatform","title":"DataPlatform","text":"<ul> <li>Due to the update to Apache Jena 5 all JSON-LD output now conforms to JSON-LD 1.1 (was JSON-LD 1.0):<ul> <li>JSON-LD 1.1 has been designed as a superset of 1.0, so we do not expect any problems, just making you aware that there might be differences when using it with JSON-LD 1.0 and 1.1 processors at the same time (e.g. to compare results)     &gt; JSON-LD 1.1 introduces new features that are compatible with JSON-LD 1.0, but if processed by a JSON-LD 1.0 processor may produce different results (cf. JSON-LD 1.1 Framing)</li> <li>Affected endpoints:<ul> <li><code>POST /proxy/{id}/resource/framed</code></li> <li><code>POST /proxy/{id}/sparql/framed</code></li> <li><code>POST /authorization/conditions/framed</code></li> <li><code>POST+GET /api/queries/jsonld/perform</code></li> </ul> </li> </ul> </li> <li>The Access Condition endpoint <code>GET /api/authorization/groups</code> returns now IRIs instead of names (including public/admin group).</li> <li>Resource API endpoints <code>/proxy/{id}/resource</code> changed, the CBD calculation does not include reifications anymore.</li> <li>The native stardog integration has been removed. As a stardog user you need to migrate your store configuration to use the generic HTTP-Store configuration option.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-2/#cmemc","title":"cmemc","text":"<ul> <li>The <code>admin status</code> command in combination with the <code>--exit-1 always</code> option now exits with status code 1 in the additional case that migrate-able workspace configurations are found<ul> <li>To avoid this, you can automatically migrate the configurations with the <code>admin store migrate</code> command.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/","title":"Corporate Memory 24.3.2","text":"<p>Corporate Memory 24.3.2 is the third major release in 2024.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Explore and Author:<ul> <li>New shacl2 engine now used in the Business Knowledge Editor sidebar for a whole new experience when viewing and editing node details.</li> </ul> </li> <li>Build:<ul> <li>Workflow reports now show a preview of the output entities being produced, allowing quick review and verification of the underlying workflow tasks..</li> </ul> </li> <li>Automate:<ul> <li>The <code>cmem</code> command group <code>admin migration</code>, which adds various migration recipes to make it easier to upgrade to new versions of Corporate Memory.</li> </ul> </li> </ul> <p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataIntegration v24.3.1</li> <li>eccenca Explore v24.3.0 (formerly DataPlatform and DataManager)</li> <li>eccenca Corporate Memory Control (cmemc) v24.3.3</li> </ul> <p>We tested this release with the following dependency components:</p> <ul> <li>Ontotext GraphDB v10.8.3</li> <li>Keycloak v25.0.6</li> </ul> <p>More detailed information for this release is provided in the next sections.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#eccenca-dataintegration-v2431","title":"eccenca DataIntegration v24.3.1","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v24.3, which introduces new features, improvements and bug fixes:</p> <p>v24.3.1 of DataIntegration adds the following new features:</p> <ul> <li>Added download button to workflow report tab.</li> </ul> <p>v24.3.1 of DataIntegration ships the following fixes:</p> <ul> <li>Task descriptions with long strings do not lead to horizontal scroll bars.</li> <li>Tag search might very shortly show old search suggestions.</li> <li>Missing unit for Matching timeout.</li> <li>Regex selection transformer has mis-formatted documentation.</li> <li>Added JDBC dataset documentation on how to configure ANSI quotes for MySQL.</li> <li>Superfluous <code>CREATE SILENT GRAPH</code> leads to slow update performance.</li> <li>Fix SQL editor inputs.</li> </ul> <p>v24.3.0 of DataIntegration adds the following new features:</p> <ul> <li>Workspace search:<ul> <li>Support to filter workflows that contain replaceable datasets.</li> <li>Display tags on workflow search items when they contain replaceable datasets.</li> <li>Add file name and graph URIs to search items as searchable tags.</li> </ul> </li> <li>Workflow editor:<ul> <li>Support creating knowledge graph datasets from DataPlatform graphs matching the search query.</li> <li>Added copy prefixes option in copy task dialog.</li> </ul> </li> <li>Integration of a Prometheus endpoint to expose many useful metrics.</li> <li>Transform operators to retrieve attributes from input tasks:<ul> <li>Input Task attributes retrieves individual attributes from the input task (such as the modified date) or the entire task as JSON.</li> <li>Input file attributes retrieves a metadata attribute from the input file (such as the file name).</li> </ul> </li> <li>JdbcDialect implementation for Trino: Fixes STRING type mapping, adds isolationLevel option to avoid Connections resetting AutoCommit mode and serves as example for the dialect concept.</li> <li>File hash transformer:<ul> <li>Calculates the hash sum of a given file</li> <li>Works on either the input file dataset or a selected file from the project</li> </ul> </li> <li>JSON special paths:<ul> <li><code>#propertyName</code> accesses the current object key</li> <li><code>*</code> selects all direct children of the current token</li> </ul> </li> <li>Add link from a task parameter description into the task\u2019s Markdown documentation for this parameter, if available.</li> <li>Show sample (output) entities for workflow operators in the workflow reports.</li> <li>Text dataset allows to configure the zip regex.</li> <li>Support setting the locale for the <code>Parse date pattern</code> and <code>Parse date</code> transform operators.<ul> <li><code>*</code> selects all direct children of the current token</li> </ul> </li> <li>More fine-grained access control:<ul> <li>In addition to a base action, it is possible to specify as many specific actions that protect specific endpoints.</li> <li>Endpoints are configured in a whitelist as URI prefixes per specific action.</li> <li>All endpoints that are protected by any specific action cannot be accessed anymore via the base action.</li> <li>Two new actions are configured by default and protect the Python plugin management and specific workspace API endpoints. See changes and migrations.</li> </ul> </li> <li> <p>Global variables can be marked sensitive for storing passwords:</p> <ul> <li>Sensitive variables can only be used in password fields.</li> <li>Using sensitive variables in other fields or in variable templates fails and does not expose the value.</li> <li> <p>Example:</p> <pre><code>config.variables = {\n    global = {\n        sensitiveVar = {\n            value = \"value 2\"\n            isSensitive = true\n        }\n    }\n}\n</code></pre> </li> </ul> </li> <li> <p>Delete project files operator: Allows to delete project files in a workflow based on a regex.</p> </li> <li>Added Snowflake dataset type.</li> </ul> <p>v24.3.0 of DataIntegration introduces the following changes:</p> <ul> <li>Optimized writing to Neo4j, resulting in a 25x speed improvement.</li> <li>Upgraded Spark to 3.5.3.</li> <li>Upgraded to typescript version 5.5.3.</li> <li>After saving a workflow the undo/redo queues are cleared which is consistent with other editors in DI/DM.</li> <li>Renamed DI action from <code>urn:eccenca:di</code> to <code>&lt;https://vocab.eccenca.com/auth/Action/Build&gt;</code>.</li> <li>Line breaks are forced for evaluation preview tooltips.</li> <li>If a project is copied to another project, all referenced project variables and their dependent variables are copied to the target project as well.</li> <li>docker image: switch to <code>eclipse-temurin:17-ubi9-minimal</code> base image</li> <li>Prefix handling:<ul> <li>Only prefixes added to a specific project are serialized/exported, no prefixes loaded by the workspace (e.g. from DP).</li> <li>Only load user prefixes and prefixes of installed vocabularies from DP into DI.</li> </ul> </li> <li>All datasets that support zips can be written now.</li> <li>Increase visibility of breadcrumbs in application header.</li> <li>Configurable Favicon in DataIntegration.</li> </ul> <p>v24.3.0 of DataIntegration ships the following fixes:</p> <ul> <li>Jinja templates can lead to OutOfMemory issues.</li> <li>Loading of JDBC Type 4 Drivers from Jar at runtime.</li> <li>Add add-opens JDK option to sbt parameters to avoid Serialization errors in executors.</li> <li>User defined function removed to prevent startup error in local dev mode.</li> <li>After saving a workflow the workflow editor can be closed without warning of unsaved changes.</li> <li>Race condition in Excel map transformer cache.</li> <li>Remote Client-Side Code Execution through CSV Injection identified in penetration testing.</li> <li>CSV datasets should not be cleared at the beginning of a workflow since they are overwritten anyway.</li> <li>Ports of datasets are shown as required in workflow validation, but are not.</li> <li>In workspace/project item search disable Enter behavior while a search is pending.</li> <li>Use correct icons for copy/clone actions.</li> <li>Workflow editor:<ul> <li>Workflow is not re-validated after undo/redo operations.</li> <li>Re-configuring a workflow node to not having a data output is not immediately visible (only after reload).</li> <li>When the <code>Create new dataset</code> operator is used it always creates a dataset even though the item type was changed.</li> <li>Caches of file base datasets are not refreshed when updated via file download operator.</li> <li>Dependency ports checkbox does not show checkmark in workflow tasks with unconnected output port.</li> <li>Fix text on node menu options that have a checkbox. Always show the enabled text.</li> </ul> </li> <li>REST task:<ul> <li>When paging is enabled and entities are output only the last request result is output.</li> <li>Add TLSv1.3 support.</li> </ul> </li> <li>Hierarchical mapping editor: Entity relationship direction input does not show current selection.</li> <li>Transform rule editor:<ul> <li>Validation errors are not shown when starting the evaluation.</li> <li>Notifications are not correctly cleared and shown.</li> </ul> </li> <li>Transform execution report:<ul> <li>Type URI validation issues are not shown in the transform execution report.</li> <li>Rule tree in transform execution report and evaluation tab has a broken collapse/expand state.</li> </ul> </li> <li>Password parameter templates are empty initially.</li> <li>Fix issues in create/update dialog:<ul> <li>Depending input gets disabled if dependent input has an empty default value.</li> <li>Data preview of dataset with nested parameters is not working.</li> </ul> </li> <li>Task config preview has a different parameter ordering than in the create/update dialog.</li> <li>Evaluation of a text path of a text dataset in a rule editor fails.</li> <li>Cannot execute SPARQL update queries with parameter templates.</li> <li><code>Evaluate template</code> operator: Changed project variable not updated without evaluating transform.</li> <li>Jinja interpreter does not clear previous errors.</li> <li>Process of opening and closing the handle tools menu.</li> <li>Manually defined project prefixes are automatically copied to other projects after reload.</li> <li>Removing a vocabulary does not remove the vocabulary prefix from the DI projects.</li> <li>Cannot reconfigure parameter values with templates in workflows.</li> <li>Workflow report shows multiple executions of some operators even though they were only executed once.</li> <li>Python Workflow status incorrect.</li> <li>Python Workflow operators could not be cancelled in some cases.</li> <li>Alignment dataset should support the clear method so it can be used in workflows.</li> <li>Drop zone in workflow editor freezes sometimes after dropping an operator.</li> <li>Transform/Linking operator\u2019s \u2018Restriction\u2019 documentation is incorrectly formatted.</li> <li>DI project \u201cItems per page\u201d cuts off \u201c100\u201d as \u201c1\u2026\u201d.</li> <li>Wide task descriptions are not nicely scrollable.</li> <li>Inline documentation of <code>Clean HTML</code> is incomplete/wrong.</li> <li>Cannot delete mapping rule target type anymore.</li> <li>SPARQL Construct task does not update its execution report.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#eccenca-explore-v2430","title":"eccenca Explore v24.3.0","text":"<p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p> <p>We are excited to announce Explore v24.3, which introduces new features, improvements and bug fixes.</p> <p>v24.3.0 of Explore adds the following new features:</p> <ul> <li>Help system<ul> <li>Implemented a renovated help system with global and local context of documentation</li> </ul> </li> <li>BusinessKnowledgeEditor (BKE)<ul> <li>Rename \u201cEasyNav\u201d to \u201cBusiness Knowledge Editor\u201d</li> <li>Keep search bar state when visualization is saved</li> <li>Set BKE as default</li> <li>Creation of customizable class on a property shape path</li> <li>Edge type selection shows shape description on hover</li> <li>Keep search bar state when visualization is saved</li> </ul> </li> <li>Notifications<ul> <li>Added a warning message, if a user is part of a fallback admin group</li> </ul> </li> <li>Query module<ul> <li>Icons added to the query dropdown functionalities</li> </ul> </li> <li>Access Condition<ul> <li>Provided custom search function for graphs in ACDetails</li> </ul> </li> <li>SHACL<ul> <li>Creation of customizable class on a property shape path</li> <li>Workflows are triggered upon editing the resource</li> </ul> </li> <li>Workspace configuration<ul> <li>Added a support for <code>GRAPH</code> placeholder in the <code>navigationSearchQuery</code></li> </ul> </li> <li>Other<ul> <li>Added endpoint for resolving node shapes of a resource evaluating target class only for explore</li> <li>Added flag to <code>/userinfo</code> response if user is root user</li> <li>Added support for multiline in turtle editor</li> <li>Added actuator proxy endpoint for GraphDB actuators<ul> <li>hidden endpoints under <code>/dataplatform/actuator/proxy/graphdb/**</code></li> </ul> </li> <li>Added support for gzip payload compression in SPARQL Graph Store endpoints<ul> <li>Content-Encoding / Accept-Encoding used with value gzip</li> <li>Added simple zip-bomb check for gzipped content<ul> <li>Configuration: <code>proxy.gspUploadGzipContentLimit</code> sets limit in bytes of uncompressed graph file in gzip (default 5 GB)</li> </ul> </li> </ul> </li> <li>Added endpoint for retrieval of resource descriptions (i.e. rdfs:comment)<ul> <li>signature same as for title resolving</li> </ul> </li> <li>Added additional prometheus endpoint under different port and no authentication<ul> <li>Configuration under deploy<ul> <li><code>deploy.additional-prometheus-endpoint.enabled</code> (default: false)</li> <li><code>deploy.additional-prometheus-endpoint.port</code> (default: 9091)</li> <li><code>deploy.additional-prometheus-endpoint.context</code> (default: /metrics)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>v24.3.0 of Explore ships the following changes:</p> <ul> <li>EasyNav<ul> <li>Created a fallback module, marked as deprecated</li> <li>Used the old EP for saving data in easynav</li> </ul> </li> <li>Charts Module<ul> <li>Sunburst chart in the Explore module - <code>Statistics</code> tab reimplemented with ECharts</li> </ul> </li> <li>CodeMirror editor<ul> <li>Replaced the library for the common usage, added linters for the Editor</li> </ul> </li> <li>SHACL<ul> <li>Split <code>ShaclContextProvider</code> into controlled and uncontrolled versions to maintain changes from outer component</li> <li>Hardcoded descriptions endpoint replaced with a proper one from DP</li> <li>Added information about different validation types to the validation control</li> </ul> </li> <li><code>RDFResourceLinkRule</code> Component<ul> <li>Renamed to <code>RDFResourceTag</code>, added the titles query for cases where only the item resource is provided, added the <code>RTKLoadingErrorElement</code> wrapper</li> </ul> </li> <li>Thesaurus<ul> <li>Translate the Thesaurus Module to our modern UI Stack</li> </ul> </li> <li>Other<ul> <li>docker image: switch to <code>eclipse-temurin:17-ubi9-minimal</code> base image</li> <li>Library Updates<ul> <li>Spring Boot 3.3</li> <li>Apache Jena 5.2.0</li> </ul> </li> <li>Removed access conditions from bootstrap data</li> <li>Add username to unauthorized graph access error in log</li> <li>Replace account information placeholders in customized queries<ul> <li>Renamed <code>{{username}}</code> placeholder in the GraphTemplateJinjaTemplate to <code>{{shuiAccountName}}</code></li> <li>Renamed <code>{{username}}</code> SPARQL Query placeholder (available in <code>onDeleteUpdate</code>, <code>onInsertUpdate</code>, <code>onUpdateUpdate</code>, <code>shui:uiQuery</code> and <code>shui:valueQuery</code>) to <code>{{shuiAccount}}</code></li> </ul> </li> <li>Changed integration of non-validating property shapes in SHACL node shape model<ul> <li>Added concept of widget integration linked to node shape which have basic SHACL Properties for form UI<ul> <li>Label, Description, Order, Group, link to widget</li> </ul> </li> <li>Widget integrations carry one of the types Workflow Trigger, Table Report or Chart</li> <li>Deprecated / Removed link from node shape to chart i.e. node shape charts are not possible anymore</li> </ul> </li> <li>Always check GraphDb license information on <code>/actuator/info</code> call</li> <li>Changed retrieval of installed vocabulary prefixes</li> <li>Actuator info endpoint secured</li> <li>Change class hierarchy resolving to SPARQL property path instead of recursion</li> <li>Removed <code>ValueView</code> and <code>ValueEdit</code> components from resource view components group</li> </ul> </li> </ul> <p>v24.3.0 of Explore ships the following fixes:</p> <ul> <li>BusinessKnowledgeEditor (BKE)<ul> <li>Disabled creation of new resources via <code>shui:denyNewResources</code> property</li> <li>Added functionality to delete a resource to the node panel</li> <li>Show a notification in case of viewing details of an unshaped node</li> <li>Changed the save request payload to include a separate change for each node shape</li> <li>Added missed \u201cremove from the canvas\u201d functionality</li> <li>Set correct node shapes order</li> <li>Prevent Modal key event propagation</li> <li>Create new node shows node shapes instead of classes</li> </ul> </li> <li>Turtle editor<ul> <li>Cursor prevented from jumping upon error</li> </ul> </li> <li><code>MultiSourceView</code> Component<ul> <li>Not imported warning displaying</li> <li>Prevent loosing state while navigation is triggered</li> <li>Starting with a blank resource gives a blank screen</li> </ul> </li> <li>SHACL<ul> <li>Source link from the validation log points to the correct graph</li> <li>Node shape description toggle shows markdown both when collapsed and opened</li> <li>Slow request getting the resources per node shape is replaced with the more performant one</li> <li>Tooltip on the resource list is shown correctly</li> <li>Adjusted \u201cAdd resource\u201d disable state for simple and complex widgets</li> <li><code>sh:name</code> shows as property shape title instead of using title helper in the dropdown</li> <li>Depictions are shown based on the vocab <code>foaf:depiction</code> property and no longer for a specific property shape</li> <li>Fetching of property values is now done purely based on pre-parameterized SPARQL queries</li> <li>Changed replacement of SHUI <code>{{username}}</code> to <code>{{shuiAccountName}}</code></li> <li>Replace SHACL Save API</li> <li>Also use <code>?_graph</code> variables for deleting when using a value query</li> <li>Display custom <code>sh:message</code> in validation results</li> <li>Expose <code>sh:name</code> of node shape in SHACL as primary name - <code>rdfs:label</code> fallback</li> </ul> </li> <li>Charts<ul> <li>Info notification shows when query results are empty</li> <li>Context graph is conveyed to the charts query replacement</li> </ul> </li> <li>Image widget<ul> <li>The check for image widget is replaced with the regex</li> </ul> </li> <li>Other<ul> <li>Notifications: Improved rendering behavior, prevent loops</li> <li>Fixed the statistics display for link rules with inverted properties</li> <li>Exit application with code 1 on expired license</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#eccenca-corporate-memory-control-cmemc-v2433","title":"eccenca Corporate Memory Control (cmemc) v24.3.3","text":"<p>We\u2019re excited to bring you the latest update to Corporate Memory Control (cmemc) v24.3, which introduces new features, improvements and bug fixes.</p> <p>v24.3.3 of cmemc provides the following fixes:</p> <ul> <li>add missing migration recipe for deprecated SPARQL datatypes</li> </ul> <p>v24.3.2 of cmemc provides the following fixes:</p> <ul> <li>remove accidentally added pip dependency</li> </ul> <p>v24.3.1 of cmemc provides the following fixes:</p> <ul> <li><code>graph import</code> command<ul> <li>use python stdlib instead rdflib to guess mime types (lower memory footprint)</li> </ul> </li> </ul> <p>v24.3.0 of cmemc adds the following new features:</p> <ul> <li><code>graph validation execute</code> command<ul> <li><code>--inspect</code> option to return the list of violations instead of the summary (includes <code>--wait</code>)</li> </ul> </li> <li><code>graph validation inpect</code> command<ul> <li>retrieval and display of titles as terminal links for resources</li> <li>completion: retrieval and display of titles as descriptions</li> </ul> </li> <li><code>graph validation list</code> command<ul> <li>retrieval and display of titles as terminal links for graphs</li> </ul> </li> <li><code>graph export</code> command<ul> <li>option <code>--compress</code> to generate compressed ttl file</li> </ul> </li> <li><code>graph import</code> command<ul> <li>support import of compressed ttl/nt files</li> </ul> </li> <li><code>admin store export</code> command<ul> <li><code>--replace</code> option to replace an existing file</li> <li>if no BACKUP_FILE is given, a default of <code>{{date}}-{{connection}}.store.zip</code> is used</li> </ul> </li> <li><code>project import</code> command<ul> <li><code>--replace</code> option to replace an existing project</li> </ul> </li> <li><code>project export</code> command<ul> <li><code>--replace</code> option to replace an existing file</li> </ul> </li> <li><code>admin workspace export</code><ul> <li><code>--replace</code> option to replace an existing file</li> </ul> </li> <li><code>admin metrics</code> command group<ul> <li>support for build / data integration metrics, e.g. <code>build:cmem_workspace_task_spec_size</code></li> <li>support for GraphDB store metrics, e.g. <code>store:graphdb_slow_queries_count</code></li> </ul> </li> <li><code>admin metrics list</code> command<ul> <li>documentation column to output table</li> <li><code>--filter</code> option to filter metrics table by job, name, ID, or type</li> </ul> </li> <li><code>admin acl</code> command group<ul> <li>support for updated 24.3 access condition vocabulary and ACL graph</li> </ul> </li> <li><code>admin migration</code> command group<ul> <li><code>admin migration list</code> command - List migration recipes</li> <li><code>admin migration execute</code> command - Execute needed migration recipes</li> <li>The following migration recipes are available:<ul> <li><code>bootstrap-data</code> - Re-import bootstrap system data to match current version</li> <li><code>workspace-configurations</code> - Forward-upgrade explore workspace configurations</li> <li><code>acl-graph-24.3</code> - Move access conditions and used queries to new ACL graph</li> <li><code>acl-vocab-24.3</code> - Migrate auth vocabulary terms (actions and other grants)</li> <li><code>chart-widgets-24.3</code> - Migrate Chart Property Shapes to Widget Integrations</li> <li><code>workflow-trigger-widgets-24.3</code> - Migrate Workflow Trigger Property Shapes to Widget Integrations</li> </ul> </li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are included:</p> <ul> <li>cmemc will not fail anymore when the config dir is not creatable (message in debug)</li> <li>cmemc will not fail anymore when the config ini is not readable (message in debug)</li> <li>For these commands <code>admin acl list</code>, <code>dataset list</code>, <code>graph list</code>, <code>project list</code>, <code>admin user list</code>, <code>project variable list</code>, <code>vocabulary list</code>, <code>workflow list</code>, <code>admin workspace python list</code>, <code>admin workspace python list-plugins</code>, <code>dataset resource list</code>, <code>workflow scheduler list</code>, and <code>vocabulary cache list</code>:<ul> <li>ommit empty tables with usage note message</li> </ul> </li> <li><code>admin status</code> command<ul> <li>component name change: DI -&gt; BUILD</li> <li>component name change: DP -&gt; EXPLORE</li> <li>component removal: DM (merged with DP into EXPLORE)</li> <li>key prefix change: dp -&gt; explore</li> <li>key prefix change: di -&gt; build</li> </ul> </li> <li><code>project export</code> command<ul> <li><code>--filename-template</code> completion examples adaption</li> </ul> </li> <li><code>dataset create</code> command<ul> <li>Support compressed zip files for dataset types including CSV, XML, JSON, YAML, and plain text.</li> </ul> </li> <li><code>admin metrics</code> command group<ul> <li>metrics identification now as combined ID of <code>job_id:metrics_name</code></li> </ul> </li> <li><code>admin metrics</code> command group<ul> <li><code>--job</code> option, use <code>--filter job job_id</code> or combined metrics ID instead</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v24.3.0 into DataIntegration v24.2.0 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v24.2.0 can be imported into DataIntegration v24.3.0.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#eccenca-dataintegration","title":"eccenca DataIntegration","text":"<ul> <li>CSV files are no longer deleted by default at the beginning of a workflow execution. This behavior can be changed in the CSV dataset configuration.</li> <li>Access control changes. Action URIs have been renamed and new actions are introduced by default:<ul> <li><code>urn:eccenca:di</code> -&gt; <code>&lt;https://vocab.eccenca.com/auth/Action/Build&gt;</code> (will be handled by <code>cmemc admin migration</code>, see below).</li> <li><code>urn:elds-backend-all-actions</code> -&gt; <code>&lt;https://vocab.eccenca.com/auth/Action/AllActions&gt;</code> (will be handled by <code>cmemc admin migration</code>, see below).</li> <li>Python plugin management endpoints are now secured via <code>&lt;https://vocab.eccenca.com/auth/Action/Build-AdminPython&gt;</code> action.</li> <li>Workspace admin functions (reload workspace, import workspace) are now secured via <code>&lt;https://vocab.eccenca.com/auth/Action/Build-AdminWorkspace&gt;</code> action.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#eccenca-explore","title":"eccenca Explore","text":"<p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p> <p>This release introduces changes to internally used graphs (access control graph) and data structures (ACL actions, workspace configuration, chart integration, workflow trigger integration, table report integration).</p> <p>For easy migration, we introduce a new cmemc command group that automates the necessary adjustments to use your configuration with v24.3: <code>cmemc admin migration</code>.</p> <p>Test if migrations are required: <code>cmemc admin migration execute --test-only --all</code>.</p> <p>Run a specific migration: <code>cmemc admin migration execute &lt;recipe-id&gt;</code>.</p> <p>Run all migrations: <code>cmemc admin migration execute --all</code>.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-24-3/#eccenca-corporate-memory-control-cmemc","title":"eccenca Corporate Memory Control (cmemc)","text":"<ul> <li>All scripts which used the <code>admin status</code> command with the <code>--key</code> option:<ul> <li>adapt the key prefixes accordingly:<ul> <li>old: <code>cmemc admin status --key dp.info.license.validDate</code></li> <li>new: <code>cmemc admin status --key explore.info.license.validDate</code></li> </ul> </li> </ul> </li> <li><code>admin store migrate</code> command deprecated<ul> <li>use the <code>admin migration</code> command group instead</li> </ul> </li> <li><code>--overwrite</code> options deprecated - will be removed with the next major version<ul> <li>affected commands:<ul> <li><code>project import</code> command</li> <li><code>project export</code> command</li> <li><code>admin workspace export</code> command</li> </ul> </li> </ul> </li> <li>All scripts which used the <code>admin metrics</code> command group:<ul> <li>use combined metrics ID of <code>job_id:metrics_name</code></li> <li>use <code>--filter job job_id</code> instead of <code>--job job_id</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-1/","title":"Corporate Memory 25.1.2","text":"<p>Corporate Memory 25.1 is the first major release in 2025.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li> <p>Build: Seamless Workflow Integration</p> <ul> <li>Directly connecting datasets with explicit schemas to workflow operators simplifies data ingestion and processing, allowing users to quickly incorporate CSV and text data into their workflows.</li> </ul> </li> <li> <p>Build: Improved Rule Editing Experience</p> <ul> <li>Enhanced copy &amp; paste functionality in rule editors boosts productivity by making it easier to manage and edit rules accurately and efficiently.</li> </ul> </li> <li> <p>Explore and Autor: Streamlined Shape Management</p> <ul> <li>The introduction of new SHACL shape quick-access options empowers users to effortlessly build, validate, and troubleshoot complex shape configuration.</li> </ul> </li> <li> <p>Automate: Lightning-fast Parameterized Queries</p> <ul> <li>The new <code>cmemc</code> query placeholder specifications enable super-fast execution of parameterized queries by running background value queries to provide dynamic completions, significantly enhancing data query responsiveness.</li> </ul> </li> </ul> <p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataIntegration v25.1.1</li> <li>eccenca Explore v25.1.2 (formerly DataPlatform and DataManager)</li> <li>eccenca Corporate Memory Control (cmemc) v25.1.1</li> </ul> <p>We tested this release with the following dependency components:</p> <ul> <li>Ontotext GraphDB v10.8.3</li> <li>Keycloak v25.0.6</li> </ul> <p>More detailed information for this release is provided in the next sections.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-1/#eccenca-dataintegration-v2511","title":"eccenca DataIntegration v25.1.1","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v25.1, which introduces new features, improvements and bug fixes:</p> <p>v25.1.1 of DataIntegration ships the following fixes and additions:</p> <ul> <li>OIDC<ul> <li>added support for Request Party-Initiated Logout as specified by OpenID Connect</li> </ul> </li> <li>S3<ul> <li>Now defaults to the AWS credentials provider chain if S3 access and secret keys are not configured.</li> </ul> </li> <li>Python<ul> <li>Prevented redundant re-imports of already loaded modules.</li> <li>Updated <code>FileEntitySchema</code> in <code>cmem-plugin-base</code> for improved compatibility with datasets.</li> </ul> </li> <li>RegexExtractionTransformer<ul> <li>Resolved an issue that incorrectly generated null values.</li> </ul> </li> </ul> <p>v25.1.0 of DataIntegration adds the following new features:</p> <ul> <li>Use colors for workbench tags.</li> <li>Added a new operator for concatenating input values into a file.</li> <li>Enabled copy &amp; paste functionality in rule editors.</li> <li>Datasets with explicit schemas can now be directly connected to workflow operators.<ul> <li>Supported for CSV and text datasets.</li> <li>If a supported dataset is connected to a workflow operator with a flexible input schema, the entire dataset (i.e., all properties of its primary type) is read.</li> <li>For CSV datasets, this results in entities being read with all columns included.</li> </ul> </li> <li>Allow changing the width of blocks in the mapping editor.</li> </ul> <p>v25.1.0 of DataIntegration introduces the following changes:</p> <ul> <li>Invisible parameters are now part of the config port schema.</li> <li>Improved file names for downloaded projects and workspaces.</li> <li>SPARQL results are streamed as JSON instead of XML.</li> <li>The root breadcrumb and the Build logo in the navigation sidebar now direct to the projects search facet instead of All types.</li> </ul> <p>v25.1.0 of DataIntegration ships the following fixes:</p> <ul> <li>Fixed URI rule evaluation failure for empty object mappings.</li> <li>No duplicate JDBC jar configuration is required anymore.</li> <li>Fixed issue with JSON datasets not always navigating into arrays.</li> <li>Fixed issue where direct transform execution does not use project variables.</li> <li>Fixed Transform Evaluation failure when a rule contains a template transformer.</li> <li>Fixed issue where URI pattern input sometimes resets to its initial value or crashes the mapping editor.</li> <li>Fixed issue where SPARQL restriction expands the wrong SPARQL pattern when using property paths with prefixed names.</li> <li>Fixed RDF file upload issue.</li> <li>Fixed issue where the reference entities cache fails to load a large number of entities from the RDF store.</li> <li>Fixed issue where tasks created in the workflow editor are not added to the recently viewed list.</li> <li>Fixed issue where adding a note to a linking rule fails to save.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-1/#eccenca-explore-v2512","title":"eccenca Explore v25.1.2","text":"<p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p> <p>We are excited to announce Explore v25.1, which introduces new features, improvements and bug fixes.</p> <p>v25.1.2 of Explore ships the following fixes:</p> <ul> <li>Spring Boot Gradle plugin patch upgrade to address CVE-2025-31651</li> <li>Consider owl import resolution in QUERY CBD resolution strategy</li> <li>Link Rules - Rule Setup: parameters of paths retain values, changes on save are shown, even if requests in the background still run.</li> <li>Fix ACL Management rights for writing access conditions</li> </ul> <p>v25.1.1 of Explore ships the following fixes:</p> <ul> <li>OIDC<ul> <li>Add deployment property for post logout redirect uri</li> </ul> </li> <li>Shacl<ul> <li>Disable adding properties if the max amount of properties is reached</li> <li>Show fields of subshapes</li> </ul> </li> <li>BKE<ul> <li>Prevent loading candidates queries for readonly properties</li> <li>Switching between nodeshapes</li> </ul> </li> </ul> <p>v25.1.0 of Explore adds the following new features:</p> <ul> <li>Other<ul> <li>Added support for Virtuoso 8.3:<ul> <li>Uses the eccenca Docker image for GitLab CI tests.</li> <li>Includes adjustments in the store connection to address specific Virtuoso issues.</li> </ul> </li> </ul> </li> <li>SHACL<ul> <li>Added a download option for value queries in the complex view.</li> <li>Values in the table view are now sorted by IRI by default; this can be overridden by setting <code>shui:disableDefaultValueSorting true</code>.</li> <li>Added a new SHACL form to the graph creation interface.</li> <li>Added a debug node shape option for quick access.</li> <li>Corrected the display of lists of <code>xsd:anyURI</code> literals with long URIs.</li> </ul> </li> <li>BKE<ul> <li>Improved whitespace formatting in the BKE dossier.</li> </ul> </li> </ul> <p>v25.1.0 of Explore ships the following changes:</p> <ul> <li>SHACL<ul> <li>Conditionally hid the remove, create, and clone buttons.</li> <li>Added support for GraphDB 10.8.3.</li> <li>Removed quad upload support for GSP and the upload endpoint (GSP quads are not supported by stores or are uploaded as triples to a single graph only).</li> </ul> </li> <li>Link Rules<ul> <li>Adjusted link rules to use the new ACL API.</li> </ul> </li> <li>SHACL<ul> <li>Stabilized the UI during loading.</li> <li>Updated graph creation forms to the current SHACL system.</li> </ul> </li> <li>BKE<ul> <li>Merged the display of relations when property shape mode is deactivated.</li> <li>Saved graph changes while preserving the visualization state.</li> </ul> </li> <li>Query Module<ul> <li>Catalogue queries are now deleted using resource deletion (CBD).</li> </ul> </li> <li>Other<ul> <li>Switched the backend build system to use Maven Central instead of Artifactory, which also removes the blocking Virtuoso dependency.</li> <li>Added <code>POST</code> endpoints for <code>GET</code> data requests that may result in long IRIs.</li> <li>Updated to Spring Boot 3.4.</li> <li>Made Apache Jena SPARQL query result streaming adjustable via the <code>proxy.proxy-sparql-streaming-format</code> configuration (default: <code>XML</code>; possible values: <code>JSON</code>, <code>XML</code>).</li> <li>Updated Apache Jena to version 5.3.0.</li> <li>Implemented a new serialization method for paged responses (currently relevant only for access condition management endpoints/clients).</li> </ul> </li> </ul> <p>v25.1.0 of Explore ships the following fixes:</p> <ul> <li>Other<ul> <li>Render node shapes without property shapes correctly\u2014that is, display their widgets.</li> <li>The root admin username now resolves to the actual account name rather than a fixed <code>admin</code>.</li> <li>GSP file uploads via multipart requests now allow file suffixes in uppercase.</li> <li>Re-enabled Prometheus cache metrics.</li> <li>URIs with escaped characters are now preserved.</li> <li>Added an indication for broken workspace configurations in the UI.</li> <li>Fixed missing translations in messages prompting necessary re-login.</li> </ul> </li> <li>SHACL<ul> <li>Resolved the <code>shuiObject</code> placeholder correctly in custom queries when a resource is created.</li> <li>In Shacline, subshapes now have a cutoff of 20; a warning is displayed when the limit is reached.</li> <li>Fixed an issue where adding a new subshape caused unwanted duplication of existing subshapes of the same type.</li> <li>Subshapes are now removed if their removal is revoked.</li> </ul> </li> <li>BusinessKnowledgeEditor (BKE)<ul> <li>Added support for value queries in BKE.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-1/#eccenca-corporate-memory-control-cmemc-v2511","title":"eccenca Corporate Memory Control (cmemc) v25.1.1","text":"<p>We\u2019re excited to bring you the latest update to Corporate Memory Control (cmemc) v25.1, which introduces new features, improvements and bug fixes.</p> <p>v25.1.1 of cmemc introduces the following changes:</p> <ul> <li>corrected target versions</li> <li>corrected migration target versions</li> </ul> <p>v25.1.0 of cmemc adds the following new features and change behaviour:</p> <ul> <li><code>query execute</code> command<ul> <li>shell completion of placeholder values (using annotated QueryPlaceholder resources)</li> </ul> </li> <li><code>admin workspace python reload</code> command<ul> <li>reload / register all installed plugins into the DataIntegration workspace</li> </ul> </li> <li><code>admin workspace python list-plugins</code> command<ul> <li>will warn now if plugins are installed but not registered</li> </ul> </li> <li><code>admin migration</code> command group<ul> <li><code>hide-header-footer-25.1</code> migration recipe</li> <li>Remove triples using deprecated shui:valueQueryHideHeader|Footer terms</li> </ul> </li> <li><code>query execute</code> command<ul> <li>in case the user does not request a specific content type, some results are shown as a table (instead of <code>text/csv</code>)</li> <li><code>--accept</code> option now has completion support</li> </ul> </li> <li>base command<ul> <li><code>--external-http-timeout</code> option to specify the timeout for non-CMEM HTTP requests</li> </ul> </li> <li>configuration via INI config file<ul> <li>allow debug and proxy settings for a connection</li> <li>allow settings in the <code>DEFAULT</code> section of the config file for all connections</li> </ul> </li> <li>change in configuration loading order, to integrate the values from the <code>DEFAULT</code> section:<ol> <li>load environment variables into options-dict (click is doing this for <code>CMEMC_</code> variables)</li> <li>load options from command line and overwrite environment (only <code>CMEMC_</code> variables)</li> <li>load <code>DEFAULT</code> value keys, but not for keys which are already set (i.e not override cli options or env variables)</li> <li>load named INI section values (in case there is an INI section given) -&gt; this will not overwrite everything</li> <li>use API defaults if there are not enough config keys (use default <code>CMEM_BASE_URI</code>, default <code>OAUTH_GRANT_TYPE</code> and default <code>OAUTH_CLIENT_ID</code>|<code>SECRET</code> if not present)</li> </ol> </li> </ul> <p>In addition the following changes and fixes are included:</p> <ul> <li><code>admin workspace python uninstall</code> command<ul> <li>shell completion uses correct connection now</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-1/#migration-notes","title":"Migration Notes","text":"<p>Info</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v25.1.0 into DataIntegration v24.3.0 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v24.3.0 can be imported into DataIntegration v25.1.0.</p> <p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-1/#eccenca-corporate-memory-control-cmemc","title":"eccenca Corporate Memory Control (cmemc)","text":"<ul> <li><code>query execute</code> command<ul> <li>use <code>--accept</code> in case you need explicit CSV output</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/","title":"Corporate Memory 25.2.0","text":"<p>Corporate Memory 25.2.0 is the second major release in 2025.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li> <p>Build: Enhanced File Management in Workflows</p> <ul> <li>New binary file dataset and project file operators enable seamless integration of PDFs, images, and other binary files directly into workflows, streamlining document processing pipelines.</li> </ul> </li> <li> <p>Explore: Dynamic Class and Property Creation</p> <ul> <li>Create classes and properties on-the-fly while defining SHACL shapes, dramatically accelerating ontology development and data modeling workflows without context switching.</li> </ul> </li> <li> <p>Explore and Automate: Multi-Graph Query Management</p> <ul> <li>The enhanced query catalog now supports multiple query graphs and arbitrary graph selection, enabling better organization and management of SPARQL queries across different knowledge domains.</li> </ul> </li> <li> <p>Build: Mapping Creator (BETA)</p> <ul> <li>New visual mapping management and GenAI based mapping environment, allowing unparalleled clarity, speed and ease in building and maintaining your mapping rules.</li> </ul> </li> </ul> <p>Important info</p> <p>Since v24.3.0, the components eccenca DataPlatform and eccenca DataManager are merged into a single component eccenca Explore.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataIntegration v25.2.0</li> <li>eccenca Explore v25.2.0</li> <li>eccenca Corporate Memory Control (cmemc) v25.4.0</li> </ul> <p>We tested this release with the following dependency components:</p> <ul> <li>Ontotext GraphDB v11.0.2</li> <li>Keycloak v25.0.6</li> </ul> <p>More detailed information for this release is provided in the next sections.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#eccenca-dataintegration-v2520","title":"eccenca DataIntegration v25.2.0","text":"<p>We are excited to announce the release of DataIntegration v25.2.0, which introduces powerful new file handling capabilities, enhanced workflow features, and important infrastructure updates.</p> <p>v25.2.0 of DataIntegration adds the following new features:</p> <ul> <li>New operators and dataset for improved file handling in workflows:<ul> <li>Add project files workflow operator - Add files to projects directly from workflows</li> <li>Get project files workflow operator - Retrieve and process project files within workflow executions</li> <li>Binary file dataset - Handle binary files (PDF, images, etc.) in data integration pipelines</li> </ul> </li> <li>Neo4j database configuration - Added parameter to configure specific databases in Neo4j connections</li> <li>Project variable autocompletion - All template operators now support autocompletion for project variables</li> <li>Camel case transform operator - Convert text to camel case format for data standardization</li> <li>Project page URL suffix configuration - New config key <code>workbench.project.defaultUrlSuffix</code> to configure the project page view (defaults to <code>?itemType=workflow&amp;page=1&amp;limit=10</code>)</li> <li>Path auto-completion - Mapping and linking rule editors now feature intelligent path auto-completion like in value mapping forms</li> </ul> <p>v25.2.0 of DataIntegration introduces the following changes:</p> <ul> <li>Infrastructure updates:<ul> <li>Migrated to Java 21 for improved performance and latest language features</li> <li>Updated Docker base image to <code>eclipse-temurin:21-ubi9-minimal</code></li> </ul> </li> <li>\u201cInternal dataset (single graph)\u201d added to plugins to properly display reports using this dataset type</li> <li>Configurable favicon - Organizations can customize the application favicon</li> <li>JSON dataset improvements:<ul> <li>New parameter to control automatic navigation into JSON arrays</li> <li>New <code>#arrayPath</code> path operator for explicit navigation into JSON arrays (available when automatic JSON array navigation is set to <code>false</code>)</li> <li>New <code>#uuid</code> path operator generates type 3 (name-based) UUIDs from JSON node string representations</li> <li>New <code>#arrayText</code> path operator for enhanced array value extraction</li> </ul> </li> </ul> <p>v25.2.0 of DataIntegration ships the following fixes:</p> <ul> <li>Fixed queries with ORDER BY clauses in SQL dataset</li> <li>Fixed create task dialog focus issues when opened via \u2018connect to newly created\u2026\u2019 menu option</li> <li>Fixed errors in Office365 dataset tests and adapted to Microsoft API changes</li> <li>Fixed display issues for workflow reports containing internal datasets</li> <li>Fixed drag-and-drop problems when adding operators to nested workflow editors</li> <li>Non-printable characters in CSV datasets are now preserved during read/write transformations</li> <li>XML datasets now return empty values for empty tags when string values are expected</li> <li>Project variable updates now properly use the triggering user\u2019s credentials</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#eccenca-explore-v2520","title":"eccenca Explore v25.2.0","text":"<p>We are pleased to announce Explore v25.2.0, which brings significant enhancements to SHACL shape management, improved graph handling, and a modernized knowledge graph exploration experience.</p> <p>v25.2.0 of Explore adds the following new features:</p> <ul> <li>Enhanced SHACL Shape Management:<ul> <li>Create target classes for node shapes on-the-fly during shape definition</li> <li>Create properties for property shapes on-the-fly without leaving the shape editor</li> <li>Create classes for property shapes on-the-fly for better data modeling</li> <li>Support for defining properties with <code>domainIncludes</code> and <code>rangeIncludes</code> predicates (as defined in either <code>schema:</code>,<code>dcam:</code> or <code>gist:</code>) )</li> </ul> </li> <li>Query Catalog Enhancements:<ul> <li>Graph selection support for Query Catalog, allowing multiple query catalog graphs and editing queries in arbitrary graphs</li> <li>Graph selection support for Charts visualization, allowing to store and edit chart visualization in arbitrary graphs</li> </ul> </li> <li>GraphDB 11.0.x Support - Full compatibility with the latest GraphDB version</li> <li>Unified Error Handling - New RTKAction handler provides consistent error handling across the application</li> </ul> <p>v25.2.0 of Explore introduces the following changes:</p> <ul> <li>Infrastructure Updates:<ul> <li>Upgraded to Spring Boot 3.5.x and Apache Jena 5.4</li> <li>Migrated to Java 21 runtime for improved performance</li> </ul> </li> <li>Timetracker Module - Complete rework of the Timetracker and reports module for better performance and usability</li> <li>Knowledge Graph Editor (BKE) Improvements:<ul> <li>Updated to React Flow v12 for enhanced graph visualization</li> <li>Automatic canvas scrolling when dragging items beyond visible area</li> <li>Advanced multi-select functionality on canvas for bulk operations</li> </ul> </li> <li>SPARQL Query Endpoints - Changed to use an explicit list of allowed content-types for better security</li> <li>Catalog Query Management - Update and SELECT queries are now differentiated by <code>rdf:type</code> using <code>shui:SparqlQuery</code> or <code>shui:UpdateQuery</code></li> </ul> <p>v25.2.0 of Explore ships the following fixes:</p> <ul> <li>Query Catalog:<ul> <li>Fixed SPARQL Query editor behavior after \u201csave as\u201d operation</li> <li>Improved error handling in Query Catalog API</li> </ul> </li> <li>Knowledge Graph Editor (BKE):<ul> <li>Property shape descriptions now consistently display as tooltips</li> <li>Fixed selection issues with expanded nodes</li> <li>Fixed greyed-out entries in Initial Search &amp; Explore Navigation Box</li> <li>Resolved highlight lag issues for better performance</li> </ul> </li> <li>General Fixes:<ul> <li>Added warning when \u201cNew graph from File\u201d overwrites existing graphs</li> <li>Empty node shapes are now properly hidden</li> <li>Fixed SHACL Edit Validation Button stability issues</li> <li>Fixed SHACL MaxCount property behavior</li> <li>Resolved duplicate entries in ResourceManager table</li> <li>Fixed broken resource selection for domain and range when graphs contain complex classes</li> <li>Enabled empty GSP multipart file uploads</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#eccenca-corporate-memory-control-cmemc-v2540","title":"eccenca Corporate Memory Control (cmemc) v25.4.0","text":"<p>Important info</p> <p>This eccenca Corporate Memory release is the first release where we introduced Semantic Versioning for our components. This cmemc release notes section reflects this by reporting multiple minor versions in one section.</p> <p>We are excited to announce cmemc v25.4.0, which introduces new features, improvements and bug fixes.</p> <p>v25.4.0 of cmemc adds the following new features:</p> <ul> <li><code>query</code> command group<ul> <li>can be used with arbitrary query graphs now</li> <li><code>query list</code> command - new <code>--catalog-graph</code> option to select query catalog</li> <li><code>query execute</code> command - new <code>--catalog-graph</code> option to select query catalog</li> <li><code>query open</code> command - new <code>--catalog-graph</code> option to select query catalog</li> </ul> </li> </ul> <p>v25.3.0 of cmemc adds the following new features:</p> <ul> <li><code>dataset create</code> command<ul> <li>support for binary file datasets<ul> <li>suggest pdf, png, jpg, jpeg, gif and tiff files as binary file dataset</li> <li>shell completion of these files</li> </ul> </li> </ul> </li> <li><code>workflow io</code> command<ul> <li>support for binary file datasets<ul> <li>accept <code>application/octet-stream</code> as mime type for input and output files</li> <li>shell completion of pdf, png, jpg, jpeg, gif and tiff files as input and output</li> </ul> </li> <li>add support for markdown documents as text datasets</li> </ul> </li> </ul> <p>v25.2.0 of cmemc adds the following new features:</p> <ul> <li><code>graph imports</code> command group<ul> <li><code>graph imports create</code> command - Add graph import to a graph</li> <li><code>graph imports delete</code> command - Delete graph import from a graph</li> <li><code>graph imports list</code> command - List accessible graph\u2019s imports</li> </ul> </li> <li><code>graph export</code> command<ul> <li><code>--include-import-statements</code> option to save a <code>*.imports</code> file preserving imports of a graph</li> </ul> </li> <li><code>graph import</code> command<ul> <li><code>--include-import-statements</code> option to read the <code>*.imports</code> files and add the preserved imports to the store</li> </ul> </li> <li><code>graph delete</code> command<ul> <li><code>--include-import-statements</code> option to delete imports from other graphs to the deleted graph</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#eccenca-dataintegration","title":"eccenca DataIntegration","text":"<ul> <li>The following plugins have been deprecated and will be removed in a future release:<ul> <li>Old Python plugins depending on Jython (Python 2.x)</li> <li>Spark scripting plugins</li> <li>Spark virtual dataset</li> <li>Legacy REST operator</li> </ul> </li> <li>To check if your instance uses any deprecated plugins, use the endpoint: <code>GET {DataIntegrationURL}/api/core/usages/deprecatedPlugins</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#eccenca-explore","title":"eccenca Explore","text":"<ul> <li>Query Catalog Query Type Changes - Catalog managed queries no longer persist <code>shui:queryType</code>. Update and SELECT queries are now differentiated by <code>rdf:type</code>:<ul> <li>SELECT queries use <code>shui:SparqlQuery</code></li> <li>UPDATE queries use <code>shui:UpdateQuery</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-25-2/#eccenca-corporate-memory-control-cmemc","title":"eccenca Corporate Memory Control (cmemc)","text":"<ul> <li>With the introduction of the <code>graph imports</code> command group, the <code>graph tree</code> command is now deprecated.<ul> <li>use <code>graph imports tree</code> instead</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"tutorials/","title":"Tutorials","text":"<p>Info</p> <p>This list of Tutorial style content can be used to learn by example and by following step-by-step guides.</p> <p>If in doubt, start with the Getting Started Tutorial.</p>"},{"location":"tutorials/#tag:beginnerstutorial","title":"BeginnersTutorial","text":"<ul> <li>            Active Learning of Linking Rules          </li> <li>            Getting Started          </li> <li>            Lift data from tabular data such as CSV, XSLX or database tables          </li> </ul>"},{"location":"tutorials/#tag:advancedtutorial","title":"AdvancedTutorial","text":"<ul> <li>            Build Knowledge Graphs from Kafka Topics          </li> <li>            Connect to Snowflake          </li> <li>            Evaluate Jinja Template and Send an Email Message          </li> <li>            Lift data from JSON and XML source          </li> <li>            Populate Data to Neo4j          </li> </ul>"},{"location":"tutorials/#tag:experttutorial","title":"ExpertTutorial","text":"<ul> <li>            Consuming Graphs with SQL Databases          </li> <li>            Extracting data from a Web API          </li> <li>            How to link Intrusion Detection Systems (IDS) to Open-Source INTelligence (OSINT)          </li> <li>            Loading JDBC datasets incrementally          </li> <li>            Processing Data with Variable Input Workflows          </li> <li>            Provide Data in any Format via a Custom API          </li> </ul>"}]}