{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00a4 This is just a markdown sandbox for Training Purpose. You can change it via the edit buttons or using a dedicated markdown editor such as obsidian .","title":"Home"},{"location":"#welcome","text":"This is just a markdown sandbox for Training Purpose. You can change it via the edit buttons or using a dedicated markdown editor such as obsidian .","title":"Welcome"},{"location":"automate/","text":"\u2606 Automate \u00a4 Setup processes and automate activities based on and towards your Knowledge Graph. cmemc - Command Line Interface \u2014 cmemc is intended for system administrators and Linked Data Expert, who wants to automate and remote control activities on eccenca Corporate Memory. Processing data with variable input workflows \u2014 This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (means, without registering datasets). Scheduling Workflows \u2014 For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator.","title":"Automate"},{"location":"automate/#automate","text":"Setup processes and automate activities based on and towards your Knowledge Graph. cmemc - Command Line Interface \u2014 cmemc is intended for system administrators and Linked Data Expert, who wants to automate and remote control activities on eccenca Corporate Memory. Processing data with variable input workflows \u2014 This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (means, without registering datasets). Scheduling Workflows \u2014 For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator.","title":"\u2606 Automate"},{"location":"automate/cmemc-command-line-interface/","tags":["cmemc","Automate"],"text":"cmemc - Command Line Interface \u00a4 cmemc is intended for system administrators and Linked Data Expert, who wants to automate and remote control activities on eccenca Corporate Memory. Main features of cmemc include: List, edit and check configurations. List, create, delete, inspect datasets as well as dataset resources. List, import, export, delete or open graphs. List, import, export, create or delete Build projects. List, execute, replay or open local and remote SPARQL queries. List, install, uninstall, import and open vocabularies. List, execute, open or inspect workflows and workflow schedulers. Import, export and reload build workspaces. Import, export and bootstrap graph stores. List, get or inspect server metrics. In order to start working with cmemc, follow one of the installation options.","title":"Overview"},{"location":"automate/cmemc-command-line-interface/#cmemc-command-line-interface","text":"cmemc is intended for system administrators and Linked Data Expert, who wants to automate and remote control activities on eccenca Corporate Memory. Main features of cmemc include: List, edit and check configurations. List, create, delete, inspect datasets as well as dataset resources. List, import, export, delete or open graphs. List, import, export, create or delete Build projects. List, execute, replay or open local and remote SPARQL queries. List, install, uninstall, import and open vocabularies. List, execute, open or inspect workflows and workflow schedulers. Import, export and reload build workspaces. Import, export and bootstrap graph stores. List, get or inspect server metrics. In order to start working with cmemc, follow one of the installation options.","title":"cmemc - Command Line Interface"},{"location":"automate/cmemc-command-line-interface/certificate-handling-and-ssl-verification/","tags":["cmemc"],"text":"Certificate handling and SSL verification \u00a4 Introduction \u00a4 In a reasonable production deployment, all client-accessible Corporate Memory APIs will be securely available as HTTPS endpoints. This document clarifies how to deal with certificates. cmemc will validate the certificates of your HTTPS endpoints and will complain if there are validation errors. If the certificates of your Corporate Memory deployment are based on a common and publicly available Certificate Authority (such as Let\u2019s Encrypt ), cmemc is able to validate your certificates out of the box. However, in some cases you need to: provide your own certificates CA bundle in order to allow cmem to trust your servers, or you need to disable SSL verification at all (for debugging and testing purpose only). Provide you own CA bundle \u00a4 cmemc will validate all used certificates of your HTTPS API endpoints by using a built-in CA bundle which comes from python\u2019s certifi package . Certifi is a carefully curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the identity of TLS hosts. If you need to configure a custom CA bundle to use with a specific connection, you can do so by using the REQUESTS_CA_BUNDLE key in the config or as an environment variable. You can validate, which CA bundle is used by turning debugging on ( \u2013debug ) and watch for a CA bundle debug line (here, line 10). using the debug mode to watch for the CA bundle $ cmemc --debug -c ssltest.eccenca.com graph list [2020-03-11 17:50:59.135898] Set config to /home/user/Library/Application Support/cmemc/config.ini [2020-03-11 17:50:59.136284] Config loaded: /home/user/Library/Application Support/cmemc/config.ini [2020-03-11 17:50:59.137476] Use connection config: ssltest.eccenca.com [2020-03-11 17:50:59.137564] CMEM_BASE_URI set by config to https://ssltest.eccenca.com [2020-03-11 17:50:59.137611] REQUESTS_CA_BUNDLE set by config to cacert.pem [2020-03-11 17:50:59.137718] OAUTH_GRANT_TYPE set by config to client_credentials [2020-03-11 17:50:59.137760] OAUTH_CLIENT_ID set by config to cmem-service-account [2020-03-11 17:50:59.137804] OAUTH_CLIENT_SECRET set by config [2020-03-11 17:50:59.137978] CA bundle loaded from /home/user/cacert.pem http://di.eccenca.com/project/cmem urn:elds-backend-access-conditions-graph The CA bundle has to be available in the PEM format. You can use the openssl command line tool to fetch all certificates from an HTTPS URL and create a PEM CA Bundle out of it. Here is an example line, producing the cacert.pem file used in the example above: $ openssl s_client -showcerts -connect ssltest.eccenca.com:443 </dev/null 2 >/dev/null | openssl x509 -outform PEM >cacert.pem $ cat cacert.pem -----BEGIN CERTIFICATE----- MIIFyzCCA7MCFDoiAY9Ry8dfH0rS/rINUb6inlvGMA0GCSqGSIb3DQEBCwUAMIGh [...] miGId7jMXd24bpfYZSiniC0+SHiCwEmzN818Ss9aIMChymAnV3RRB/UqKLlOMnA= -----END CERTIFICATE----- Disabling SSL Verification at all \u00a4 You can also disable SSL Verification completely by setting the SSL_VERIFY key in the config or environment to false . However, this will lead to warnings: $ cmemc -c ssltest.eccenca.com graph list SSL verification is disabled (SSL_VERIFY=False). http://di.eccenca.com/project/cmem urn:elds-backend-access-conditions-graph","title":"Certificate handling and SSL verification"},{"location":"automate/cmemc-command-line-interface/certificate-handling-and-ssl-verification/#certificate-handling-and-ssl-verification","text":"","title":"Certificate handling and SSL verification"},{"location":"automate/cmemc-command-line-interface/certificate-handling-and-ssl-verification/#introduction","text":"In a reasonable production deployment, all client-accessible Corporate Memory APIs will be securely available as HTTPS endpoints. This document clarifies how to deal with certificates. cmemc will validate the certificates of your HTTPS endpoints and will complain if there are validation errors. If the certificates of your Corporate Memory deployment are based on a common and publicly available Certificate Authority (such as Let\u2019s Encrypt ), cmemc is able to validate your certificates out of the box. However, in some cases you need to: provide your own certificates CA bundle in order to allow cmem to trust your servers, or you need to disable SSL verification at all (for debugging and testing purpose only).","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/certificate-handling-and-ssl-verification/#provide-you-own-ca-bundle","text":"cmemc will validate all used certificates of your HTTPS API endpoints by using a built-in CA bundle which comes from python\u2019s certifi package . Certifi is a carefully curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the identity of TLS hosts. If you need to configure a custom CA bundle to use with a specific connection, you can do so by using the REQUESTS_CA_BUNDLE key in the config or as an environment variable. You can validate, which CA bundle is used by turning debugging on ( \u2013debug ) and watch for a CA bundle debug line (here, line 10). using the debug mode to watch for the CA bundle $ cmemc --debug -c ssltest.eccenca.com graph list [2020-03-11 17:50:59.135898] Set config to /home/user/Library/Application Support/cmemc/config.ini [2020-03-11 17:50:59.136284] Config loaded: /home/user/Library/Application Support/cmemc/config.ini [2020-03-11 17:50:59.137476] Use connection config: ssltest.eccenca.com [2020-03-11 17:50:59.137564] CMEM_BASE_URI set by config to https://ssltest.eccenca.com [2020-03-11 17:50:59.137611] REQUESTS_CA_BUNDLE set by config to cacert.pem [2020-03-11 17:50:59.137718] OAUTH_GRANT_TYPE set by config to client_credentials [2020-03-11 17:50:59.137760] OAUTH_CLIENT_ID set by config to cmem-service-account [2020-03-11 17:50:59.137804] OAUTH_CLIENT_SECRET set by config [2020-03-11 17:50:59.137978] CA bundle loaded from /home/user/cacert.pem http://di.eccenca.com/project/cmem urn:elds-backend-access-conditions-graph The CA bundle has to be available in the PEM format. You can use the openssl command line tool to fetch all certificates from an HTTPS URL and create a PEM CA Bundle out of it. Here is an example line, producing the cacert.pem file used in the example above: $ openssl s_client -showcerts -connect ssltest.eccenca.com:443 </dev/null 2 >/dev/null | openssl x509 -outform PEM >cacert.pem $ cat cacert.pem -----BEGIN CERTIFICATE----- MIIFyzCCA7MCFDoiAY9Ry8dfH0rS/rINUb6inlvGMA0GCSqGSIb3DQEBCwUAMIGh [...] miGId7jMXd24bpfYZSiniC0+SHiCwEmzN818Ss9aIMChymAnV3RRB/UqKLlOMnA= -----END CERTIFICATE-----","title":"Provide you own CA bundle"},{"location":"automate/cmemc-command-line-interface/certificate-handling-and-ssl-verification/#disabling-ssl-verification-at-all","text":"You can also disable SSL Verification completely by setting the SSL_VERIFY key in the config or environment to false . However, this will lead to warnings: $ cmemc -c ssltest.eccenca.com graph list SSL verification is disabled (SSL_VERIFY=False). http://di.eccenca.com/project/cmem urn:elds-backend-access-conditions-graph","title":"Disabling SSL Verification at all"},{"location":"automate/cmemc-command-line-interface/command-line-completion/","tags":["cmemc"],"text":"Command-Line Completion \u00a4 Introduction \u00a4 In case you are using bash or zsh as your terminal shell, you should enable Command-line tab completion for cmemc. Tab completion is a powerful feature and will save you a lot of typing work. Furthermore, it will help you to learn the different commands, parameters and options and will auto-complete parameter values taken live from your Corporate Memory instance (such as graph IRIs, project IDs, \u2026). We suggest to use zsh so you can take advantage of the advanced menu-completion feature of zsh. Installation \u00a4 In order to enable tab completion with zsh run the following command: completion setup for zsh $ eval \" $( _CMEMC_COMPLETE = source_zsh cmemc ) \" In order to enable tab completion with bash run the following command: completion setup for bash $ eval \" $( _CMEMC_COMPLETE = source cmemc ) \" You may want to add this line to your .bashrc or .zshrc .","title":"Command-Line Completion"},{"location":"automate/cmemc-command-line-interface/command-line-completion/#command-line-completion","text":"","title":"Command-Line Completion"},{"location":"automate/cmemc-command-line-interface/command-line-completion/#introduction","text":"In case you are using bash or zsh as your terminal shell, you should enable Command-line tab completion for cmemc. Tab completion is a powerful feature and will save you a lot of typing work. Furthermore, it will help you to learn the different commands, parameters and options and will auto-complete parameter values taken live from your Corporate Memory instance (such as graph IRIs, project IDs, \u2026). We suggest to use zsh so you can take advantage of the advanced menu-completion feature of zsh.","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/command-line-completion/#installation","text":"In order to enable tab completion with zsh run the following command: completion setup for zsh $ eval \" $( _CMEMC_COMPLETE = source_zsh cmemc ) \" In order to enable tab completion with bash run the following command: completion setup for bash $ eval \" $( _CMEMC_COMPLETE = source cmemc ) \" You may want to add this line to your .bashrc or .zshrc .","title":"Installation"},{"location":"automate/cmemc-command-line-interface/command-reference/","tags":["cmemc","Reference"],"text":"Command Reference \u00a4 This section lists the help texts of all commands as a reference and to search for it. Command group: admin \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... Import bootstrap data, backup/restore workspace or get status. This command group consists of commands for setting up and configuring eccenca Corporate Memory. Options: -h, --help Show this message and exit. Commands: bootstrap Update/Import bootstrap data. metrics List and get metrics. showcase Create showcase data. status Output health and version information. store Import, export and bootstrap the knowledge graph store. token Fetch and output an access token. workspace Import, export and reload the project workspace. Command: admin showcase \u00a4 Usage: cmemc [OPTIONS] Create showcase data. This command creates a showcase scenario of multiple graphs including integration graphs, shapes, statement annotations etc. Note: There is currently no deletion mechanism for the showcase data, so you need to remove the showcase graphs manually (or just remove all graphs). Options: --scale INTEGER The scale factor provides a way to set the target size of the scenario. A value of 10 results in around 40k triples, a value of 50 in around 350k triples. [default: 10] --create Delete old showcase data if present and create new showcase databased on the given scale factor. --delete Delete existing showcase data if present. -h, --help Show this message and exit. Command: admin bootstrap \u00a4 Usage: cmemc [OPTIONS] Update/Import bootstrap data. This command imports the bootstrap data needed for managing shapes, access conditions, the query catalog and the vocabulary catalog. Note: There is currently no deletion mechanism for the bootstrap data, so you need to remove the graphs manually (or just remove all graphs). Options: --import Delete existing bootstrap data if present and import bootstrap data which was delivered -h, --help Show this message and exit. Command: admin status \u00a4 Usage: cmemc [OPTIONS] Output health and version information. This command outputs version and health information of the selected deployment. If the version information can not be retrieved, UNKNOWN is shown if the endpoint is not available or ERROR is shown, if the endpoints returns an error. In addition to that, this command warns you if the target version of your cmemc client is newer than the version of your backend and if the ShapeCatalog has a different version then your DataPlatform component. To get status information of all configured deployments use this command in combination with parallel: cmemc config list | parallel --ctag cmemc -c {} admin status Options: -h, --help Show this message and exit. Command: admin token \u00a4 Usage: cmemc [OPTIONS] Fetch and output an access token. This command can be used to check for correct authentication as well as to use the token with wget / curl or similar standard tools: Example Usage: curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug Please be aware that this command can reveal secrets, which you do not want to have in log files or on the screen. Options: --raw Outputs raw JSON. Note that this option will always try to fetch a new JSON token response. In case you are working with OAUTH_GRANT_TYPE=prefetched_token, this may lead to an error. --decode Decode the access token and outputs the raw JSON. Note that the access token is only decoded and esp. not validated. -h, --help Show this message and exit. Command group: admin metrics \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List and get metrics. This command group consists of commands for reading and listing internal monitoring metrics of eccenca Corporate Memory. A deployment consists of multiple jobs (e.g. DP, DI), which provide multiple metric families on an endpoint. Each metric family can consist of different samples identified by labels with a name and a value (dimensions). A metric has a specific type (counter, gauge, summary and histogram) and additional metadata. Please have a look at https://prometheus.io/docs/concepts/data_model/ for further details. Options: -h, --help Show this message and exit. Commands: get Get sample data of a metric. inspect Inspect a metric. list List metrics for a specific job. Command: admin metrics get \u00a4 Usage: cmemc [OPTIONS] METRIC_ID Get sample data of a metric. A metric of a specific job is identified by a metric ID. Possible metric IDs of a job can be retrieved with the `metrics list` command. A metric can contain multiple samples. These samples are distinguished by labels (name and value). Options: --job [DP] The job from which the metrics data is fetched. [default: DP] --filter <TEXT TEXT>... A set of label name/value pairs in order to filter the samples of the requested metric family. Each metric has a different set of labels with different values. In order to get a list of possible label names and values, use the command without this option. The label names are then shown as column headers and label values as cell values of this column. --enforce-table A single sample value will be returned as plain text instead of the normal table. This allows for more easy integration with scripts. This flag enforces the use of tabular output, even for single row tables. --raw Outputs raw prometheus sample classes. -h, --help Show this message and exit. Command: admin metrics inspect \u00a4 Usage: cmemc [OPTIONS] METRIC_ID Inspect a metric. This command outputs the data of a metric. The first table includes basic meta data about the metric. The second table includes sample labels and values. Options: --job [DP] The job from which the metrics data is fetched. [default: DP] --raw Outputs raw JSON of the table data. -h, --help Show this message and exit. Command: admin metrics list \u00a4 Usage: cmemc [OPTIONS] List metrics for a specific job. For each metric, the output table shows the metric ID, the type of the metric, a count of how many labels (label names) are describing the samples (L) and a count of how many samples are currently available for a metric (S). Options: --job [DP] The job from which the metrics data is fetched. [default: DP] --id-only Lists metric identifier only. This is useful for piping the IDs into other commands. --raw Outputs (sorted) JSON dict, parsed from the metrics API output. -h, --help Show this message and exit. Command group: admin workspace \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... Import, export and reload the project workspace. Options: -h, --help Show this message and exit. Commands: export Export the complete workspace (all projects) to a ZIP file. import Import the workspace from a file. python List, install, or uninstall python packages. reload Reload the workspace from the backend. Command: admin workspace export \u00a4 Usage: cmemc [OPTIONS] [FILE] Export the complete workspace (all projects) to a ZIP file. Depending on the requested type, this ZIP contains either a turtle file for each project (type rdfTurtle) or a substructure of resource files and XML descriptions (type xmlZip). The file name is optional and will be generated with by the template if absent. Options: -o, --overwrite Overwrite existing files. This is a dangerous option, so use it with care. --type TEXT Type of the exported workspace file. [default: xmlZip] -t, --filename-template TEXT Template for the export file name. Possible placeholders are (Jinja2): {{connection}} (from the --connection option) and {{date}} (the current date as YYYY-MM-DD). The file suffix will be appended. Needed directories will be created. [default: {{date}}-{{connection}}.workspace] -h, --help Show this message and exit. Command: admin workspace import \u00a4 Usage: cmemc [OPTIONS] FILE Import the workspace from a file. Options: --type TEXT Type of the exported workspace file. [default: xmlZip] -h, --help Show this message and exit. Command: admin workspace reload \u00a4 Usage: cmemc [OPTIONS] Reload the workspace from the backend. Options: -h, --help Show this message and exit. Command group: admin workspace python \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, install, or uninstall python packages. Python packages are used to extend the DataIntegration workspace with python plugins. To get a list of installed packages, execute the list command. Warning: Installing packages from unknown sources is not recommended. Plugins are not verified for malicious code. Options: -h, --help Show this message and exit. Commands: install Install a python package to the workspace. list List installed python packages. list-plugins List installed workspace plugins. uninstall Uninstall a python package from the workspace. Command: admin workspace python install \u00a4 Usage: cmemc [OPTIONS] PACKAGE Install a python package to the workspace. This command is basically a 'pip install' in the remote python environment. You can install a package by uploading a source distribution .tar.gz file, or by uploading a build distribution .whl file, or by specifying a package name, more precisely, a pip requirement specifier with a package name available on pypi.org (e.g. 'requests==2.27.1'). Options: -h, --help Show this message and exit. Command: admin workspace python uninstall \u00a4 Usage: cmemc [OPTIONS] PACKAGE_NAME Uninstall a python package from the workspace. This command is basically a 'pip uninstall' in the remote python environment. Options: -h, --help Show this message and exit. Command: admin workspace python list \u00a4 Usage: cmemc [OPTIONS] List installed python packages. This command is basically a 'pip list' in the remote python environment. It outputs a table of python package identifiers with version information. Options: --raw Outputs raw JSON. --id-only Lists only package identifier. This is useful for piping the IDs into other commands. -h, --help Show this message and exit. Command: admin workspace python list-plugins \u00a4 Usage: cmemc [OPTIONS] List installed workspace plugins. This commands lists all discovered plugins. Note that the plugin discovery is limited to specific packages. Options: --raw Outputs raw JSON. --id-only Lists only plugin identifier. --package-id-only Lists only plugin package identifier. -h, --help Show this message and exit. Command group: admin store \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... Import, export and bootstrap the knowledge graph store. This command group consist of commands to administrate the knowledge graph store as a whole. Options: -h, --help Show this message and exit. Commands: bootstrap Update/Import bootstrap data. export Backup all knowledge graphs to a ZIP archive. import Restore graphs from a ZIP archive. showcase Create showcase data. Command: admin store showcase \u00a4 Usage: cmemc [OPTIONS] Create showcase data. This command creates a showcase scenario of multiple graphs including integration graphs, shapes, statement annotations etc. Note: There is currently no deletion mechanism for the showcase data, so you need to remove the showcase graphs manually (or just remove all graphs). Options: --scale INTEGER The scale factor provides a way to set the target size of the scenario. A value of 10 results in around 40k triples, a value of 50 in around 350k triples. [default: 10] --create Delete old showcase data if present and create new showcase databased on the given scale factor. --delete Delete existing showcase data if present. -h, --help Show this message and exit. Command: admin store bootstrap \u00a4 Usage: cmemc [OPTIONS] Update/Import bootstrap data. This command imports the bootstrap data needed for managing shapes, access conditions, the query catalog and the vocabulary catalog. Note: There is currently no deletion mechanism for the bootstrap data, so you need to remove the graphs manually (or just remove all graphs). Options: --import Delete existing bootstrap data if present and import bootstrap data which was delivered -h, --help Show this message and exit. Command: admin store export \u00a4 Usage: cmemc [OPTIONS] BACKUP_FILE Backup all knowledge graphs to a ZIP archive. The backup file is a ZIP archive containing all knowledge graphs as Turtle files + configuration file for each graph. This command will create lots of load on the server. It can take a long time to complete. Options: --overwrite Overwrite existing files. This is a dangerous option, so use it with care. -h, --help Show this message and exit. Command: admin store import \u00a4 Usage: cmemc [OPTIONS] BACKUP_FILE Restore graphs from a ZIP archive. The backup file is a ZIP archive containing all knowledge graphs as Turtle files + configuration file for each graph. The command will load a single backup ZIP archive into the triple store, by replacing all graphs with the content of the Turtle files in the archive and deleting all graphs which are not in the archive. This command will create lots of load on the server. It can take a long time to complete. The backup file will be transferred to the server, then unzipped and imported graph by graph. After the initial transfer, the network connection is not used anymore, so it will be closed by proxies sometimes. This does not mean that the import failed. Options: -h, --help Show this message and exit. Command group: config \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List and edit configs as well as get config values. Configurations are identified by the section identifier in the config file. Each configuration represent a Corporate Memory deployment with its specific access method as well as credentials. A minimal configuration which uses client credentials has the following entries: [example.org] CMEM_BASE_URI=https://cmem.example.org/ OAUTH_GRANT_TYPE=client_credentials OAUTH_CLIENT_ID=cmem-service-account OAUTH_CLIENT_SECRET=my-secret-account-pass Note that OAUTH_GRANT_TYPE can be either client_credentials, password or prefetched_token. In addition to that, the following config parameters can be used as well: SSL_VERIFY=False - for ignoring certificate issues (not recommended) DP_API_ENDPOINT=URL - to point to a non-standard DataPlatform location DI_API_ENDPOINT=URL - to point to a non-standard DataIntegration location OAUTH_TOKEN_URI=URL - to point to an external IdentityProvider location OAUTH_USER=username - only if OAUTH_GRANT_TYPE=password OAUTH_PASSWORD=password - only if OAUTH_GRANT_TYPE=password OAUTH_ACCESS_TOKEN=token - only if OAUTH_GRANT_TYPE=prefetched_token In order to get credential information from an external process, you can use the parameter OAUTH_PASSWORD_PROCESS, OAUTH_CLIENT_SECRET_PROCESS and OAUTH_ACCESS_TOKEN_PROCESS to setup an external executable. OAUTH_CLIENT_SECRET_PROCESS=/path/to/getpass.sh OAUTH_PASSWORD_PROCESS=[\"getpass.sh\", \"parameter1\", \"parameter2\"] The credential executable can use the cmemc environment for fetching the credential (e.g. CMEM_BASE_URI and OAUTH_USER). If the credential executable is not given with a full path, cmemc will look into your environment PATH for something which can be executed. The configured process needs to return the credential on the first line of stdout. In addition to that, the process needs to exit with exit code 0 (without failure). There are examples available in the online manual. Options: -h, --help Show this message and exit. Commands: edit Edit the user-scope configuration file. eval Export all configuration values of a configuration for evaluation. get Get the value of a known cmemc configuration key. list List configured connections. Command: config list \u00a4 Usage: cmemc [OPTIONS] List configured connections. This command lists all configured connections from the currently used config file. The connection identifier can be used with the --connection option in order to use a specific Corporate Memory instance. In order to apply commands on more than one instance, you need to use typical unix gear such as xargs or parallel: cmemc config list | xargs -I % sh -c 'cmemc -c % admin status' cmemc config list | parallel --jobs 5 cmemc -c {} admin status Options: -h, --help Show this message and exit. Command: config edit \u00a4 Usage: cmemc [OPTIONS] Edit the user-scope configuration file. Options: -h, --help Show this message and exit. Command: config get \u00a4 Usage: cmemc [OPTIONS] [CMEM_BASE_URI|SSL_VERIFY|REQUESTS_CA_BUNDLE|DP_API_END POINT|DI_API_ENDPOINT|OAUTH_TOKEN_URI|OAUTH_GRANT_TYPE|OAUTH_USER |OAUTH_PASSWORD|OAUTH_CLIENT_ID|OAUTH_CLIENT_SECRET|OAUTH_ACCESS_ TOKEN] Get the value of a known cmemc configuration key. In order to automate processes such as fetching custom API data from multiple Corporate Memory instances, this command provides a way to get the value of a cmemc configuration key for the selected deployment. Example Usage: curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug The commands returns with exit code 1 if the config key is not used in the current configuration. Options: -h, --help Show this message and exit. Command: config eval \u00a4 Usage: cmemc [OPTIONS] Export all configuration values of a configuration for evaluation. The output of this command is suitable to be used by a shells eval command. It will output the complete configuration as 'export key=\"value\"' statements. This allows for preparation of a shell environment. eval $(cmemc -c my config eval) Please be aware that credential details are shown in cleartext with this command. Options: --unset Instead of export all configuration keys, this option will unset all key. -h, --help Show this message and exit. Command group: dataset \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, create, delete, inspect, up-/download or open datasets. This command group allows for managing workspace datasets as well as dataset file resources. Datasets can be created and deleted. File resources can be uploaded and downloaded. Details of dataset parameter can be listed with inspect. Datasets are identified with a combined key of the project ID and the project internal dataset ID (e.g: my-project:my-dataset). To get a list of datasets, use the list command. Options: -h, --help Show this message and exit. Commands: create Create a dataset. delete Delete datasets. download Download the resource file of a dataset. inspect Display meta data of a dataset. list List available datasets. open Open datasets in the browser. resource List, inspect or delete dataset file resources. upload Upload a resource file to a dataset. Command: dataset list \u00a4 Usage: cmemc [OPTIONS] List available datasets. Outputs a list of datasets IDs which can be used as reference for the dataset create and delete commands. Options: --project TEXT The project, from which you want to list the datasets. Project IDs can be listed with the 'project list' command. --raw Outputs raw JSON objects of dataset search API response. --id-only Lists only dataset identifier and no labels or other meta data. This is useful for piping the ids into other cmemc commands. -h, --help Show this message and exit. Command: dataset delete \u00a4 Usage: cmemc [OPTIONS] [DATASET_IDS]... Delete datasets. This deletes existing datasets in integration projects from Corporate Memory. Datasets will be deleted without prompting! Dataset resources will not be deleted. Example: cmemc dataset delete my_project:my_dataset Datasets can be listed by using the 'cmemc dataset list' command. Options: -a, --all Delete all datasets. This is a dangerous option, so use it with care. --project TEXT In combination with the '--all' flag, this option allows for deletion of all datasets of a certain project. The behaviour is similar to the 'dataset list --project' command. -h, --help Show this message and exit. Command: dataset download \u00a4 Usage: cmemc [OPTIONS] DATASET_ID OUTPUT_PATH Download the resource file of a dataset. This command downloads the file resource of a dataset to your local file system or to standard out (-). Note that this is not possible for dataset types such as Knowledge Graph (eccencaDataplatform) or SQL endpoint (sqlEndpoint). Without providing an output path, the output file name will be the same as the remote file resource. Datasets can be listed by using the 'cmemc dataset list' command. Options: --replace Replace existing files. This is a dangerous option, so use it with care. -h, --help Show this message and exit. Command: dataset upload \u00a4 Usage: cmemc [OPTIONS] DATASET_ID INPUT_PATH Upload a resource file to a dataset. This command uploads a file to a dataset. The content of the uploaded file replaces the remote file resource. The name of the remote file resource is not changed. Warning: If the remote file resource is used in more than one dataset, the other datasets are also affected by this command. Warning: The content of the uploaded file is not tested, so uploading a json file to an xml dataset will result in errors. Datasets can be listed by using the 'cmemc dataset list' command. Example: cmemc dataset upload cmem:my-dataset new-file.csv Options: -h, --help Show this message and exit. Command: dataset inspect \u00a4 Usage: cmemc [OPTIONS] DATASET_ID Display meta data of a dataset. Options: --raw Outputs raw JSON. -h, --help Show this message and exit. Command: dataset create \u00a4 Usage: cmemc [OPTIONS] [DATASET_FILE] Create a dataset. Datasets are created in projects and can have associated file resources. Each dataset has a type (such as 'csv') and a list of parameter which can change or specify the dataset behaviour. To get more information on possible dataset types and parameter on these types, use the '--help-types' and '--help-parameter' options. Example: cmemc dataset create --project my-project --type csv my-file.csv Options: -t, --type TEXT The dataset type of the dataset to create. Example types are 'csv','json' and 'eccencaDataPlatform' (-> Knowledge Graph). --project TEXT The project, where you want to create the dataset in. If there is only one project in the workspace, this option can be omitted. -p, --parameter <TEXT TEXT>... A set of key/value pairs. Each dataset type has different parameters (such as charset, arraySeparator, ignoreBadLines, ...). In order to get a list of possible parameter, use the'--help-parameter' option. --replace Replace remote file resources in case there already exists a file with the same name. --id TEXT The dataset ID of the dataset to create. The dataset ID will be automatically created in case it is not present. --help-types Lists all possible dataset types on given Corporate Memory instance. Note that this option already needs access to the instance. --help-parameter Lists all possible (optional and mandatory) parameter for a dataset type. Note that this option already needs access to the instance. -h, --help Show this message and exit. Command: dataset open \u00a4 Usage: cmemc [OPTIONS] DATASET_IDS... Open datasets in the browser. With this command, you can open a dataset in the workspace in your browser. The command accepts multiple dataset IDs which results in opening multiple browser tabs. Options: -h, --help Show this message and exit. Command group: dataset resource \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, inspect or delete dataset file resources. File resources are identified by its name and project ID. Options: -h, --help Show this message and exit. Commands: delete Delete file resources. inspect Display all meta data of a file resource. list List available file resources. usage Display all usage data of a file resource. Command: dataset resource list \u00a4 Usage: cmemc [OPTIONS] List available file resources. Outputs a table or a list of dataset resources (files). Options: --raw Outputs raw JSON. --id-only Lists only resource names and no other meta data. This is useful for piping the IDs into other commands. --filter <TEXT TEXT>... Filter file resources based on a meta data. First parameter CHOICE can be one of ['project', 'regex']. The second parameter is based on CHOICE, e.g. a project ID or a regular expression string. -h, --help Show this message and exit. Command: dataset resource delete \u00a4 Usage: cmemc [OPTIONS] [RESOURCE_IDS]... Delete file resources. You have three selection mechanisms: with specific IDs, you will delete only these resources; by using --filter your will delete resources based on the filter type and value; by using --all will delete all resources. Options: --force Delete resource even if in use by a task. -a, --all Delete all resources. This is a dangerous option, so use it with care. --filter <TEXT TEXT>... Filter file resources based on a meta data. First parameter CHOICE can be one of ['project', 'regex']. The second parameter is based on CHOICE, e.g. a project ID or a regular expression string. -h, --help Show this message and exit. Command: dataset resource inspect \u00a4 Usage: cmemc [OPTIONS] RESOURCE_ID Display all meta data of a file resource. Options: --raw Outputs raw JSON. -h, --help Show this message and exit. Command: dataset resource usage \u00a4 Usage: cmemc [OPTIONS] RESOURCE_ID Display all usage data of a file resource. Options: --raw Outputs raw JSON. -h, --help Show this message and exit. Command group: graph \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, import, export, delete, count, tree or open graphs. Graphs are identified by an IRI. The get a list of existing graphs, execute the list command or use tab-completion. Options: -h, --help Show this message and exit. Commands: count Count triples in graph(s). delete Delete graph(s) from the store. export Export graph(s) as NTriples to stdout (-), file or directory. import Import graph(s) to the store. list List accessible graphs. open Open / explore a graph in the browser. tree Show graph tree(s) of the owl:imports hierarchy. Command: graph count \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Count triples in graph(s). This command lists graphs with their triple count. Counts are done without following imported graphs. Options: -a, --all Count all graphs -s, --summarize Display only a sum of all counted graphs together -h, --help Show this message and exit. Command: graph tree \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Show graph tree(s) of the owl:imports hierarchy. You can can output one or more trees of the import hierarchy. Imported graphs which do not exists are shown as [missing: IRI]. Imported graphs which will result in an import cycle are shown as [ignored: IRI]. Each graph is shown with label and IRI. Options: -a, --all Show tree of all (readable) graphs. --raw Outputs raw JSON of the graph importTree API response. --id-only Lists only graph identifier (IRIs) and no labels or other meta data. This is useful for piping the IRIs into other commands. The output with this option is a sorted, flat, de-duplicated list of existing graphs. -h, --help Show this message and exit. Command: graph list \u00a4 Usage: cmemc [OPTIONS] List accessible graphs. Options: --raw Outputs raw JSON of the graphs list API response. --id-only Lists only graph identifier (IRIs) and no labels or other meta data. This is useful for piping the IRIs into other commands. --filter <CHOICE TEXT>... Filter graphs based on effective access conditions or import closure. First parameter CHOICE can be 'access' or 'imported-by'. The second parameter can be 'readonly' or 'writeable' in case of 'access' or any readable graph in case of 'imported-by'. -h, --help Show this message and exit. Command: graph export \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Export graph(s) as NTriples to stdout (-), file or directory. In case of file export, data from all selected graphs will be concatenated in one file. In case of directory export, .graph and .ttl files will be created for each graph. Options: -a, --all Export all readable graphs. --include-imports Export selected graph(s) and all graphs which are imported from these selected graph(s). --create-catalog In addition to the .ttl and .graph files, cmemc will create an XML catalog file (catalog-v001.xml) which can be used by applications such as Prot\u00e9g\u00e9. --output-dir DIRECTORY Export to this directory. --output-file FILE Export to this file. [default: -] -t, --filename-template TEXT Template for the export file name(s). Used together with --output-dir. Possible placeholders are (Jinja2): {{hash}} - sha256 hash of the graph IRI, {{iriname}} - graph IRI converted to filename, {{connection}} - from the --connection option and {{date}} - the current date as YYYY-MM-DD. The file suffix will be appended. Needed directories will be created. [default: {{hash}}] --mime-type [application/n-triples|text/turtle] Define the requested mime type [default: application/n-triples] -h, --help Show this message and exit. Command: graph delete \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Delete graph(s) from the store. Options: -a, --all Delete all writeable graphs. --include-imports Delete selected graph(s) and all writeable graphs which are imported from these selected graph(s). -h, --help Show this message and exit. Command: graph import \u00a4 Usage: cmemc [OPTIONS] INPUT_PATH [IRI] Import graph(s) to the store. If input is an directory, it scans for file-pairs such as xxx.ttl and xxx.ttl.graph where xxx.ttl is the actual triples file and xxx.ttl.graph contains the graph IRI as one string: \"https://mygraph.de/xxx/\". If input is a file, content will be uploaded to IRI. If --replace is set, the data will be overwritten, if not, it will be added. Options: --replace Replace / overwrite the graph - instead of just adding new triples the graph. --skip-existing Skip importing a file if the target graph already exists in the store. Note that the graph list is fetched once at the beginning of the process, so that you can still add multiple files to one single graph (if it does not exist). -h, --help Show this message and exit. Command: graph open \u00a4 Usage: cmemc [OPTIONS] IRI Open / explore a graph in the browser. Options: -h, --help Show this message and exit. Command group: project \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, import, export, create, delete or open projects. Projects are identified by an PROJECTID. The get a list of existing projects, execute the list command or use tab-completion. Options: -h, --help Show this message and exit. Commands: create Create empty new project(s). delete Delete project(s). export Export project(s) to file(s). import Import a project from a file or directory. list List available projects. open Open projects in the browser. Command: project open \u00a4 Usage: cmemc [OPTIONS] PROJECT_IDS... Open projects in the browser. With this command, you can open a project in the workspace in your browser to change them. The command accepts multiple projects IDs which results in opening multiple browser tabs. Options: -h, --help Show this message and exit. Command: project list \u00a4 Usage: cmemc [OPTIONS] List available projects. Outputs a list of project IDs which can be used as reference for the project create, delete, export and import commands. Options: --raw Outputs raw JSON. --id-only Lists only project identifier and no labels or other meta data. This is useful for piping the IDs into other commands. -h, --help Show this message and exit. Command: project export \u00a4 Usage: cmemc [OPTIONS] [PROJECT_IDS]... Export project(s) to file(s). Projects can be exported with different export formats. The default type is a zip archive which includes meta data as well as dataset resources. If more than one project is exported, a file is created for each project. By default, these files are created in the current directory and with a descriptive name (see --template option default). Example: cmemc project export my_project Available projects can be listed by using the 'cmemc project list' command. You can use the template string to create subdirectories as well: cmemc config list | parallel -I% cmemc -c % project export --all -t \"dump/{{connection}}/{{date}}-{{id}}.project\" Options: -a, --all Export all projects. -o, --overwrite Overwrite existing files. This is a dangerous option, so use it with care. --output-dir DIRECTORY The base directory, where the project files will be created. If this directory does not exist, it will be silently created. [default: .] --type TEXT Type of the exported project file(s). Use the --help-types option or tab completion to see a list of possible types. [default: xmlZip] -t, --filename-template TEXT Template for the export file name(s). Possible placeholders are (Jinja2): {{id}} (the project ID), {{connection}} (from the --connection option) and {{date}} (the current date as YYYY-MM-DD). The file suffix will be appended. Needed directories will be created. [default: {{date}}-{{connection}}-{{id}}.project] --extract Export projects to a directory structure instead of a ZIP archive. Note that the --filename-template option is ignored here. Instead, a sub-directory per exported project is created under the output directory. Also note that not all export types are extractable. --help-types Lists all possible export types. -h, --help Show this message and exit. Command: project import \u00a4 Usage: cmemc [OPTIONS] PATH [PROJECT_ID] Import a project from a file or directory. Example: cmemc project import my_project.zip my_project Options: -o, --overwrite Overwrite an existing project. This is a dangerous option, so use it with care. -h, --help Show this message and exit. Command: project delete \u00a4 Usage: cmemc [OPTIONS] [PROJECT_IDS]... Delete project(s). This deletes existing data integration projects from Corporate Memory. Projects will be deleted without prompting! Example: cmemc project delete my_project Projects can be listed by using the 'cmemc project list' command. Options: -a, --all Delete all projects. This is a dangerous option, so use it with care. -h, --help Show this message and exit. Command: project create \u00a4 Usage: cmemc [OPTIONS] PROJECT_IDS... Create empty new project(s). This creates one or more new projects. Existing projects will not be overwritten. Example: cmemc project create my_project Projects can be listed by using the 'cmemc project list' command. Options: -h, --help Show this message and exit. Command group: query \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, execute, get status or open SPARQL queries. Queries are identified either by a file path, a URI from the query catalog or a shortened URI (qname, using a default namespace). In order to get a list of queries from the query catalog, use the list command. One or more queries can be executed one after the other with the execute command. With open command you can jump to the query editor in your browser. Queries can use a mustache like syntax to specify placeholder for parameter values (e.g. {{resourceUri}}). These parameter values need to be given as well, before the query can be executed (use the -p option). Options: -h, --help Show this message and exit. Commands: execute Execute queries which are loaded from files or the query catalog. list List available queries from the catalog. open Open queries in the editor of the query catalog in your browser. replay Re-execute queries from a replay file. status Get status information of executed and running queries. Command: query execute \u00a4 Usage: cmemc [OPTIONS] QUERIES... Execute queries which are loaded from files or the query catalog. Queries are identified either by a file path, a URI from the query catalog, or a shortened URI (qname, using a default namespace). If multiple queries are executed one after the other, the first failing query stops the whole execution chain. Limitations: All optional parameters (e.g. accept, base64, ...) are provided for ALL queries in an execution chain. If you need different parameters for each query in a chain, run cmemc multiple times and use the logical operators && and || of your shell instead. Options: --accept TEXT Accept header for the HTTP request(s). Setting this to 'default' means that cmemc uses an appropriate accept header for terminal output (text/csv for tables, text/turtle for graphs, * otherwise). Please refer to the Corporate Memory system manual for a list of accepted mime types. [default: default] --no-imports Graphs which include other graphs (using owl:imports) will be queried as merged overall-graph. This flag disables this default behaviour. The flag has no effect on update queries. --base64 Enables base64 encoding of the query parameter for the SPARQL requests (the response is not touched). This can be useful in case there is an aggressive firewall between cmemc and Corporate Memory. -p, --parameter <TEXT TEXT>... In case of a parameterized query (placeholders with the '{{key}}' syntax), this option fills all placeholder with a given value before the query is executed.Pairs of placeholder/value need to be given as a tuple 'KEY VALUE'. A key can be used only once. --limit INTEGER Override or set the LIMIT in the executed SELECT query. Note that this option will never give you more results than the LIMIT given in the query itself. --offset INTEGER Override or set the OFFSET in the executed SELECT query. --distinct Override the SELECT query by make the result set DISTINCT. --timeout INTEGER Set max execution time for query evaluation (in milliseconds). -h, --help Show this message and exit. Command: query list \u00a4 Usage: cmemc [OPTIONS] List available queries from the catalog. Outputs a list of query URIs which can be used as reference for the query execute command. Options: --id-only Lists only query identifier and no labels or other meta data. This is useful for piping the ids into other cmemc commands. -h, --help Show this message and exit. Command: query open \u00a4 Usage: cmemc [OPTIONS] QUERIES... Open queries in the editor of the query catalog in your browser. With this command, you can open (remote) queries from the query catalog in the query editor in your browser (e.g. in order to change them). You can also load local query files into the query editor, in order to import them into the query catalog. The command accepts multiple query URIs or files which results in opening multiple browser tabs. Options: -h, --help Show this message and exit. Command: query status \u00a4 Usage: cmemc [OPTIONS] [QUERY_UUID] Get status information of executed and running queries. With this command, you can access the latest executed SPARQL queries on the DataPlatform. These queries are identified by UUIDs and listed ordered by starting timestamp. You can filter queries based on status and runtime in order to investigate slow queries. In addition to that, you can get the details of a specific query by using the ID as a parameter. Options: --id-only Lists only query identifier and no labels or other meta data. This is useful for piping the ids into other cmemc commands. --raw Outputs raw JSON response of the query status API. --filter <TEXT TEXT>... Filter queries based on execution status and time. First parameter --filter CHOICE can be one of ['status', 'slower-than', 'type', 'regex']. The second parameter is based on CHOICE, e.g. int in case of slower-than, or a regular expression string. -h, --help Show this message and exit. Command: query replay \u00a4 Usage: cmemc [OPTIONS] REPLAY_FILE Re-execute queries from a replay file. This command reads a REPLAY_FILE and re-executes the logged queries. A REPLAY_FILE is a JSON document which is an array of JSON objects with at least a key `queryString` holding the query text OR a key 'iri' holding the IRI of the query in the query catalog. It can be created with the `query status` command, e.g. `query status --raw > replay.json` The output of this command shows basic query execution statistics. The queries are executed one after another in the order given in the input REPLAY_FILE. Query placeholders / parameters are ignored. If a query results in an error, the duration is not counted. The optional output file is the same JSON document which is used as input, but each query object is annotated with an additional 'replays' object, which is an array of JSON objects which hold values for the replay|loop|run IDs, start and end time as well as duration and other data. Options: --raw Output the execution statistic as raw JSON. --loops INTEGER Number of loops to run the replay file. [default: 1] --wait INTEGER Number of seconds to wait between query executions. [default: 0] --output-file FILE Save the optional output to this file. Input and output of the command can be the same file. The output is written at the end of a successful command execution. The output can be stdout ('-') - in this case, the execution statistic output is oppressed. --run-label TEXT Optional label of this replay run. -h, --help Show this message and exit. Command group: vocabulary \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, (un-)install, import or open vocabs / manage cache. Options: -h, --help Show this message and exit. Commands: cache List und update the vocabulary cache. import Import a turtle file as a vocabulary. install Install one or more vocabularies from the catalog. list Output a list of vocabularies. open Open / explore a vocabulary graph in the browser. uninstall Uninstall one or more vocabularies. Command: vocabulary open \u00a4 Usage: cmemc [OPTIONS] IRI Open / explore a vocabulary graph in the browser. Vocabularies are identified by their graph IRI. Installed vocabularies can be listed with the \"vocabulary list\" command. Options: -h, --help Show this message and exit. Command: vocabulary list \u00a4 Usage: cmemc [OPTIONS] Output a list of vocabularies. Vocabularies are graphs (see 'cmemc graph' command group) which consists of class and property descriptions. Options: --id-only Lists only vocabulary identifier (IRIs) and no labels or other meta data. This is useful for piping the ids into other cmemc commands. --filter [all|installed|installable] Filter list based on status. [default: installed] --raw Outputs raw JSON. -h, --help Show this message and exit. Command: vocabulary install \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Install one or more vocabularies from the catalog. Vocabularies are identified by their graph IRI. Installable vocabularies can be listed with the \"vocabulary list --filter installable\" command. Options: -a, --all Install all vocabularies from the catalog. -h, --help Show this message and exit. Command: vocabulary uninstall \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Uninstall one or more vocabularies. Vocabularies are identified by their graph IRI. Already installed vocabularies can be listed with the \"vocabulary list --filter installed\" command. Options: -a, --all Uninstall all installed vocabularies. -h, --help Show this message and exit. Command: vocabulary import \u00a4 Usage: cmemc [OPTIONS] FILE Import a turtle file as a vocabulary. With this command, you can import a local ontology file as a named graph. and create a corresponding vocabulary catalog entry. The uploaded ontology file is analysed locally in order to discover the named graph and the prefix declaration. This requires an OWL ontology description which correctly uses the vann:preferredNamespacePrefix and vann:preferredNamespaceUri properties. Options: --replace Replace (overwrite) existing vocabulary, if present. -h, --help Show this message and exit. Command group: vocabulary cache \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List und update the vocabulary cache. Options: -h, --help Show this message and exit. Commands: list Output the content of the global vocabulary cache. update Reload / updates the data integration cache for a vocabulary. Command: vocabulary cache update \u00a4 Usage: cmemc [OPTIONS] [IRIS]... Reload / updates the data integration cache for a vocabulary. Options: -a, --all Update cache for all installed vocabularies. -h, --help Show this message and exit. Command: vocabulary cache list \u00a4 Usage: cmemc [OPTIONS] Output the content of the global vocabulary cache. Options: --id-only Lists only vocabulary term identifier (IRIs) and no labels or other meta data. This is useful for piping the ids into other cmemc commands. --raw Outputs raw JSON. -h, --help Show this message and exit. Command group: workflow \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, execute, status or open (io) workflows. Workflows are identified by a WORKFLOW_ID. The get a list of existing workflows, execute the list command or use tab-completion. The WORKFLOW_ID is a concatenation of an PROJECT_ID and a TASK_ID, such as \"my-project:my- workflow\". Options: -h, --help Show this message and exit. Commands: execute Execute workflow(s). io Execute a workflow with file input/output. list List available workflow ids. open Open a workflow in your browser. scheduler List, inspect, enable/disable or open scheduler. status Get status information of workflow(s). Command: workflow execute \u00a4 Usage: cmemc [OPTIONS] [WORKFLOW_IDS]... Execute workflow(s). With this command, you can start one or more workflows at the same time or in a sequence, depending on the result of the predecessor. Executing a workflow can be done in two ways: Without --wait just sends the starting signal and does not look for the workflow and its result (fire and forget). Starting workflows in this way, starts all given workflows at the same time. The optional --wait option starts the workflows in the same way, but also polls the status of a workflow until it is finished. In case of an error of a workflow, the next workflow is not started. Options: -a, --all Execute all available workflows. --wait Wait until all executed workflows are completed. --polling-interval INTEGER RANGE How many seconds to wait between status polls. Status polls are cheap, so a higher polling interval is most likely not needed. [default: 1] -h, --help Show this message and exit. Command: workflow io \u00a4 Usage: cmemc [OPTIONS] WORKFLOW_ID Execute a workflow with file input/output. With this command, you can execute a workflow that uses variable datasets as input, output or for configuration. Use the input parameter to feed data into the workflow. Likewise use output for retrieval of the workflow result. Workflows without a variable dataset will throw an error. Options: -i, --input FILE From which file the input is taken: note that the maximum file size to upload is limited to a server configured value. If the workflow has no defined variable input dataset, this can be ignored. -o, --output FILE To which file the result is written to: use '-' in order to output the result to stdout. If the workflow has no defined variable output dataset, this can be ignored. Please note that the io command will not warn you on overwriting existing output files. --input-mimetype [guess|application/xml|application/json|text/csv] Which input format should be processed: If not given, cmemc will try to guess the mime type based on the file extension or will fail --output-mimetype [guess|application/xml|application/json|application/n-triples|application/vnd.openxmlformats-officedocument.spreadsheetml.sheet|text/csv] Which output format should be requested: If not given, cmemc will try to guess the mime type based on the file extension or will fail. In case of an output to stdout, a default mime type will be used (currently xml). -h, --help Show this message and exit. Command: workflow list \u00a4 Usage: cmemc [OPTIONS] List available workflow ids. Options: --raw Outputs raw JSON objects of workflow task search API response. --id-only Lists only workflow identifier and no labels or other meta data. This is useful for piping the IDs into other commands. --filter <CHOICE TEXT>... Filter workflows based on project or suitability for the io command .First parameter CHOICE can be 'project' or 'io'. The second parameter has to be a project ID in case of 'project' or 'input- only|output-only|input-output|any' in case of 'io' filter. -h, --help Show this message and exit. Command: workflow status \u00a4 Usage: cmemc [OPTIONS] [WORKFLOW_IDS]... Get status information of workflow(s). Options: --project TEXT The project, from which you want to list the workflows. Project IDs can be listed with the 'project list' command. --raw Output raw JSON info. --filter [Idle|Not executed|Finished|Cancelled|Failed|Successful|Canceling|Running|Waiting] Show only workflows of a specific status. -h, --help Show this message and exit. Command: workflow open \u00a4 Usage: cmemc [OPTIONS] WORKFLOW_ID Open a workflow in your browser. Options: -h, --help Show this message and exit. Command group: workflow scheduler \u00a4 Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, inspect, enable/disable or open scheduler. Schedulers execute workflows in specified intervals. They are identified with a SCHEDULERID. To get a list of existing schedulers, execute the list command or use tab-completion. Options: -h, --help Show this message and exit. Commands: disable Disable scheduler(s). enable Enable scheduler(s). inspect Display all meta data of a scheduler. list List available scheduler. open Open scheduler(s) in the browser. Command: workflow scheduler open \u00a4 Usage: cmemc [OPTIONS] SCHEDULER_IDS... Open scheduler(s) in the browser. With this command, you can open a scheduler in the workspace in your browser to change it. The command accepts multiple scheduler IDs which results in opening multiple browser tabs. Options: --workflow Instead of opening the scheduler page, open the page of the scheduled workflow. -h, --help Show this message and exit. Command: workflow scheduler list \u00a4 Usage: cmemc [OPTIONS] List available scheduler. Outputs a table or a list of scheduler IDs which can be used as reference for the scheduler commands. Options: --raw Outputs raw JSON. --id-only Lists only task identifier and no labels or other meta data. This is useful for piping the IDs into other commands. -h, --help Show this message and exit. Command: workflow scheduler inspect \u00a4 Usage: cmemc [OPTIONS] SCHEDULER_ID Display all meta data of a scheduler. Options: --raw Outputs raw JSON. -h, --help Show this message and exit. Command: workflow scheduler disable \u00a4 Usage: cmemc [OPTIONS] [SCHEDULER_IDS]... Disable scheduler(s). The command accepts multiple scheduler IDs which results in disabling them one after the other. Options: -a, --all Disable all scheduler. -h, --help Show this message and exit. Command: workflow scheduler enable \u00a4 Usage: cmemc [OPTIONS] [SCHEDULER_IDS]... Enable scheduler(s). The command accepts multiple scheduler IDs which results in enabling them one after the other. Options: -a, --all Enable all scheduler. -h, --help Show this message and exit.","title":"Command Reference"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-reference","text":"This section lists the help texts of all commands as a reference and to search for it.","title":"Command Reference"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-admin","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... Import bootstrap data, backup/restore workspace or get status. This command group consists of commands for setting up and configuring eccenca Corporate Memory. Options: -h, --help Show this message and exit. Commands: bootstrap Update/Import bootstrap data. metrics List and get metrics. showcase Create showcase data. status Output health and version information. store Import, export and bootstrap the knowledge graph store. token Fetch and output an access token. workspace Import, export and reload the project workspace.","title":"Command group: admin"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-showcase","text":"Usage: cmemc [OPTIONS] Create showcase data. This command creates a showcase scenario of multiple graphs including integration graphs, shapes, statement annotations etc. Note: There is currently no deletion mechanism for the showcase data, so you need to remove the showcase graphs manually (or just remove all graphs). Options: --scale INTEGER The scale factor provides a way to set the target size of the scenario. A value of 10 results in around 40k triples, a value of 50 in around 350k triples. [default: 10] --create Delete old showcase data if present and create new showcase databased on the given scale factor. --delete Delete existing showcase data if present. -h, --help Show this message and exit.","title":"Command: admin showcase"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-bootstrap","text":"Usage: cmemc [OPTIONS] Update/Import bootstrap data. This command imports the bootstrap data needed for managing shapes, access conditions, the query catalog and the vocabulary catalog. Note: There is currently no deletion mechanism for the bootstrap data, so you need to remove the graphs manually (or just remove all graphs). Options: --import Delete existing bootstrap data if present and import bootstrap data which was delivered -h, --help Show this message and exit.","title":"Command: admin bootstrap"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-status","text":"Usage: cmemc [OPTIONS] Output health and version information. This command outputs version and health information of the selected deployment. If the version information can not be retrieved, UNKNOWN is shown if the endpoint is not available or ERROR is shown, if the endpoints returns an error. In addition to that, this command warns you if the target version of your cmemc client is newer than the version of your backend and if the ShapeCatalog has a different version then your DataPlatform component. To get status information of all configured deployments use this command in combination with parallel: cmemc config list | parallel --ctag cmemc -c {} admin status Options: -h, --help Show this message and exit.","title":"Command: admin status"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-token","text":"Usage: cmemc [OPTIONS] Fetch and output an access token. This command can be used to check for correct authentication as well as to use the token with wget / curl or similar standard tools: Example Usage: curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug Please be aware that this command can reveal secrets, which you do not want to have in log files or on the screen. Options: --raw Outputs raw JSON. Note that this option will always try to fetch a new JSON token response. In case you are working with OAUTH_GRANT_TYPE=prefetched_token, this may lead to an error. --decode Decode the access token and outputs the raw JSON. Note that the access token is only decoded and esp. not validated. -h, --help Show this message and exit.","title":"Command: admin token"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-admin-metrics","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List and get metrics. This command group consists of commands for reading and listing internal monitoring metrics of eccenca Corporate Memory. A deployment consists of multiple jobs (e.g. DP, DI), which provide multiple metric families on an endpoint. Each metric family can consist of different samples identified by labels with a name and a value (dimensions). A metric has a specific type (counter, gauge, summary and histogram) and additional metadata. Please have a look at https://prometheus.io/docs/concepts/data_model/ for further details. Options: -h, --help Show this message and exit. Commands: get Get sample data of a metric. inspect Inspect a metric. list List metrics for a specific job.","title":"Command group: admin metrics"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-metrics-get","text":"Usage: cmemc [OPTIONS] METRIC_ID Get sample data of a metric. A metric of a specific job is identified by a metric ID. Possible metric IDs of a job can be retrieved with the `metrics list` command. A metric can contain multiple samples. These samples are distinguished by labels (name and value). Options: --job [DP] The job from which the metrics data is fetched. [default: DP] --filter <TEXT TEXT>... A set of label name/value pairs in order to filter the samples of the requested metric family. Each metric has a different set of labels with different values. In order to get a list of possible label names and values, use the command without this option. The label names are then shown as column headers and label values as cell values of this column. --enforce-table A single sample value will be returned as plain text instead of the normal table. This allows for more easy integration with scripts. This flag enforces the use of tabular output, even for single row tables. --raw Outputs raw prometheus sample classes. -h, --help Show this message and exit.","title":"Command: admin metrics get"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-metrics-inspect","text":"Usage: cmemc [OPTIONS] METRIC_ID Inspect a metric. This command outputs the data of a metric. The first table includes basic meta data about the metric. The second table includes sample labels and values. Options: --job [DP] The job from which the metrics data is fetched. [default: DP] --raw Outputs raw JSON of the table data. -h, --help Show this message and exit.","title":"Command: admin metrics inspect"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-metrics-list","text":"Usage: cmemc [OPTIONS] List metrics for a specific job. For each metric, the output table shows the metric ID, the type of the metric, a count of how many labels (label names) are describing the samples (L) and a count of how many samples are currently available for a metric (S). Options: --job [DP] The job from which the metrics data is fetched. [default: DP] --id-only Lists metric identifier only. This is useful for piping the IDs into other commands. --raw Outputs (sorted) JSON dict, parsed from the metrics API output. -h, --help Show this message and exit.","title":"Command: admin metrics list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-admin-workspace","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... Import, export and reload the project workspace. Options: -h, --help Show this message and exit. Commands: export Export the complete workspace (all projects) to a ZIP file. import Import the workspace from a file. python List, install, or uninstall python packages. reload Reload the workspace from the backend.","title":"Command group: admin workspace"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-export","text":"Usage: cmemc [OPTIONS] [FILE] Export the complete workspace (all projects) to a ZIP file. Depending on the requested type, this ZIP contains either a turtle file for each project (type rdfTurtle) or a substructure of resource files and XML descriptions (type xmlZip). The file name is optional and will be generated with by the template if absent. Options: -o, --overwrite Overwrite existing files. This is a dangerous option, so use it with care. --type TEXT Type of the exported workspace file. [default: xmlZip] -t, --filename-template TEXT Template for the export file name. Possible placeholders are (Jinja2): {{connection}} (from the --connection option) and {{date}} (the current date as YYYY-MM-DD). The file suffix will be appended. Needed directories will be created. [default: {{date}}-{{connection}}.workspace] -h, --help Show this message and exit.","title":"Command: admin workspace export"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-import","text":"Usage: cmemc [OPTIONS] FILE Import the workspace from a file. Options: --type TEXT Type of the exported workspace file. [default: xmlZip] -h, --help Show this message and exit.","title":"Command: admin workspace import"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-reload","text":"Usage: cmemc [OPTIONS] Reload the workspace from the backend. Options: -h, --help Show this message and exit.","title":"Command: admin workspace reload"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-admin-workspace-python","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, install, or uninstall python packages. Python packages are used to extend the DataIntegration workspace with python plugins. To get a list of installed packages, execute the list command. Warning: Installing packages from unknown sources is not recommended. Plugins are not verified for malicious code. Options: -h, --help Show this message and exit. Commands: install Install a python package to the workspace. list List installed python packages. list-plugins List installed workspace plugins. uninstall Uninstall a python package from the workspace.","title":"Command group: admin workspace python"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-python-install","text":"Usage: cmemc [OPTIONS] PACKAGE Install a python package to the workspace. This command is basically a 'pip install' in the remote python environment. You can install a package by uploading a source distribution .tar.gz file, or by uploading a build distribution .whl file, or by specifying a package name, more precisely, a pip requirement specifier with a package name available on pypi.org (e.g. 'requests==2.27.1'). Options: -h, --help Show this message and exit.","title":"Command: admin workspace python install"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-python-uninstall","text":"Usage: cmemc [OPTIONS] PACKAGE_NAME Uninstall a python package from the workspace. This command is basically a 'pip uninstall' in the remote python environment. Options: -h, --help Show this message and exit.","title":"Command: admin workspace python uninstall"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-python-list","text":"Usage: cmemc [OPTIONS] List installed python packages. This command is basically a 'pip list' in the remote python environment. It outputs a table of python package identifiers with version information. Options: --raw Outputs raw JSON. --id-only Lists only package identifier. This is useful for piping the IDs into other commands. -h, --help Show this message and exit.","title":"Command: admin workspace python list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-workspace-python-list-plugins","text":"Usage: cmemc [OPTIONS] List installed workspace plugins. This commands lists all discovered plugins. Note that the plugin discovery is limited to specific packages. Options: --raw Outputs raw JSON. --id-only Lists only plugin identifier. --package-id-only Lists only plugin package identifier. -h, --help Show this message and exit.","title":"Command: admin workspace python list-plugins"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-admin-store","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... Import, export and bootstrap the knowledge graph store. This command group consist of commands to administrate the knowledge graph store as a whole. Options: -h, --help Show this message and exit. Commands: bootstrap Update/Import bootstrap data. export Backup all knowledge graphs to a ZIP archive. import Restore graphs from a ZIP archive. showcase Create showcase data.","title":"Command group: admin store"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-store-showcase","text":"Usage: cmemc [OPTIONS] Create showcase data. This command creates a showcase scenario of multiple graphs including integration graphs, shapes, statement annotations etc. Note: There is currently no deletion mechanism for the showcase data, so you need to remove the showcase graphs manually (or just remove all graphs). Options: --scale INTEGER The scale factor provides a way to set the target size of the scenario. A value of 10 results in around 40k triples, a value of 50 in around 350k triples. [default: 10] --create Delete old showcase data if present and create new showcase databased on the given scale factor. --delete Delete existing showcase data if present. -h, --help Show this message and exit.","title":"Command: admin store showcase"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-store-bootstrap","text":"Usage: cmemc [OPTIONS] Update/Import bootstrap data. This command imports the bootstrap data needed for managing shapes, access conditions, the query catalog and the vocabulary catalog. Note: There is currently no deletion mechanism for the bootstrap data, so you need to remove the graphs manually (or just remove all graphs). Options: --import Delete existing bootstrap data if present and import bootstrap data which was delivered -h, --help Show this message and exit.","title":"Command: admin store bootstrap"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-store-export","text":"Usage: cmemc [OPTIONS] BACKUP_FILE Backup all knowledge graphs to a ZIP archive. The backup file is a ZIP archive containing all knowledge graphs as Turtle files + configuration file for each graph. This command will create lots of load on the server. It can take a long time to complete. Options: --overwrite Overwrite existing files. This is a dangerous option, so use it with care. -h, --help Show this message and exit.","title":"Command: admin store export"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-admin-store-import","text":"Usage: cmemc [OPTIONS] BACKUP_FILE Restore graphs from a ZIP archive. The backup file is a ZIP archive containing all knowledge graphs as Turtle files + configuration file for each graph. The command will load a single backup ZIP archive into the triple store, by replacing all graphs with the content of the Turtle files in the archive and deleting all graphs which are not in the archive. This command will create lots of load on the server. It can take a long time to complete. The backup file will be transferred to the server, then unzipped and imported graph by graph. After the initial transfer, the network connection is not used anymore, so it will be closed by proxies sometimes. This does not mean that the import failed. Options: -h, --help Show this message and exit.","title":"Command: admin store import"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-config","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List and edit configs as well as get config values. Configurations are identified by the section identifier in the config file. Each configuration represent a Corporate Memory deployment with its specific access method as well as credentials. A minimal configuration which uses client credentials has the following entries: [example.org] CMEM_BASE_URI=https://cmem.example.org/ OAUTH_GRANT_TYPE=client_credentials OAUTH_CLIENT_ID=cmem-service-account OAUTH_CLIENT_SECRET=my-secret-account-pass Note that OAUTH_GRANT_TYPE can be either client_credentials, password or prefetched_token. In addition to that, the following config parameters can be used as well: SSL_VERIFY=False - for ignoring certificate issues (not recommended) DP_API_ENDPOINT=URL - to point to a non-standard DataPlatform location DI_API_ENDPOINT=URL - to point to a non-standard DataIntegration location OAUTH_TOKEN_URI=URL - to point to an external IdentityProvider location OAUTH_USER=username - only if OAUTH_GRANT_TYPE=password OAUTH_PASSWORD=password - only if OAUTH_GRANT_TYPE=password OAUTH_ACCESS_TOKEN=token - only if OAUTH_GRANT_TYPE=prefetched_token In order to get credential information from an external process, you can use the parameter OAUTH_PASSWORD_PROCESS, OAUTH_CLIENT_SECRET_PROCESS and OAUTH_ACCESS_TOKEN_PROCESS to setup an external executable. OAUTH_CLIENT_SECRET_PROCESS=/path/to/getpass.sh OAUTH_PASSWORD_PROCESS=[\"getpass.sh\", \"parameter1\", \"parameter2\"] The credential executable can use the cmemc environment for fetching the credential (e.g. CMEM_BASE_URI and OAUTH_USER). If the credential executable is not given with a full path, cmemc will look into your environment PATH for something which can be executed. The configured process needs to return the credential on the first line of stdout. In addition to that, the process needs to exit with exit code 0 (without failure). There are examples available in the online manual. Options: -h, --help Show this message and exit. Commands: edit Edit the user-scope configuration file. eval Export all configuration values of a configuration for evaluation. get Get the value of a known cmemc configuration key. list List configured connections.","title":"Command group: config"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-config-list","text":"Usage: cmemc [OPTIONS] List configured connections. This command lists all configured connections from the currently used config file. The connection identifier can be used with the --connection option in order to use a specific Corporate Memory instance. In order to apply commands on more than one instance, you need to use typical unix gear such as xargs or parallel: cmemc config list | xargs -I % sh -c 'cmemc -c % admin status' cmemc config list | parallel --jobs 5 cmemc -c {} admin status Options: -h, --help Show this message and exit.","title":"Command: config list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-config-edit","text":"Usage: cmemc [OPTIONS] Edit the user-scope configuration file. Options: -h, --help Show this message and exit.","title":"Command: config edit"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-config-get","text":"Usage: cmemc [OPTIONS] [CMEM_BASE_URI|SSL_VERIFY|REQUESTS_CA_BUNDLE|DP_API_END POINT|DI_API_ENDPOINT|OAUTH_TOKEN_URI|OAUTH_GRANT_TYPE|OAUTH_USER |OAUTH_PASSWORD|OAUTH_CLIENT_ID|OAUTH_CLIENT_SECRET|OAUTH_ACCESS_ TOKEN] Get the value of a known cmemc configuration key. In order to automate processes such as fetching custom API data from multiple Corporate Memory instances, this command provides a way to get the value of a cmemc configuration key for the selected deployment. Example Usage: curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug The commands returns with exit code 1 if the config key is not used in the current configuration. Options: -h, --help Show this message and exit.","title":"Command: config get"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-config-eval","text":"Usage: cmemc [OPTIONS] Export all configuration values of a configuration for evaluation. The output of this command is suitable to be used by a shells eval command. It will output the complete configuration as 'export key=\"value\"' statements. This allows for preparation of a shell environment. eval $(cmemc -c my config eval) Please be aware that credential details are shown in cleartext with this command. Options: --unset Instead of export all configuration keys, this option will unset all key. -h, --help Show this message and exit.","title":"Command: config eval"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-dataset","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, create, delete, inspect, up-/download or open datasets. This command group allows for managing workspace datasets as well as dataset file resources. Datasets can be created and deleted. File resources can be uploaded and downloaded. Details of dataset parameter can be listed with inspect. Datasets are identified with a combined key of the project ID and the project internal dataset ID (e.g: my-project:my-dataset). To get a list of datasets, use the list command. Options: -h, --help Show this message and exit. Commands: create Create a dataset. delete Delete datasets. download Download the resource file of a dataset. inspect Display meta data of a dataset. list List available datasets. open Open datasets in the browser. resource List, inspect or delete dataset file resources. upload Upload a resource file to a dataset.","title":"Command group: dataset"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-list","text":"Usage: cmemc [OPTIONS] List available datasets. Outputs a list of datasets IDs which can be used as reference for the dataset create and delete commands. Options: --project TEXT The project, from which you want to list the datasets. Project IDs can be listed with the 'project list' command. --raw Outputs raw JSON objects of dataset search API response. --id-only Lists only dataset identifier and no labels or other meta data. This is useful for piping the ids into other cmemc commands. -h, --help Show this message and exit.","title":"Command: dataset list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-delete","text":"Usage: cmemc [OPTIONS] [DATASET_IDS]... Delete datasets. This deletes existing datasets in integration projects from Corporate Memory. Datasets will be deleted without prompting! Dataset resources will not be deleted. Example: cmemc dataset delete my_project:my_dataset Datasets can be listed by using the 'cmemc dataset list' command. Options: -a, --all Delete all datasets. This is a dangerous option, so use it with care. --project TEXT In combination with the '--all' flag, this option allows for deletion of all datasets of a certain project. The behaviour is similar to the 'dataset list --project' command. -h, --help Show this message and exit.","title":"Command: dataset delete"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-download","text":"Usage: cmemc [OPTIONS] DATASET_ID OUTPUT_PATH Download the resource file of a dataset. This command downloads the file resource of a dataset to your local file system or to standard out (-). Note that this is not possible for dataset types such as Knowledge Graph (eccencaDataplatform) or SQL endpoint (sqlEndpoint). Without providing an output path, the output file name will be the same as the remote file resource. Datasets can be listed by using the 'cmemc dataset list' command. Options: --replace Replace existing files. This is a dangerous option, so use it with care. -h, --help Show this message and exit.","title":"Command: dataset download"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-upload","text":"Usage: cmemc [OPTIONS] DATASET_ID INPUT_PATH Upload a resource file to a dataset. This command uploads a file to a dataset. The content of the uploaded file replaces the remote file resource. The name of the remote file resource is not changed. Warning: If the remote file resource is used in more than one dataset, the other datasets are also affected by this command. Warning: The content of the uploaded file is not tested, so uploading a json file to an xml dataset will result in errors. Datasets can be listed by using the 'cmemc dataset list' command. Example: cmemc dataset upload cmem:my-dataset new-file.csv Options: -h, --help Show this message and exit.","title":"Command: dataset upload"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-inspect","text":"Usage: cmemc [OPTIONS] DATASET_ID Display meta data of a dataset. Options: --raw Outputs raw JSON. -h, --help Show this message and exit.","title":"Command: dataset inspect"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-create","text":"Usage: cmemc [OPTIONS] [DATASET_FILE] Create a dataset. Datasets are created in projects and can have associated file resources. Each dataset has a type (such as 'csv') and a list of parameter which can change or specify the dataset behaviour. To get more information on possible dataset types and parameter on these types, use the '--help-types' and '--help-parameter' options. Example: cmemc dataset create --project my-project --type csv my-file.csv Options: -t, --type TEXT The dataset type of the dataset to create. Example types are 'csv','json' and 'eccencaDataPlatform' (-> Knowledge Graph). --project TEXT The project, where you want to create the dataset in. If there is only one project in the workspace, this option can be omitted. -p, --parameter <TEXT TEXT>... A set of key/value pairs. Each dataset type has different parameters (such as charset, arraySeparator, ignoreBadLines, ...). In order to get a list of possible parameter, use the'--help-parameter' option. --replace Replace remote file resources in case there already exists a file with the same name. --id TEXT The dataset ID of the dataset to create. The dataset ID will be automatically created in case it is not present. --help-types Lists all possible dataset types on given Corporate Memory instance. Note that this option already needs access to the instance. --help-parameter Lists all possible (optional and mandatory) parameter for a dataset type. Note that this option already needs access to the instance. -h, --help Show this message and exit.","title":"Command: dataset create"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-open","text":"Usage: cmemc [OPTIONS] DATASET_IDS... Open datasets in the browser. With this command, you can open a dataset in the workspace in your browser. The command accepts multiple dataset IDs which results in opening multiple browser tabs. Options: -h, --help Show this message and exit.","title":"Command: dataset open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-dataset-resource","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, inspect or delete dataset file resources. File resources are identified by its name and project ID. Options: -h, --help Show this message and exit. Commands: delete Delete file resources. inspect Display all meta data of a file resource. list List available file resources. usage Display all usage data of a file resource.","title":"Command group: dataset resource"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-resource-list","text":"Usage: cmemc [OPTIONS] List available file resources. Outputs a table or a list of dataset resources (files). Options: --raw Outputs raw JSON. --id-only Lists only resource names and no other meta data. This is useful for piping the IDs into other commands. --filter <TEXT TEXT>... Filter file resources based on a meta data. First parameter CHOICE can be one of ['project', 'regex']. The second parameter is based on CHOICE, e.g. a project ID or a regular expression string. -h, --help Show this message and exit.","title":"Command: dataset resource list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-resource-delete","text":"Usage: cmemc [OPTIONS] [RESOURCE_IDS]... Delete file resources. You have three selection mechanisms: with specific IDs, you will delete only these resources; by using --filter your will delete resources based on the filter type and value; by using --all will delete all resources. Options: --force Delete resource even if in use by a task. -a, --all Delete all resources. This is a dangerous option, so use it with care. --filter <TEXT TEXT>... Filter file resources based on a meta data. First parameter CHOICE can be one of ['project', 'regex']. The second parameter is based on CHOICE, e.g. a project ID or a regular expression string. -h, --help Show this message and exit.","title":"Command: dataset resource delete"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-resource-inspect","text":"Usage: cmemc [OPTIONS] RESOURCE_ID Display all meta data of a file resource. Options: --raw Outputs raw JSON. -h, --help Show this message and exit.","title":"Command: dataset resource inspect"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-dataset-resource-usage","text":"Usage: cmemc [OPTIONS] RESOURCE_ID Display all usage data of a file resource. Options: --raw Outputs raw JSON. -h, --help Show this message and exit.","title":"Command: dataset resource usage"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-graph","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, import, export, delete, count, tree or open graphs. Graphs are identified by an IRI. The get a list of existing graphs, execute the list command or use tab-completion. Options: -h, --help Show this message and exit. Commands: count Count triples in graph(s). delete Delete graph(s) from the store. export Export graph(s) as NTriples to stdout (-), file or directory. import Import graph(s) to the store. list List accessible graphs. open Open / explore a graph in the browser. tree Show graph tree(s) of the owl:imports hierarchy.","title":"Command group: graph"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-count","text":"Usage: cmemc [OPTIONS] [IRIS]... Count triples in graph(s). This command lists graphs with their triple count. Counts are done without following imported graphs. Options: -a, --all Count all graphs -s, --summarize Display only a sum of all counted graphs together -h, --help Show this message and exit.","title":"Command: graph count"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-tree","text":"Usage: cmemc [OPTIONS] [IRIS]... Show graph tree(s) of the owl:imports hierarchy. You can can output one or more trees of the import hierarchy. Imported graphs which do not exists are shown as [missing: IRI]. Imported graphs which will result in an import cycle are shown as [ignored: IRI]. Each graph is shown with label and IRI. Options: -a, --all Show tree of all (readable) graphs. --raw Outputs raw JSON of the graph importTree API response. --id-only Lists only graph identifier (IRIs) and no labels or other meta data. This is useful for piping the IRIs into other commands. The output with this option is a sorted, flat, de-duplicated list of existing graphs. -h, --help Show this message and exit.","title":"Command: graph tree"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-list","text":"Usage: cmemc [OPTIONS] List accessible graphs. Options: --raw Outputs raw JSON of the graphs list API response. --id-only Lists only graph identifier (IRIs) and no labels or other meta data. This is useful for piping the IRIs into other commands. --filter <CHOICE TEXT>... Filter graphs based on effective access conditions or import closure. First parameter CHOICE can be 'access' or 'imported-by'. The second parameter can be 'readonly' or 'writeable' in case of 'access' or any readable graph in case of 'imported-by'. -h, --help Show this message and exit.","title":"Command: graph list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-export","text":"Usage: cmemc [OPTIONS] [IRIS]... Export graph(s) as NTriples to stdout (-), file or directory. In case of file export, data from all selected graphs will be concatenated in one file. In case of directory export, .graph and .ttl files will be created for each graph. Options: -a, --all Export all readable graphs. --include-imports Export selected graph(s) and all graphs which are imported from these selected graph(s). --create-catalog In addition to the .ttl and .graph files, cmemc will create an XML catalog file (catalog-v001.xml) which can be used by applications such as Prot\u00e9g\u00e9. --output-dir DIRECTORY Export to this directory. --output-file FILE Export to this file. [default: -] -t, --filename-template TEXT Template for the export file name(s). Used together with --output-dir. Possible placeholders are (Jinja2): {{hash}} - sha256 hash of the graph IRI, {{iriname}} - graph IRI converted to filename, {{connection}} - from the --connection option and {{date}} - the current date as YYYY-MM-DD. The file suffix will be appended. Needed directories will be created. [default: {{hash}}] --mime-type [application/n-triples|text/turtle] Define the requested mime type [default: application/n-triples] -h, --help Show this message and exit.","title":"Command: graph export"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-delete","text":"Usage: cmemc [OPTIONS] [IRIS]... Delete graph(s) from the store. Options: -a, --all Delete all writeable graphs. --include-imports Delete selected graph(s) and all writeable graphs which are imported from these selected graph(s). -h, --help Show this message and exit.","title":"Command: graph delete"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-import","text":"Usage: cmemc [OPTIONS] INPUT_PATH [IRI] Import graph(s) to the store. If input is an directory, it scans for file-pairs such as xxx.ttl and xxx.ttl.graph where xxx.ttl is the actual triples file and xxx.ttl.graph contains the graph IRI as one string: \"https://mygraph.de/xxx/\". If input is a file, content will be uploaded to IRI. If --replace is set, the data will be overwritten, if not, it will be added. Options: --replace Replace / overwrite the graph - instead of just adding new triples the graph. --skip-existing Skip importing a file if the target graph already exists in the store. Note that the graph list is fetched once at the beginning of the process, so that you can still add multiple files to one single graph (if it does not exist). -h, --help Show this message and exit.","title":"Command: graph import"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-graph-open","text":"Usage: cmemc [OPTIONS] IRI Open / explore a graph in the browser. Options: -h, --help Show this message and exit.","title":"Command: graph open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-project","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, import, export, create, delete or open projects. Projects are identified by an PROJECTID. The get a list of existing projects, execute the list command or use tab-completion. Options: -h, --help Show this message and exit. Commands: create Create empty new project(s). delete Delete project(s). export Export project(s) to file(s). import Import a project from a file or directory. list List available projects. open Open projects in the browser.","title":"Command group: project"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-project-open","text":"Usage: cmemc [OPTIONS] PROJECT_IDS... Open projects in the browser. With this command, you can open a project in the workspace in your browser to change them. The command accepts multiple projects IDs which results in opening multiple browser tabs. Options: -h, --help Show this message and exit.","title":"Command: project open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-project-list","text":"Usage: cmemc [OPTIONS] List available projects. Outputs a list of project IDs which can be used as reference for the project create, delete, export and import commands. Options: --raw Outputs raw JSON. --id-only Lists only project identifier and no labels or other meta data. This is useful for piping the IDs into other commands. -h, --help Show this message and exit.","title":"Command: project list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-project-export","text":"Usage: cmemc [OPTIONS] [PROJECT_IDS]... Export project(s) to file(s). Projects can be exported with different export formats. The default type is a zip archive which includes meta data as well as dataset resources. If more than one project is exported, a file is created for each project. By default, these files are created in the current directory and with a descriptive name (see --template option default). Example: cmemc project export my_project Available projects can be listed by using the 'cmemc project list' command. You can use the template string to create subdirectories as well: cmemc config list | parallel -I% cmemc -c % project export --all -t \"dump/{{connection}}/{{date}}-{{id}}.project\" Options: -a, --all Export all projects. -o, --overwrite Overwrite existing files. This is a dangerous option, so use it with care. --output-dir DIRECTORY The base directory, where the project files will be created. If this directory does not exist, it will be silently created. [default: .] --type TEXT Type of the exported project file(s). Use the --help-types option or tab completion to see a list of possible types. [default: xmlZip] -t, --filename-template TEXT Template for the export file name(s). Possible placeholders are (Jinja2): {{id}} (the project ID), {{connection}} (from the --connection option) and {{date}} (the current date as YYYY-MM-DD). The file suffix will be appended. Needed directories will be created. [default: {{date}}-{{connection}}-{{id}}.project] --extract Export projects to a directory structure instead of a ZIP archive. Note that the --filename-template option is ignored here. Instead, a sub-directory per exported project is created under the output directory. Also note that not all export types are extractable. --help-types Lists all possible export types. -h, --help Show this message and exit.","title":"Command: project export"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-project-import","text":"Usage: cmemc [OPTIONS] PATH [PROJECT_ID] Import a project from a file or directory. Example: cmemc project import my_project.zip my_project Options: -o, --overwrite Overwrite an existing project. This is a dangerous option, so use it with care. -h, --help Show this message and exit.","title":"Command: project import"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-project-delete","text":"Usage: cmemc [OPTIONS] [PROJECT_IDS]... Delete project(s). This deletes existing data integration projects from Corporate Memory. Projects will be deleted without prompting! Example: cmemc project delete my_project Projects can be listed by using the 'cmemc project list' command. Options: -a, --all Delete all projects. This is a dangerous option, so use it with care. -h, --help Show this message and exit.","title":"Command: project delete"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-project-create","text":"Usage: cmemc [OPTIONS] PROJECT_IDS... Create empty new project(s). This creates one or more new projects. Existing projects will not be overwritten. Example: cmemc project create my_project Projects can be listed by using the 'cmemc project list' command. Options: -h, --help Show this message and exit.","title":"Command: project create"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-query","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, execute, get status or open SPARQL queries. Queries are identified either by a file path, a URI from the query catalog or a shortened URI (qname, using a default namespace). In order to get a list of queries from the query catalog, use the list command. One or more queries can be executed one after the other with the execute command. With open command you can jump to the query editor in your browser. Queries can use a mustache like syntax to specify placeholder for parameter values (e.g. {{resourceUri}}). These parameter values need to be given as well, before the query can be executed (use the -p option). Options: -h, --help Show this message and exit. Commands: execute Execute queries which are loaded from files or the query catalog. list List available queries from the catalog. open Open queries in the editor of the query catalog in your browser. replay Re-execute queries from a replay file. status Get status information of executed and running queries.","title":"Command group: query"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-query-execute","text":"Usage: cmemc [OPTIONS] QUERIES... Execute queries which are loaded from files or the query catalog. Queries are identified either by a file path, a URI from the query catalog, or a shortened URI (qname, using a default namespace). If multiple queries are executed one after the other, the first failing query stops the whole execution chain. Limitations: All optional parameters (e.g. accept, base64, ...) are provided for ALL queries in an execution chain. If you need different parameters for each query in a chain, run cmemc multiple times and use the logical operators && and || of your shell instead. Options: --accept TEXT Accept header for the HTTP request(s). Setting this to 'default' means that cmemc uses an appropriate accept header for terminal output (text/csv for tables, text/turtle for graphs, * otherwise). Please refer to the Corporate Memory system manual for a list of accepted mime types. [default: default] --no-imports Graphs which include other graphs (using owl:imports) will be queried as merged overall-graph. This flag disables this default behaviour. The flag has no effect on update queries. --base64 Enables base64 encoding of the query parameter for the SPARQL requests (the response is not touched). This can be useful in case there is an aggressive firewall between cmemc and Corporate Memory. -p, --parameter <TEXT TEXT>... In case of a parameterized query (placeholders with the '{{key}}' syntax), this option fills all placeholder with a given value before the query is executed.Pairs of placeholder/value need to be given as a tuple 'KEY VALUE'. A key can be used only once. --limit INTEGER Override or set the LIMIT in the executed SELECT query. Note that this option will never give you more results than the LIMIT given in the query itself. --offset INTEGER Override or set the OFFSET in the executed SELECT query. --distinct Override the SELECT query by make the result set DISTINCT. --timeout INTEGER Set max execution time for query evaluation (in milliseconds). -h, --help Show this message and exit.","title":"Command: query execute"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-query-list","text":"Usage: cmemc [OPTIONS] List available queries from the catalog. Outputs a list of query URIs which can be used as reference for the query execute command. Options: --id-only Lists only query identifier and no labels or other meta data. This is useful for piping the ids into other cmemc commands. -h, --help Show this message and exit.","title":"Command: query list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-query-open","text":"Usage: cmemc [OPTIONS] QUERIES... Open queries in the editor of the query catalog in your browser. With this command, you can open (remote) queries from the query catalog in the query editor in your browser (e.g. in order to change them). You can also load local query files into the query editor, in order to import them into the query catalog. The command accepts multiple query URIs or files which results in opening multiple browser tabs. Options: -h, --help Show this message and exit.","title":"Command: query open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-query-status","text":"Usage: cmemc [OPTIONS] [QUERY_UUID] Get status information of executed and running queries. With this command, you can access the latest executed SPARQL queries on the DataPlatform. These queries are identified by UUIDs and listed ordered by starting timestamp. You can filter queries based on status and runtime in order to investigate slow queries. In addition to that, you can get the details of a specific query by using the ID as a parameter. Options: --id-only Lists only query identifier and no labels or other meta data. This is useful for piping the ids into other cmemc commands. --raw Outputs raw JSON response of the query status API. --filter <TEXT TEXT>... Filter queries based on execution status and time. First parameter --filter CHOICE can be one of ['status', 'slower-than', 'type', 'regex']. The second parameter is based on CHOICE, e.g. int in case of slower-than, or a regular expression string. -h, --help Show this message and exit.","title":"Command: query status"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-query-replay","text":"Usage: cmemc [OPTIONS] REPLAY_FILE Re-execute queries from a replay file. This command reads a REPLAY_FILE and re-executes the logged queries. A REPLAY_FILE is a JSON document which is an array of JSON objects with at least a key `queryString` holding the query text OR a key 'iri' holding the IRI of the query in the query catalog. It can be created with the `query status` command, e.g. `query status --raw > replay.json` The output of this command shows basic query execution statistics. The queries are executed one after another in the order given in the input REPLAY_FILE. Query placeholders / parameters are ignored. If a query results in an error, the duration is not counted. The optional output file is the same JSON document which is used as input, but each query object is annotated with an additional 'replays' object, which is an array of JSON objects which hold values for the replay|loop|run IDs, start and end time as well as duration and other data. Options: --raw Output the execution statistic as raw JSON. --loops INTEGER Number of loops to run the replay file. [default: 1] --wait INTEGER Number of seconds to wait between query executions. [default: 0] --output-file FILE Save the optional output to this file. Input and output of the command can be the same file. The output is written at the end of a successful command execution. The output can be stdout ('-') - in this case, the execution statistic output is oppressed. --run-label TEXT Optional label of this replay run. -h, --help Show this message and exit.","title":"Command: query replay"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-vocabulary","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, (un-)install, import or open vocabs / manage cache. Options: -h, --help Show this message and exit. Commands: cache List und update the vocabulary cache. import Import a turtle file as a vocabulary. install Install one or more vocabularies from the catalog. list Output a list of vocabularies. open Open / explore a vocabulary graph in the browser. uninstall Uninstall one or more vocabularies.","title":"Command group: vocabulary"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-open","text":"Usage: cmemc [OPTIONS] IRI Open / explore a vocabulary graph in the browser. Vocabularies are identified by their graph IRI. Installed vocabularies can be listed with the \"vocabulary list\" command. Options: -h, --help Show this message and exit.","title":"Command: vocabulary open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-list","text":"Usage: cmemc [OPTIONS] Output a list of vocabularies. Vocabularies are graphs (see 'cmemc graph' command group) which consists of class and property descriptions. Options: --id-only Lists only vocabulary identifier (IRIs) and no labels or other meta data. This is useful for piping the ids into other cmemc commands. --filter [all|installed|installable] Filter list based on status. [default: installed] --raw Outputs raw JSON. -h, --help Show this message and exit.","title":"Command: vocabulary list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-install","text":"Usage: cmemc [OPTIONS] [IRIS]... Install one or more vocabularies from the catalog. Vocabularies are identified by their graph IRI. Installable vocabularies can be listed with the \"vocabulary list --filter installable\" command. Options: -a, --all Install all vocabularies from the catalog. -h, --help Show this message and exit.","title":"Command: vocabulary install"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-uninstall","text":"Usage: cmemc [OPTIONS] [IRIS]... Uninstall one or more vocabularies. Vocabularies are identified by their graph IRI. Already installed vocabularies can be listed with the \"vocabulary list --filter installed\" command. Options: -a, --all Uninstall all installed vocabularies. -h, --help Show this message and exit.","title":"Command: vocabulary uninstall"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-import","text":"Usage: cmemc [OPTIONS] FILE Import a turtle file as a vocabulary. With this command, you can import a local ontology file as a named graph. and create a corresponding vocabulary catalog entry. The uploaded ontology file is analysed locally in order to discover the named graph and the prefix declaration. This requires an OWL ontology description which correctly uses the vann:preferredNamespacePrefix and vann:preferredNamespaceUri properties. Options: --replace Replace (overwrite) existing vocabulary, if present. -h, --help Show this message and exit.","title":"Command: vocabulary import"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-vocabulary-cache","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List und update the vocabulary cache. Options: -h, --help Show this message and exit. Commands: list Output the content of the global vocabulary cache. update Reload / updates the data integration cache for a vocabulary.","title":"Command group: vocabulary cache"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-cache-update","text":"Usage: cmemc [OPTIONS] [IRIS]... Reload / updates the data integration cache for a vocabulary. Options: -a, --all Update cache for all installed vocabularies. -h, --help Show this message and exit.","title":"Command: vocabulary cache update"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-vocabulary-cache-list","text":"Usage: cmemc [OPTIONS] Output the content of the global vocabulary cache. Options: --id-only Lists only vocabulary term identifier (IRIs) and no labels or other meta data. This is useful for piping the ids into other cmemc commands. --raw Outputs raw JSON. -h, --help Show this message and exit.","title":"Command: vocabulary cache list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-workflow","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, execute, status or open (io) workflows. Workflows are identified by a WORKFLOW_ID. The get a list of existing workflows, execute the list command or use tab-completion. The WORKFLOW_ID is a concatenation of an PROJECT_ID and a TASK_ID, such as \"my-project:my- workflow\". Options: -h, --help Show this message and exit. Commands: execute Execute workflow(s). io Execute a workflow with file input/output. list List available workflow ids. open Open a workflow in your browser. scheduler List, inspect, enable/disable or open scheduler. status Get status information of workflow(s).","title":"Command group: workflow"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-execute","text":"Usage: cmemc [OPTIONS] [WORKFLOW_IDS]... Execute workflow(s). With this command, you can start one or more workflows at the same time or in a sequence, depending on the result of the predecessor. Executing a workflow can be done in two ways: Without --wait just sends the starting signal and does not look for the workflow and its result (fire and forget). Starting workflows in this way, starts all given workflows at the same time. The optional --wait option starts the workflows in the same way, but also polls the status of a workflow until it is finished. In case of an error of a workflow, the next workflow is not started. Options: -a, --all Execute all available workflows. --wait Wait until all executed workflows are completed. --polling-interval INTEGER RANGE How many seconds to wait between status polls. Status polls are cheap, so a higher polling interval is most likely not needed. [default: 1] -h, --help Show this message and exit.","title":"Command: workflow execute"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-io","text":"Usage: cmemc [OPTIONS] WORKFLOW_ID Execute a workflow with file input/output. With this command, you can execute a workflow that uses variable datasets as input, output or for configuration. Use the input parameter to feed data into the workflow. Likewise use output for retrieval of the workflow result. Workflows without a variable dataset will throw an error. Options: -i, --input FILE From which file the input is taken: note that the maximum file size to upload is limited to a server configured value. If the workflow has no defined variable input dataset, this can be ignored. -o, --output FILE To which file the result is written to: use '-' in order to output the result to stdout. If the workflow has no defined variable output dataset, this can be ignored. Please note that the io command will not warn you on overwriting existing output files. --input-mimetype [guess|application/xml|application/json|text/csv] Which input format should be processed: If not given, cmemc will try to guess the mime type based on the file extension or will fail --output-mimetype [guess|application/xml|application/json|application/n-triples|application/vnd.openxmlformats-officedocument.spreadsheetml.sheet|text/csv] Which output format should be requested: If not given, cmemc will try to guess the mime type based on the file extension or will fail. In case of an output to stdout, a default mime type will be used (currently xml). -h, --help Show this message and exit.","title":"Command: workflow io"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-list","text":"Usage: cmemc [OPTIONS] List available workflow ids. Options: --raw Outputs raw JSON objects of workflow task search API response. --id-only Lists only workflow identifier and no labels or other meta data. This is useful for piping the IDs into other commands. --filter <CHOICE TEXT>... Filter workflows based on project or suitability for the io command .First parameter CHOICE can be 'project' or 'io'. The second parameter has to be a project ID in case of 'project' or 'input- only|output-only|input-output|any' in case of 'io' filter. -h, --help Show this message and exit.","title":"Command: workflow list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-status","text":"Usage: cmemc [OPTIONS] [WORKFLOW_IDS]... Get status information of workflow(s). Options: --project TEXT The project, from which you want to list the workflows. Project IDs can be listed with the 'project list' command. --raw Output raw JSON info. --filter [Idle|Not executed|Finished|Cancelled|Failed|Successful|Canceling|Running|Waiting] Show only workflows of a specific status. -h, --help Show this message and exit.","title":"Command: workflow status"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-open","text":"Usage: cmemc [OPTIONS] WORKFLOW_ID Open a workflow in your browser. Options: -h, --help Show this message and exit.","title":"Command: workflow open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-group-workflow-scheduler","text":"Usage: cmemc [OPTIONS] COMMAND [ARGS]... List, inspect, enable/disable or open scheduler. Schedulers execute workflows in specified intervals. They are identified with a SCHEDULERID. To get a list of existing schedulers, execute the list command or use tab-completion. Options: -h, --help Show this message and exit. Commands: disable Disable scheduler(s). enable Enable scheduler(s). inspect Display all meta data of a scheduler. list List available scheduler. open Open scheduler(s) in the browser.","title":"Command group: workflow scheduler"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-scheduler-open","text":"Usage: cmemc [OPTIONS] SCHEDULER_IDS... Open scheduler(s) in the browser. With this command, you can open a scheduler in the workspace in your browser to change it. The command accepts multiple scheduler IDs which results in opening multiple browser tabs. Options: --workflow Instead of opening the scheduler page, open the page of the scheduled workflow. -h, --help Show this message and exit.","title":"Command: workflow scheduler open"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-scheduler-list","text":"Usage: cmemc [OPTIONS] List available scheduler. Outputs a table or a list of scheduler IDs which can be used as reference for the scheduler commands. Options: --raw Outputs raw JSON. --id-only Lists only task identifier and no labels or other meta data. This is useful for piping the IDs into other commands. -h, --help Show this message and exit.","title":"Command: workflow scheduler list"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-scheduler-inspect","text":"Usage: cmemc [OPTIONS] SCHEDULER_ID Display all meta data of a scheduler. Options: --raw Outputs raw JSON. -h, --help Show this message and exit.","title":"Command: workflow scheduler inspect"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-scheduler-disable","text":"Usage: cmemc [OPTIONS] [SCHEDULER_IDS]... Disable scheduler(s). The command accepts multiple scheduler IDs which results in disabling them one after the other. Options: -a, --all Disable all scheduler. -h, --help Show this message and exit.","title":"Command: workflow scheduler disable"},{"location":"automate/cmemc-command-line-interface/command-reference/#command-workflow-scheduler-enable","text":"Usage: cmemc [OPTIONS] [SCHEDULER_IDS]... Enable scheduler(s). The command accepts multiple scheduler IDs which results in enabling them one after the other. Options: -a, --all Enable all scheduler. -h, --help Show this message and exit.","title":"Command: workflow scheduler enable"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/","tags":["cmemc"],"text":"Installation and Configuration \u00a4 cmemc can be installed using the python package from pypi.org / the release package or by pulling the docker image. Installation via pypi.org \u00a4 cmemc is available as an official pypi package so installation can be done with pip or pipx (preferred): $ pipx install cmem-cmemc Installation via release package \u00a4 The cmemc release package consists of the following files: cmem_cmemc-vXX.YY.tar.gz - the source package of cmemc cmem_cmempy-vXX.YY.tar.gz - the source package of cmempy (the used python API to access Corporate Memory) cmemc_vXX.YY_Manual.pdf - the cmemc documentation manual (this document) cmemc_vXX.YY_Manual.ttl - the cmemc documentation as structured data (RDF graph) requirements.txt - additional requirements needed by cmemc The following script demonstrates how to install cmemc from these files: $ pip install -r requirements.txt ... $ pip install cmem_cmempy-v22.1.tar.gz ... $ pip install cmem_cmemc-v22.1.tar.gz ... $ cmemc --version cmemc, version 22.1, running under python 3.9.11 Installation via docker image \u00a4 This topic is described on a stand-alone page . Configuration \u00a4 Once you installed cmemc, you need to configure a connection with a config file or learn how to use environment variables to control cmemc.","title":"Installation and Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/#installation-and-configuration","text":"cmemc can be installed using the python package from pypi.org / the release package or by pulling the docker image.","title":"Installation and Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/#installation-via-pypiorg","text":"cmemc is available as an official pypi package so installation can be done with pip or pipx (preferred): $ pipx install cmem-cmemc","title":"Installation via pypi.org"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/#installation-via-release-package","text":"The cmemc release package consists of the following files: cmem_cmemc-vXX.YY.tar.gz - the source package of cmemc cmem_cmempy-vXX.YY.tar.gz - the source package of cmempy (the used python API to access Corporate Memory) cmemc_vXX.YY_Manual.pdf - the cmemc documentation manual (this document) cmemc_vXX.YY_Manual.ttl - the cmemc documentation as structured data (RDF graph) requirements.txt - additional requirements needed by cmemc The following script demonstrates how to install cmemc from these files: $ pip install -r requirements.txt ... $ pip install cmem_cmempy-v22.1.tar.gz ... $ pip install cmem_cmemc-v22.1.tar.gz ... $ cmemc --version cmemc, version 22.1, running under python 3.9.11","title":"Installation via release package"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/#installation-via-docker-image","text":"This topic is described on a stand-alone page .","title":"Installation via docker image"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/#configuration","text":"Once you installed cmemc, you need to configure a connection with a config file or learn how to use environment variables to control cmemc.","title":"Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/","tags":["cmemc"],"text":"Environment based Configuration \u00a4 Introduction \u00a4 In addition to using configuration files, cmemc can be widely configured and parameterised with environment variables. Typical use cases why you should want this includes: set a default connection (see below), enable session wide debugging output, control cmemc with variables from a calling process, avoid having client and user credentials laying around in a file, \u2026 There are two major categories of exploitable environment variables. Environment variables for configuration \u00a4 For these variables the rules are simple: You can use any variable from the config file in the same way also as an environment variable. The following commands provide the same result as given in the basic example here : $ export CMEM_BASE_URI = http://localhost/ $ export OAUTH_GRANT_TYPE = client_credentials $ export OAUTH_CLIENT_ID = cmem-service-account $ export OAUTH_CLIENT_SECRET = ... Info When you combine file based and environment based configuration, the config file always overwrite the environment. Environment variables for parameter or options \u00a4 The general pattern for parameter and option settings via environment variables is: all variables start with the prefix CMEMC_ , command group and command follow the prefix uppercased and separated by _ , finally, the option is uppercased at the end. The naming scheme is: CMEM[_<COMMAND-GROUP>_<COMMAND>][_<OPTION>] The next sections demonstrate this pattern with examples. Example: Set a default connection \u00a4 To give an example, we first run a cmemc command via command line parameter: $ cmemc --config-file cmemc.ini --connection mycmem graph list --raw [ { \"iri\": \"urn:elds-backend-access-conditions-graph\", ... more JSON output ... As a next step, we exchange all connection parameter with environment variables: $ export CMEMC_CONFIG_FILE = cmemc.ini $ export CMEMC_CONNECTION = mycmem This alone allows us to save a lot of typing for a series of commands on the same Corporate Memory instance. $ cmemc graph list --raw [... same output as above ...] But you also can pre-define command options in the same way: $ export CMEMC_GRAPH_LIST_RAW = true Again, the same command but \u2013raw is set default. $ cmemc graph list [... same output as above ...] Example: enable session wide debugging output \u00a4 Since there is a top level --debug option, the corresponding variable name is CMEMC_DEBUG : $ export CMEMC_DEBUG = true Configuration environment export from the config file \u00a4 Beginning with v21.11, cmemc can export a configuration environment from a configuration file to setup an environment for later use with the config eval command. $ cmemc -c my-cmem.example.org config eval export CMEM_BASE_URI=\"https://my-cmem.example.org\" export DI_API_ENDPOINT=\"https://my-cmem.example.org/dataintegration\" export DP_API_ENDPOINT=\"https://my-cmem.example.org/dataplatform\" unset OAUTH_ACCESS_TOKEN export OAUTH_CLIENT_ID=\"cmem-service-account\" export OAUTH_CLIENT_SECRET=\"...\" export OAUTH_GRANT_TYPE=\"client_credentials\" unset OAUTH_PASSWORD export OAUTH_TOKEN_URI=\"https://my-cmem.example.org/auth/realms/cmem/protocol/openid-connect/token\" unset OAUTH_USER export REQUESTS_CA_BUNDLE=\".../certifi/cacert.pem\" export SSL_VERIFY=\"True\" This can be used to export a full config.env or to eval it in an environment for other processes: $ cmemc -c my-cmem.example.org config eval > config.env $ eval $( cmemc -c my-cmem.example.org config eval ) Please note that the following command has the same effect but needs the cmemc.ini for evaluating the config values for the config section my-cmem.example.org: $ export CMEMC_CONNECTION = \"my-cmem.example.org\"","title":"Environment based Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#environment-based-configuration","text":"","title":"Environment based Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#introduction","text":"In addition to using configuration files, cmemc can be widely configured and parameterised with environment variables. Typical use cases why you should want this includes: set a default connection (see below), enable session wide debugging output, control cmemc with variables from a calling process, avoid having client and user credentials laying around in a file, \u2026 There are two major categories of exploitable environment variables.","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#environment-variables-for-configuration","text":"For these variables the rules are simple: You can use any variable from the config file in the same way also as an environment variable. The following commands provide the same result as given in the basic example here : $ export CMEM_BASE_URI = http://localhost/ $ export OAUTH_GRANT_TYPE = client_credentials $ export OAUTH_CLIENT_ID = cmem-service-account $ export OAUTH_CLIENT_SECRET = ... Info When you combine file based and environment based configuration, the config file always overwrite the environment.","title":"Environment variables for configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#environment-variables-for-parameter-or-options","text":"The general pattern for parameter and option settings via environment variables is: all variables start with the prefix CMEMC_ , command group and command follow the prefix uppercased and separated by _ , finally, the option is uppercased at the end. The naming scheme is: CMEM[_<COMMAND-GROUP>_<COMMAND>][_<OPTION>] The next sections demonstrate this pattern with examples.","title":"Environment variables for parameter or options"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#example-set-a-default-connection","text":"To give an example, we first run a cmemc command via command line parameter: $ cmemc --config-file cmemc.ini --connection mycmem graph list --raw [ { \"iri\": \"urn:elds-backend-access-conditions-graph\", ... more JSON output ... As a next step, we exchange all connection parameter with environment variables: $ export CMEMC_CONFIG_FILE = cmemc.ini $ export CMEMC_CONNECTION = mycmem This alone allows us to save a lot of typing for a series of commands on the same Corporate Memory instance. $ cmemc graph list --raw [... same output as above ...] But you also can pre-define command options in the same way: $ export CMEMC_GRAPH_LIST_RAW = true Again, the same command but \u2013raw is set default. $ cmemc graph list [... same output as above ...]","title":"Example: Set a default connection"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#example-enable-session-wide-debugging-output","text":"Since there is a top level --debug option, the corresponding variable name is CMEMC_DEBUG : $ export CMEMC_DEBUG = true","title":"Example: enable session wide debugging output"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/environment-based-configuration/#configuration-environment-export-from-the-config-file","text":"Beginning with v21.11, cmemc can export a configuration environment from a configuration file to setup an environment for later use with the config eval command. $ cmemc -c my-cmem.example.org config eval export CMEM_BASE_URI=\"https://my-cmem.example.org\" export DI_API_ENDPOINT=\"https://my-cmem.example.org/dataintegration\" export DP_API_ENDPOINT=\"https://my-cmem.example.org/dataplatform\" unset OAUTH_ACCESS_TOKEN export OAUTH_CLIENT_ID=\"cmem-service-account\" export OAUTH_CLIENT_SECRET=\"...\" export OAUTH_GRANT_TYPE=\"client_credentials\" unset OAUTH_PASSWORD export OAUTH_TOKEN_URI=\"https://my-cmem.example.org/auth/realms/cmem/protocol/openid-connect/token\" unset OAUTH_USER export REQUESTS_CA_BUNDLE=\".../certifi/cacert.pem\" export SSL_VERIFY=\"True\" This can be used to export a full config.env or to eval it in an environment for other processes: $ cmemc -c my-cmem.example.org config eval > config.env $ eval $( cmemc -c my-cmem.example.org config eval ) Please note that the following command has the same effect but needs the cmemc.ini for evaluating the config values for the config section my-cmem.example.org: $ export CMEMC_CONNECTION = \"my-cmem.example.org\"","title":"Configuration environment export from the config file"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/","tags":["cmemc"],"text":"File based Configuration \u00a4 Introduction \u00a4 This page documents how to configure cmemc via configuration files. cmemc looks for a default configuration file on a reasonable place depending on your operating system: For Linux, this is $HOME/.config/cmemc/config.ini . For Windows, this is %APPDATA%\\cmemc\\config.ini . If you need to change this location and want use another config file, you have the following options: you can always run cmemc with the --config-file path/to/your/config.ini option, or you can set a new config file with the environment variable CMEMC_CONFIG_FILE , or you can set the XDG_CONFIG_HOME variable to something different than $HOME/.config/ (this will also change the config location of all other XDG Base Directory Specification aware tools). However, once you start cmemc the first time and without any command or option, it will create an empty config file at this location and will output a general introduction. First cmemc run \u2026 $ cmemc Empty config created: /home/user/.config/cmemc/config.ini Usage: cmemc [OPTIONS] COMMAND [ARGS]... eccenca Corporate Memory Control (cmemc). cmemc is the eccenca Corporate Memory Command Line Interface (CLI). Available commands are grouped by affecting resource type (such as graph, project and query). Each command and group has a separate --help screen for detailed documentation. In order to see possible commands in a group, simply execute the group command without further parameter (e.g. cmemc project). If your terminal supports colors, these coloring rules are applied: Groups are colored in white; Commands which change data are colored in red; all other commands as well as options are colored in green. Please also have a look at the cmemc online documentation: https://eccenca.com/go/cmemc cmemc is \u00a9 2022 eccenca GmbH, licensed under the Apache License 2.0. Options: -c, --connection TEXT Use a specific connection from the config file. --config-file FILE Use this config file instead of the default one. [default: /Users/seebi/Library/Application Support/cmemc/config.ini] -q, --quiet Suppress any non-error info messages. -d, --debug Output debug messages and stack traces after errors. --version Show the version and exit. -h, --help Show this message and exit. Commands: admin Import bootstrap data, backup/restore workspace or get status. config List and edit configs as well as get config values. dataset List, create, delete, inspect, up-/download or open datasets. graph List, import, export, delete, count, tree or open graphs. project List, import, export, create, delete or open projects. query List, execute, get status or open SPARQL queries. vocabulary List, (un-)install, import or open vocabs / manage cache. workflow List, execute, status or open (io) workflows. You can now edit your config file and add credentials and URL parameter for your Corporate Memory deployment. You either search the config manually in your home directory or you can use the config edit command, which opens the config file in your default text editor (specified with the EDITOR variable ). $ cmemc config edit Open editor for config file /home/user/.config/cmemc/config.ini The rules for the config file are similar to a Windows INI file and are explained in detail at docs.python.org . Examples \u00a4 Example [my-local] CMEM_BASE_URI = http://localhost/ OAUTH_GRANT_TYPE = client_credentials OAUTH_CLIENT_ID = cmem-service-account OAUTH_CLIENT_SECRET = ... Here is a minimal example using the client_credentials grant type. This creates a named section my-local which is a connection to a Corporate Memory deployment on http://localhost/ . The authorization will be done with a system account cmem-service-account and the given client secret. Using this combination of config parameter is based on a typical installation where, all components are available under the same hostname. Example [my-local] CMEM_BASE_URI = http://localhost/ OAUTH_GRANT_TYPE = password OAUTH_CLIENT_ID = cmemc OAUTH_USERNAME = user OAUTH_PASSWORD = ... Another example using password grant type. This creates a named section my-local which is a connection to a Corporate Memory deployment on http://localhost/ . The authorization will be done with the given OAUTH_USERNAME and the OAUTH_PASSWORD . Configuration Variables \u00a4 The above example provides access to an installation where all components including keycloak are deployed with the default URL base. However, if you need to fine tune all locations or want to use special functionality, the following config file parameters can be used to do this. Location related \u00a4 The following configuration variables specify where cmemc can find the relevant HTTP endpoints. Most of them are optional. CMEM_BASE_URI \u00a4 This is the base location (HTTP(S) URL) of your eccenca Corporate Memory deployment. You always have to set this configuration variable. This variable defaults to http://docker.localhost/ . DI_API_ENDPOINT \u00a4 This is the base location (HTTP(S) URL) of all Data Integration APIs. Usually you do not need to set this configuration variable. This variable defaults to $CMEM_BASE_URI/dataintegration/ . DP_API_ENDPOINT \u00a4 This is the base location (HTTP(S) URL) of all Data Platform APIs. Usually you do not need to set this configuration variable. This variable defaults to $CMEM_BASE_URI/dataplatform/ . OAUTH_TOKEN_URI \u00a4 This is OpenID Connect (OIDC) OAuth 2.0 token endpoint location (HTTP(S) URL). Usually you do not need to set this configuration variable. This variable defaults to $CMEM_BASE_URI/auth/realms/cmem/protocol/openid-connect/token . Authentication related \u00a4 The following configuration variables specify how cmemc can fetch a token in order to authenticate with this token on the endpoints. OAUTH_GRANT_TYPE \u00a4 This configures the used OAuth Grant Type in order to specify how cmemc is able to get a valid token for accessing the Corporate Memory APIs. Depending on the value of this variable, other authentication related variables will become mandatory or obsolete. The following values can be used: client_credentials - this refers to the OAuth 2.0 Client Credentials Grant Type . Mandatory variables for this grant type are OAUTH_CLIENT_ID , OAUTH_CLIENT_SECRET or OAUTH_CLIENT_SECRET_PROCESS . password - this refers to the OAuth 2.0 Password Grant Type . Mandatory variables for this grant type are OAUTH_CLIENT_ID , OAUTH_USER , OAUTH_PASSWORD or OAUTH_PASSWORD_PROCESS . prefetched_token - this value can be used in case you can provide a token which was fetched outside of cmemc. Mandatory variables for this grant type are OAUTH_ACCESS_TOKEN or OAUTH_ACCESS_TOKEN_PROCESS . OAUTH_CLIENT_ID \u00a4 This configures the used client ID. Ususally, the following cmemc related clients are configured in the standard Corporate Memory realm: cmem-service-account is the client which is configured to be used with the client_credentials grant type. cmemc is the client which is configured to be used with the password grant type. You usually have to set this configuration variable (exception: you use the prefetched_token grant type). This variable defaults to cmem-service-account . OAUTH_USER \u00a4 This variable specifies your user account. You only need to set this configuration variable in case you use the password grant type. This variable defaults to admin . OAUTH_PASSWORD \u00a4 This variable specifies your user password. You only need to set this configuration variable in case you use the password grant type. OAUTH_CLIENT_SECRET \u00a4 This variable specifies your client secret (password). You only need to set this configuration variable in case you use the client_credentials grant type. OAUTH_ACCESS_TOKEN \u00a4 This variable specifies a prefetched access token. You only need to set this configuration variable in case you use the prefetched_token grant type. OAUTH_PASSWORD_PROCESS \u00a4 In order to avoid saving credentials in config files you can use this optional configuration variable instead of the OAUTH_PASSWORD variable. Please refer to Getting Credentials from external Processes for more information. This variable defaults to none . OAUTH_CLIENT_SECRET_PROCESS \u00a4 In order to avoid saving credentials in config files you can use this optional configuration variable instead of the OAUTH_CLIENT_SECRET variable. Please refer to Getting Credentials from external Processes for more information. This variable defaults to none . OAUTH_ACCESS_TOKEN_PROCESS \u00a4 In order to avoid saving credentials in config files you can use this optional configuration variable instead of the OAUTH_ACCESS_TOKEN variable. Please refer to Getting Credentials from external Processes for more information. This variable defaults to none . Network related \u00a4 SSL_VERIFY \u00a4 Setting this to True will disable certification verification (not recommended). Please refer to Certificate handling and SSL verification for more information. This variable defaults to False . REQUESTS_CA_BUNDLE \u00a4 Setting this to a PEM file allow for using private Certificate Authorities for certificate validation. Please refer to Certificate handling and SSL verification for more information. This variable defaults to $PYTHON_HOME/site-packages/certifi/cacert.pem .","title":"File based Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#file-based-configuration","text":"","title":"File based Configuration"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#introduction","text":"This page documents how to configure cmemc via configuration files. cmemc looks for a default configuration file on a reasonable place depending on your operating system: For Linux, this is $HOME/.config/cmemc/config.ini . For Windows, this is %APPDATA%\\cmemc\\config.ini . If you need to change this location and want use another config file, you have the following options: you can always run cmemc with the --config-file path/to/your/config.ini option, or you can set a new config file with the environment variable CMEMC_CONFIG_FILE , or you can set the XDG_CONFIG_HOME variable to something different than $HOME/.config/ (this will also change the config location of all other XDG Base Directory Specification aware tools). However, once you start cmemc the first time and without any command or option, it will create an empty config file at this location and will output a general introduction. First cmemc run \u2026 $ cmemc Empty config created: /home/user/.config/cmemc/config.ini Usage: cmemc [OPTIONS] COMMAND [ARGS]... eccenca Corporate Memory Control (cmemc). cmemc is the eccenca Corporate Memory Command Line Interface (CLI). Available commands are grouped by affecting resource type (such as graph, project and query). Each command and group has a separate --help screen for detailed documentation. In order to see possible commands in a group, simply execute the group command without further parameter (e.g. cmemc project). If your terminal supports colors, these coloring rules are applied: Groups are colored in white; Commands which change data are colored in red; all other commands as well as options are colored in green. Please also have a look at the cmemc online documentation: https://eccenca.com/go/cmemc cmemc is \u00a9 2022 eccenca GmbH, licensed under the Apache License 2.0. Options: -c, --connection TEXT Use a specific connection from the config file. --config-file FILE Use this config file instead of the default one. [default: /Users/seebi/Library/Application Support/cmemc/config.ini] -q, --quiet Suppress any non-error info messages. -d, --debug Output debug messages and stack traces after errors. --version Show the version and exit. -h, --help Show this message and exit. Commands: admin Import bootstrap data, backup/restore workspace or get status. config List and edit configs as well as get config values. dataset List, create, delete, inspect, up-/download or open datasets. graph List, import, export, delete, count, tree or open graphs. project List, import, export, create, delete or open projects. query List, execute, get status or open SPARQL queries. vocabulary List, (un-)install, import or open vocabs / manage cache. workflow List, execute, status or open (io) workflows. You can now edit your config file and add credentials and URL parameter for your Corporate Memory deployment. You either search the config manually in your home directory or you can use the config edit command, which opens the config file in your default text editor (specified with the EDITOR variable ). $ cmemc config edit Open editor for config file /home/user/.config/cmemc/config.ini The rules for the config file are similar to a Windows INI file and are explained in detail at docs.python.org .","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#examples","text":"Example [my-local] CMEM_BASE_URI = http://localhost/ OAUTH_GRANT_TYPE = client_credentials OAUTH_CLIENT_ID = cmem-service-account OAUTH_CLIENT_SECRET = ... Here is a minimal example using the client_credentials grant type. This creates a named section my-local which is a connection to a Corporate Memory deployment on http://localhost/ . The authorization will be done with a system account cmem-service-account and the given client secret. Using this combination of config parameter is based on a typical installation where, all components are available under the same hostname. Example [my-local] CMEM_BASE_URI = http://localhost/ OAUTH_GRANT_TYPE = password OAUTH_CLIENT_ID = cmemc OAUTH_USERNAME = user OAUTH_PASSWORD = ... Another example using password grant type. This creates a named section my-local which is a connection to a Corporate Memory deployment on http://localhost/ . The authorization will be done with the given OAUTH_USERNAME and the OAUTH_PASSWORD .","title":"Examples"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#configuration-variables","text":"The above example provides access to an installation where all components including keycloak are deployed with the default URL base. However, if you need to fine tune all locations or want to use special functionality, the following config file parameters can be used to do this.","title":"Configuration Variables"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#location-related","text":"The following configuration variables specify where cmemc can find the relevant HTTP endpoints. Most of them are optional.","title":"Location related"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#cmem_base_uri","text":"This is the base location (HTTP(S) URL) of your eccenca Corporate Memory deployment. You always have to set this configuration variable. This variable defaults to http://docker.localhost/ .","title":"CMEM_BASE_URI"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#di_api_endpoint","text":"This is the base location (HTTP(S) URL) of all Data Integration APIs. Usually you do not need to set this configuration variable. This variable defaults to $CMEM_BASE_URI/dataintegration/ .","title":"DI_API_ENDPOINT"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#dp_api_endpoint","text":"This is the base location (HTTP(S) URL) of all Data Platform APIs. Usually you do not need to set this configuration variable. This variable defaults to $CMEM_BASE_URI/dataplatform/ .","title":"DP_API_ENDPOINT"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_token_uri","text":"This is OpenID Connect (OIDC) OAuth 2.0 token endpoint location (HTTP(S) URL). Usually you do not need to set this configuration variable. This variable defaults to $CMEM_BASE_URI/auth/realms/cmem/protocol/openid-connect/token .","title":"OAUTH_TOKEN_URI"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#authentication-related","text":"The following configuration variables specify how cmemc can fetch a token in order to authenticate with this token on the endpoints.","title":"Authentication related"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_grant_type","text":"This configures the used OAuth Grant Type in order to specify how cmemc is able to get a valid token for accessing the Corporate Memory APIs. Depending on the value of this variable, other authentication related variables will become mandatory or obsolete. The following values can be used: client_credentials - this refers to the OAuth 2.0 Client Credentials Grant Type . Mandatory variables for this grant type are OAUTH_CLIENT_ID , OAUTH_CLIENT_SECRET or OAUTH_CLIENT_SECRET_PROCESS . password - this refers to the OAuth 2.0 Password Grant Type . Mandatory variables for this grant type are OAUTH_CLIENT_ID , OAUTH_USER , OAUTH_PASSWORD or OAUTH_PASSWORD_PROCESS . prefetched_token - this value can be used in case you can provide a token which was fetched outside of cmemc. Mandatory variables for this grant type are OAUTH_ACCESS_TOKEN or OAUTH_ACCESS_TOKEN_PROCESS .","title":"OAUTH_GRANT_TYPE"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_client_id","text":"This configures the used client ID. Ususally, the following cmemc related clients are configured in the standard Corporate Memory realm: cmem-service-account is the client which is configured to be used with the client_credentials grant type. cmemc is the client which is configured to be used with the password grant type. You usually have to set this configuration variable (exception: you use the prefetched_token grant type). This variable defaults to cmem-service-account .","title":"OAUTH_CLIENT_ID"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_user","text":"This variable specifies your user account. You only need to set this configuration variable in case you use the password grant type. This variable defaults to admin .","title":"OAUTH_USER"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_password","text":"This variable specifies your user password. You only need to set this configuration variable in case you use the password grant type.","title":"OAUTH_PASSWORD"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_client_secret","text":"This variable specifies your client secret (password). You only need to set this configuration variable in case you use the client_credentials grant type.","title":"OAUTH_CLIENT_SECRET"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_access_token","text":"This variable specifies a prefetched access token. You only need to set this configuration variable in case you use the prefetched_token grant type.","title":"OAUTH_ACCESS_TOKEN"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_password_process","text":"In order to avoid saving credentials in config files you can use this optional configuration variable instead of the OAUTH_PASSWORD variable. Please refer to Getting Credentials from external Processes for more information. This variable defaults to none .","title":"OAUTH_PASSWORD_PROCESS"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_client_secret_process","text":"In order to avoid saving credentials in config files you can use this optional configuration variable instead of the OAUTH_CLIENT_SECRET variable. Please refer to Getting Credentials from external Processes for more information. This variable defaults to none .","title":"OAUTH_CLIENT_SECRET_PROCESS"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#oauth_access_token_process","text":"In order to avoid saving credentials in config files you can use this optional configuration variable instead of the OAUTH_ACCESS_TOKEN variable. Please refer to Getting Credentials from external Processes for more information. This variable defaults to none .","title":"OAUTH_ACCESS_TOKEN_PROCESS"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#network-related","text":"","title":"Network related"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#ssl_verify","text":"Setting this to True will disable certification verification (not recommended). Please refer to Certificate handling and SSL verification for more information. This variable defaults to False .","title":"SSL_VERIFY"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/file-based-configuration/#requests_ca_bundle","text":"Setting this to a PEM file allow for using private Certificate Authorities for certificate validation. Please refer to Certificate handling and SSL verification for more information. This variable defaults to $PYTHON_HOME/site-packages/certifi/cacert.pem .","title":"REQUESTS_CA_BUNDLE"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/getting-credentials-from-external-processes/","tags":["cmemc"],"text":"Getting Credentials from external Processes \u00a4 Introduction \u00a4 This page discusses how to avoid passwords in configuration files by using configured credential processes or environment variables. This is especially needed when credentials change often and / or are stored in central infrastructure such as personal or company wide password managers. In addition to that, you might find it useful when working with cmemc in CI/CD pipelines. Environment Variables \u00a4 As described in the Configuration with Environment Variables document, cmemc can be configured with environment variables. The following code snippet demonstrates behaviour: $ export CMEM_BASE_URI = \"https://your-cmem.eccenca.dev/\" $ export OAUTH_GRANT_TYPE = \"client_credentials\" $ export OAUTH_CLIENT_ID = \"cmem-service-account\" $ export OAUTH_CLIENT_SECRET = \"...secret...\" $ cmemc graph list In the context of a CI/CD pipeline e.g. on github, these credentials can be taken from the repository secrets: jobs : build : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - name : run cmemc env : CMEM_BASE_URI : https://your-cmem.eccenca.dev/ OAUTH_GRANT_TYPE : client_credentials OAUTH_CLIENT_ID : cmem-service-account OAUTH_CLIENT_SECRET : ${{ secrets.OAUTH_CLIENT_SECRET }} run : | cmemc graph list If in shell context, you can fetch the secret from an external process to the variable: $ export OAUTH_CLIENT_SECRET = $( get-my-secret.sh ) External Processes \u00a4 Another option, which is interesting when working with multiple Corporate Memory instances, is to configure an external process in your cmemc configuration file . In order to get credential information from an external process, you need to use the following configuration variables to setup an external executable: OAUTH_PASSWORD_PROCESS , to setup the process to get the use password when using the password grant type. OAUTH_CLIENT_SECRET_PROCESS , to setup the process to get the client secret when using client_credentials grant type . OAUTH_ACCESS_TOKEN_PROCESS , to setup the process to get the direct access token ( prefetched_token ). The credential executable can use the other cmemc environment keys of the configuration block for fetching the credential (e.g. CMEM_BASE_URI and OAUTH_USER ). If the credential executable is not given with a a full path, cmemc will look into your environment PATH for something which can be executed. The configured process needs to return the credential on the first line of stdout . In addition to that, the process needs to exit with exit code 0 (without failure). The following config section demonstrates this behaviour: [your-cmem] CMEM_BASE_URI = https://your-cmem.eccenca.dev/ OAUTH_GRANT_TYPE = client_credentials OAUTH_CLIENT_ID = cmem-service-account OAUTH_CLIENT_SECRET_PROCESS = get-my-secret.sh If you need to add options to the call, you can write the call as a list: [your-cmem] CMEM_BASE_URI = https://your-cmem.eccenca.dev/ OAUTH_GRANT_TYPE = client_credentials OAUTH_CLIENT_ID = cmem-service-account OAUTH_CLIENT_SECRET_PROCESS = [\"getpass.sh\", \"parameter1\", \"parameter2\"] Example: MacOS Keychain \u00a4 Here is an working example with the MacOS Keychain, which can be queried with the command line tool security . This example fetches a password for the account cmem-service-account on the service https://your-cmem.eccenca.dev/ . OAUTH_CLIENT_SECRET_PROCESS = [\"security\", \"find-generic-password\", \"-w\", \"-a\", \"cmem-service-account\", \"-s\", \"https://your-cmem.eccenca.dev/\" ] The corresponding keychain entry looks like this: In order to avoid repeating this long line in a cmemc configuration with lots of entries, this could be wrapped in a shell script like this: #!/usr/bin/env bash if [ \" ${ OAUTH_GRANT_TYPE } \" = \"client_credentials\" ] ; then security find-generic-password -w -a \" ${ OAUTH_CLIENT_ID } \" -s \" ${ CMEM_BASE_URI } \" || exit 1 exit 0 fi if [ \" ${ OAUTH_GRANT_TYPE } \" = \"password\" ] ; then security find-generic-password -w -a \" ${ OAUTH_USER } \" -s \" ${ CMEM_BASE_URI } \" || exit 1 exit 0 fi exit 1","title":"Getting Credentials from external Processes"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/getting-credentials-from-external-processes/#getting-credentials-from-external-processes","text":"","title":"Getting Credentials from external Processes"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/getting-credentials-from-external-processes/#introduction","text":"This page discusses how to avoid passwords in configuration files by using configured credential processes or environment variables. This is especially needed when credentials change often and / or are stored in central infrastructure such as personal or company wide password managers. In addition to that, you might find it useful when working with cmemc in CI/CD pipelines.","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/getting-credentials-from-external-processes/#environment-variables","text":"As described in the Configuration with Environment Variables document, cmemc can be configured with environment variables. The following code snippet demonstrates behaviour: $ export CMEM_BASE_URI = \"https://your-cmem.eccenca.dev/\" $ export OAUTH_GRANT_TYPE = \"client_credentials\" $ export OAUTH_CLIENT_ID = \"cmem-service-account\" $ export OAUTH_CLIENT_SECRET = \"...secret...\" $ cmemc graph list In the context of a CI/CD pipeline e.g. on github, these credentials can be taken from the repository secrets: jobs : build : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - name : run cmemc env : CMEM_BASE_URI : https://your-cmem.eccenca.dev/ OAUTH_GRANT_TYPE : client_credentials OAUTH_CLIENT_ID : cmem-service-account OAUTH_CLIENT_SECRET : ${{ secrets.OAUTH_CLIENT_SECRET }} run : | cmemc graph list If in shell context, you can fetch the secret from an external process to the variable: $ export OAUTH_CLIENT_SECRET = $( get-my-secret.sh )","title":"Environment Variables"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/getting-credentials-from-external-processes/#external-processes","text":"Another option, which is interesting when working with multiple Corporate Memory instances, is to configure an external process in your cmemc configuration file . In order to get credential information from an external process, you need to use the following configuration variables to setup an external executable: OAUTH_PASSWORD_PROCESS , to setup the process to get the use password when using the password grant type. OAUTH_CLIENT_SECRET_PROCESS , to setup the process to get the client secret when using client_credentials grant type . OAUTH_ACCESS_TOKEN_PROCESS , to setup the process to get the direct access token ( prefetched_token ). The credential executable can use the other cmemc environment keys of the configuration block for fetching the credential (e.g. CMEM_BASE_URI and OAUTH_USER ). If the credential executable is not given with a a full path, cmemc will look into your environment PATH for something which can be executed. The configured process needs to return the credential on the first line of stdout . In addition to that, the process needs to exit with exit code 0 (without failure). The following config section demonstrates this behaviour: [your-cmem] CMEM_BASE_URI = https://your-cmem.eccenca.dev/ OAUTH_GRANT_TYPE = client_credentials OAUTH_CLIENT_ID = cmem-service-account OAUTH_CLIENT_SECRET_PROCESS = get-my-secret.sh If you need to add options to the call, you can write the call as a list: [your-cmem] CMEM_BASE_URI = https://your-cmem.eccenca.dev/ OAUTH_GRANT_TYPE = client_credentials OAUTH_CLIENT_ID = cmem-service-account OAUTH_CLIENT_SECRET_PROCESS = [\"getpass.sh\", \"parameter1\", \"parameter2\"]","title":"External Processes"},{"location":"automate/cmemc-command-line-interface/installation-and-configuration/getting-credentials-from-external-processes/#example-macos-keychain","text":"Here is an working example with the MacOS Keychain, which can be queried with the command line tool security . This example fetches a password for the account cmem-service-account on the service https://your-cmem.eccenca.dev/ . OAUTH_CLIENT_SECRET_PROCESS = [\"security\", \"find-generic-password\", \"-w\", \"-a\", \"cmem-service-account\", \"-s\", \"https://your-cmem.eccenca.dev/\" ] The corresponding keychain entry looks like this: In order to avoid repeating this long line in a cmemc configuration with lots of entries, this could be wrapped in a shell script like this: #!/usr/bin/env bash if [ \" ${ OAUTH_GRANT_TYPE } \" = \"client_credentials\" ] ; then security find-generic-password -w -a \" ${ OAUTH_CLIENT_ID } \" -s \" ${ CMEM_BASE_URI } \" || exit 1 exit 0 fi if [ \" ${ OAUTH_GRANT_TYPE } \" = \"password\" ] ; then security find-generic-password -w -a \" ${ OAUTH_USER } \" -s \" ${ CMEM_BASE_URI } \" || exit 1 exit 0 fi exit 1","title":"Example: MacOS Keychain"},{"location":"automate/cmemc-command-line-interface/sparql-scripts/","tags":["cmemc","SPARQL"],"text":"SPARQL Scripts \u00a4 By prepending a Shebang line to a SPARQL query file and making this file executable, the query file can be treated as an executable script. As an example, here is a simple text file with a generic SPARQL query which counts all triples in all graphs and outputs an ordered list: count-graphs.sh SELECT DISTINCT ?graph ( COUNT ( ?graph ) AS ?triples ) WHERE { GRAPH ?graph { ?s ?p ?o } } GROUP BY ?graph ORDER BY DESC ( ?triples ) In order to make a SPARQL script out of this text file you need to add the following line on top of this file: shebang line for SPARQL scripts #!/usr/bin/env -S cmemc query execute --accept text/csv This will set cmemc as an interpreter for the rest of the file and by using the query execute command, the rest of the file will be used as a SPARQL query. Now you need to define your SPARQL file as executable and run it: $ chmod a+x ./count-graphs.sh $ ./count-graphs.sh graph,triples https://vocab.eccenca.com/shacl/,1796 https://vocab.eccenca.com/dsm/,736 https://vocab.eccenca.com/sketch/,395 https://ns.eccenca.com/example/data/dataset/,233 https://ns.eccenca.com/example/data/vocabs/,128 urn:elds-backend-access-conditions-graph,97 https://ns.eccenca.com/data/queries/,32 http://di.eccenca.com/project/cmem,7","title":"SPARQL Scripts"},{"location":"automate/cmemc-command-line-interface/sparql-scripts/#sparql-scripts","text":"By prepending a Shebang line to a SPARQL query file and making this file executable, the query file can be treated as an executable script. As an example, here is a simple text file with a generic SPARQL query which counts all triples in all graphs and outputs an ordered list: count-graphs.sh SELECT DISTINCT ?graph ( COUNT ( ?graph ) AS ?triples ) WHERE { GRAPH ?graph { ?s ?p ?o } } GROUP BY ?graph ORDER BY DESC ( ?triples ) In order to make a SPARQL script out of this text file you need to add the following line on top of this file: shebang line for SPARQL scripts #!/usr/bin/env -S cmemc query execute --accept text/csv This will set cmemc as an interpreter for the rest of the file and by using the query execute command, the rest of the file will be used as a SPARQL query. Now you need to define your SPARQL file as executable and run it: $ chmod a+x ./count-graphs.sh $ ./count-graphs.sh graph,triples https://vocab.eccenca.com/shacl/,1796 https://vocab.eccenca.com/dsm/,736 https://vocab.eccenca.com/sketch/,395 https://ns.eccenca.com/example/data/dataset/,233 https://ns.eccenca.com/example/data/vocabs/,128 urn:elds-backend-access-conditions-graph,97 https://ns.eccenca.com/data/queries/,32 http://di.eccenca.com/project/cmem,7","title":"SPARQL Scripts"},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/","tags":["cmemc"],"text":"Troubleshooting and Caveats \u00a4 This page lists and documents possible issues and warnings when working with cmemc. Proxy is in the way \u00a4 If you feel that your systems proxy configuration troubles the communication between cmemc and Corporate Memory, you can disable the usage of any proxy by setting this variable: export no_proxy='*' This is due to the python requests library proxy handling . The no_proxy environment variable can be used to specify hosts which shouldn\u2019t be reached via proxy; if set, it should be a comma-separated list of hostname suffixes, optionally with :port appended, for example cern.ch,ncsa.uiuc.edu,some.host:8080. Gateway Time-out \u00a4 A gateway time-out occurs if your Corporate Memory infrastructure ins not setup correctly. $ cmemc -c my-cmem project import my-project.zip my-project Import file my-project.zip to project my-project ... 504 Server Error: Gateway Time-out for url: https://my-cmem/dataintegration/workspace/projects This can have multiple reasons - please check in the following order: application.yaml of DataIntegration reverse proxy configuration","title":"Troubleshooting and Caveats"},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#troubleshooting-and-caveats","text":"This page lists and documents possible issues and warnings when working with cmemc.","title":"Troubleshooting and Caveats"},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#proxy-is-in-the-way","text":"If you feel that your systems proxy configuration troubles the communication between cmemc and Corporate Memory, you can disable the usage of any proxy by setting this variable: export no_proxy='*' This is due to the python requests library proxy handling . The no_proxy environment variable can be used to specify hosts which shouldn\u2019t be reached via proxy; if set, it should be a comma-separated list of hostname suffixes, optionally with :port appended, for example cern.ch,ncsa.uiuc.edu,some.host:8080.","title":"Proxy is in the way"},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#gateway-time-out","text":"A gateway time-out occurs if your Corporate Memory infrastructure ins not setup correctly. $ cmemc -c my-cmem project import my-project.zip my-project Import file my-project.zip to project my-project ... 504 Server Error: Gateway Time-out for url: https://my-cmem/dataintegration/workspace/projects This can have multiple reasons - please check in the following order: application.yaml of DataIntegration reverse proxy configuration","title":"Gateway Time-out"},{"location":"automate/cmemc-command-line-interface/using-the-docker-image/","tags":["cmemc","Docker"],"text":"Using the docker image \u00a4 Introduction \u00a4 In addition to the cmemc distribution package, you can use the eccenca cmemc docker image which is based on the official debian slim image . This is especially needed when you want to use cmemc in orchestrations. Image and Tags \u00a4 The following image - tag combinations are available for public use: docker-registry.eccenca.com/eccenca-cmemc:v22.1 - a specific release docker-registry.eccenca.com/eccenca-cmemc:latest - same as the latest release Image retrieval and check cmemc version $ docker run -it --rm docker-registry.eccenca.com/eccenca-cmemc:v22.1 --version Unable to find image 'docker-registry.eccenca.com/eccenca-cmemc:v22.1' locally v22.1: Pulling from eccenca-cmemc Digest: sha256:29bdd320e02f1b7758df22528740964225b62530c73c773a55c36c0e9e18b647 Status: Downloaded newer image for docker-registry.eccenca.com/eccenca-cmemc:v22.1 cmemc, version v22.1.1, running under python 3.9.13 Volumes \u00a4 cmemc processes a configuration file and can import and export files which represent graph, project or workspace payloads. These files need to be mounted via docker volumes to be accessible for the dockerized cmemc. /config/cmemc.ini (file) - the loaded configuration file /data (directory) - the working directory Using a volume to mount the config. $ cat cmemc.ini [my-deployment] CMEM_BASE_URI=https://data.example.org/ OAUTH_GRANT_TYPE=client_credentials OAUTH_CLIENT_ID=cmem-service-account OAUTH_CLIENT_SECRET=credentialshere $ docker run -it --rm -v \" $( pwd ) \" /cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.1 config list my-deployment Using a volume to additionally mount the data directory. $ cat list-graphs.sparql SELECT DISTINCT ?graph (COUNT(?graph) AS ?triples) WHERE { GRAPH ?graph { ?s ?p ?o } } GROUP BY ?graph ORDER BY DESC(?triples) $ docker run -it --rm -v $( pwd ) :/data -v $( pwd ) /cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.1 -c my-deployment query execute ./list-graphs.sparql graph,triples http://schema.org/,8809 https://vocab.eccenca.com/shacl/,1752 [...]","title":"Using the docker image"},{"location":"automate/cmemc-command-line-interface/using-the-docker-image/#using-the-docker-image","text":"","title":"Using the docker image"},{"location":"automate/cmemc-command-line-interface/using-the-docker-image/#introduction","text":"In addition to the cmemc distribution package, you can use the eccenca cmemc docker image which is based on the official debian slim image . This is especially needed when you want to use cmemc in orchestrations.","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/using-the-docker-image/#image-and-tags","text":"The following image - tag combinations are available for public use: docker-registry.eccenca.com/eccenca-cmemc:v22.1 - a specific release docker-registry.eccenca.com/eccenca-cmemc:latest - same as the latest release Image retrieval and check cmemc version $ docker run -it --rm docker-registry.eccenca.com/eccenca-cmemc:v22.1 --version Unable to find image 'docker-registry.eccenca.com/eccenca-cmemc:v22.1' locally v22.1: Pulling from eccenca-cmemc Digest: sha256:29bdd320e02f1b7758df22528740964225b62530c73c773a55c36c0e9e18b647 Status: Downloaded newer image for docker-registry.eccenca.com/eccenca-cmemc:v22.1 cmemc, version v22.1.1, running under python 3.9.13","title":"Image and Tags"},{"location":"automate/cmemc-command-line-interface/using-the-docker-image/#volumes","text":"cmemc processes a configuration file and can import and export files which represent graph, project or workspace payloads. These files need to be mounted via docker volumes to be accessible for the dockerized cmemc. /config/cmemc.ini (file) - the loaded configuration file /data (directory) - the working directory Using a volume to mount the config. $ cat cmemc.ini [my-deployment] CMEM_BASE_URI=https://data.example.org/ OAUTH_GRANT_TYPE=client_credentials OAUTH_CLIENT_ID=cmem-service-account OAUTH_CLIENT_SECRET=credentialshere $ docker run -it --rm -v \" $( pwd ) \" /cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.1 config list my-deployment Using a volume to additionally mount the data directory. $ cat list-graphs.sparql SELECT DISTINCT ?graph (COUNT(?graph) AS ?triples) WHERE { GRAPH ?graph { ?s ?p ?o } } GROUP BY ?graph ORDER BY DESC(?triples) $ docker run -it --rm -v $( pwd ) :/data -v $( pwd ) /cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.1 -c my-deployment query execute ./list-graphs.sparql graph,triples http://schema.org/,8809 https://vocab.eccenca.com/shacl/,1752 [...]","title":"Volumes"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/","tags":["cmemc","Workflow"],"text":"Workflow execution and orchestration \u00a4 Introduction \u00a4 In some cases, you need to automate a complete graph of integration workflows, which depend on each other and can sometimes run in parallel or after each other. Although cmemc is not a workflow orchestration tool, you can easily use it do some basic workflow orchestration. This page describes how you can execute and orchestrate workflows together. For simplicity all given examples do not select a specific connection ( --connection your-cmem ). We simply assume that you selected your instance via an environment variable ( export CMEMC_CONNECTION=your-cmem ). Simple Execution \u00a4 cmemc allows for execution of workflows with the workflow execute command. To start a workflow, simply use this command: workflow execute command $ cmemc workflow execute cmem:my-workflow cmem:my-workflow ... Started Workflow identifier can be extended with command-line completion on the command line but you can also get a list of workflows with the workflow list command: workflow list command $ cmemc workflow list cmem:my-workflow cmem:second-workflow The default working mode of the workflow execute command starts a workflow without waiting for a response. In order to wait until the workflow is finished you need to use the --wait option: workflow execute command with wait option $ cmemc workflow execute cmem:my-workflow --wait cmem:my-workflow ... Started ... Finished (Finished in 32.931s, just now) For a reference on the workflow execute command, have a look at the Command Reference or look at the command specific help ( cmemc workflow execute --help ). Retrieve Status Information \u00a4 At any time, you can retrieve status information of a workflow with the workflow status command: workflow status command $ cmemc workflow status cmem:my-workflow cmem:my-workflow ... Finished (Finished in 32.931s, 4 minutes ago) In addition to that, you can retrieve raw JSON data about a workflow, which can used for post-processing: workflow status command with JSON output $ cmemc -workflow status cmem:my-workflow --raw { \"activity\": \"ExecuteLocalWorkflow\", \"runtime\": 32931, \"project\": \"cmem\", \"failed\": false, \"message\": \"Finished in 32.931s\", \"task\": \"my-workflow\", \"isRunning\": false, \"statusName\": \"Finished\", \"progress\": 100, \"cancelled\": false, \"startTime\": 1593679211989, \"exceptionMessage\": null, \"lastUpdateTime\": 1593679244920 } For a reference on the workflow status command, have a look at the Command Reference or look at the command specific help ( cmemc workflow status --help ). Serial Execution \u00a4 The workflow execute command is able to start multiple workflows in a chain, waiting for each of the workflow and exit if there is an error with one of the workflows. To do this, use the --wait option and simply add more than one workflow identifier as parameter to the the command: workflow execute command $ cmemc workflow execute cmem:my-workflow cmem:second-workflow --wait cmem:my-workflow ... Started ... Finished (Finished in 30.984s, just now) cmem:second-workflow ... Started ... Finished (Finished in 50.579s, just now) Warning Starting these workflows in this way means that cmemc exits with an error code 1 in the moment the first workflow throws an error. All later workflows will not be executed. Parallel Execution \u00a4 Sometimes you want to execute workflows in parallel, because they do not depend on each other and it can fasten up the overall runtime. To do this, there is a little bit of extra scripting needed at the moment. The main idea is, to start the parallel workflows without waiting and then poll the status information until they are not running anymore. Here is an example script which does exactly this. cmemc-parallel-workflows.sh #!/usr/bin/env bash # @(#) Example: execute two workflows in parallel and wait for the results (exit 1 on failure) # Use the unofficial bash strict mode: http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail ; export FS = $'\\n\\t' # setup the used instance export CMEMC_CONNECTION = your-cmem # check SSL config if [ \" $( cmemc config get SSL_VERIFY 2 > & 1 ) \" == True ] then NUM = 0 else NUM = 1 fi # start the given set of workflows WORKFLOW_IDS = \"cmem:my-workflow cmem:second-workflow\" cmemc workflow execute $WORKFLOW_IDS # loop until they are not running anymore RUNNING = -1 until [ $RUNNING == $NUM ] do # wait 5 seconds - polling time sleep 5 # use the the filter option to show only running workflows RUNNING = $( cmemc workflow status $WORKFLOW_IDS --filter running 2 > & 1 | wc -l ) if [ $RUNNING ! = $NUM ] ; then echo \"We still have $RUNNING running workflows ...\" fi done # look for failed workflows FAILED = $( cmemc workflow status $WORKFLOW_IDS --filter failed 2 > & 1 | wc -l ) if [ $FAILED ! = $NUM ] ; then echo \"Some workflows failed :-(\" exit 1 else echo \"All workflows finished successfully :-)\" exit 0 fi","title":"Workflow execution and orchestration"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#workflow-execution-and-orchestration","text":"","title":"Workflow execution and orchestration"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#introduction","text":"In some cases, you need to automate a complete graph of integration workflows, which depend on each other and can sometimes run in parallel or after each other. Although cmemc is not a workflow orchestration tool, you can easily use it do some basic workflow orchestration. This page describes how you can execute and orchestrate workflows together. For simplicity all given examples do not select a specific connection ( --connection your-cmem ). We simply assume that you selected your instance via an environment variable ( export CMEMC_CONNECTION=your-cmem ).","title":"Introduction"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#simple-execution","text":"cmemc allows for execution of workflows with the workflow execute command. To start a workflow, simply use this command: workflow execute command $ cmemc workflow execute cmem:my-workflow cmem:my-workflow ... Started Workflow identifier can be extended with command-line completion on the command line but you can also get a list of workflows with the workflow list command: workflow list command $ cmemc workflow list cmem:my-workflow cmem:second-workflow The default working mode of the workflow execute command starts a workflow without waiting for a response. In order to wait until the workflow is finished you need to use the --wait option: workflow execute command with wait option $ cmemc workflow execute cmem:my-workflow --wait cmem:my-workflow ... Started ... Finished (Finished in 32.931s, just now) For a reference on the workflow execute command, have a look at the Command Reference or look at the command specific help ( cmemc workflow execute --help ).","title":"Simple Execution"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#retrieve-status-information","text":"At any time, you can retrieve status information of a workflow with the workflow status command: workflow status command $ cmemc workflow status cmem:my-workflow cmem:my-workflow ... Finished (Finished in 32.931s, 4 minutes ago) In addition to that, you can retrieve raw JSON data about a workflow, which can used for post-processing: workflow status command with JSON output $ cmemc -workflow status cmem:my-workflow --raw { \"activity\": \"ExecuteLocalWorkflow\", \"runtime\": 32931, \"project\": \"cmem\", \"failed\": false, \"message\": \"Finished in 32.931s\", \"task\": \"my-workflow\", \"isRunning\": false, \"statusName\": \"Finished\", \"progress\": 100, \"cancelled\": false, \"startTime\": 1593679211989, \"exceptionMessage\": null, \"lastUpdateTime\": 1593679244920 } For a reference on the workflow status command, have a look at the Command Reference or look at the command specific help ( cmemc workflow status --help ).","title":"Retrieve Status Information"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#serial-execution","text":"The workflow execute command is able to start multiple workflows in a chain, waiting for each of the workflow and exit if there is an error with one of the workflows. To do this, use the --wait option and simply add more than one workflow identifier as parameter to the the command: workflow execute command $ cmemc workflow execute cmem:my-workflow cmem:second-workflow --wait cmem:my-workflow ... Started ... Finished (Finished in 30.984s, just now) cmem:second-workflow ... Started ... Finished (Finished in 50.579s, just now) Warning Starting these workflows in this way means that cmemc exits with an error code 1 in the moment the first workflow throws an error. All later workflows will not be executed.","title":"Serial Execution"},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#parallel-execution","text":"Sometimes you want to execute workflows in parallel, because they do not depend on each other and it can fasten up the overall runtime. To do this, there is a little bit of extra scripting needed at the moment. The main idea is, to start the parallel workflows without waiting and then poll the status information until they are not running anymore. Here is an example script which does exactly this. cmemc-parallel-workflows.sh #!/usr/bin/env bash # @(#) Example: execute two workflows in parallel and wait for the results (exit 1 on failure) # Use the unofficial bash strict mode: http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail ; export FS = $'\\n\\t' # setup the used instance export CMEMC_CONNECTION = your-cmem # check SSL config if [ \" $( cmemc config get SSL_VERIFY 2 > & 1 ) \" == True ] then NUM = 0 else NUM = 1 fi # start the given set of workflows WORKFLOW_IDS = \"cmem:my-workflow cmem:second-workflow\" cmemc workflow execute $WORKFLOW_IDS # loop until they are not running anymore RUNNING = -1 until [ $RUNNING == $NUM ] do # wait 5 seconds - polling time sleep 5 # use the the filter option to show only running workflows RUNNING = $( cmemc workflow status $WORKFLOW_IDS --filter running 2 > & 1 | wc -l ) if [ $RUNNING ! = $NUM ] ; then echo \"We still have $RUNNING running workflows ...\" fi done # look for failed workflows FAILED = $( cmemc workflow status $WORKFLOW_IDS --filter failed 2 > & 1 | wc -l ) if [ $FAILED ! = $NUM ] ; then echo \"Some workflows failed :-(\" exit 1 else echo \"All workflows finished successfully :-)\" exit 0 fi","title":"Parallel Execution"},{"location":"automate/scheduling-workflows/","text":"Scheduling Workflows \u00a4 Introduction \u00a4 For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator. Please note that, in case you want to schedule workflows externally, cmemc can be used for that . Create a scheduler \u00a4 Navigate to Build \u2192 Projects section in the workspace and Click Create . Select the Item type Scheduler . Click Add - then the Create new item of type Scheduler dialog box appears. Set the properties of the Scheduler: Select the target project. Define the label of your scheduler Specify the workflow (task) to be executed. Define the interval for the scheduler to be executed again. Example: PT15MD (Every 15 minutes) Define the start time for the scheduler to be executed for the first time. Click Enable to enable the scheduler. Click Stop on error to stop the scheduler on after a failed run. Once you are ready with the configurations, click Create button. Now, the scheduler will be executed with the given settings. Modify, enable or disable a scheduler \u00a4 Navigate to Build \u2192 Projects section in the workspace. Search the scheduler you want to modify. Select it or click on Open Details Page in the context menu. Click on the Configure button in the Configuration section. Change the values according to your needs. Time Interval Specification \u00a4 The scheduler interval is represented an ISO-8601 time duration string . The following values are possible: P is the duration designator (referred to as \u201cperiod\u201d), and is always placed at the beginning of the duration. Y for defining the number of years. M for defining the number of months. W for defining the number of weeks. D for defining the number of days. T is the time designator that precedes the time components. H for defining the number of hours. M for defining the number of minutes. S for defining the number of seconds. A duration with all values being used: P2Y6M4DT12H30M10S (defines a a period of 2 years, 6 months, 4 days, 12 hours, 30 minutes and 10 seconds). More common examples: PT30M - every half hour PT1H - every hour P1D - every day","title":"Scheduling Workflows"},{"location":"automate/scheduling-workflows/#scheduling-workflows","text":"","title":"Scheduling Workflows"},{"location":"automate/scheduling-workflows/#introduction","text":"For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator. Please note that, in case you want to schedule workflows externally, cmemc can be used for that .","title":"Introduction"},{"location":"automate/scheduling-workflows/#create-a-scheduler","text":"Navigate to Build \u2192 Projects section in the workspace and Click Create . Select the Item type Scheduler . Click Add - then the Create new item of type Scheduler dialog box appears. Set the properties of the Scheduler: Select the target project. Define the label of your scheduler Specify the workflow (task) to be executed. Define the interval for the scheduler to be executed again. Example: PT15MD (Every 15 minutes) Define the start time for the scheduler to be executed for the first time. Click Enable to enable the scheduler. Click Stop on error to stop the scheduler on after a failed run. Once you are ready with the configurations, click Create button. Now, the scheduler will be executed with the given settings.","title":"Create a scheduler"},{"location":"automate/scheduling-workflows/#modify-enable-or-disable-a-scheduler","text":"Navigate to Build \u2192 Projects section in the workspace. Search the scheduler you want to modify. Select it or click on Open Details Page in the context menu. Click on the Configure button in the Configuration section. Change the values according to your needs.","title":"Modify, enable or disable a scheduler"},{"location":"automate/scheduling-workflows/#time-interval-specification","text":"The scheduler interval is represented an ISO-8601 time duration string . The following values are possible: P is the duration designator (referred to as \u201cperiod\u201d), and is always placed at the beginning of the duration. Y for defining the number of years. M for defining the number of months. W for defining the number of weeks. D for defining the number of days. T is the time designator that precedes the time components. H for defining the number of hours. M for defining the number of minutes. S for defining the number of seconds. A duration with all values being used: P2Y6M4DT12H30M10S (defines a a period of 2 years, 6 months, 4 days, 12 hours, 30 minutes and 10 seconds). More common examples: PT30M - every half hour PT1H - every hour P1D - every day","title":"Time Interval Specification"},{"location":"build/","text":"\u2605 Build \u00a4 Lift your data by integrating multiple datasets into a Knowledge Graph. The Build stage is used to turn your legacy data points from existing datasets into an Enterprise Knowledge Graph structure. The subsections introduce the features of Corporate Memory that support this stage and provide guidance through your first lifting activities. Cool IRIs \u2014 URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company. Define Prefixes / Namespaces \u2014 Define Prefixes / Namespaces \u2014 Namespace declarations allow for abbreviation of IRIs by using a prefixed name instead of an IRI, in particular when writing SPARQL queries or Turtle. Extracting data from a Web API \u2014 This tutorial shows how you can build a Knowledge Graph based on input data from a Web API. Introduction to the user interface \u2014 This page provides a short introduction to the BUILD / Data Integration workspace incl. projects and different tasks. Lift data from JSON and XML sources \u2014 This tutorial shows how you can build a Knowledge Graph based on input data from hierarchical sources like a JavaScript Object Notation file (.json) or an Extensible Markup Language file (.xml).","title":"Build"},{"location":"build/#build","text":"Lift your data by integrating multiple datasets into a Knowledge Graph. The Build stage is used to turn your legacy data points from existing datasets into an Enterprise Knowledge Graph structure. The subsections introduce the features of Corporate Memory that support this stage and provide guidance through your first lifting activities. Cool IRIs \u2014 URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company. Define Prefixes / Namespaces \u2014 Define Prefixes / Namespaces \u2014 Namespace declarations allow for abbreviation of IRIs by using a prefixed name instead of an IRI, in particular when writing SPARQL queries or Turtle. Extracting data from a Web API \u2014 This tutorial shows how you can build a Knowledge Graph based on input data from a Web API. Introduction to the user interface \u2014 This page provides a short introduction to the BUILD / Data Integration workspace incl. projects and different tasks. Lift data from JSON and XML sources \u2014 This tutorial shows how you can build a Knowledge Graph based on input data from hierarchical sources like a JavaScript Object Notation file (.json) or an Extensible Markup Language file (.xml).","title":"\u2605 Build"},{"location":"build/cool-iris/","text":"Cool IRIs \u00a4 Introduction \u00a4 URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company. RFC 3986 defines a generic syntax for URIs: <scheme>:<scheme-specific-part> Scheme-specific part often structured: <authority>/<path>?<query> URIs are limited to ASCII characters. IRIs (Internationalized Resource Identifiers) allow Unicode ( RFC 3987 ). The following list of example IRIs demonstrate the broad scope of this concept: ftp://ftp.is.co.za/rfc/rfc1808.txt http://www.ietf.org/rfc/rfc2396.txt ldap://[2001:db8::7]/c=GB?objectClass?one mailto:John.Doe@example.com news:comp.infosystems.www.servers.unix tel:+1-816-555-1212 telnet://192.0.2.16:80/ urn:oasis:names:specification:docbook:dtd:xml:4.1.2 Best practices in Corporate Memory \u00a4 A good IRI is unique, stable, simple and manageable. Define a useful IRI-Scheme that can be used for resources. Define a Base URI which is the common authority for all resources in your graph. Example: https://data.company.org/ Define subspaces where necessary, e.g. for each subproject or domain. Provide a prefix for each subspace. Examples: https://data.company.org/hardware/ for hardware artifacts https://data.company.org/software/ for software artifacts PREFIX cohw: <https://data.company.org/hardware/> PREFIX cosw: <https://data.company.org/software/> Based on these build consistent schemes that define how your IRIs have to be build. Examples: https://data.company.org/hardware/<ProductClass>/<Serialnumber> to identify an individual product https://data.company.org/hardware/<ProductClass>/<Modelnumber> to identify a product model Warning Do not put a trailing slash on the end of a resource IRIs. As such can not be used with prefix definitions in Turtle or SPARQL, which makes them more difficult to use. More information \u00a4 Spanish Government, URIs for Open Data resources European Union, URIs for Legal Resources UK, \u201cDesigning URI sets for the UK public sector\u201d Other Resources https://www.w3.org/TR/cooluris/ https://www.w3.org/Provider/Style/URI.html https://www.w3.org/wiki/GoodURIs https://www.w3.org/TR/dwbp/ https://www.w3.org/TR/ld-bp/","title":"Cool IRIs"},{"location":"build/cool-iris/#cool-iris","text":"","title":"Cool IRIs"},{"location":"build/cool-iris/#introduction","text":"URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company. RFC 3986 defines a generic syntax for URIs: <scheme>:<scheme-specific-part> Scheme-specific part often structured: <authority>/<path>?<query> URIs are limited to ASCII characters. IRIs (Internationalized Resource Identifiers) allow Unicode ( RFC 3987 ). The following list of example IRIs demonstrate the broad scope of this concept: ftp://ftp.is.co.za/rfc/rfc1808.txt http://www.ietf.org/rfc/rfc2396.txt ldap://[2001:db8::7]/c=GB?objectClass?one mailto:John.Doe@example.com news:comp.infosystems.www.servers.unix tel:+1-816-555-1212 telnet://192.0.2.16:80/ urn:oasis:names:specification:docbook:dtd:xml:4.1.2","title":"Introduction"},{"location":"build/cool-iris/#best-practices-in-corporate-memory","text":"A good IRI is unique, stable, simple and manageable. Define a useful IRI-Scheme that can be used for resources. Define a Base URI which is the common authority for all resources in your graph. Example: https://data.company.org/ Define subspaces where necessary, e.g. for each subproject or domain. Provide a prefix for each subspace. Examples: https://data.company.org/hardware/ for hardware artifacts https://data.company.org/software/ for software artifacts PREFIX cohw: <https://data.company.org/hardware/> PREFIX cosw: <https://data.company.org/software/> Based on these build consistent schemes that define how your IRIs have to be build. Examples: https://data.company.org/hardware/<ProductClass>/<Serialnumber> to identify an individual product https://data.company.org/hardware/<ProductClass>/<Modelnumber> to identify a product model Warning Do not put a trailing slash on the end of a resource IRIs. As such can not be used with prefix definitions in Turtle or SPARQL, which makes them more difficult to use.","title":"Best practices in Corporate Memory"},{"location":"build/cool-iris/#more-information","text":"Spanish Government, URIs for Open Data resources European Union, URIs for Legal Resources UK, \u201cDesigning URI sets for the UK public sector\u201d Other Resources https://www.w3.org/TR/cooluris/ https://www.w3.org/Provider/Style/URI.html https://www.w3.org/wiki/GoodURIs https://www.w3.org/TR/dwbp/ https://www.w3.org/TR/ld-bp/","title":"More information"},{"location":"build/define-prefixes-namespaces/","text":"Define Prefixes / Namespaces \u00a4 Introduction \u00a4 Namespace declarations allow for abbreviation of IRIs by using a prefixed name instead of an IRI, in particular when writing SPARQL queries or Turtle.A namespace declaration consists of a prefix name and a namespace IRI . For example, after defining a namespace with the values prefix name = cohw , and the namespace IRI = https://data.company.org/hardware/ you can use the term cohw:test as an abbreviation for the full IRI https://data.company.org/hardware/test . This is in particular useful when you have to write source code in Turtle and SPARQL. Using the Vocabulary Catalog \u00a4 After installing a vocabulary from the Vocabulary Catalog , the vocabulary namespace declaration is automatically added to all integration projects. In order to get the prefix name and the namespace IRI from the vocabulary graph, the following terms from the VANN vocabulary need to be used on the Ontology resource. vann:preferredNamespacePrefix - to specify the prefix name vann:preferredNamespaceUri - to specify the namespace IRI In the Explore area, an Ontology with a correct namespace declaration looks like this. Using the Project Configuration \u00a4 In addition to the used vocabulary namespace declarations, you may want to add well-known namespaces for organizing to Knowledge Graphs. Such organization use cases include: Namespaces per class / resource type: prefix name = persons , namespace IRI = https://example.org/data/persons/ Namespaces per data owner or origin: prefix name = sales , namespace IRI = https://example.org/data/sales/ Prefixes in Data Integration are defined on a project base. When creating a new project, a list of well-know prefixes is already declared. After selecting a project from the search results the prefix management is available in the project configuration in the lower right area: By using the Edit Prefix Settings button in this Configuration are, you will see the Manage Prefixes dialog: In this dialog, you are able to Delete a namespace declaration \u2192 Delete Prefix Add a new namespace declaration \u2192 Add Validating Namespace Declarations \u00a4 After adding namespace declarations to a project, you are able to the abbreviated IRIs in user interface, such as the mapping editor:","title":"Define Prefixes / Namespaces"},{"location":"build/define-prefixes-namespaces/#define-prefixes-namespaces","text":"","title":"Define Prefixes / Namespaces"},{"location":"build/define-prefixes-namespaces/#introduction","text":"Namespace declarations allow for abbreviation of IRIs by using a prefixed name instead of an IRI, in particular when writing SPARQL queries or Turtle.A namespace declaration consists of a prefix name and a namespace IRI . For example, after defining a namespace with the values prefix name = cohw , and the namespace IRI = https://data.company.org/hardware/ you can use the term cohw:test as an abbreviation for the full IRI https://data.company.org/hardware/test . This is in particular useful when you have to write source code in Turtle and SPARQL.","title":"Introduction"},{"location":"build/define-prefixes-namespaces/#using-the-vocabulary-catalog","text":"After installing a vocabulary from the Vocabulary Catalog , the vocabulary namespace declaration is automatically added to all integration projects. In order to get the prefix name and the namespace IRI from the vocabulary graph, the following terms from the VANN vocabulary need to be used on the Ontology resource. vann:preferredNamespacePrefix - to specify the prefix name vann:preferredNamespaceUri - to specify the namespace IRI In the Explore area, an Ontology with a correct namespace declaration looks like this.","title":"Using the Vocabulary Catalog"},{"location":"build/define-prefixes-namespaces/#using-the-project-configuration","text":"In addition to the used vocabulary namespace declarations, you may want to add well-known namespaces for organizing to Knowledge Graphs. Such organization use cases include: Namespaces per class / resource type: prefix name = persons , namespace IRI = https://example.org/data/persons/ Namespaces per data owner or origin: prefix name = sales , namespace IRI = https://example.org/data/sales/ Prefixes in Data Integration are defined on a project base. When creating a new project, a list of well-know prefixes is already declared. After selecting a project from the search results the prefix management is available in the project configuration in the lower right area: By using the Edit Prefix Settings button in this Configuration are, you will see the Manage Prefixes dialog: In this dialog, you are able to Delete a namespace declaration \u2192 Delete Prefix Add a new namespace declaration \u2192 Add","title":"Using the Project Configuration"},{"location":"build/define-prefixes-namespaces/#validating-namespace-declarations","text":"After adding namespace declarations to a project, you are able to the abbreviated IRIs in user interface, such as the mapping editor:","title":"Validating Namespace Declarations"},{"location":"build/extracting-data-from-a-web-api/","text":"Extracting data from a Web API \u00a4 Introduction \u00a4 This tutorial shows how you can build a Knowledge Graph based on input data from a Web API.The tutorial is based on the GitHub API (v3) , which we will use to fetch repository data of a certain organization and create a Knowledge Graph from the response. Info The complete tutorial is available as a project file . You can import this project: by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-webapi.project.zip web-api In order to get familiar with the API, just fetch an example response with this command: curl https://api.github.com/orgs/vocol/repos The HTTP Get request retrieves all repositories of a GitHub organization named vocol. The JSON response includes the data for all repositories ( mobivoc , vocol , \u2026). You can also download the response file here: repos.json . [ { ... \"id\" : 22646219 , \"name\" : \"mobivoc\" , ... }, { ... \"id\" : 22646629 , \"name\" : \"vocol\" , ... }, { ... \"id\" : 30964669 , \"name\" : \"scor\" , ... }, ... ] 1 Register a Web API \u00a4 Press the Create button (top right) in the data integration workspace and select the type REST request . Define a Label, Description and the URL of the Web API. Example input: https://api.github.com/orgs/vocol/repos . 2 Create a JSON parser \u00a4 As we are only interested in the HTTP Message Body which holds the JSON repository data, we first have to parse the body from the entire HTTP response. Press the Create button (top right) in the data integration workspace and select the type Parse JSON. Define a Label , a Description , and the Input path. Every other field can keep the default settings. The default input path is always: <http://silkframework.org/vocab/taskSpec/RestTaskResult/responseBody> 3 Create a JSON Dataset \u00a4 To create a JSON-to-RDF-mapping within Corporate Memory, we have to first register an example response from the API (repos.json). Based on the schema of the response, we can then define step-by-step the mappings, which are used to build the Knowledge Graph. Press the Create button (top right) in the data integration workspace and select the type JSON . Upload the JSON file repos.json (API response) as a Dataset into Corporate Memory. 4 Create a Knowledge Graph \u00a4 The Knowledge Graph will be used to integrate all data coming from one or more APIs. The Knowledge Graph receives RDF triples from the defined Transformations for each API. Press the Create button (top right) in the data integration workspace and select the type Knowledge Graph . Provide the Knowledge Graph with a Label and Description , as well as the following (example) Graph URI: http://ld.company.org/repository-data/ 5 Create a Transformation \u00a4 In order to transform the input data from the API, which is structured in our example in JSON, we have to define a mapping to create RDF triples which are then written into the Knowledge Graph. Press the Create button (top right) in the data integration workspace and select the type Transform . Provide the Transformation with a Label and Description , configure the Input Dataset (Repos.json) as well as the Output Dataset (Repository Knowledge Graph). In order to transform the input data from the API, which is structured in our example in JSON, we have to define a mapping to create RDF triples which are then written into the Knowledge Graph. Press the Mapping Editor button in the previously defined Transformation. In the following screenshots, we provide an example mapping for the data received by the GitHub API. For more complex mappings, we recommend the Tutorial Lift data from JSON and XML sources . 6 Create a Workflow \u00a4 To build a workflow which combines all the elements we previously built, we define now a workflow for (1) requesting the data from the GitHub API, (2) parsing the HTTP response we receive, (3) transforming the JSON data into RDF triples and finally (5) to write the RDF triples into the Knowledge Graph. Press the Create button (top right) in the data integration workspace and select the type Workflow . Provide the Transformation with a Label and a Description . Press the Workflow Editor button in the menu of the created workflow. Drag and drop the different items into the Workflow Editor and combine them with one another (see example screenshot). Save the workflow, and press the run symbol to execute the workflow. Validate the result by clicking on the Workflow Report tab and see the result of your execution. In this example, 15x repositories were found from the GitHub API request.","title":"Extracting data from a Web API"},{"location":"build/extracting-data-from-a-web-api/#extracting-data-from-a-web-api","text":"","title":"Extracting data from a Web API"},{"location":"build/extracting-data-from-a-web-api/#introduction","text":"This tutorial shows how you can build a Knowledge Graph based on input data from a Web API.The tutorial is based on the GitHub API (v3) , which we will use to fetch repository data of a certain organization and create a Knowledge Graph from the response. Info The complete tutorial is available as a project file . You can import this project: by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-webapi.project.zip web-api In order to get familiar with the API, just fetch an example response with this command: curl https://api.github.com/orgs/vocol/repos The HTTP Get request retrieves all repositories of a GitHub organization named vocol. The JSON response includes the data for all repositories ( mobivoc , vocol , \u2026). You can also download the response file here: repos.json . [ { ... \"id\" : 22646219 , \"name\" : \"mobivoc\" , ... }, { ... \"id\" : 22646629 , \"name\" : \"vocol\" , ... }, { ... \"id\" : 30964669 , \"name\" : \"scor\" , ... }, ... ]","title":"Introduction"},{"location":"build/extracting-data-from-a-web-api/#1-register-a-web-api","text":"Press the Create button (top right) in the data integration workspace and select the type REST request . Define a Label, Description and the URL of the Web API. Example input: https://api.github.com/orgs/vocol/repos .","title":"1 Register a Web API"},{"location":"build/extracting-data-from-a-web-api/#2-create-a-json-parser","text":"As we are only interested in the HTTP Message Body which holds the JSON repository data, we first have to parse the body from the entire HTTP response. Press the Create button (top right) in the data integration workspace and select the type Parse JSON. Define a Label , a Description , and the Input path. Every other field can keep the default settings. The default input path is always: <http://silkframework.org/vocab/taskSpec/RestTaskResult/responseBody>","title":"2 Create a JSON parser"},{"location":"build/extracting-data-from-a-web-api/#3-create-a-json-dataset","text":"To create a JSON-to-RDF-mapping within Corporate Memory, we have to first register an example response from the API (repos.json). Based on the schema of the response, we can then define step-by-step the mappings, which are used to build the Knowledge Graph. Press the Create button (top right) in the data integration workspace and select the type JSON . Upload the JSON file repos.json (API response) as a Dataset into Corporate Memory.","title":"3 Create a JSON Dataset"},{"location":"build/extracting-data-from-a-web-api/#4-create-a-knowledge-graph","text":"The Knowledge Graph will be used to integrate all data coming from one or more APIs. The Knowledge Graph receives RDF triples from the defined Transformations for each API. Press the Create button (top right) in the data integration workspace and select the type Knowledge Graph . Provide the Knowledge Graph with a Label and Description , as well as the following (example) Graph URI: http://ld.company.org/repository-data/","title":"4 Create a Knowledge Graph"},{"location":"build/extracting-data-from-a-web-api/#5-create-a-transformation","text":"In order to transform the input data from the API, which is structured in our example in JSON, we have to define a mapping to create RDF triples which are then written into the Knowledge Graph. Press the Create button (top right) in the data integration workspace and select the type Transform . Provide the Transformation with a Label and Description , configure the Input Dataset (Repos.json) as well as the Output Dataset (Repository Knowledge Graph). In order to transform the input data from the API, which is structured in our example in JSON, we have to define a mapping to create RDF triples which are then written into the Knowledge Graph. Press the Mapping Editor button in the previously defined Transformation. In the following screenshots, we provide an example mapping for the data received by the GitHub API. For more complex mappings, we recommend the Tutorial Lift data from JSON and XML sources .","title":"5 Create a Transformation"},{"location":"build/extracting-data-from-a-web-api/#6-create-a-workflow","text":"To build a workflow which combines all the elements we previously built, we define now a workflow for (1) requesting the data from the GitHub API, (2) parsing the HTTP response we receive, (3) transforming the JSON data into RDF triples and finally (5) to write the RDF triples into the Knowledge Graph. Press the Create button (top right) in the data integration workspace and select the type Workflow . Provide the Transformation with a Label and a Description . Press the Workflow Editor button in the menu of the created workflow. Drag and drop the different items into the Workflow Editor and combine them with one another (see example screenshot). Save the workflow, and press the run symbol to execute the workflow. Validate the result by clicking on the Workflow Report tab and see the result of your execution. In this example, 15x repositories were found from the GitHub API request.","title":"6 Create a Workflow"},{"location":"build/introduction-to-the-user-interface/","text":"Introduction to the user interface \u00a4 This page provides a short introduction to the BUILD / Data Integration workspace incl. projects and different tasks. Workbench \u00a4 The workbench is the main entry point to the BUILD interface and provides a list view of your work, structured in projects . On the left-hand side, a facet list provides an overview over different item types: Projects are the main structure and consists of datasets, workflows and different tasks (transformations, linking and other tasks) Datasets are registered data sources from files, endpoints or internally managed. Transform tasks take an input dataset and execute a (hierarchical) set of mapping and transformation rules and generate data for an output dataset. Linking tasks compare entities from two datasets according to (hierarchical) set of comparators and generate links between these datasets in an output (link) dataset. Workflows can combine all items in a project in order to create a structured work plan which can be executed on demand, controlled remotely (e.g. via cmemc ) or executed from a scheduler . The interface can be used to browse from a global level to a project level as well as to search for items globally and project wide. On the top right side in the header, there is a big blue Create (+) button which allows for creation of all possible data integration items: Projects \u00a4 Selecting a project from the search results opens the project details view (see above). The project is the main metaphor to structure your work. In the workspace, you can review your projects, create new or delete old ones, and import or export them. Each project is self-contained and holds all relevant parts, such as the following: the raw data resources (local or remote data files) that form the starting point for your work the data sets that provide uniform access to different kinds of data resources transformation tasks to transform data sets (e.g., from one format or schema to another) linking tasks to integrate different data sets workflows to orchestrate the different tasks definitions of URI prefixes used in the project Resources, data sets, tasks and other parts of a project can be added and configured from within the workspace. By clicking the Show Details button on any item in a project, you can \u2018look inside\u2019 it and define the actual transformation or linking rules, or assemble the actual workflow. Datasets \u00a4 Selecting a dataset in a project or from the search results opens the dataset details view. Here you have the following options: Change the meta data of the dataset ( Summary - label and description, top left main area), Change the Configuration parameters of the dataset (lower right side area), Browse to other Related Items , such as transform, linking or workflow tasks which use this dataset (top right side area), Get a Data Preview as well as generate and view profiling information (lower left main area). A dataset represents an abstraction over raw data. In order to work with data, you have to make it available in the form of a dataset. A dataset provides a source or destination for the different kinds of tasks. I.e., it may be used to read entities for transformation or linking, and it can be used to write transformed entities and generated links. There is a range of different dataset types for different kinds of source data. Some most important ones are: Knowledge Graph - Read RDF from or write RDF to a Knowledge Graph embedded in Corporate Memory. CSV - Read from or write to an CSV file. XML - Read from or write to an XML file. JSON - Read from or write to a JSON file. JDBC endpoint - Connect to an existing JDBC endpoint. Variable dataset - Dataset that acts as a placeholder in workflows and is replaced at request time. Excel - Read from or write to an Excel workbook in Open XML format (XLSX). Multi CSV ZIP - Reads from or writes to multiple CSV files from/to a single ZIP file. Other options include: Internal dataset - Dataset for storing entities between workflow steps. RDF - Dataset which retrieves and writes all entities from/to an RDF file. The dataset is loaded in-memory and thus the size is restricted by the available memory. Large datasets should be loaded into an external RDF store and retrieved using the SPARQL dataset instead. SPARQL endpoint - Connect to an existing SPARQL endpoint. In-memory dataset - A Dataset that holds all data in-memory. Avro - Read from or write to an Apache Avro file. ORC - Read from or write to an Apache ORC file. Parquet - Read from or write to an Apache Parquet file. Hive database - Read from or write to an embedded Apache Hive endpoint. SQL endpoint - Provides a JDBC endpoint that exposes workflow or transformation results as tables, which can be queried using SQL. Neo4j - Connect to an existing Neo4j property graph database system. Transform tasks \u00a4 The purpose of transform tasks is to transform data sets (e.g., from one format or schema to another). More specifically Transform Task create Knowledge Graphs out of raw data sources. Selecting a transform task in a project or from the search results opens the transform task details view. Here you have the following options: Change the meta data of the transformation task ( Summary - label and description, top left main area) Change the Configuration parameters of the transformation task (lower right side area), Browse to other Related Items , such as used datasets or workflows using this task (top right side area), Open the Mapping Editor in order to change the transformation rules, Evaluate the Transformation to check for issues with the rules, Execute the Transformation rules in order to create new data, Clone the task, creating a new transform task with the same rules. Linking tasks \u00a4 The purpose of linking tasks is to integrate two datasets (source and target) by generating a linkset that contains links from the source to the target dataset. Selecting a linking task in a project or from the search results opens the transform task details view. Here you have the following options: Change the meta data of the linking task ( Summary - label and description, top left main area), Change the Configuration parameters of the linking task (lower right side area), Browse to other Related Items , such as used datasets or workflows using this task (top right side area), Open the Linking Editor in order to change the comparison rules, Evaluate the Linking to check for issues with the rules, Execute the Linking task in order to create a linkset, Clone the task, creating a new linking task with the same comparisons. As in the case of transform tasks, the output linkset of a linking task can either be written directly to an output data set, or provide the input to another task if integrated into a workflow. Workflows \u00a4 For projects that go beyond one or two simple transform or linking tasks, you can create complex workflows. In a workflow, you can orchestrate datasets, transform and linking tasks in terms of inputs and outputs to each other, and in this way perform complex tasks. Selecting a workflow in a project or from the search results opens the workflow details view. Here you have the following options: Change the meta data of the linking task ( Summary - label and description, top left main area) Browse to Related Items , such as used datasets and tasks (top right side area), Open the Worflow Editor in order to change the workflow (here you can as well start the workflow), Clone the workflow, creating a new workflow with the same task.","title":"Introduction to the user interface"},{"location":"build/introduction-to-the-user-interface/#introduction-to-the-user-interface","text":"This page provides a short introduction to the BUILD / Data Integration workspace incl. projects and different tasks.","title":"Introduction to the user interface"},{"location":"build/introduction-to-the-user-interface/#workbench","text":"The workbench is the main entry point to the BUILD interface and provides a list view of your work, structured in projects . On the left-hand side, a facet list provides an overview over different item types: Projects are the main structure and consists of datasets, workflows and different tasks (transformations, linking and other tasks) Datasets are registered data sources from files, endpoints or internally managed. Transform tasks take an input dataset and execute a (hierarchical) set of mapping and transformation rules and generate data for an output dataset. Linking tasks compare entities from two datasets according to (hierarchical) set of comparators and generate links between these datasets in an output (link) dataset. Workflows can combine all items in a project in order to create a structured work plan which can be executed on demand, controlled remotely (e.g. via cmemc ) or executed from a scheduler . The interface can be used to browse from a global level to a project level as well as to search for items globally and project wide. On the top right side in the header, there is a big blue Create (+) button which allows for creation of all possible data integration items:","title":"Workbench"},{"location":"build/introduction-to-the-user-interface/#projects","text":"Selecting a project from the search results opens the project details view (see above). The project is the main metaphor to structure your work. In the workspace, you can review your projects, create new or delete old ones, and import or export them. Each project is self-contained and holds all relevant parts, such as the following: the raw data resources (local or remote data files) that form the starting point for your work the data sets that provide uniform access to different kinds of data resources transformation tasks to transform data sets (e.g., from one format or schema to another) linking tasks to integrate different data sets workflows to orchestrate the different tasks definitions of URI prefixes used in the project Resources, data sets, tasks and other parts of a project can be added and configured from within the workspace. By clicking the Show Details button on any item in a project, you can \u2018look inside\u2019 it and define the actual transformation or linking rules, or assemble the actual workflow.","title":"Projects"},{"location":"build/introduction-to-the-user-interface/#datasets","text":"Selecting a dataset in a project or from the search results opens the dataset details view. Here you have the following options: Change the meta data of the dataset ( Summary - label and description, top left main area), Change the Configuration parameters of the dataset (lower right side area), Browse to other Related Items , such as transform, linking or workflow tasks which use this dataset (top right side area), Get a Data Preview as well as generate and view profiling information (lower left main area). A dataset represents an abstraction over raw data. In order to work with data, you have to make it available in the form of a dataset. A dataset provides a source or destination for the different kinds of tasks. I.e., it may be used to read entities for transformation or linking, and it can be used to write transformed entities and generated links. There is a range of different dataset types for different kinds of source data. Some most important ones are: Knowledge Graph - Read RDF from or write RDF to a Knowledge Graph embedded in Corporate Memory. CSV - Read from or write to an CSV file. XML - Read from or write to an XML file. JSON - Read from or write to a JSON file. JDBC endpoint - Connect to an existing JDBC endpoint. Variable dataset - Dataset that acts as a placeholder in workflows and is replaced at request time. Excel - Read from or write to an Excel workbook in Open XML format (XLSX). Multi CSV ZIP - Reads from or writes to multiple CSV files from/to a single ZIP file. Other options include: Internal dataset - Dataset for storing entities between workflow steps. RDF - Dataset which retrieves and writes all entities from/to an RDF file. The dataset is loaded in-memory and thus the size is restricted by the available memory. Large datasets should be loaded into an external RDF store and retrieved using the SPARQL dataset instead. SPARQL endpoint - Connect to an existing SPARQL endpoint. In-memory dataset - A Dataset that holds all data in-memory. Avro - Read from or write to an Apache Avro file. ORC - Read from or write to an Apache ORC file. Parquet - Read from or write to an Apache Parquet file. Hive database - Read from or write to an embedded Apache Hive endpoint. SQL endpoint - Provides a JDBC endpoint that exposes workflow or transformation results as tables, which can be queried using SQL. Neo4j - Connect to an existing Neo4j property graph database system.","title":"Datasets"},{"location":"build/introduction-to-the-user-interface/#transform-tasks","text":"The purpose of transform tasks is to transform data sets (e.g., from one format or schema to another). More specifically Transform Task create Knowledge Graphs out of raw data sources. Selecting a transform task in a project or from the search results opens the transform task details view. Here you have the following options: Change the meta data of the transformation task ( Summary - label and description, top left main area) Change the Configuration parameters of the transformation task (lower right side area), Browse to other Related Items , such as used datasets or workflows using this task (top right side area), Open the Mapping Editor in order to change the transformation rules, Evaluate the Transformation to check for issues with the rules, Execute the Transformation rules in order to create new data, Clone the task, creating a new transform task with the same rules.","title":"Transform tasks"},{"location":"build/introduction-to-the-user-interface/#linking-tasks","text":"The purpose of linking tasks is to integrate two datasets (source and target) by generating a linkset that contains links from the source to the target dataset. Selecting a linking task in a project or from the search results opens the transform task details view. Here you have the following options: Change the meta data of the linking task ( Summary - label and description, top left main area), Change the Configuration parameters of the linking task (lower right side area), Browse to other Related Items , such as used datasets or workflows using this task (top right side area), Open the Linking Editor in order to change the comparison rules, Evaluate the Linking to check for issues with the rules, Execute the Linking task in order to create a linkset, Clone the task, creating a new linking task with the same comparisons. As in the case of transform tasks, the output linkset of a linking task can either be written directly to an output data set, or provide the input to another task if integrated into a workflow.","title":"Linking tasks"},{"location":"build/introduction-to-the-user-interface/#workflows","text":"For projects that go beyond one or two simple transform or linking tasks, you can create complex workflows. In a workflow, you can orchestrate datasets, transform and linking tasks in terms of inputs and outputs to each other, and in this way perform complex tasks. Selecting a workflow in a project or from the search results opens the workflow details view. Here you have the following options: Change the meta data of the linking task ( Summary - label and description, top left main area) Browse to Related Items , such as used datasets and tasks (top right side area), Open the Worflow Editor in order to change the workflow (here you can as well start the workflow), Clone the workflow, creating a new workflow with the same task.","title":"Workflows"},{"location":"build/lift-data-from-json-and-xml-sources/","tags":["AdvancedTutorial"],"text":"Lift data from JSON and XML source \u00a4 Introduction \u00a4 This tutorial shows how you can build a Knowledge Graph based on input data from hierarchical sources like a JavaScript Object Notation file (.json) or an Extensible Markup Language file (.xml). Info The complete tutorial is available as a project file (XML) and a project file (JSON) . You can import these projects: by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-xml.project.zip xml-transformation cmemc -c my-cmem project import tutorial-json.project.zip json-transformation The documentation consists of the following steps, which are described in detail below: The following material is used in this tutorial: Sample vocabulary which describes the data in the JSON and XML files: products_vocabulary.nt Sample JSON file: services.json [ { \"Price\" : \"748,40 EUR\" , \"ProductManager\" : \"Lambert.Faust@company.org\" , \"Products\" : \"O491-3823912, I965-1821441, Z655-3173353, ...\" , \"ServiceID\" : \"Y704-9764759\" , \"ServiceName\" : \"Product Analysis\" }, { \"Price\" : \"1082,00 EUR\" , \"ProductManager\" : \"Corinna.Ludwig@company.org\" , \"Products\" : \"Z249-1364492, L557-1467804, C721-7900144, ...\" , \"ServiceID\" : \"I241-8776317\" , \"ServiceName\" : \"Component Confabulation\" }, ... ] Sample XML file: orgmap.xml <orgmap> <dept id= \"73191\" name= \"Engineering\" > <manager> <email> Thomas.Mueller@company.org </email> <name> Thomas Mueller </name> <address> Karl-Liebknecht-Stra\u00dfe 885, 82003 Tettnang </address> <phone> +49-8200-38218301 </phone> </manager> <employees> <employee> <email> Corinna.Ludwig@company.org </email> <name> Corinna Ludwig </name> <address> Ringstra\u00dfe 276 </address> <phone> +49-1743-24836762 </phone> <productExpert> Memristor, Gauge, Encoder </productExpert> </employee> <employee> <email> Karen.Brant@company.org </email> <name> Karen Brant </name> <address> Friedrichstra\u00dfe 664, 30805 Willich </address> <phone> (00530) 5040048 </phone> <productExpert> Inductor </productExpert> </employee> ... </employees> <products> <product id= \"Z249-1364492\" /> <product id= \"O184-6903943\" /> <product id= \"V404-9975399\" /> <product id= \"F344-7012314\" /> <product id= \"N463-8050264\" /> <product id= \"M605-5951566\" /> <product id= \"N733-1946687\" /> </products> <services> <service id= \"I241-8776317\" /> <service id= \"D215-3449390\" /> </services> </dept> <dept id= \"22183\" name= \"Product Management\" > ... </dept> ... </orgmap> 1 Register the vocabulary \u00a4 The vocabulary contains the classes and properties needed to map the source data into entities in the Knowledge Graph. In Corporate Memory, click Vocabularies in the under EXPLORE in the navigation on the left side of the page. Click Register new vocabulary on the top right of the Vocabulary catalog page in Corporate Memory. Define a Name , a Graph URI and a Description of the vocabulary. In this example we will use: Name: Product Vocabulary Graph URI: http://ld.company.org/prod-vocab/ Description: Example vocabulary modeled to describe relations between products and services. Click REGISTER . 2 Upload the data file \u00a4 To add the data files, click Projects under BUILD in the navigation on the left side of the page. Follow the steps below for adding JSON and XML datasets. JSON XML Click Create at the top of the page. In Create new item window, select JSON and click Add. Define a Label for the dataset and upload the services.json file. You can leave all the other fields at default values. Click Create . Press the Create button and select XML Define a Label for the dataset and upload the orgmap.xml example file. You can leave all the other fields at default values. Click Create . 3 Create a Knowledge Graph \u00a4 Click Create at the top of the page.\u202f In Create new item window, select Knowledge Graph and click Add . The Create new item of type Knowledge Graph window appears. Fill in the required details such as Label and Description. JSON XML Define a Label for the Knowlege Graph and provide Graph uri. You can leave all the other fields at default values. In this example we use: Name: Service Knowledge Graph Graph: http://ld.company.org/prod-instances/ Define a Label for the Knowledge Graph and provide Graph uri. You can leave all the other fields at default values. In this example we will use: Name: Organization Knowledge Graph Graph: http://ld.company.org/organization-data/ 4 Create a Transformation \u00a4 The transformation defines how an input dataset (e.g.: JSON or XML) will be transformed into an output dataset (e.g.: Knowledge Graph). Click Create in your project.\u202f On the Create New Item window, select Transform and click Add to create a new transformation. In the Create new item of type Transform window, enter the required fields. JSON XML For this example, enter the following: Name: Create Service Triples (optional) Description: Lifts the Service file into the Knowledge Graph Select the Source Dataset: Services JSON Select the Output Dataset: Service_Knowledge_Graph Click Create . For this example, enter the following: Name: Create Organization Triples (optional) Description: Lifts the Orgmap XML file into the Knowledge GraphOrgmap XML Select the Source Dataset: Orgmap XML Type: dept (define the Source Type, which defines the XML element that should be iterated when creating resources) Select the Output Dataset: Organization_Knowledge_Graph Click Create . Expand the menu by clicking the arrow on the right side of the page to expand the menu. Click Edit to create a base mapping. Define the Target entity type from the vocabulary, the URI pattern and a Label for the mapping. JSON XML Target Entity Type defines the class that will be instantiated when the mapping rule is applied. The URI pattern that defines the URI that shall be generated for each individual http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case, service-instances/ complements the instances prefix by adding a common prefix for all service instances and finally {ServiceID} is a placeholder that will resolve to the json-key ServiceID (e.g. \u201cServiceID\u201d: \u201cY704-9764759\u201d ) In this example we will use: Target Entity Type: Service URI Pattern: http://ld.company.org/prod-inst/service-instances/{ServiceID} An optional Label: Service Click SAVE . Example RDF triple in our Knowledge Graph based on the mapping definition: <http : //ld.company.org/services-instances/Y704-9764759> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://ld.company.org/prod-vocab/Service> Target Entity Type defines the class that will be instantiated when the mapping rule is applied: Department The URI pattern that defines the URI that shall be generated for each individual: http://ld.company.org/department/{@id} http://ld.company.org/department/ is a common prefix for the department instances in this use case, and finally {@id} is a placeholder that will resolve the XML attribute of the XML tag dept, which was configured as the Source Type of this transformation (see previous steps) In this example we will use: Target Entity Type: Department URI Pattern: http://ld.company.org/department/{@id} An optional Label: Department Click Save . Example RDF triple in our Knowledge Graph based on the mapping definition: <http : //ld.company.org/department/73191 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://ld.company.org/prod-vocab/Department> Evaluate your mapping by pressing on the button in the Examples of target data property to see at most three generated base URIs. We have now created the Service entities in the Knowledge Graph. Next we will now add the name of our entity. Click the circular blue + icon on the lower right and select Add value mapping . JSON XML Define the Target property , the Data type , the Value path (column name) and a Label for your value mapping. In this example, enter the following: Target Property: has product manager Data type: StringValueType Value path: ProductManager/name which corresponds to the following element in the json-file: [ {\u201cProductManager\u201d: { \u201cname\u201d: \u201cCorinna Ludwig\u201d} \u2026 } \u2026] An optional Label: has Product Manager Click Save . Define the Target property , the Data type , the Value path (column name) and a Label for your value mapping. In this example we will use: Target Property: name Data type: StringValueType Value path: dept/@name which corresponds to the department name attribute in the XML file An optional Label: department name Click Save . By clicking on the button in the Examples of target data property, you can get a preview for 3x value mapping to be created. JSON XML 5 Evaluate a Transformation \u00a4 Click Transform evaluation to evaluate the transformed entities. 6 Build the Knowledge Graph \u00a4 Click Transform execution Press the button and validate the results. In this example, 9x Service entities were created in our Knowledge Graph based on the mapping. You can click Knowledge Graphs under EXPLORE to (re-)view of the created Knowledge Graphs Enter the following URIs in the Enter search term for JSON and XML respectively. JSON / Service: http://ld.company.org/prod-instances/ XML / Department: http://ld.company.org/organization-data/ JSON XML","title":"Lift data from JSON and XML source"},{"location":"build/lift-data-from-json-and-xml-sources/#lift-data-from-json-and-xml-source","text":"","title":"Lift data from JSON and XML source"},{"location":"build/lift-data-from-json-and-xml-sources/#introduction","text":"This tutorial shows how you can build a Knowledge Graph based on input data from hierarchical sources like a JavaScript Object Notation file (.json) or an Extensible Markup Language file (.xml). Info The complete tutorial is available as a project file (XML) and a project file (JSON) . You can import these projects: by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-xml.project.zip xml-transformation cmemc -c my-cmem project import tutorial-json.project.zip json-transformation The documentation consists of the following steps, which are described in detail below: The following material is used in this tutorial: Sample vocabulary which describes the data in the JSON and XML files: products_vocabulary.nt Sample JSON file: services.json [ { \"Price\" : \"748,40 EUR\" , \"ProductManager\" : \"Lambert.Faust@company.org\" , \"Products\" : \"O491-3823912, I965-1821441, Z655-3173353, ...\" , \"ServiceID\" : \"Y704-9764759\" , \"ServiceName\" : \"Product Analysis\" }, { \"Price\" : \"1082,00 EUR\" , \"ProductManager\" : \"Corinna.Ludwig@company.org\" , \"Products\" : \"Z249-1364492, L557-1467804, C721-7900144, ...\" , \"ServiceID\" : \"I241-8776317\" , \"ServiceName\" : \"Component Confabulation\" }, ... ] Sample XML file: orgmap.xml <orgmap> <dept id= \"73191\" name= \"Engineering\" > <manager> <email> Thomas.Mueller@company.org </email> <name> Thomas Mueller </name> <address> Karl-Liebknecht-Stra\u00dfe 885, 82003 Tettnang </address> <phone> +49-8200-38218301 </phone> </manager> <employees> <employee> <email> Corinna.Ludwig@company.org </email> <name> Corinna Ludwig </name> <address> Ringstra\u00dfe 276 </address> <phone> +49-1743-24836762 </phone> <productExpert> Memristor, Gauge, Encoder </productExpert> </employee> <employee> <email> Karen.Brant@company.org </email> <name> Karen Brant </name> <address> Friedrichstra\u00dfe 664, 30805 Willich </address> <phone> (00530) 5040048 </phone> <productExpert> Inductor </productExpert> </employee> ... </employees> <products> <product id= \"Z249-1364492\" /> <product id= \"O184-6903943\" /> <product id= \"V404-9975399\" /> <product id= \"F344-7012314\" /> <product id= \"N463-8050264\" /> <product id= \"M605-5951566\" /> <product id= \"N733-1946687\" /> </products> <services> <service id= \"I241-8776317\" /> <service id= \"D215-3449390\" /> </services> </dept> <dept id= \"22183\" name= \"Product Management\" > ... </dept> ... </orgmap>","title":"Introduction"},{"location":"build/lift-data-from-json-and-xml-sources/#1-register-the-vocabulary","text":"The vocabulary contains the classes and properties needed to map the source data into entities in the Knowledge Graph. In Corporate Memory, click Vocabularies in the under EXPLORE in the navigation on the left side of the page. Click Register new vocabulary on the top right of the Vocabulary catalog page in Corporate Memory. Define a Name , a Graph URI and a Description of the vocabulary. In this example we will use: Name: Product Vocabulary Graph URI: http://ld.company.org/prod-vocab/ Description: Example vocabulary modeled to describe relations between products and services. Click REGISTER .","title":"1 Register the vocabulary"},{"location":"build/lift-data-from-json-and-xml-sources/#2-upload-the-data-file","text":"To add the data files, click Projects under BUILD in the navigation on the left side of the page. Follow the steps below for adding JSON and XML datasets. JSON XML Click Create at the top of the page. In Create new item window, select JSON and click Add. Define a Label for the dataset and upload the services.json file. You can leave all the other fields at default values. Click Create . Press the Create button and select XML Define a Label for the dataset and upload the orgmap.xml example file. You can leave all the other fields at default values. Click Create .","title":"2 Upload the data file"},{"location":"build/lift-data-from-json-and-xml-sources/#3-create-a-knowledge-graph","text":"Click Create at the top of the page.\u202f In Create new item window, select Knowledge Graph and click Add . The Create new item of type Knowledge Graph window appears. Fill in the required details such as Label and Description. JSON XML Define a Label for the Knowlege Graph and provide Graph uri. You can leave all the other fields at default values. In this example we use: Name: Service Knowledge Graph Graph: http://ld.company.org/prod-instances/ Define a Label for the Knowledge Graph and provide Graph uri. You can leave all the other fields at default values. In this example we will use: Name: Organization Knowledge Graph Graph: http://ld.company.org/organization-data/","title":"3 Create a Knowledge Graph"},{"location":"build/lift-data-from-json-and-xml-sources/#4-create-a-transformation","text":"The transformation defines how an input dataset (e.g.: JSON or XML) will be transformed into an output dataset (e.g.: Knowledge Graph). Click Create in your project.\u202f On the Create New Item window, select Transform and click Add to create a new transformation. In the Create new item of type Transform window, enter the required fields. JSON XML For this example, enter the following: Name: Create Service Triples (optional) Description: Lifts the Service file into the Knowledge Graph Select the Source Dataset: Services JSON Select the Output Dataset: Service_Knowledge_Graph Click Create . For this example, enter the following: Name: Create Organization Triples (optional) Description: Lifts the Orgmap XML file into the Knowledge GraphOrgmap XML Select the Source Dataset: Orgmap XML Type: dept (define the Source Type, which defines the XML element that should be iterated when creating resources) Select the Output Dataset: Organization_Knowledge_Graph Click Create . Expand the menu by clicking the arrow on the right side of the page to expand the menu. Click Edit to create a base mapping. Define the Target entity type from the vocabulary, the URI pattern and a Label for the mapping. JSON XML Target Entity Type defines the class that will be instantiated when the mapping rule is applied. The URI pattern that defines the URI that shall be generated for each individual http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case, service-instances/ complements the instances prefix by adding a common prefix for all service instances and finally {ServiceID} is a placeholder that will resolve to the json-key ServiceID (e.g. \u201cServiceID\u201d: \u201cY704-9764759\u201d ) In this example we will use: Target Entity Type: Service URI Pattern: http://ld.company.org/prod-inst/service-instances/{ServiceID} An optional Label: Service Click SAVE . Example RDF triple in our Knowledge Graph based on the mapping definition: <http : //ld.company.org/services-instances/Y704-9764759> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://ld.company.org/prod-vocab/Service> Target Entity Type defines the class that will be instantiated when the mapping rule is applied: Department The URI pattern that defines the URI that shall be generated for each individual: http://ld.company.org/department/{@id} http://ld.company.org/department/ is a common prefix for the department instances in this use case, and finally {@id} is a placeholder that will resolve the XML attribute of the XML tag dept, which was configured as the Source Type of this transformation (see previous steps) In this example we will use: Target Entity Type: Department URI Pattern: http://ld.company.org/department/{@id} An optional Label: Department Click Save . Example RDF triple in our Knowledge Graph based on the mapping definition: <http : //ld.company.org/department/73191 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://ld.company.org/prod-vocab/Department> Evaluate your mapping by pressing on the button in the Examples of target data property to see at most three generated base URIs. We have now created the Service entities in the Knowledge Graph. Next we will now add the name of our entity. Click the circular blue + icon on the lower right and select Add value mapping . JSON XML Define the Target property , the Data type , the Value path (column name) and a Label for your value mapping. In this example, enter the following: Target Property: has product manager Data type: StringValueType Value path: ProductManager/name which corresponds to the following element in the json-file: [ {\u201cProductManager\u201d: { \u201cname\u201d: \u201cCorinna Ludwig\u201d} \u2026 } \u2026] An optional Label: has Product Manager Click Save . Define the Target property , the Data type , the Value path (column name) and a Label for your value mapping. In this example we will use: Target Property: name Data type: StringValueType Value path: dept/@name which corresponds to the department name attribute in the XML file An optional Label: department name Click Save . By clicking on the button in the Examples of target data property, you can get a preview for 3x value mapping to be created. JSON XML","title":"4 Create a Transformation"},{"location":"build/lift-data-from-json-and-xml-sources/#5-evaluate-a-transformation","text":"Click Transform evaluation to evaluate the transformed entities.","title":"5 Evaluate a Transformation"},{"location":"build/lift-data-from-json-and-xml-sources/#6-build-the-knowledge-graph","text":"Click Transform execution Press the button and validate the results. In this example, 9x Service entities were created in our Knowledge Graph based on the mapping. You can click Knowledge Graphs under EXPLORE to (re-)view of the created Knowledge Graphs Enter the following URIs in the Enter search term for JSON and XML respectively. JSON / Service: http://ld.company.org/prod-instances/ XML / Department: http://ld.company.org/organization-data/ JSON XML","title":"6 Build the Knowledge Graph"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/","tags":["BeginnersTutorial"],"text":"Lift data from tabular data such as CSV, XSLX or database tables \u00a4 Introduction \u00a4 This beginner-level tutorial shows how you can build a Knowledge Graph based on input data from a comma-separated value file (.csv), an excel file (.xlsx) or a database table (jdbc). Info The complete tutorial is available as a project file . You can import this project by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface $ cmemc -c my-cmem project import tutorial-csv.project.zip tutorial-csv This step is optional and makes some of the following steps of the tutorial superfluous. The documentation consists of the following steps, which are described in detail below: Registration of the target vocabulary Uploading of the data (file)/Connect to JDBC endpoint (Re-)View your data table Creation of a (target) graph Creation of the transformation rules Evaluation of the results of the transformation rules Execution of the transformation to populate the target graph Sample Material \u00a4 The following material is used in this tutorial, you should download the files and have them at hand throughout the tutorial: Sample vocabulary which describes the data in the CSV files: products_vocabulary.nt Sample CSV file: services.csv Info ServiceID ServiceName Products ProductManager Price Y704-9764759 Product Analysis O491-3823912, I965-1821441, Z655-3173353, \u2026 Lambert.Faust@company.org 748,40 EUR I241-8776317 Component Confabulation Z249-1364492, L557-1467804, C721-7900144, \u2026 Corinna.Ludwig@company.org 1082,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 Sample Excel file: products.xlsx Info ProductID ProductName Height Width Depth Weigth ProductManager Price I241-8776317 Strain Compensator 12 68 15 8 Baldwin.Dirksen@company.org 0,50 EUR D215-3449390 Gauge Crystal 77 58 19 15 Wanja.Hoffmann@company.org 2,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 1 Register the vocabulary \u00a4 The vocabulary contains the classes and properties needed to map the data into the new structure in the Knowledge Graph. Corporate Memory cmemc In Corporate Memory, click Vocabularies under EXPLORE in the navigation on the left side of the page. Click Register new vocabulary on the top right. Define a Name , a Graph URI and a Description of the vocabulary. In this example we will use: Name: Product Vocabulary Graph URI: http://ld.company.org/prod-vocab/ Description: Example vocabulary modeled to describe relations between products and services. Vocabulary File: Browse in your filesystem for the products_vocabulary.nt file and select it to be uploaded. $ cmemc vocabulary import products_vocabulary.nt 2 Upload the data file / Connect to the JDBC endpoint \u00a4 CSV + XLSX JDBC cmemc In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page. Click Create at the top of the page.\u202f In Create new item window, select Project and click Add . The Create new item of type Project window appears.\u202f Fill in the required details such as Title and Description. Alternatively, import the existing project by clicking Import Project File and selecting the file from your system.\u202f Click Create . Your project is created. Within your project, click Create or Create item . In the Create new item dialog, select CSV . Fill out a label and upload the services.csv sample file . Click Create .** Leave all other parameters at their default values. Create a second dataset . Choose Excel and upload the products.xlsx file. Instead of uploading the services.csv sample file into Corporate Memory, you can also load it into a SQL database and access it from Corporate Memory using the JDBC protocol. In the project, Click Create and select the JDBC endpoint type. Define a Label for the dataset, specify the JDBC Driver connection URL , the table name and the user and password to connect to the database. In this example we will use: Name: Services_ServiceDB JDBC Driver Connection URL: jdbc:mysql://mysql:3306/ServicesDB table: Services username: root password: **** The general form of the JDBC connection string is: jdbc:<vendor>://<hostname>:<portNumber>/<databaseName> Default JDBC connection strings for popular Relational Database Management Systems: Vendor Default JDBC Connection String Default Port Microsoft SQL Server jdbc:sqlserver: :1433/ 1433 PostgreSQL jdbc:postgresql: :5432/ 5432 MySQL jdbc:mysql: :3306/ 3306 MariaDB jdbc:mariadb: :3306/ 3306 IBM DB2* jdbc:db2: :50000/ 50000 Oracle* jdbc:oracle:thin: :1521/ 1521 Info * IBM DB2 and Oracle JDBC drivers are not by default part of Corporate Memory, but can be added. Info Instead of selecting a table you can also specify a custom SQL query in the source query field. $ cmemc project create tutorial-csv $ cmemc dataset create --project tutorial-csv services.csv $ cmemc dataset create --project tutorial-csv products.xlsx 3 (Re-)View your Data Table \u00a4 To validate that the input data is correct, you can preview the data table in Corporate Memory. On the dataset page, press the Load preview button Once the preview is loaded, you can view a couple of rows to check that your data is accessible. Optionally, you can click start profiling and explore statistics about the dataset. 4 Create a Knowledge Graph \u00a4 Click Create at the top of the page. In Create new item window, select Knowledge Graph and click Add. The Create new item of type Knowledge Graph window appears. Define a Label for the Knowledge Graph and provide a graph uri. Leave all the other parameters at the default values. In this example we will use: Label: Service Knowledge Graph Graph: http://ld.company.org/prod-instances/ Click Create . 5 Create a Transformation \u00a4 The transformation defines how an input dataset (e.g. CSV) will be transformed into an output dataset (e.g. Knowledge Graph). Click Create in your project.\u202f On the Create New Item window, select Transform and click Add to create a new transformation. Fill out the the details leaving the target vocabularies field at its default value all installed vocabularies, which will enable us to create a transformation to the previously installed products vocabulary. In this example we will use: Name: Lift Service Database In the section INPUT TASK in the field Dataset , select the previously created dataset: Services (Input Dataset). Select the previously created dataset as the Output Dataset: Service Knowledge Graph In the main area you will find the Mapping editor . Click Mapping in the main area to expand its menu. Click Edit to create a base mapping. Define the Target entity type from the vocabulary, the URI pattern and a label for the mapping. In this example we will use: Target entity type: Service URI pattern: Click Create custom pattern Insert http://ld.company.org/prod-inst/{ServiceID} where http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case, and {ServiceID} is a placeholder that will resolve to the column of that name An optional Label: Service Click Save Example RDF triple in our Knowledge Graph based on the mapping definition: <http : //ld.company.org/prod-inst/Y704-9764759> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://ld.company.org/prod-vocab/Service> Evaluate your mapping by clicking the Expand button in the Examples of target data property to see at most three generated base URIs. We have now created the Service entities in the Knowledge Graph. As a next step, we will add the name of the Service entity. Press the circular Blue + button on the lower right and select Add value mapping . Define the Target property , the Data type , the Value path (column name) and a Label for your value mapping. In this example we will use: Target Property: name Data type: StringValueType Value path: ServiceName (which corresponds to the column of that name) An optional Label: service name Click Save. 6 Evaluate a Transformation \u00a4 Go the Transform evaluation tab of your transformation to view a list of generated entities. By clicking one of the generated entities, more details are provided. 7 Build the Knowledge Graph \u00a4 Go into the mapping and visit the Transform execution tab. Press the button and validate the results. In this example, 9x Service triples were created in our Knowledge Graph based on the mapping. Finally you can use the DataManager Knowledge Graphs module to (re-)view of the created Knowledge Graph: http://ld.company.org/prod-instances/","title":"Lift data from tabular data such as CSV, XSLX or database tables"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables","text":"","title":"Lift data from tabular data such as CSV, XSLX or database tables"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#introduction","text":"This beginner-level tutorial shows how you can build a Knowledge Graph based on input data from a comma-separated value file (.csv), an excel file (.xlsx) or a database table (jdbc). Info The complete tutorial is available as a project file . You can import this project by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface $ cmemc -c my-cmem project import tutorial-csv.project.zip tutorial-csv This step is optional and makes some of the following steps of the tutorial superfluous. The documentation consists of the following steps, which are described in detail below: Registration of the target vocabulary Uploading of the data (file)/Connect to JDBC endpoint (Re-)View your data table Creation of a (target) graph Creation of the transformation rules Evaluation of the results of the transformation rules Execution of the transformation to populate the target graph","title":"Introduction"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#sample-material","text":"The following material is used in this tutorial, you should download the files and have them at hand throughout the tutorial: Sample vocabulary which describes the data in the CSV files: products_vocabulary.nt Sample CSV file: services.csv Info ServiceID ServiceName Products ProductManager Price Y704-9764759 Product Analysis O491-3823912, I965-1821441, Z655-3173353, \u2026 Lambert.Faust@company.org 748,40 EUR I241-8776317 Component Confabulation Z249-1364492, L557-1467804, C721-7900144, \u2026 Corinna.Ludwig@company.org 1082,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 Sample Excel file: products.xlsx Info ProductID ProductName Height Width Depth Weigth ProductManager Price I241-8776317 Strain Compensator 12 68 15 8 Baldwin.Dirksen@company.org 0,50 EUR D215-3449390 Gauge Crystal 77 58 19 15 Wanja.Hoffmann@company.org 2,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026","title":"Sample Material"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#1-register-the-vocabulary","text":"The vocabulary contains the classes and properties needed to map the data into the new structure in the Knowledge Graph. Corporate Memory cmemc In Corporate Memory, click Vocabularies under EXPLORE in the navigation on the left side of the page. Click Register new vocabulary on the top right. Define a Name , a Graph URI and a Description of the vocabulary. In this example we will use: Name: Product Vocabulary Graph URI: http://ld.company.org/prod-vocab/ Description: Example vocabulary modeled to describe relations between products and services. Vocabulary File: Browse in your filesystem for the products_vocabulary.nt file and select it to be uploaded. $ cmemc vocabulary import products_vocabulary.nt","title":"1 Register the vocabulary"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#2-upload-the-data-file-connect-to-the-jdbc-endpoint","text":"CSV + XLSX JDBC cmemc In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page. Click Create at the top of the page.\u202f In Create new item window, select Project and click Add . The Create new item of type Project window appears.\u202f Fill in the required details such as Title and Description. Alternatively, import the existing project by clicking Import Project File and selecting the file from your system.\u202f Click Create . Your project is created. Within your project, click Create or Create item . In the Create new item dialog, select CSV . Fill out a label and upload the services.csv sample file . Click Create .** Leave all other parameters at their default values. Create a second dataset . Choose Excel and upload the products.xlsx file. Instead of uploading the services.csv sample file into Corporate Memory, you can also load it into a SQL database and access it from Corporate Memory using the JDBC protocol. In the project, Click Create and select the JDBC endpoint type. Define a Label for the dataset, specify the JDBC Driver connection URL , the table name and the user and password to connect to the database. In this example we will use: Name: Services_ServiceDB JDBC Driver Connection URL: jdbc:mysql://mysql:3306/ServicesDB table: Services username: root password: **** The general form of the JDBC connection string is: jdbc:<vendor>://<hostname>:<portNumber>/<databaseName> Default JDBC connection strings for popular Relational Database Management Systems: Vendor Default JDBC Connection String Default Port Microsoft SQL Server jdbc:sqlserver: :1433/ 1433 PostgreSQL jdbc:postgresql: :5432/ 5432 MySQL jdbc:mysql: :3306/ 3306 MariaDB jdbc:mariadb: :3306/ 3306 IBM DB2* jdbc:db2: :50000/ 50000 Oracle* jdbc:oracle:thin: :1521/ 1521 Info * IBM DB2 and Oracle JDBC drivers are not by default part of Corporate Memory, but can be added. Info Instead of selecting a table you can also specify a custom SQL query in the source query field. $ cmemc project create tutorial-csv $ cmemc dataset create --project tutorial-csv services.csv $ cmemc dataset create --project tutorial-csv products.xlsx","title":"2 Upload the data file / Connect to the JDBC endpoint"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#3-re-view-your-data-table","text":"To validate that the input data is correct, you can preview the data table in Corporate Memory. On the dataset page, press the Load preview button Once the preview is loaded, you can view a couple of rows to check that your data is accessible. Optionally, you can click start profiling and explore statistics about the dataset.","title":"3 (Re-)View your Data Table"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#4-create-a-knowledge-graph","text":"Click Create at the top of the page. In Create new item window, select Knowledge Graph and click Add. The Create new item of type Knowledge Graph window appears. Define a Label for the Knowledge Graph and provide a graph uri. Leave all the other parameters at the default values. In this example we will use: Label: Service Knowledge Graph Graph: http://ld.company.org/prod-instances/ Click Create .","title":"4 Create a Knowledge Graph"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#5-create-a-transformation","text":"The transformation defines how an input dataset (e.g. CSV) will be transformed into an output dataset (e.g. Knowledge Graph). Click Create in your project.\u202f On the Create New Item window, select Transform and click Add to create a new transformation. Fill out the the details leaving the target vocabularies field at its default value all installed vocabularies, which will enable us to create a transformation to the previously installed products vocabulary. In this example we will use: Name: Lift Service Database In the section INPUT TASK in the field Dataset , select the previously created dataset: Services (Input Dataset). Select the previously created dataset as the Output Dataset: Service Knowledge Graph In the main area you will find the Mapping editor . Click Mapping in the main area to expand its menu. Click Edit to create a base mapping. Define the Target entity type from the vocabulary, the URI pattern and a label for the mapping. In this example we will use: Target entity type: Service URI pattern: Click Create custom pattern Insert http://ld.company.org/prod-inst/{ServiceID} where http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case, and {ServiceID} is a placeholder that will resolve to the column of that name An optional Label: Service Click Save Example RDF triple in our Knowledge Graph based on the mapping definition: <http : //ld.company.org/prod-inst/Y704-9764759> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://ld.company.org/prod-vocab/Service> Evaluate your mapping by clicking the Expand button in the Examples of target data property to see at most three generated base URIs. We have now created the Service entities in the Knowledge Graph. As a next step, we will add the name of the Service entity. Press the circular Blue + button on the lower right and select Add value mapping . Define the Target property , the Data type , the Value path (column name) and a Label for your value mapping. In this example we will use: Target Property: name Data type: StringValueType Value path: ServiceName (which corresponds to the column of that name) An optional Label: service name Click Save.","title":"5 Create a Transformation"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#6-evaluate-a-transformation","text":"Go the Transform evaluation tab of your transformation to view a list of generated entities. By clicking one of the generated entities, more details are provided.","title":"6 Evaluate a Transformation"},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#7-build-the-knowledge-graph","text":"Go into the mapping and visit the Transform execution tab. Press the button and validate the results. In this example, 9x Service triples were created in our Knowledge Graph based on the mapping. Finally you can use the DataManager Knowledge Graphs module to (re-)view of the created Knowledge Graph: http://ld.company.org/prod-instances/","title":"7 Build the Knowledge Graph"},{"location":"build/populate-data-to-neo4j/","text":"","title":"Index"},{"location":"build/processing-data-with-variable-input-workflows/","text":"Processing data with variable input workflows \u00a4 Introduction \u00a4 This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (means, without registering datasets). The concept to achieve this is named a Variable Dataset . A variable dataset is created and used inside of a workflow as an input for other tasks (e.g. a transformation) at the place where a \u201cnormal\u201d dataset (such as register CSV file) would be placed. The workflow is then called via an HTTP REST call (or via cmemc ), thus uploading the payload data \u201cat the place\u201d of this variable input dataset and executing all following parts of the workflow. This allows for solving all kinds of \u2606 Automation tasks when you need to process lots of small data snippets or similar. Info The complete tutorial is available as a project file . You can import this project by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-varinput.project.zip varinput 1 Install the required vocabularies \u00a4 First, install all required ontologies/vocabularies which are needed for mappings later in the tab VOCABULARIES. In this tutorial, we need the Schema.org and the RDFS vocabulary. Press the (toggle switch) button on the right to install them. 2 Create a new project \u00a4 Second, create in the tab DATA INTEGRATION a new project. Provide it with a Title and Description . The project will include everything you need to build a workflow for extracting Feed XML data, transforming it into RDF, and loading it into a Knowledge Graph. 3 Create an (example) feed dataset and target graph dataset \u00a4 Upload a sample XML dataset (feed data) into your project: Create \u2192 XML \u2192 Upload new file. For this tutorial, you may take this file: feed.xml Feed source: https://www.ecdc.europa.eu/en/taxonomy/term/2942/feed 4 Create the feed transformation \u00a4 Based on the added sample feed XML Dataset, create a mapping to generate RDF triples. The screenshot provides an example mapping to generate WebPages, which include a label, a URL, a text, and the date they were published in the feed. The mappings are based on classes and properties defined by the Schema.org and RDFS vocabulary. In case you need help with mapping data from XML to RDF, feel free to visit your respective tutorial: Lift data from JSON and XML sources . 5 Create the variable input and workflow \u00a4 Create a new workflow in your project. Move the input XML feed dataset and the Feed Data Graph into the workflow editor and connect them with your created Transform feed . 6 Use cmemc to feed data into the workflow \u00a4 Finally, you can process all the feeds you want, by executing the created workflow with a dynamic XML payload. For this, you need to use the workflow io command: # process one specific feed xml document $ cmemc workflow io varinput:process-feed -i feed.xml You can easily automate this for a list of feeds like this: $ cat feeds.txt https://feeds.npr.org/500005/podcast.xml http://rss.cnn.com/rss/cnn_topstories.rss https://lifehacker.com/rss http://feeds.bbci.co.uk/news/rss.xml \u2026 # fetch the list of urls one by one and feed the content to the corporate memory workflow $ cat feeds.txt | xargs -I % sh -c '{ echo %; curl -s % -o feed.xml; cmemc workflow io varinput:process-feed -i feed.xml; rm feed.xml; }' https://feeds.npr.org/500005/podcast.xml http://rss.cnn.com/rss/cnn_topstories.rss https://lifehacker.com/rss http://feeds.bbci.co.uk/news/rss.xml \u2026 7 Explore the fetched Knowledge Graph \u00a4 In EXPLORATION , you can study the ingested feed data in your Knowledge Graph.","title":"Processing data with variable input workflows"},{"location":"build/processing-data-with-variable-input-workflows/#processing-data-with-variable-input-workflows","text":"","title":"Processing data with variable input workflows"},{"location":"build/processing-data-with-variable-input-workflows/#introduction","text":"This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (means, without registering datasets). The concept to achieve this is named a Variable Dataset . A variable dataset is created and used inside of a workflow as an input for other tasks (e.g. a transformation) at the place where a \u201cnormal\u201d dataset (such as register CSV file) would be placed. The workflow is then called via an HTTP REST call (or via cmemc ), thus uploading the payload data \u201cat the place\u201d of this variable input dataset and executing all following parts of the workflow. This allows for solving all kinds of \u2606 Automation tasks when you need to process lots of small data snippets or similar. Info The complete tutorial is available as a project file . You can import this project by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-varinput.project.zip varinput","title":"Introduction"},{"location":"build/processing-data-with-variable-input-workflows/#1-install-the-required-vocabularies","text":"First, install all required ontologies/vocabularies which are needed for mappings later in the tab VOCABULARIES. In this tutorial, we need the Schema.org and the RDFS vocabulary. Press the (toggle switch) button on the right to install them.","title":"1 Install the required vocabularies"},{"location":"build/processing-data-with-variable-input-workflows/#2-create-a-new-project","text":"Second, create in the tab DATA INTEGRATION a new project. Provide it with a Title and Description . The project will include everything you need to build a workflow for extracting Feed XML data, transforming it into RDF, and loading it into a Knowledge Graph.","title":"2 Create a new project"},{"location":"build/processing-data-with-variable-input-workflows/#3-create-an-example-feed-dataset-and-target-graph-dataset","text":"Upload a sample XML dataset (feed data) into your project: Create \u2192 XML \u2192 Upload new file. For this tutorial, you may take this file: feed.xml Feed source: https://www.ecdc.europa.eu/en/taxonomy/term/2942/feed","title":"3 Create an (example) feed dataset and target graph dataset"},{"location":"build/processing-data-with-variable-input-workflows/#4-create-the-feed-transformation","text":"Based on the added sample feed XML Dataset, create a mapping to generate RDF triples. The screenshot provides an example mapping to generate WebPages, which include a label, a URL, a text, and the date they were published in the feed. The mappings are based on classes and properties defined by the Schema.org and RDFS vocabulary. In case you need help with mapping data from XML to RDF, feel free to visit your respective tutorial: Lift data from JSON and XML sources .","title":"4 Create the feed transformation"},{"location":"build/processing-data-with-variable-input-workflows/#5-create-the-variable-input-and-workflow","text":"Create a new workflow in your project. Move the input XML feed dataset and the Feed Data Graph into the workflow editor and connect them with your created Transform feed .","title":"5 Create the variable input and workflow"},{"location":"build/processing-data-with-variable-input-workflows/#6-use-cmemc-to-feed-data-into-the-workflow","text":"Finally, you can process all the feeds you want, by executing the created workflow with a dynamic XML payload. For this, you need to use the workflow io command: # process one specific feed xml document $ cmemc workflow io varinput:process-feed -i feed.xml You can easily automate this for a list of feeds like this: $ cat feeds.txt https://feeds.npr.org/500005/podcast.xml http://rss.cnn.com/rss/cnn_topstories.rss https://lifehacker.com/rss http://feeds.bbci.co.uk/news/rss.xml \u2026 # fetch the list of urls one by one and feed the content to the corporate memory workflow $ cat feeds.txt | xargs -I % sh -c '{ echo %; curl -s % -o feed.xml; cmemc workflow io varinput:process-feed -i feed.xml; rm feed.xml; }' https://feeds.npr.org/500005/podcast.xml http://rss.cnn.com/rss/cnn_topstories.rss https://lifehacker.com/rss http://feeds.bbci.co.uk/news/rss.xml \u2026","title":"6 Use cmemc to feed data into the workflow"},{"location":"build/processing-data-with-variable-input-workflows/#7-explore-the-fetched-knowledge-graph","text":"In EXPLORATION , you can study the ingested feed data in your Knowledge Graph.","title":"7 Explore the fetched Knowledge Graph"},{"location":"build/rule-operators/","text":"Rule Operators \u00a4 Introduction \u00a4 This page outlines the basic operators that can be used to build linkage and transformation rules. Transformation rules are trees that consist of two types of operators: Path Operator: Retrieves all values of a specific property path of each entity, such as its label property. The purpose of the path operator is to enable the access of values from the dataset. Transformation Operator: Transforms the values of path or transformation operators according to a specific data transformation function. Linkage rules may use two additional operator types in addition: Comparison Operator: Evaluates the similarity between two entities based on the values that are returned by two path or transformation operators by applying a distance measure and a distance threshold. Examples of distance measures include Levenshtein, Jaccard, or geographic distance. Aggregation Operator: Due to the fact that, in most cases, the similarity of two entities cannot be determined by evaluating a single comparison, an aggregation operator combines the similarity scores from multiple comparison or aggregation operators into a single score according to a specific aggregation function. Examples of common aggregation functions include the weighted average or yielding the minimum score of all operators. Path Operator \u00a4 A path operator retrieves all values, which are connected to the entities by a specific path. Every path statement consists of a series of path elements. If a path cannot be resolved due to a missing property or a too restrictive filter, an empty result set is returned. The following operators can be used to traverse the dataset: Operator Example Description /<property> /dbpedia:director/rdfs:label Moves forward from a subject resource through an operator property to its value(s). \\<property> \\dbpedia:artist Moves backward from a subject resource through an operator property to its value(s). [<property> <comp_operator> value] /dbpedia:work[rdf:type = dbpedia:Album] Filters the currently selected values using a filter expression. comp_operator may be one of > , < , >= , <= , = , != [@lang = 'lang'] /rdfs:label[@lang = 'en'] Filter literal values by their language . Transformation Operator \u00a4 As different datasets usually use different data formats, a transformation can be used to normalize the values prior to comparison. Examples of transformation functions include case normalization, tokenization or concatenation of values from multiple operators. Multiple transformation operators can be nested in order to apply a chain of transformations. Comparison Operator \u00a4 A comparison operator evaluates two inputs and computes the similarity based on a user-defined distance measure and a user-defined threshold . The distance measure always outputs 0 for a perfect match, and a higher value for an imperfect match. Only distance values between 0 and the threshold will result in a positive similarity score. Therefore it is important to know how the distance measures work and what the range of their output values is, in order to set a threshold value sensibly. The following parameters can be set for each comparison: Parameter Description required If required is true, the parent aggregation only yields a confidence value if the given inputs have values for both instances. weight Weight of this operator in the parent aggregation. The weight is used by some aggregations such as the weighted average aggregation. threshold The maximum distance. For normalized distance measures, the threshold should be between 0.0 and 1.0. The threshold is used to convert the computed distance to a confidence between -1.0 and 1.0. Links will be generated for confidences above 0 while higher confidence values imply a higher similarity between the compared entities. If distance measures generate multiple distance scores the lowest is used to generate the confidence. Aggregation Operator \u00a4 An aggregation combines multiple confidence values into a single value. In order to determine if two entities are duplicates it is usually not sufficient to compare a single property. For instance, when comparing geographic entities, an aggregation may aggregate the similarities between the names of the entities and the similarities based on the distance between the entities. If an aggregation is fed with missing values (e.g., if inputs paths returned no values), the behavior is as follows: Boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d. Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing. If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.","title":"Rule Operators"},{"location":"build/rule-operators/#rule-operators","text":"","title":"Rule Operators"},{"location":"build/rule-operators/#introduction","text":"This page outlines the basic operators that can be used to build linkage and transformation rules. Transformation rules are trees that consist of two types of operators: Path Operator: Retrieves all values of a specific property path of each entity, such as its label property. The purpose of the path operator is to enable the access of values from the dataset. Transformation Operator: Transforms the values of path or transformation operators according to a specific data transformation function. Linkage rules may use two additional operator types in addition: Comparison Operator: Evaluates the similarity between two entities based on the values that are returned by two path or transformation operators by applying a distance measure and a distance threshold. Examples of distance measures include Levenshtein, Jaccard, or geographic distance. Aggregation Operator: Due to the fact that, in most cases, the similarity of two entities cannot be determined by evaluating a single comparison, an aggregation operator combines the similarity scores from multiple comparison or aggregation operators into a single score according to a specific aggregation function. Examples of common aggregation functions include the weighted average or yielding the minimum score of all operators.","title":"Introduction"},{"location":"build/rule-operators/#path-operator","text":"A path operator retrieves all values, which are connected to the entities by a specific path. Every path statement consists of a series of path elements. If a path cannot be resolved due to a missing property or a too restrictive filter, an empty result set is returned. The following operators can be used to traverse the dataset: Operator Example Description /<property> /dbpedia:director/rdfs:label Moves forward from a subject resource through an operator property to its value(s). \\<property> \\dbpedia:artist Moves backward from a subject resource through an operator property to its value(s). [<property> <comp_operator> value] /dbpedia:work[rdf:type = dbpedia:Album] Filters the currently selected values using a filter expression. comp_operator may be one of > , < , >= , <= , = , != [@lang = 'lang'] /rdfs:label[@lang = 'en'] Filter literal values by their language .","title":"Path Operator"},{"location":"build/rule-operators/#transformation-operator","text":"As different datasets usually use different data formats, a transformation can be used to normalize the values prior to comparison. Examples of transformation functions include case normalization, tokenization or concatenation of values from multiple operators. Multiple transformation operators can be nested in order to apply a chain of transformations.","title":"Transformation Operator"},{"location":"build/rule-operators/#comparison-operator","text":"A comparison operator evaluates two inputs and computes the similarity based on a user-defined distance measure and a user-defined threshold . The distance measure always outputs 0 for a perfect match, and a higher value for an imperfect match. Only distance values between 0 and the threshold will result in a positive similarity score. Therefore it is important to know how the distance measures work and what the range of their output values is, in order to set a threshold value sensibly. The following parameters can be set for each comparison: Parameter Description required If required is true, the parent aggregation only yields a confidence value if the given inputs have values for both instances. weight Weight of this operator in the parent aggregation. The weight is used by some aggregations such as the weighted average aggregation. threshold The maximum distance. For normalized distance measures, the threshold should be between 0.0 and 1.0. The threshold is used to convert the computed distance to a confidence between -1.0 and 1.0. Links will be generated for confidences above 0 while higher confidence values imply a higher similarity between the compared entities. If distance measures generate multiple distance scores the lowest is used to generate the confidence.","title":"Comparison Operator"},{"location":"build/rule-operators/#aggregation-operator","text":"An aggregation combines multiple confidence values into a single value. In order to determine if two entities are duplicates it is usually not sufficient to compare a single property. For instance, when comparing geographic entities, an aggregation may aggregate the similarities between the names of the entities and the similarities based on the distance between the entities. If an aggregation is fed with missing values (e.g., if inputs paths returned no values), the behavior is as follows: Boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d. Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing. If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.","title":"Aggregation Operator"},{"location":"build/workflow-reconfiguration/","text":"Workflow Reconfiguration \u00a4 Introduction \u00a4 The operators of a workflow can be reconfigured completely in the context of a workflow. During its execution, new parameters are loaded from any possible source and translated by a transformation task to allow an injection into the dataset configuration that overwrites originally set parameters. To reconfigure a workflow operator, the transformation task has to be connected to the red dot at the top of this operator as shown in the following image: Although this feature has been developed to support the ingestion of database deltas, the possible applications are various since any parameter can be overwritten to make workflow operators even more dynamic and reusable in various contexts. The incremental ingestion of database content that was implemented as a first use-case can be found the application section of this page. However, we intend to add other use-cases that have been implemented. The following parameters seem to be good starting points for possible applications: Transformation Task: Source Type Source Restriction JDBC endpoint (remote) Source Query Write Strategy Restriction Knowledge Graph (embedded) Clear Graph before workflow execution Scheduler Interval Enabled \u2026 Implementation \u00a4 To reconfigure a workflow operator, you need to create a transformation task, the data source of which is the intended source of the dynamic parameters of the workflow operator. Once you have created this task, you need to create a data value mapping for each parameter you want to overwrite. Info Only one transformation task can be used to reconfigure the workflow operator and one source can be used for a transformation task\u2019s source. Thus, it is necessary to pre-process all parameters that need to be rewritten into one single dataset, e.g. a CSV file or a in-memory dataset. Then, you can use this dataset to inject all parameters with one transformation task. Once you are sure, that your mapping rule entails the correct value, you can set the workflow operator parameter as the target property of the mapping rule. After this is done, you can reconfigure any workflow operator that uses this parameter as part of its configuration. Info The transformation task needs a suffix of the workflow parameter\u2019s URI in the workflow operator\u2019s serialization as its target property. This differs from the documentation that just refers to the parameter\u2019s name . If you want to overwrite the source query of a JDBC endpoint, you need to define sourceQuery as the target property, which is the suffix of <https://vocab.eccenca.com/di/functions/param_Jdbc_sourceQuery> . Applications \u00a4 Tutorials that showcase this function in an application context: Loading JDBC datasets incrementally","title":"Workflow Reconfiguration"},{"location":"build/workflow-reconfiguration/#workflow-reconfiguration","text":"","title":"Workflow Reconfiguration"},{"location":"build/workflow-reconfiguration/#introduction","text":"The operators of a workflow can be reconfigured completely in the context of a workflow. During its execution, new parameters are loaded from any possible source and translated by a transformation task to allow an injection into the dataset configuration that overwrites originally set parameters. To reconfigure a workflow operator, the transformation task has to be connected to the red dot at the top of this operator as shown in the following image: Although this feature has been developed to support the ingestion of database deltas, the possible applications are various since any parameter can be overwritten to make workflow operators even more dynamic and reusable in various contexts. The incremental ingestion of database content that was implemented as a first use-case can be found the application section of this page. However, we intend to add other use-cases that have been implemented. The following parameters seem to be good starting points for possible applications: Transformation Task: Source Type Source Restriction JDBC endpoint (remote) Source Query Write Strategy Restriction Knowledge Graph (embedded) Clear Graph before workflow execution Scheduler Interval Enabled \u2026","title":"Introduction"},{"location":"build/workflow-reconfiguration/#implementation","text":"To reconfigure a workflow operator, you need to create a transformation task, the data source of which is the intended source of the dynamic parameters of the workflow operator. Once you have created this task, you need to create a data value mapping for each parameter you want to overwrite. Info Only one transformation task can be used to reconfigure the workflow operator and one source can be used for a transformation task\u2019s source. Thus, it is necessary to pre-process all parameters that need to be rewritten into one single dataset, e.g. a CSV file or a in-memory dataset. Then, you can use this dataset to inject all parameters with one transformation task. Once you are sure, that your mapping rule entails the correct value, you can set the workflow operator parameter as the target property of the mapping rule. After this is done, you can reconfigure any workflow operator that uses this parameter as part of its configuration. Info The transformation task needs a suffix of the workflow parameter\u2019s URI in the workflow operator\u2019s serialization as its target property. This differs from the documentation that just refers to the parameter\u2019s name . If you want to overwrite the source query of a JDBC endpoint, you need to define sourceQuery as the target property, which is the suffix of <https://vocab.eccenca.com/di/functions/param_Jdbc_sourceQuery> .","title":"Implementation"},{"location":"build/workflow-reconfiguration/#applications","text":"Tutorials that showcase this function in an application context: Loading JDBC datasets incrementally","title":"Applications"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/","text":"Loading JDBC datasets incrementally \u00a4 Introduction \u00a4 This tutorial walks you through the process of loading data incrementally from a JDBC Dataset (relational database Table) into a Knowledge Graph. Info The complete tutorial is available as a project file . You can import this project: by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-webapi.project.zip web-api Example SQL query for selecting a predefined range of rows We use the LIMIT and OFFSET clauses in SQL to retrieve only a portion of rows. This prevents loading all data at once from a table, SQL Query with OFFSET AND LIMIT: SELECT * FROM services OFFSET 0 LIMIT 5 This query retrieves the first 5 rows of a table named \u201cservices\u201d. If the OFFSET is changed to 5, it will retrieve the next 5 rows. 1 Create the JDBC dataset \u00a4 To extract data from a relational database, you need to first register a JDBC endpoint in Corporate Memory. This tutorial assumes that you have access to the relational database from the Corporate Memory instance. In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page. Click Create at the top of the page. In Create new item window, select Project and click Add. The Create new item of type Project window appears. In the Create new item window, select Dataset under Item Type , search for JDBC endpoint, and click Add . Provide the required configuration details for JDBC endpoint: Label : Provide a table name. Description: Optionally describe your table. JDBC Driver Connection URL: Provide the JDBC connection. In this tutorial, we use a MySQL database. The database server is named mysql and the database is named serviceDB . Table: Provide the name of the table in the database. Source query : Provide a default source query. In this tutorial, the source query will be changed later, as the OFFSET changes. Limit: Provide a LIMIT for the SQL query. In this tutorial, we choose 5 for demonstrating the functionality. You may select any value which works for your use case. Query strategy : Select: Execute the given source query. No paging or virtual Query. In this tutorial, this needs to be changed, such that when this JDBC endpoint is being used, Corporate Memory will always check for the Source Query which was provided earlier. User : Provide the user name which is allowed to access the database. Password : Provide the user password that is allowed to access the database. 2 Create a Metadata Graph \u00a4 To incrementally extract data in Corporate Memory, we need to store the information about the OFFSET, which will change with each extraction. For that, we need to define a new Graph, named Services Metadata Graph that will hold this information. To identify the changing OFFSET with the JDBC endpoint we previously created, we will use the Graph IRI that Corporate Memory created for us. To find the JDBC endpoint IRI Visit the Exploration Tab of Corporate Memory Select in Graph (top left) your project, which starts with \u201c CMEM DI Project \u2026 \u201d (if you can\u2019t see it, you might not have access to it, please contact in your case your administrator) Select in Navigation (bottom left): functions_Plugins_Jdbc Select the previously created JDBC endpoint (in our example: \u201cServices Table (JDBC)\u201d Press the Turtle tab inside your JDBC endpoint view (right) In our example, the JDBC Endpoint IRI looks like this: http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC See screenshot below for example: Now that we have the JDBC endpoint IRI, we will build the Metadata Graph to store the OFFSET information. The following three RDF triples hold the (minimal) necessary information we need for this tutorial: The first triple imports into our Metadata Graph the CMEM DI Project graph to have access to the LIMIT property defined and in case we need more in the future. The second triple defines a label for the Graph. The third triple defines the <\u2026 lastOffset > property, which we need for this tutorial. As a default, we set it to 0 to start with the first row in the table. services_metadata_graph <http : //di.eccenca.com/project/services/metadata> <http : //www.w3.org/2002/07/owl#imports> <http : //di.eccenca.com/project/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload> . # import the original project <http : //di.eccenca.com/project/services/metadata> <http : //www.w3.org/2000/01/rdf-schema#label> \"Services Metadata\"@en . # provide the graph with a label <http : //dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC> <https : //vocab.eccenca.com/di/functions/param_Jdbc_lastOffset> \"0\" . # set the initial offset to zero to start with the first row in the table For your project, please: adjust the CMEM DI Project IRI and the JDBC endpoint IRI. Import the Graph in the Exploration tab \u2192 Graph (menu) \u2192 Add new Graph \u2192 Provide Graph IRI + Select file In our example, we used the following Graph IRI for the Metadata Graph: http://di.eccenca.com/project/services/metadata 3 Create a Transformation to dynamically compose a SQL Query \u00a4 To extract rows based on the predefined (changing) OFFSET and LIMIT from a table, we need to create a Transformation to compose the SQL with each execution. Click Create (top right) in the data integration workspace and select the type Transformation . Provide a Label . Provide Description (Optional). Select the Services Metadata Graph we previously created. Create only a value mapping with the property sourceQuery . The sourceQuery will be used as an input for the JDBC endpoint. A root mapping does not need to be defined. In this screenshot, everything is already configured, your\u2019s will be empty when you create it for the first time. Press the circular pen button to jump into the advanced mapping editor. As source paths, we select the data from our Metadata Graph: table , lastOffset and limit . Everything else is defined as a constant, as it does not change in the query. For our source paths, we defined a \u201cdi\u201d prefix. In case that is missing, your source path may look longer (full IRI). 4 Create a Transformation to update the SQL Offset \u00a4 Each time we execute the transformation, we want to forward the OFFSET in our SQL Query to extract the next rows. As an example, we have a start OFFSET of 0, and LIMIT of 5. After one execution, we want to have an OFFSET of 5, after another execution, an OFFSET of 10 and so on. In this tutorial, we assume that the table contains an ID column which incrementally increases by 1 in each row. To store the updated OFFSET, we update the triple with a SPARQL Update query: Press the Create button (top right) in the data integration workspace and select the type Transformation Provide a Label Provide optionally a Description Paste the query into the SPARQL update query form. The following IRIs need to be adapted for your use cases: Service Metadata Graph JDBC endpoint jdbc_table_data_config Knowledge Graph http://ld.company.org/services/ This query will look for the last max service ID found in the Knowledge Graph, and update the OFFSET information in the Metadata Graph. Last Offset \u00a4 PREFIX service_metadata_graph : <http://di.eccenca.com/project/services/metadata> PREFIX jdbc_table_dataset_config : <http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC> PREFIX func : <https://vocab.eccenca.com/di/functions/param_Jdbc_> PREFIX prod : <http://ld.company.org/prod-vocab/> WITH service_metadata_graph : DELETE { jdbc_table_dataset_config : func : lastOffset ?lastOffset .} INSERT { jdbc_table_dataset_config : func : lastOffset ?newOffset . } WHERE { { SELECT ?lastOffset WHERE { GRAPH service_metadata_graph : { jdbc_table_dataset_config : func : lastOffset ?lastOffset } } } { SELECT ( max ( ?id ) as ?newOffset ) WHERE { GRAPH <http://ld.company.org/services/> { ?services a prod : Service . ?services prod : id ?id . } } } } Finally, we can build a Workflow which demonstrates how each step works. We compose the SQL query based on the OFFSET and LIMIT information in our Metadata Graph. This SQL query will be used to configure the sourceQuery of the JDBC endpoint. Next, we do a \u201cnormal\u201d transformation of data from a JDBC endpoint to RDF. As this step was omitted here, please feel free to read how this Transformation can be built here: Lift data from tabular data such as CSV, XSLX or database tables . As a final step, we use our SPARQL update query, to select the max service ID in our Knowledge Graph, and update the RDF Triples in our Metadata Graph accordingly.","title":"Loading JDBC datasets incrementally"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#loading-jdbc-datasets-incrementally","text":"","title":"Loading JDBC datasets incrementally"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#introduction","text":"This tutorial walks you through the process of loading data incrementally from a JDBC Dataset (relational database Table) into a Knowledge Graph. Info The complete tutorial is available as a project file . You can import this project: by using the web interface (Create \u2192 Project \u2192 Import project file) or by using the command line interface cmemc -c my-cmem project import tutorial-webapi.project.zip web-api Example SQL query for selecting a predefined range of rows We use the LIMIT and OFFSET clauses in SQL to retrieve only a portion of rows. This prevents loading all data at once from a table, SQL Query with OFFSET AND LIMIT: SELECT * FROM services OFFSET 0 LIMIT 5 This query retrieves the first 5 rows of a table named \u201cservices\u201d. If the OFFSET is changed to 5, it will retrieve the next 5 rows.","title":"Introduction"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#1-create-the-jdbc-dataset","text":"To extract data from a relational database, you need to first register a JDBC endpoint in Corporate Memory. This tutorial assumes that you have access to the relational database from the Corporate Memory instance. In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page. Click Create at the top of the page. In Create new item window, select Project and click Add. The Create new item of type Project window appears. In the Create new item window, select Dataset under Item Type , search for JDBC endpoint, and click Add . Provide the required configuration details for JDBC endpoint: Label : Provide a table name. Description: Optionally describe your table. JDBC Driver Connection URL: Provide the JDBC connection. In this tutorial, we use a MySQL database. The database server is named mysql and the database is named serviceDB . Table: Provide the name of the table in the database. Source query : Provide a default source query. In this tutorial, the source query will be changed later, as the OFFSET changes. Limit: Provide a LIMIT for the SQL query. In this tutorial, we choose 5 for demonstrating the functionality. You may select any value which works for your use case. Query strategy : Select: Execute the given source query. No paging or virtual Query. In this tutorial, this needs to be changed, such that when this JDBC endpoint is being used, Corporate Memory will always check for the Source Query which was provided earlier. User : Provide the user name which is allowed to access the database. Password : Provide the user password that is allowed to access the database.","title":"1 Create the JDBC dataset"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#2-create-a-metadata-graph","text":"To incrementally extract data in Corporate Memory, we need to store the information about the OFFSET, which will change with each extraction. For that, we need to define a new Graph, named Services Metadata Graph that will hold this information. To identify the changing OFFSET with the JDBC endpoint we previously created, we will use the Graph IRI that Corporate Memory created for us. To find the JDBC endpoint IRI Visit the Exploration Tab of Corporate Memory Select in Graph (top left) your project, which starts with \u201c CMEM DI Project \u2026 \u201d (if you can\u2019t see it, you might not have access to it, please contact in your case your administrator) Select in Navigation (bottom left): functions_Plugins_Jdbc Select the previously created JDBC endpoint (in our example: \u201cServices Table (JDBC)\u201d Press the Turtle tab inside your JDBC endpoint view (right) In our example, the JDBC Endpoint IRI looks like this: http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC See screenshot below for example: Now that we have the JDBC endpoint IRI, we will build the Metadata Graph to store the OFFSET information. The following three RDF triples hold the (minimal) necessary information we need for this tutorial: The first triple imports into our Metadata Graph the CMEM DI Project graph to have access to the LIMIT property defined and in case we need more in the future. The second triple defines a label for the Graph. The third triple defines the <\u2026 lastOffset > property, which we need for this tutorial. As a default, we set it to 0 to start with the first row in the table. services_metadata_graph <http : //di.eccenca.com/project/services/metadata> <http : //www.w3.org/2002/07/owl#imports> <http : //di.eccenca.com/project/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload> . # import the original project <http : //di.eccenca.com/project/services/metadata> <http : //www.w3.org/2000/01/rdf-schema#label> \"Services Metadata\"@en . # provide the graph with a label <http : //dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC> <https : //vocab.eccenca.com/di/functions/param_Jdbc_lastOffset> \"0\" . # set the initial offset to zero to start with the first row in the table For your project, please: adjust the CMEM DI Project IRI and the JDBC endpoint IRI. Import the Graph in the Exploration tab \u2192 Graph (menu) \u2192 Add new Graph \u2192 Provide Graph IRI + Select file In our example, we used the following Graph IRI for the Metadata Graph: http://di.eccenca.com/project/services/metadata","title":"2 Create a Metadata Graph"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#3-create-a-transformation-to-dynamically-compose-a-sql-query","text":"To extract rows based on the predefined (changing) OFFSET and LIMIT from a table, we need to create a Transformation to compose the SQL with each execution. Click Create (top right) in the data integration workspace and select the type Transformation . Provide a Label . Provide Description (Optional). Select the Services Metadata Graph we previously created. Create only a value mapping with the property sourceQuery . The sourceQuery will be used as an input for the JDBC endpoint. A root mapping does not need to be defined. In this screenshot, everything is already configured, your\u2019s will be empty when you create it for the first time. Press the circular pen button to jump into the advanced mapping editor. As source paths, we select the data from our Metadata Graph: table , lastOffset and limit . Everything else is defined as a constant, as it does not change in the query. For our source paths, we defined a \u201cdi\u201d prefix. In case that is missing, your source path may look longer (full IRI).","title":"3 Create a Transformation to dynamically compose a SQL Query"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#4-create-a-transformation-to-update-the-sql-offset","text":"Each time we execute the transformation, we want to forward the OFFSET in our SQL Query to extract the next rows. As an example, we have a start OFFSET of 0, and LIMIT of 5. After one execution, we want to have an OFFSET of 5, after another execution, an OFFSET of 10 and so on. In this tutorial, we assume that the table contains an ID column which incrementally increases by 1 in each row. To store the updated OFFSET, we update the triple with a SPARQL Update query: Press the Create button (top right) in the data integration workspace and select the type Transformation Provide a Label Provide optionally a Description Paste the query into the SPARQL update query form. The following IRIs need to be adapted for your use cases: Service Metadata Graph JDBC endpoint jdbc_table_data_config Knowledge Graph http://ld.company.org/services/ This query will look for the last max service ID found in the Knowledge Graph, and update the OFFSET information in the Metadata Graph.","title":"4 Create a Transformation to update the SQL Offset"},{"location":"build/workflow-reconfiguration/loading-jdbc-datasets-incrementally/#last-offset","text":"PREFIX service_metadata_graph : <http://di.eccenca.com/project/services/metadata> PREFIX jdbc_table_dataset_config : <http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC> PREFIX func : <https://vocab.eccenca.com/di/functions/param_Jdbc_> PREFIX prod : <http://ld.company.org/prod-vocab/> WITH service_metadata_graph : DELETE { jdbc_table_dataset_config : func : lastOffset ?lastOffset .} INSERT { jdbc_table_dataset_config : func : lastOffset ?newOffset . } WHERE { { SELECT ?lastOffset WHERE { GRAPH service_metadata_graph : { jdbc_table_dataset_config : func : lastOffset ?lastOffset } } } { SELECT ( max ( ?id ) as ?newOffset ) WHERE { GRAPH <http://ld.company.org/services/> { ?services a prod : Service . ?services prod : id ?id . } } } } Finally, we can build a Workflow which demonstrates how each step works. We compose the SQL query based on the OFFSET and LIMIT information in our Metadata Graph. This SQL query will be used to configure the sourceQuery of the JDBC endpoint. Next, we do a \u201cnormal\u201d transformation of data from a JDBC endpoint to RDF. As this step was omitted here, please feel free to read how this Transformation can be built here: Lift data from tabular data such as CSV, XSLX or database tables . As a final step, we use our SPARQL update query, to select the max service ID in our Knowledge Graph, and update the RDF Triples in our Metadata Graph accordingly.","title":"Last Offset"},{"location":"consume/","text":"\u2605 Consume \u00a4 This section outlines how to consume data from the Knowledge Graph. While there are several options to retrieve information from the Knowledge Graph, the most direct way is to issue SPARQL queries. SPARQL queries can be managed and executed in the Query Module UI . External applications may access the query catalog and execute queries through the REST API directly or more conveniently by using the cmemc - Command Line Interface . Since not all applications allow the direct use of SPARQL, this section includes tutorials to access the Knowledge Graph using BI tools (such as Power BI) as well as relational databases. Accessing Graphs with Java Applications \u2014 This short recipe covers how to connect to Corporate Memory using a Java program. cmemc - Command Line Interface \u2014 cmemc is intended for system administrators and Linked Data Expert, who wants to automate and remote control activities on eccenca Corporate Memory. Consuming Graphs in Power BI \u2014 Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector.","title":"\u2605 Consume"},{"location":"consume/#consume","text":"This section outlines how to consume data from the Knowledge Graph. While there are several options to retrieve information from the Knowledge Graph, the most direct way is to issue SPARQL queries. SPARQL queries can be managed and executed in the Query Module UI . External applications may access the query catalog and execute queries through the REST API directly or more conveniently by using the cmemc - Command Line Interface . Since not all applications allow the direct use of SPARQL, this section includes tutorials to access the Knowledge Graph using BI tools (such as Power BI) as well as relational databases. Accessing Graphs with Java Applications \u2014 This short recipe covers how to connect to Corporate Memory using a Java program. cmemc - Command Line Interface \u2014 cmemc is intended for system administrators and Linked Data Expert, who wants to automate and remote control activities on eccenca Corporate Memory. Consuming Graphs in Power BI \u2014 Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector.","title":"\u2605 Consume"},{"location":"consume/consuming-graphs-in-power-bi/","text":"Consuming Graphs in Power BI \u00a4 Introduction \u00a4 Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector. This manual and tutorial describes how you can consume data from your knowledge graph in Microsoft Power BI through our Corporate Memory Power-BI-Connector. Power BI is a business analytics service by Microsoft. It aims to provide interactive visualizations and business intelligence capabilities with an interface simple enough for end users to create their own reports and dashboards. Power BI can be obtained from the official Microsoft page and/or in the Windows Software Store. The latest (unsigned) version of our Power-BI-Connector is available from its source repository a version signed by eccenca is available with each Corporate Memory release. eccenca github.com repository (unsigned .mez file) eccenca Corporate Memory Releases (signed .pqx file) Thumbprint of the signature: FB6C562BD0B08107AAA420EDDE94507420C7FE1A Installation \u00a4 Download the .pqx or .mez file from the locations linked above. Move the file into the folder Documents\\Power BI Desktop\\Custom Connectors . Create the folder if it does not exist. In case you are running Windows on Parallels Desktop: Do not use the Local Disk\\Users\\UserName\\Documents folder but your shared folder with macOS. Register the Thumbprint (for .pqx) or setup PowerBI Desktop to allow any 3rd party connector (for .pqx or .mez) (we recommend to register the Thumbprint) Setup Register Thumbprint (.pqx) Allow 3rd Party Connectors (.mez and .pqx) In order to allow the eccenca Corporate Memory Power-BI-Connector in your Power BI Desktop installation you need to register the Thumbprint of the file signature in the windows registry. Cf. official Microsoft documentation The registry path is HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Power BI Desktop . Make sure the path exists, or create it. Add a new value under the path specified above. The type should be \u201cMulti-String Value\u201d ( REG_MULTI_SZ ), and it should be called TrustedCertificateThumbprints Add the thumbprints of the certificates you want to trust. You can add multiple certificates by using \u201c\\0\u201d as a delimiter, or in the registry editor, right click \u2192 modify and put each thumbprint on a new line. (Re-)Start Power BI Desktop If you wish to automate this setup you can use the reg windows command line tool to make this entry like: REM list existing entries in Power BI Desktop > TrustedCertificateThumbprints reg query \"\" HKEY_LOCAL_MACHINE \\ SOFTWARE \\ Policies \\ Microsoft \\ Power BI Desktop \" /v TrustedCertificateThumbprints REM add eccenca Corporate Memory Power-BI-Connector Thumbprint reg add \" HKEY_LOCAL_MACHINE \\ SOFTWARE \\ Policies \\ Microsoft \\ Power BI Desktop \" /v TrustedCertificateThumbprints /t REG_MULTI_SZ /d FB6C562BD0B08107AAA420EDDE94507420C7FE1A In case you are using the .mez (works for .pqx file too) file or simply want to trust any third party connector extension (Re-)Start Power BI Desktop, go to File \u2192 Options and settings \u2192 Options \u2192 Security Under Data Extensions, select (Not Recommended). Allow any extension to load without validation or warning. Select OK, and then restart Power BI Desktop. Add a Data Source \u00a4 Use the Power-BI-Connector to login with your Corporate Memory Instance: Open Power BI Desktop Click Edit Queries \u2192 New Source (or directly Get Data ) In the dialog search for eccenca Corporate Memory , which is listed in the Database category Select the connector and click Connect Read and accept the 3rd party connector notification In the following dialog you need to specify the connection and information and access credentials, ask your Corporate Memory administrator for assistance if you miss any of the requested details. You have the option to use username + password or a client secret for login. In case of a custom setup is used advanced configuration can be provided: Access-configuration Username + Password Client Advanced Configuration In order to use username + password based login you need to fill the details shown below: First Step Corporate Memory Base URI Grant type = password Client ID Second Step Password / Client Secret In order to use Client Secret based login you need to fill the details shown below: First Step Corporate Memory Base URI Grant type = client_credentials Client ID Second Step Password / Client Secret In case you installation uses a custom service endpoint layout the individual URIs for DataPlatform and Keycloak can be configured individually. The configuration keys are the same as for cmemc. The following configuration parameter can be provided: DP_API_ENDPOINT - specifies the DataPlatform URI OAUTH_TOKEN_URI - specifies the keycloak token URI SSL_VERIFY - can be used to set certificate verification to False In case a Corporate Memory Base URI is configured too, the values from the Config ini section take precedence Get Data \u00a4 With the eccenca Corporate Memory Power-BI-Connector you can load data from SELECT queries stored in the query catalog of Corporate Memory. You can use queries without or with placeholders. The steps are different depending if your query uses placeholder: Queries Without placeholders With placeholders SELECT queries that use no placeholders are shown with a table icon (e.g. ) When selected a preview will be loaded. Check the one(s) you want to load and click OK . The tables will be added to your list of queries and to the fields inventory in Power BI Start using your data in transformations, dashboards and analytics SELECT queries that take placeholder arguments are shown with a function icon. (e.g. ) You need to be in Edit Queries mode in Power BI so you can enter the required query parameter. Check the one(s) to be added. Power BI will add the selected query as a query entry. Click \u201cTransform Data\u201d in order to fill in the parameter. This adds a new entry to the list of Power BI queries, which contains the actual data you requested. The new entry will be named \u201cInvoked Function\u201d. It is recommended to rename this automatic generated name to a more speaking one. Right click on \u201cInvoked Function\u201d or select \u201cInvoked Function\u201d and press F2. Rename the table (e.g. to \u201c_search via regex match\u201d). Click \u201cClose & Apply\u201c to save changes. Hint You can call the function multi times with different parameter values to get different result tables into Power BI. Start using your data in transformations, dashboards and analytics","title":"Consuming Graphs in Power BI"},{"location":"consume/consuming-graphs-in-power-bi/#consuming-graphs-in-power-bi","text":"","title":"Consuming Graphs in Power BI"},{"location":"consume/consuming-graphs-in-power-bi/#introduction","text":"Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector. This manual and tutorial describes how you can consume data from your knowledge graph in Microsoft Power BI through our Corporate Memory Power-BI-Connector. Power BI is a business analytics service by Microsoft. It aims to provide interactive visualizations and business intelligence capabilities with an interface simple enough for end users to create their own reports and dashboards. Power BI can be obtained from the official Microsoft page and/or in the Windows Software Store. The latest (unsigned) version of our Power-BI-Connector is available from its source repository a version signed by eccenca is available with each Corporate Memory release. eccenca github.com repository (unsigned .mez file) eccenca Corporate Memory Releases (signed .pqx file) Thumbprint of the signature: FB6C562BD0B08107AAA420EDDE94507420C7FE1A","title":"Introduction"},{"location":"consume/consuming-graphs-in-power-bi/#installation","text":"Download the .pqx or .mez file from the locations linked above. Move the file into the folder Documents\\Power BI Desktop\\Custom Connectors . Create the folder if it does not exist. In case you are running Windows on Parallels Desktop: Do not use the Local Disk\\Users\\UserName\\Documents folder but your shared folder with macOS. Register the Thumbprint (for .pqx) or setup PowerBI Desktop to allow any 3rd party connector (for .pqx or .mez) (we recommend to register the Thumbprint) Setup Register Thumbprint (.pqx) Allow 3rd Party Connectors (.mez and .pqx) In order to allow the eccenca Corporate Memory Power-BI-Connector in your Power BI Desktop installation you need to register the Thumbprint of the file signature in the windows registry. Cf. official Microsoft documentation The registry path is HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Power BI Desktop . Make sure the path exists, or create it. Add a new value under the path specified above. The type should be \u201cMulti-String Value\u201d ( REG_MULTI_SZ ), and it should be called TrustedCertificateThumbprints Add the thumbprints of the certificates you want to trust. You can add multiple certificates by using \u201c\\0\u201d as a delimiter, or in the registry editor, right click \u2192 modify and put each thumbprint on a new line. (Re-)Start Power BI Desktop If you wish to automate this setup you can use the reg windows command line tool to make this entry like: REM list existing entries in Power BI Desktop > TrustedCertificateThumbprints reg query \"\" HKEY_LOCAL_MACHINE \\ SOFTWARE \\ Policies \\ Microsoft \\ Power BI Desktop \" /v TrustedCertificateThumbprints REM add eccenca Corporate Memory Power-BI-Connector Thumbprint reg add \" HKEY_LOCAL_MACHINE \\ SOFTWARE \\ Policies \\ Microsoft \\ Power BI Desktop \" /v TrustedCertificateThumbprints /t REG_MULTI_SZ /d FB6C562BD0B08107AAA420EDDE94507420C7FE1A In case you are using the .mez (works for .pqx file too) file or simply want to trust any third party connector extension (Re-)Start Power BI Desktop, go to File \u2192 Options and settings \u2192 Options \u2192 Security Under Data Extensions, select (Not Recommended). Allow any extension to load without validation or warning. Select OK, and then restart Power BI Desktop.","title":"Installation"},{"location":"consume/consuming-graphs-in-power-bi/#add-a-data-source","text":"Use the Power-BI-Connector to login with your Corporate Memory Instance: Open Power BI Desktop Click Edit Queries \u2192 New Source (or directly Get Data ) In the dialog search for eccenca Corporate Memory , which is listed in the Database category Select the connector and click Connect Read and accept the 3rd party connector notification In the following dialog you need to specify the connection and information and access credentials, ask your Corporate Memory administrator for assistance if you miss any of the requested details. You have the option to use username + password or a client secret for login. In case of a custom setup is used advanced configuration can be provided: Access-configuration Username + Password Client Advanced Configuration In order to use username + password based login you need to fill the details shown below: First Step Corporate Memory Base URI Grant type = password Client ID Second Step Password / Client Secret In order to use Client Secret based login you need to fill the details shown below: First Step Corporate Memory Base URI Grant type = client_credentials Client ID Second Step Password / Client Secret In case you installation uses a custom service endpoint layout the individual URIs for DataPlatform and Keycloak can be configured individually. The configuration keys are the same as for cmemc. The following configuration parameter can be provided: DP_API_ENDPOINT - specifies the DataPlatform URI OAUTH_TOKEN_URI - specifies the keycloak token URI SSL_VERIFY - can be used to set certificate verification to False In case a Corporate Memory Base URI is configured too, the values from the Config ini section take precedence","title":"Add a Data Source"},{"location":"consume/consuming-graphs-in-power-bi/#get-data","text":"With the eccenca Corporate Memory Power-BI-Connector you can load data from SELECT queries stored in the query catalog of Corporate Memory. You can use queries without or with placeholders. The steps are different depending if your query uses placeholder: Queries Without placeholders With placeholders SELECT queries that use no placeholders are shown with a table icon (e.g. ) When selected a preview will be loaded. Check the one(s) you want to load and click OK . The tables will be added to your list of queries and to the fields inventory in Power BI Start using your data in transformations, dashboards and analytics SELECT queries that take placeholder arguments are shown with a function icon. (e.g. ) You need to be in Edit Queries mode in Power BI so you can enter the required query parameter. Check the one(s) to be added. Power BI will add the selected query as a query entry. Click \u201cTransform Data\u201d in order to fill in the parameter. This adds a new entry to the list of Power BI queries, which contains the actual data you requested. The new entry will be named \u201cInvoked Function\u201d. It is recommended to rename this automatic generated name to a more speaking one. Right click on \u201cInvoked Function\u201d or select \u201cInvoked Function\u201d and press F2. Rename the table (e.g. to \u201c_search via regex match\u201d). Click \u201cClose & Apply\u201c to save changes. Hint You can call the function multi times with different parameter values to get different result tables into Power BI. Start using your data in transformations, dashboards and analytics","title":"Get Data"},{"location":"consume/consuming-graphs-with-sql-databases/","text":"Consuming Graphs with SQL Databases \u00a4 Introduction \u00a4 If direct access to the knowledge graph is not sufficient, fragments of the Knowledge Graph may also be pushed into external SQL databases. While in general all supported databases can also be written into, optimized writing support is available and packaged for MySQL and MariaDB. See the documentation of the JDBC dataset for more details. Writing a single table into a SQL database \u00a4 Three buildings blocks are required in eccenca DataIntegration to write into a remote SQL database: A dataset that allows access to the Knowledge Graph. A transformation that builds tables from specified resources in the Knowledge Graph. A dataset that configures access to the SQL database using JDBC. A simple workflow to write the contents of a Knowledge Graph into a SQL database looks like this: In the following, we have a more detailed look at each of the three operators. Create Knowledge Graph dataset \u00a4 Create a dataset of the type Knowledge Graph (embedded) and set the graph parameter to the URI of the graph that contains the resources to be exported: Create Transformation \u00a4 Create a transformation that covers the type of the RDF resources to be exported into a table: For each column of the target table, add a value mapping to the transformation: Basic Mapping The shown transformation will create a table with two columns: A column name that contains values of the property foaf:name . A column runtime that contains values of the property dbpediaow:runtime . Create SQL dataset \u00a4 Create a dataset of the type JDBC endpoint (remote): The most relevant parameters are: The JDBC Driver Connection URL should contain the database-specific JDBC URL. The table parameter defines the name of the table to be written. The write strategy specifies the behavior if the configured table already exists in the target SQL database. Writing multiple tables into a SQL database \u00a4 If multiple tables should be written from several type of resources, there are two options: If the types are connected by properties, a single transformation with multiple object mappings can be used. The root mapping will write the table specified by the SQL dataset. Each object mapping does write an additional table. The name of the table is generated based on the target type, which is defined in the object mapping. If the types are not directly connected by properties, multiple transformations can be created. Note that for each transformation, a separate target SQL dataset needs to be created, since the table name is specified in it.","title":"Consuming Graphs with SQL Databases"},{"location":"consume/consuming-graphs-with-sql-databases/#consuming-graphs-with-sql-databases","text":"","title":"Consuming Graphs with SQL Databases"},{"location":"consume/consuming-graphs-with-sql-databases/#introduction","text":"If direct access to the knowledge graph is not sufficient, fragments of the Knowledge Graph may also be pushed into external SQL databases. While in general all supported databases can also be written into, optimized writing support is available and packaged for MySQL and MariaDB. See the documentation of the JDBC dataset for more details.","title":"Introduction"},{"location":"consume/consuming-graphs-with-sql-databases/#writing-a-single-table-into-a-sql-database","text":"Three buildings blocks are required in eccenca DataIntegration to write into a remote SQL database: A dataset that allows access to the Knowledge Graph. A transformation that builds tables from specified resources in the Knowledge Graph. A dataset that configures access to the SQL database using JDBC. A simple workflow to write the contents of a Knowledge Graph into a SQL database looks like this: In the following, we have a more detailed look at each of the three operators.","title":"Writing a single table into a SQL database"},{"location":"consume/consuming-graphs-with-sql-databases/#create-knowledge-graph-dataset","text":"Create a dataset of the type Knowledge Graph (embedded) and set the graph parameter to the URI of the graph that contains the resources to be exported:","title":"Create Knowledge Graph dataset"},{"location":"consume/consuming-graphs-with-sql-databases/#create-transformation","text":"Create a transformation that covers the type of the RDF resources to be exported into a table: For each column of the target table, add a value mapping to the transformation: Basic Mapping The shown transformation will create a table with two columns: A column name that contains values of the property foaf:name . A column runtime that contains values of the property dbpediaow:runtime .","title":"Create Transformation"},{"location":"consume/consuming-graphs-with-sql-databases/#create-sql-dataset","text":"Create a dataset of the type JDBC endpoint (remote): The most relevant parameters are: The JDBC Driver Connection URL should contain the database-specific JDBC URL. The table parameter defines the name of the table to be written. The write strategy specifies the behavior if the configured table already exists in the target SQL database.","title":"Create SQL dataset"},{"location":"consume/consuming-graphs-with-sql-databases/#writing-multiple-tables-into-a-sql-database","text":"If multiple tables should be written from several type of resources, there are two options: If the types are connected by properties, a single transformation with multiple object mappings can be used. The root mapping will write the table specified by the SQL dataset. Each object mapping does write an additional table. The name of the table is generated based on the target type, which is defined in the object mapping. If the types are not directly connected by properties, multiple transformations can be created. Note that for each transformation, a separate target SQL dataset needs to be created, since the table name is specified in it.","title":"Writing multiple tables into a SQL database"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/","text":"Provide data in any format via a custom API \u00a4 Introduction \u00a4 Learn how to provide data via a customized Corporate Memory API in a text format of your choice and how to consume it in your applications. This tutorial describes how you can provide data in a text format of your choice via your own custom Corporate Memory API, and how you request those APIs. As an example, we describe how you can set-up an endpoint which provides iCalendar data. If you want to rebuild the example, you can download this iCalendar RDF data and import it into your Corporate Memory instance: ical_data.ttl iCalendar Event data 1 2 3 4 5 6 7 8 9 10 11 BEGIN:VCALENDAR VERSION:2.0 PRODID:-//hacksw/handcal//NONSGML v1.0//EN BEGIN:VEVENT UID:20020630T230353Z-3895-69-1-0@jammer DTSTAMP:20020630T230353Z DTSTART:20020630T090000Z DTEND:20020630T103000Z SUMMARY:Church END:VEVENT END:VCALENDAR Define a SPARQL query \u00a4 This query selects the event data in our graph which will be provided via the customized API. To rebuild the iCalendar format, we need at least the unique identifier (uid), the datetime start (dtstart), the datetime end (dtend), and the summary of the event. The query filters (replace) at the end the special characters \u201c:\u201d and \u201c-\u201c, as they are not needed in the iCal DateTime format. Sample iCalendar SPARQL Query 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PREFIX ical : < http : // www . w3 . org / 2002 / 12 / cal / icaltzd #> SELECT DISTINCT ? vevent ? uid ? dtstamp ? dtstart ? dtend ? summary WHERE { ? vevent a ical : Vevent . ? vevent ical : uid ? uid . ? vevent ical : dtstamp ? dtstamp_raw . ? vevent ical : dtstart ? dtstart_raw . ? vevent ical : dtend ? dtend_raw . ? vevent ical : summary ? summary . BIND ( REPLACE ( STR ( ? dtstamp_raw ), \"[: -]\" , \"\" ) AS ? dtstamp ) . BIND ( REPLACE ( STR ( ? dtstart_raw ), \"[: -]\" , \"\" ) AS ? dtstart ) . BIND ( REPLACE ( STR ( ? dtend_raw ), \"[: -]\" , \"\" ) AS ? dtend ) . } Define a Template for the iCal format \u00a4 As a next step, we will define a template which generates iCalendar data from our previously defined SPARQL query. Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Result Template and click Create a new Select Result Template to create a new one. Define a Name , a Description and the Body format. You may also define a Header or a Footer, but this is not necessary for this example. The template engine we are using Jinja . In Jinja, dynamic data within a template needs to be referenced via double curly brackets {{\u2026}}. So the line {{result.uid}} inserts at execution time the ?uid value from our previously defined SPARQL query into this template. Everything outside curly brackets it static. As static data in our example, we define the full iCalendar format (..BEGIN:EVENT..). As we receive from the SPARQL query multiple results (iCalendar Events), we have to iterate through each of them. To define this iteration in the template, the following line needs to be added: {% for result in results %} and for the conclusion of the iteration, this line needs to be added at the end: {% endfor %} Jira Template for our iCalendar format 1 2 3 4 5 6 7 8 9 10 11 12 13 14 BEGIN:VCALENDAR VERSION:2.0 PRODID:-//hacksw/handcal//NONSGML v1.0//EN { % for result in results % } BEGIN:VEVENT UID: {{ result.uid }} DTSTAMP: {{ result.dtstamp }} DTSTART: {{ result.dtstart }} DTEND: {{ result.dtend }} SUMMARY: {{ result.summary }} END:VEVENT { % endfor % } END:VCALENDAR Create an API based on your template \u00a4 As a next step, we will set-up the API which serves the data in the format you defined in the previous template. Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Query Endpoint and click \u201cCreate a new Select Query Endpoint\u201d to create a new one. Define a Name, a human-readable keyword (aka URL Slug ) for the API path, choose if it is a Streaming endpoint (false in our example), a Description, select the defined SPARQL Query from our first step and select the Template we created in the second step. Once you press save, your endpoint it set up! Consume data via the endpoint \u00a4 Now that the endpoint is defined, it is possible to make a request to receive the iCal data. The endpoint URL consist of the path /dataplatform/api/custom and the previously defined URL Slug (/ical). /ical request curl curl 'https://<cmem_instance>/dataplatform/api/custom/ical' -H 'Authorization: Bearer <token>' The <token> can be fetched by: cmemc admin token /ical response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 BEGIN:VCALENDAR VERSION:2.0 PRODID:-//hacksw/handcal//NONSGML v1.0//EN BEGIN:VEVENT UID:20020630T230353Z-3895-69-1-0@jammer DTSTAMP:20020630T230353Z DTSTART:20020630T090000Z DTEND:20020630T103000Z SUMMARY:Church END:VEVENT BEGIN:VEVENT UID:20020630T230445Z-3895-69-1-7@jammer DTSTAMP:20020630T230445Z DTSTART:20020703 DTEND:20020706 SUMMARY:Scooby Conference END:VEVENT BEGIN:VEVENT UID:20020630T230600Z-3895-69-1-16@jammer DTSTAMP:20020630T230600Z DTSTART:20020718T090000 DTEND:20020718T093000 SUMMARY:Federal Reserve Board Meeting END:VEVENT END:VCALENDAR This result represent as valid ICalendar (ics) format and can be imported into your calendar client. event_data.ics Configuration remarks \u00a4 Streaming \u00a4 If Is Streaming is set to false for the endpoint (as in the given example) the respective Jinja Template needs to resolve a results variable, which is a list of all query results and you need to iterate over the variable using Jinja constructs {% for result in results %} . A non-streaming result set (the SPARQL query) is limited to 1000 elements. If more results are expected Is Streaming should be set to true. If Is Streaming is set to true the Jinja Template has to resolve a result variable (without the \u2018s\u2019), which is a single query result and the template engine iterates over the results, i.e. the Body template is repeated for each query result.","title":"Provide data in any format via a custom API"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#provide-data-in-any-format-via-a-custom-api","text":"","title":"Provide data in any format via a custom API"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#introduction","text":"Learn how to provide data via a customized Corporate Memory API in a text format of your choice and how to consume it in your applications. This tutorial describes how you can provide data in a text format of your choice via your own custom Corporate Memory API, and how you request those APIs. As an example, we describe how you can set-up an endpoint which provides iCalendar data. If you want to rebuild the example, you can download this iCalendar RDF data and import it into your Corporate Memory instance: ical_data.ttl iCalendar Event data 1 2 3 4 5 6 7 8 9 10 11 BEGIN:VCALENDAR VERSION:2.0 PRODID:-//hacksw/handcal//NONSGML v1.0//EN BEGIN:VEVENT UID:20020630T230353Z-3895-69-1-0@jammer DTSTAMP:20020630T230353Z DTSTART:20020630T090000Z DTEND:20020630T103000Z SUMMARY:Church END:VEVENT END:VCALENDAR","title":"Introduction"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#define-a-sparql-query","text":"This query selects the event data in our graph which will be provided via the customized API. To rebuild the iCalendar format, we need at least the unique identifier (uid), the datetime start (dtstart), the datetime end (dtend), and the summary of the event. The query filters (replace) at the end the special characters \u201c:\u201d and \u201c-\u201c, as they are not needed in the iCal DateTime format. Sample iCalendar SPARQL Query 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PREFIX ical : < http : // www . w3 . org / 2002 / 12 / cal / icaltzd #> SELECT DISTINCT ? vevent ? uid ? dtstamp ? dtstart ? dtend ? summary WHERE { ? vevent a ical : Vevent . ? vevent ical : uid ? uid . ? vevent ical : dtstamp ? dtstamp_raw . ? vevent ical : dtstart ? dtstart_raw . ? vevent ical : dtend ? dtend_raw . ? vevent ical : summary ? summary . BIND ( REPLACE ( STR ( ? dtstamp_raw ), \"[: -]\" , \"\" ) AS ? dtstamp ) . BIND ( REPLACE ( STR ( ? dtstart_raw ), \"[: -]\" , \"\" ) AS ? dtstart ) . BIND ( REPLACE ( STR ( ? dtend_raw ), \"[: -]\" , \"\" ) AS ? dtend ) . }","title":"Define a SPARQL query"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#define-a-template-for-the-ical-format","text":"As a next step, we will define a template which generates iCalendar data from our previously defined SPARQL query. Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Result Template and click Create a new Select Result Template to create a new one. Define a Name , a Description and the Body format. You may also define a Header or a Footer, but this is not necessary for this example. The template engine we are using Jinja . In Jinja, dynamic data within a template needs to be referenced via double curly brackets {{\u2026}}. So the line {{result.uid}} inserts at execution time the ?uid value from our previously defined SPARQL query into this template. Everything outside curly brackets it static. As static data in our example, we define the full iCalendar format (..BEGIN:EVENT..). As we receive from the SPARQL query multiple results (iCalendar Events), we have to iterate through each of them. To define this iteration in the template, the following line needs to be added: {% for result in results %} and for the conclusion of the iteration, this line needs to be added at the end: {% endfor %} Jira Template for our iCalendar format 1 2 3 4 5 6 7 8 9 10 11 12 13 14 BEGIN:VCALENDAR VERSION:2.0 PRODID:-//hacksw/handcal//NONSGML v1.0//EN { % for result in results % } BEGIN:VEVENT UID: {{ result.uid }} DTSTAMP: {{ result.dtstamp }} DTSTART: {{ result.dtstart }} DTEND: {{ result.dtend }} SUMMARY: {{ result.summary }} END:VEVENT { % endfor % } END:VCALENDAR","title":"Define a Template for the iCal format"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#create-an-api-based-on-your-template","text":"As a next step, we will set-up the API which serves the data in the format you defined in the previous template. Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Query Endpoint and click \u201cCreate a new Select Query Endpoint\u201d to create a new one. Define a Name, a human-readable keyword (aka URL Slug ) for the API path, choose if it is a Streaming endpoint (false in our example), a Description, select the defined SPARQL Query from our first step and select the Template we created in the second step. Once you press save, your endpoint it set up!","title":"Create an API based on your template"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#consume-data-via-the-endpoint","text":"Now that the endpoint is defined, it is possible to make a request to receive the iCal data. The endpoint URL consist of the path /dataplatform/api/custom and the previously defined URL Slug (/ical). /ical request curl curl 'https://<cmem_instance>/dataplatform/api/custom/ical' -H 'Authorization: Bearer <token>' The <token> can be fetched by: cmemc admin token /ical response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 BEGIN:VCALENDAR VERSION:2.0 PRODID:-//hacksw/handcal//NONSGML v1.0//EN BEGIN:VEVENT UID:20020630T230353Z-3895-69-1-0@jammer DTSTAMP:20020630T230353Z DTSTART:20020630T090000Z DTEND:20020630T103000Z SUMMARY:Church END:VEVENT BEGIN:VEVENT UID:20020630T230445Z-3895-69-1-7@jammer DTSTAMP:20020630T230445Z DTSTART:20020703 DTEND:20020706 SUMMARY:Scooby Conference END:VEVENT BEGIN:VEVENT UID:20020630T230600Z-3895-69-1-16@jammer DTSTAMP:20020630T230600Z DTSTART:20020718T090000 DTEND:20020718T093000 SUMMARY:Federal Reserve Board Meeting END:VEVENT END:VCALENDAR This result represent as valid ICalendar (ics) format and can be imported into your calendar client. event_data.ics","title":"Consume data via the endpoint"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#configuration-remarks","text":"","title":"Configuration remarks"},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#streaming","text":"If Is Streaming is set to false for the endpoint (as in the given example) the respective Jinja Template needs to resolve a results variable, which is a list of all query results and you need to iterate over the variable using Jinja constructs {% for result in results %} . A non-streaming result set (the SPARQL query) is limited to 1000 elements. If more results are expected Is Streaming should be set to true. If Is Streaming is set to true the Jinja Template has to resolve a result variable (without the \u2018s\u2019), which is a single query result and the template engine iterates over the results, i.e. the Body template is repeated for each query result.","title":"Streaming"},{"location":"deploy-and-configure/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/access-conditions/","text":"Access Conditions \u00a4 Introduction \u00a4 The Access control module shows the list of all access conditions manageable by your account. Access conditions specify access rights for users and groups to graphs and actions. To open the Access control, open the menu in the Module bar and click Access control. Access conditions \u00a4 The main window shows the list of all access conditions manageable by your account. Access conditions specify access rights for users and groups to graphs and actions. Click a specific condition to get an expanded view with more details. Expanded view of an access condition Adding a new condition \u00a4 To add a new access condition: Click the context menu in the upper right Select \u201cCreate access condition\u201d In the dialog box, enter the new access condition rule as needed. Each access condition can have a Name and a Description as well as conditions and grants for actions and graphs. Note The application uses a set of specific URIs with a precise meaning as listed below: Resource Explanation urn:elds-backend-all-graphs Represents all RDF named graphs. You can use it in the Allow reading graph or Allow writing graph field. urn:elds-backend-all-actions Represents all actions. You can use it in the Allowed actions field. urn:elds-backend-public-group Represents the group which every user is member of (incl. anonymous users). You can use it in the Requires group field. urn:elds-backend-anonymous-user Represents the anonymous user account. You can use it in the Requires account field. urn:elds-backend-actions-revision-api Represents the Revision API (see the Developer Manual). You can use it in the Allowed actions field. urn:elds-backend-actions-auth-access-control Represents the Authorization management API (see the Developer Manual). You can use it in the Allowed actions field. urn:eccenca:di Represents the action needed to use eccenca DataIntegration component of eccenca Corporate Memory. You can use it in the Allowed actions field. urn:eccenca:ThesaurusUserInterface Represents the action needed to use the Thesaurus Catalog as well as Thesaurus Project editing interface (needs access to specific thesaurus graphs as well). You can use it in the Allowed actions field. urn:eccenca:AccessInternalGraphs Represents the action needed to list Corporate Memory Internal graphs in the exploration tab. You can use it in the Allowed actions field. urn:eccenca:QueryUserInterface Represents the action needed to use the Query Catalog (needs access to catalog graph as well if changes should be allowed). You can use it in the Allowed actions field. urn:eccenca:VocabularyUserInterface Represents the action needed to use the Vocabulary Catalog (needs access to specific vocabulary graphs as well). You can use it in the Allowed actions field. urn:eccenca:ExploreUserInterface Represents the action needed to use the Explore Tab (needs access to at least one graph as well). You can use it in the Allowed actions field. Click CREATE to create the new condition or abort your action with CANCEL. Create a new condition Edit an existing condition \u00a4 In the expanded view of an access condition, click DELETE to remove the access condition or EDIT to apply changes. For changing a condition refer to Access Conditions . Click on SAVE to apply your changes or discard them with CANCEL Adding Access Conditions from Command Line \u00a4 Additional to the Access Control module you can add the access conditions directly to the triple store. In this case, the access conditions need to be defined in an RDF file, for example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix eccurn: <urn:eccenca:> . @prefix ecc: <http://eccenca.com/> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix eccauth: <https://vocab.eccenca.com/auth/> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix xsd: <http://www.w3.org/2001/XMLSchema#> . ecc : f7f3b9c5-4e94-4e7f-a14b-15e39ae046ed dcterms : created \"2020-06-12T11:10:19Z\" ^^ xsd : dateTime ; dcterms : creator ecc : admin ; a eccauth : AccessCondition ; rdfs : comment \"single access condition which describes all rights for users in the local-admins group\" ; rdfs : label \"Active: access rights of all users of the local-admins group\" ; eccauth : allowedAction eccurn : AccessInternalGraphs , eccurn : QueryUserInterface , eccurn : ThesaurusUserInterface , eccurn : VocabularyUserInterface , eccurn : di , <urn:elds-backend-actions-auth-access-control> ; eccauth : requiresGroup ecc : local-admins ; eccauth : writeGraph <urn:elds-backend-all-graphs> . ecc : 5318ffd4-4ca7-46bb-8e0c-8a910376c6b9 dcterms : created \"2020-06-12T11:10:32Z\" ^^ xsd : dateTime ; dcterms : creator ecc : admin ; a eccauth : AccessCondition ; rdfs : comment \"single access conditions which describes the rights of users from the local-users group\" ; rdfs : label \"Active: access rights of all users of the local-users group\" ; eccauth : allowedAction eccurn : ExploreUserInterface , eccurn : QueryUserInterface , eccurn : ThesaurusUserInterface , eccurn : VocabularyUserInterface , eccurn : di ; eccauth : requiresGroup ecc : local-users ; eccauth : writeGraph <urn:elds-backend-all-graphs> . In this example, we have listed default access conditions for the docker-compose based orchestration . The file defines two access conditions. The allowedActions correspond to the URIs listed in the table above. Both access conditions allow the users to write to all graphs. You can define several access conditions for the same group. When you are finished with creating your access conditions RDF file, you can add it directly to the urn:elds-backend-access-conditions-graph graph (defined in DataPlatform configuration ). With cmemc , you can do this with the following command line: 1 2 $ cmemc -c my-cmem-instance graph import --replace access-conditions.ttl urn:elds-backend-access-conditions-graph Import graph 1 /1: urn:elds-backend-access-conditions-graph from access-conditions.ttl ... done","title":"Access Conditions"},{"location":"deploy-and-configure/configuration/access-conditions/#access-conditions","text":"","title":"Access Conditions"},{"location":"deploy-and-configure/configuration/access-conditions/#introduction","text":"The Access control module shows the list of all access conditions manageable by your account. Access conditions specify access rights for users and groups to graphs and actions. To open the Access control, open the menu in the Module bar and click Access control.","title":"Introduction"},{"location":"deploy-and-configure/configuration/access-conditions/#access-conditions_1","text":"The main window shows the list of all access conditions manageable by your account. Access conditions specify access rights for users and groups to graphs and actions. Click a specific condition to get an expanded view with more details. Expanded view of an access condition","title":"Access conditions"},{"location":"deploy-and-configure/configuration/access-conditions/#adding-a-new-condition","text":"To add a new access condition: Click the context menu in the upper right Select \u201cCreate access condition\u201d In the dialog box, enter the new access condition rule as needed. Each access condition can have a Name and a Description as well as conditions and grants for actions and graphs. Note The application uses a set of specific URIs with a precise meaning as listed below: Resource Explanation urn:elds-backend-all-graphs Represents all RDF named graphs. You can use it in the Allow reading graph or Allow writing graph field. urn:elds-backend-all-actions Represents all actions. You can use it in the Allowed actions field. urn:elds-backend-public-group Represents the group which every user is member of (incl. anonymous users). You can use it in the Requires group field. urn:elds-backend-anonymous-user Represents the anonymous user account. You can use it in the Requires account field. urn:elds-backend-actions-revision-api Represents the Revision API (see the Developer Manual). You can use it in the Allowed actions field. urn:elds-backend-actions-auth-access-control Represents the Authorization management API (see the Developer Manual). You can use it in the Allowed actions field. urn:eccenca:di Represents the action needed to use eccenca DataIntegration component of eccenca Corporate Memory. You can use it in the Allowed actions field. urn:eccenca:ThesaurusUserInterface Represents the action needed to use the Thesaurus Catalog as well as Thesaurus Project editing interface (needs access to specific thesaurus graphs as well). You can use it in the Allowed actions field. urn:eccenca:AccessInternalGraphs Represents the action needed to list Corporate Memory Internal graphs in the exploration tab. You can use it in the Allowed actions field. urn:eccenca:QueryUserInterface Represents the action needed to use the Query Catalog (needs access to catalog graph as well if changes should be allowed). You can use it in the Allowed actions field. urn:eccenca:VocabularyUserInterface Represents the action needed to use the Vocabulary Catalog (needs access to specific vocabulary graphs as well). You can use it in the Allowed actions field. urn:eccenca:ExploreUserInterface Represents the action needed to use the Explore Tab (needs access to at least one graph as well). You can use it in the Allowed actions field. Click CREATE to create the new condition or abort your action with CANCEL. Create a new condition","title":"Adding a new condition"},{"location":"deploy-and-configure/configuration/access-conditions/#edit-an-existing-condition","text":"In the expanded view of an access condition, click DELETE to remove the access condition or EDIT to apply changes. For changing a condition refer to Access Conditions . Click on SAVE to apply your changes or discard them with CANCEL","title":"Edit an existing condition"},{"location":"deploy-and-configure/configuration/access-conditions/#adding-access-conditions-from-command-line","text":"Additional to the Access Control module you can add the access conditions directly to the triple store. In this case, the access conditions need to be defined in an RDF file, for example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix eccurn: <urn:eccenca:> . @prefix ecc: <http://eccenca.com/> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix eccauth: <https://vocab.eccenca.com/auth/> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix xsd: <http://www.w3.org/2001/XMLSchema#> . ecc : f7f3b9c5-4e94-4e7f-a14b-15e39ae046ed dcterms : created \"2020-06-12T11:10:19Z\" ^^ xsd : dateTime ; dcterms : creator ecc : admin ; a eccauth : AccessCondition ; rdfs : comment \"single access condition which describes all rights for users in the local-admins group\" ; rdfs : label \"Active: access rights of all users of the local-admins group\" ; eccauth : allowedAction eccurn : AccessInternalGraphs , eccurn : QueryUserInterface , eccurn : ThesaurusUserInterface , eccurn : VocabularyUserInterface , eccurn : di , <urn:elds-backend-actions-auth-access-control> ; eccauth : requiresGroup ecc : local-admins ; eccauth : writeGraph <urn:elds-backend-all-graphs> . ecc : 5318ffd4-4ca7-46bb-8e0c-8a910376c6b9 dcterms : created \"2020-06-12T11:10:32Z\" ^^ xsd : dateTime ; dcterms : creator ecc : admin ; a eccauth : AccessCondition ; rdfs : comment \"single access conditions which describes the rights of users from the local-users group\" ; rdfs : label \"Active: access rights of all users of the local-users group\" ; eccauth : allowedAction eccurn : ExploreUserInterface , eccurn : QueryUserInterface , eccurn : ThesaurusUserInterface , eccurn : VocabularyUserInterface , eccurn : di ; eccauth : requiresGroup ecc : local-users ; eccauth : writeGraph <urn:elds-backend-all-graphs> . In this example, we have listed default access conditions for the docker-compose based orchestration . The file defines two access conditions. The allowedActions correspond to the URIs listed in the table above. Both access conditions allow the users to write to all graphs. You can define several access conditions for the same group. When you are finished with creating your access conditions RDF file, you can add it directly to the urn:elds-backend-access-conditions-graph graph (defined in DataPlatform configuration ). With cmemc , you can do this with the following command line: 1 2 $ cmemc -c my-cmem-instance graph import --replace access-conditions.ttl urn:elds-backend-access-conditions-graph Import graph 1 /1: urn:elds-backend-access-conditions-graph from access-conditions.ttl ... done","title":"Adding Access Conditions from Command Line"},{"location":"deploy-and-configure/configuration/dataintegration/","text":"DataIntegration \u00a4 This section is intended to be a reference for all available eccenca DataIntegration configuration options.The configuration format is based on HOCON . The following sections introduce the most important configuration parameters. The entire list of available configuration parameters can be found in the dataintegration.conf file found in the release package. OAuth \u00a4 Authorization in eccenca DataIntegration is based on OAuth. Typically the eccenca DataPlatform is used as an OAuth endpoint, but external endpoints can be used as well. The Authorization Code Grant workflow is used for retrieving the OAuth token. The default configuration is the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # The URL of the eccenca DataPlatform. eccencaDataPlatform.url = \"http://localhost:9090\" # Use OAuth for authentification against DataPlatform eccencaDataPlatform.oauth = false # Enable if user information should be fetched via DP and OAuth. Only uncomment if OAuth is enabled and DP is configured. user.manager.web.plugin = oauthUserManager # The DataPlatform endpoint that is used for authentification eccencaDataPlatform.endpointId = \"default\" # Define the protocol used for accessing the workbench (http or https), defaults to http workbench.protocol = \"http\" # Optional parameter for specifying the host. workbench.host = \"localhost:9090\" # The URL to redirect to after logout. # If not set, the user will be redirected to the internal logout page. oauth.logoutRedirectUrl = \"http://localhost:9090/loggedOut\" # The OAuth client that will be used to load the workspace initially and run the schedulers. workbench.superuser.client = \"elds\" workbench.superuser.clientSecret = \"elds\" # Optional parameter for specifying an alternative OAuth authorization endpoint. # If not specified, the default OAuth authorization endpoint of the specified eccenca Platform URL is used. # Note that if the eccenca Platform URL is internal and not accessible for the user, a public authorization URL must be set here. oauth.authorizationUrl = \"http://localhost:9090/oauth/authorize\" # Optional parameter for specifying an alternative OAuth authorization endpoint. oauth.tokenUrl = \"http://localhost:9090/oauth/token\" # Optional parameter for specifying an alternative OAuth client ID. oauth.clientId = \"eldsClient\" # Optional parameter for specifying an alternative OAuth client secret. oauth.clientSecret = \"secret\" # Additional request parameters to append to all OAuth authentication requests. # oauth.requestParameters = \"&resource=value\" DataPlatform configuration \u00a4 eccenca DataIntegration can only be run by OAuth users that are granted the urn:eccenca:di action by the DataPlatform. An example configuration that grants all users from a predefined group dataIntegrators access to DataIntegration is shown in the following: 1 2 3 4 5 6 7 8 @prefix eccauth: <https://vocab.eccenca.com/auth/> . @prefix : <http://eccenca.com/> . <urn:readAndWriteAllGraphs> a eccauth : AccessCondition ; eccauth : requiresGroup : dataIntegrators ; eccauth : allowedAction <urn:eccenca:di> ; eccauth : readGraph <urn:elds-backend-all-graphs> ; eccauth : writeGraph <urn:elds-backend-all-graphs> . In the shown configuration, the users also get access to all graphs in the RDF store. This is not a requirement for working with DataIntegration. Access may also be restricted to graphs that the data integration users are allowed to work with. Note The urn:elds-backend-all-actions action set also includes the urn:eccenca:di action, i.e., users that are granted urn:elds-backend-all-actions also get access to eccencca DataIntegration. In order to activate OAuth using the eccenca DataPlatform, the following minimal configuration is required: 1 2 3 4 eccencaDataPlatform.url = \"http://localhost:9090\" eccencaDataPlatform.oauth = true oauth.clientId = \"eldsClient\" oauth.clientSecret = \"secret\" Super User \u00a4 By default, the workspace is loaded the first time a user opens eccenca DataIntegration in the browser using their credentials. Any scheduler that is part of a project must be started manually. By configuring a super user, the workspace will be loaded at startup. After loading, all schedulers will be started using the credentials of the super user. In addition to the configuration of the eccencaDataPlatform according to the previous section, a super user is configured by specifying the following two parameters: 1 2 workbench.superuser.client = \"superUserClient\" workbench.superuser.clientSecret = \"superUserClientSecret\" Note The client credentials grant type is used to retrieve a token for the super user. Note that the schedulers are only started automatically when running in production mode. Workspace Providers \u00a4 The backend that holds the workspace can be configured using the workspace.provider.plugin parameter in dataintegration.conf 1 workspace.provider.plugin = <workspace-provider-plugin-name> The following sections describe the available workspace provider plugins and how they are configured. RDF-store Workspace - backend \u00a4 When running in Corporate Memory, by default the workspace is held in the RDF store configured in the eccenca DataPlatform. The workspace is held using the eccenca DataPlatform, i.e., it requires the eccencaDataPlatform.url parameter to be configured. This workspace can be configured using the following parameter: Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ cacheDir String Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start. <empty> By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true. The corresponding configuration in your dataintegration.conf looks like the following: eccencaDataPlatform.url = <DATAPLATFORM_URL> ... workspace.provider.plugin = backend ... workspace.provider.backend = { # Load prefixes defined by all known vocabularies. loadAllVocabularyPrefixes = false # Load prefixes defined by vocabularies that are actually loaded in the RDF store. loadInstalledVocabularyPrefixes = true # The graph that contains the vocabulary meta data. vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\" # Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start. cacheDir = \"\" } File-based Workspace - file \u00a4 The workspace can also be held on the filesystem. This workspace can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the workspace is persisted. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 workspace.provider.plugin = file ... workspace.provider.file = { # The directory to which the workspace is persisted. dir = ${user.home}\"/myWorkspace\" } Hybrid workspace - fileAndDataPlatform \u00a4 The so called hybrid workspace holds the workspace in the filesystem and in eccenca DataPlatform simultaneously. Each time a task is updated, it is written to both the filesystem and to the project RDF graph. In addition, on each (re)load of the Workspace, the contents of the file based workspace are pushed to the RDF store, to make sure that both workspace backends stay synchronized. Contents of the file system may be changed manually (reload the workspace afterwards). Contents of the RDF store are supposed to be read only and will be overwritten on reload. The workspace is held using the eccenca DataPlatform, i.e., it requires the eccencaDataPlatform.url parameter to be configured. This workspace can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the workspace is persisted. no default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ failOnDataPlatformError boolean If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning. false By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true. The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 eccencaDataPlatform.url = <DATAPLATFORM_URL> ... workspace.provider.plugin = fileAndDataPlatform ... workspace.provider.fileAndDataPlatform = { # The directory to which the workspace is persisted. dir = ${user.home}\"/myWorkspace\" # Load prefixes defined by all known vocabularies. loadAllVocabularyPrefixes = false # Load prefixes defined by vocabularies that are actually loaded in the RDF store. loadInstalledVocabularyPrefixes = true # The graph that contains the vocabulary meta data. vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\" # If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning. failOnDataPlatformError = false } In-memory Workspace - inMemory \u00a4 A workspace provider that holds all projects in memory. All contents will be gone on restart. The corresponding configuration in your dataintegration.conf looks like the following: 1 workspace.provider.plugin = inMemory In-memory RDF Workspace - inMemoryRdfWorkspace \u00a4 A workspace that is held in a in-memory RDF store and loses all its content on restart (mainly used for testing). Needed if operators are used that require an RDF store backend. This workspace can be configured using the following parameter: Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true. The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 9 10 workspace.provider.plugin = inMemoryRdfWorkspace ... workspace.provider.inMemoryRdfWorkspace = { # Load prefixes defined by all known vocabularies. loadAllVocabularyPrefixes = false # Load prefixes defined by vocabularies that are actually loaded in the RDF store. loadInstalledVocabularyPrefixes = true # The graph that contains the vocabulary meta data. vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\" } Resource Repositories \u00a4 Project resources are held by a resource repository which is configured using the workspace.repository.plugin parameter in dataintegration.conf 1 workspace.repository.plugin = <resource-repository-plugin-name> The following sections describe the available resource repository plugins and how they are configured. Project Specific Directories - projectFile \u00a4 By default, resources are held in project specific directories. This plugin can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the resources are persisted. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 workspace.repository.plugin = projectFile ... workspace.repository.projectFile = { dir = ${elds.home}\"/var/dataintegration/workspace/\" } Shared Directory - file \u00a4 Alternatively, all resources across all DataIntegration projects can be held in a single directory on the file system. This plugin can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the resources are persisted. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 workspace.repository.plugin = file ... workspace.repository.file = { dir = ${elds.home}\"/var/dataintegration/resources/\" } HDFS resources - hdfs \u00a4 Holds all resources on the HDFS file system. This plugin can be configured using the following parameter: Parameter Type Description Default path String The directory to which the resources are persisted. no default user String The hadoop user. hadoopuser The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 workspace.repository.plugin = hdfs ... workspace.repository.hdfs = { # The directory to which the resources are persisted. dir = \"/data/hdfs-datalake/\" # The hadoop user. user = \"hadoopuser\" } S3 Bucket, Project Specific - projectS3 \u00a4 In addition to storing files in the local filesystem an AWS S3 bucket can be used as the resource repository backend. To use resources stored on S3 the AWS keyID , secretKey need to be configured in the dataintegration.conf file. This and the region are used to connect to S3. Further, one bucket name has to be given. This is analog to the root folder of filesystem based resource repositories. This plugin can be configured using the following parameter: Parameter Type Description Default bucket String The S3 bucket name. no default accessKeyId String The S3 access key ID. no default secretKey String The S3 secret key. no default region String The AWS region the S3 bucket is located in. no default path String OPTIONAL. Path (absolute to the bucket (root)) that defines the folder that will be used to hold the workspace. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 workspace.repository.plugin = projectS3 # project individual resources ... workspace.repository.projectS3 = { # The S3 bucket name. bucket = \"your-bucket-name\" # The S3 access key ID. accessKeyId = \"BUCKET-ACCESS-KEY\" # The S3 secret key. secretKey = \"BUCKET-SECRET-KEY\" # The AWS region the S3 bucket is located in. region = \"eu-central-1\" # OPTIONAL path in the bucket used to hold the DataIntegration workspace # /path/to/my-workspace/ } S3 Bucket, Shared Directory - s3 \u00a4 Holds all resources shared across all your DataIntegration projects in a single S3 bucket. The available configuration options are the same as for projectS3 . The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 workspace.repository.plugin = s3 # resources shared across projects ... workspace.repository.s3 = { ... # same configuration as for projectS3 } In-Memory - inMemory \u00a4 Resources can also be held in-memory. The corresponding configuration in your dataintegration.conf looks like the following: 1 workspace.repository.plugin = inMemory Note In-memory repositories will be emptied on restart. No resources - empty \u00a4 In case you do not want to allow to store file resources you can define an empty repository. The empty resource repository that does not allow storing any resources. The resource repository can also be specified as empty. The corresponding configuration in your dataintegration.conf looks like the following: 1 workspace.repository.plugin = empty Execution Report Manager \u00a4 The execution report manager is used to persist execution reports. It allows to retrieve previous reports. you can use it with file and in-memory models. In addition you can specify a retention time. Reports older than this time will be deleted, if a new report is added. The retention time is expressed as a Java Duration string, see https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence- for details. Disabled - None \u00a4 Discards execution reports and does not persist them. 1 workspace.reportManager.plugin = none # Discards execution reports and does not persist them. In-Memory - inMemory \u00a4 Holds the reports in memory. 1 2 3 4 5 workspace.reportManager.plugin = inMemory # Holds the reports in memory. workspace.reportManager.inMemory = { retentionTime = \"P30D\" # duration how long to keep - default is 30 Days } File based - file \u00a4 Holds the reports in a specified directory on the filesystem. 1 2 3 4 5 6 workspace.reportManager.plugin = file # Holds the reports in a specified directory on the filesystem. workspace.reportManager.file = { dir = \"/data/reports\" # directory where the reports will be stored retentionTime = \"P30D\" # duration how long to keep - default is 30 Days } Internal Datasets \u00a4 Internal datasets hold intermediate results during the execution of a DataIntegration Workflow. The type of internal dataset that is used can be configured. By default an in memory dataset is used for storing data between tasks: 1 dataset.internal.plugin = inMemory Alternatively, the internal data can also be held in the eccenca DataPlatform: 1 2 3 4 dataset.internal.plugin = eccencaDataPlatform dataset.internal.eccencaDataPlatform = { graph = \"https://ns.eccenca.com/dataintegration/internal\" } If the eccenca DataPlatform is not available, an external store may also be specified: 1 2 3 4 5 dataset.internal.plugin = sparqlEndpoint dataset.internal.sparqlEndpoint = { endpointURI = \"http://localhost:8890/sparql\" graph = \"https://ns.eccenca.com/dataintegration/internal\" } Warning If an RDF store based internal dataset is used, all internal data is stored in the same graph. For this reason, multiple different internal datasets cannot be used safely in that case. Timeouts \u00a4 DataIntegration has a number of timeouts to maintain operation while connection issues or problems in other applications occur. The following sections list the most important global timeouts. Note In addition to these global timeouts, many datasets, such as the Knowledge Graph dataset, do provide additional timeout parameters. Refer to the documentation of datasets for individual timeout mechanisms of different datasets. Warning All timeouts set need to be lower than the gateway timeout in the network infrastructure. Request timeout \u00a4 The request timeout specifies how long a HTTP(S) request may take until it times out and is closed: 1 play.server.akka.requestTimeout = 10m This timeout mechanism can be disabled, by setting the timeout to \"infinite\" . Idle request timeout \u00a4 The idle timeout specifies the maximum inactivity time of a HTTP(S) connection: 1 play.server.http.idleTimeout = 10m The connection will be closed after it has been open for the configured idle timeout, without any request or response being written. This timeout mechanism can be disabled, by setting the timeout to \"infinite\" . DataPlatform timeouts \u00a4 As DataIntegration depends on DataPlatform for managing the Knowledge Graph, it will query the health of DataPlatform as part of its own health check. The timeout to wait for DataPlatform\u2019s response to a health request can be configured: 1 healthCheck.dataplatform.timeout = 10000 # milliseconds In addition, there is a timeout when requesting authorization information from the eccenca DataPlatform, which is fixed to 61 seconds. RDF store timeouts \u00a4 When reading and writing RDF there are a number of timeouts applied, depending on whether the Graph Store protocol or the SPARQL endpoint are used. For the Graph Store protocol, the following timeouts can be configured: 1 2 3 4 5 6 7 8 9 10 graphstore.default = { # Timeout in which a connection must be established connection.timeout.ms = 15000 # 15s # Timeout in which a response must be read read.timeout.ms = 150000 # 150s # Max request size of a single GraphStore request, larger data is split into multiple requests max.request.size = 300000000 # 300MB # Timeout in which a file upload of size max.request.size must be uploaded fileUpload.timeout.ms = 1800000 # half hour } For the SPARQL endpoint, the following parameters are applicable: 1 2 3 4 silk.remoteSparqlEndpoint.defaults = { connection.timeout.ms = 15000 # 15s read.timeout.ms = 180000 # 180s } Note The Knowledge Graph dataset provides additional timeout parameters for more fine-grained control. Linking execution timeout \u00a4 When executing linking rules there are a number of timeouts to prevent possibly erroneous linkage rules from generating too many links or staling the execution: 1 2 3 4 5 6 7 8 9 10 11 linking.execution = { # The maximum amount of links that are generated in the linking execution/evaluation linkLimit = { # The default value a link spec is initialized with, this can be changed for each link spec. default = 1000000 # 1 million # The absolute maximum of links that can be generated. This is necessary since the links are kept in-memory. max = 10000000 # 10 million } # The maximum time the matching task is allowed to run, this does not limit the loading time. matching.timeout.seconds = 3600 # 1 hour } Maximum Upload Size \u00a4 By default, the size of uploaded resources is limited to 10 MB. The upload limit can be increased: 1 play.http.parser.maxDiskBuffer = 100MB While uploading, resources are cached on the disk, i.e., the limit may exceed the size of the available memory. Provenance \u00a4 By default, no provenance data is written to the RDF store. To enable writing provenance data to a named graph, a provenance plugin needs to be configured. Provenance output can be configured using the following parameter: Parameter Type Description Default provenance.graph String Set the graph where generated provenance will be written to in the RDF workspace provider. https://ns.eccenca.com/example/data/dataset/ provenance.persistWorkflowProvenancePlugin.plugin String Provenance plugin to set where provenance data should be written to. Possible options: - rdfWorkflowProvenance - writes provenance data to RDF backend - nopWorkflowProvenance - do NOT write provenance data (disable it) nopWorkflowProvenance To enable provenance output, the following lines can be added to dataintegration.conf : 1 2 provenance.graph = https://ns.eccenca.com/example/data/dataset/ provenance.persistWorkflowProvenancePlugin.plugin = rdfWorkflowProvenance Logging \u00a4 Logging for eccenca DataIntegration is based on the Logback logging framework. There are two ways to change the logging behavior from the default, the first is to provide a logback.xml file, the second is to set various logging properties in the dataintegration.conf file. Logback Configuration File \u00a4 The logback.xml file can be added to the ${ELDS_HOME}/etc/dataintegration/conf/ folder, from where it is read on application start-up and replaces the default logging config. Configuration Example \u00a4 The following example logback.xml file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <appender name= \"TIME_BASED_FILE\" class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> /opt/elds/var/log/dataintegration.log </file> <rollingPolicy class= \"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\" > <!-- daily rollover, history for 1 week --> <fileNamePattern> /opt/elds/var/log/dataintegration.%d{yyyy-MM-dd}.log </fileNamePattern> <maxHistory> 7 </maxHistory> </rollingPolicy> <encoder> <pattern> %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n </pattern> </encoder> </appender> <logger name= \"com.eccenca\" level= \"INFO\" > <appender-ref ref= \"TIME_BASED_FILE\" /> </logger> </configuration> Logging Properties \u00a4 For debugging purposes and smaller adaptions it is possible to change log levels for any logger in the DataIntegration config file. There are following possibilities: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # The following log level properties will overwrite the config from the logback.xml file # Set the root logger level, valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL. # This affects any logger that is not explicitly defined in the logback.xml config file logging.root.level = DEBUG # Set the DI root log level. This affects all logger in DI packages that are not explicitly specified in the logback.xml logging.di.level = TRACE # Set the Silk root log level. This affects all logger in Silk packages that are not explicitly specified in the logback.xml logging.silk.level = WARN # Generic log level config: inside logging.level enter package path as key and log level as value. # Valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL # This can be used to override any log level, also these defined in the logback.xml file. logging.level { # Set log level of oauth package to TRACE, this overrides the config in the default logback config oauth=TRACE } Plugin Configuration \u00a4 The plugin architecture of eccenca DataIntegration allows to configure certain characteristics of the application, e.g. the persistence backend for the workspace.\\ A full list over all plugins are given in the eccenca DataIntegration user manual in the sections Plugin Reference as well as Activity Reference . Blacklisting Plugins \u00a4 In some cases the usage of specific plugins might pose a risk. In order to avoid to load a specific plugin, it can be blacklisted in the configuration file by setting the pluginRegistry.plugins.{pluginID}.enabled config parameter to false. The parameter takes a comma-separated list of plugin IDs. The corresponding plugins will not be loaded into the plugin registry and can thus not be selected or executed anymore. The plugin ID for each plugin can be found in the Plugin Reference and Activity Reference section of the eccenca DataIntegration user manual. Example config: 1 2 3 4 5 pluginRegistry { plugins { pluginToBeBlacklisted.enabled = false } } Spark configuration \u00a4 The following chapters describe configuration options relevant for the execution of eccenca DataIntegration workflows on Spark. All options regarding the execution of DataIntegration on Spark are set in the dataintegration.conf file. The option to define SparkExecutor as the execution engine is execution.manager.plugin and needs to be changed from the default value: 1 execution.manager.plugin = LocalExecutionManager To the value that specifies the use of the SparkExecutor: 1 execution.manager.plugin = SparkExecutionManager Execution in Spark client Mode \u00a4 Spark provides a simple standalone cluster manager. You can launch a standalone cluster either manually, by starting a master and workers by hand, or by using the provided launch scripts. Spark can still run alongside Hive, Hadoop and other services in this mode. But in general this mode is preferred if only Spark applications are running in a cluster. When multiple cluster applications are running in parallel (e.g. different databases, interpreters or any software running on top of Yarn) or more advanced monitoring is needed the execution with Yarn is often recommended. For running DataIntegration in client mode the following configuration can be used 1 2 3 4 5 6 7 8 9 10 11 spark.interpreter.options = { # Specifies local or client-mode, required: local, cluster or client deploymentMode = \"client\" # URL of the Spark cluster master node sparkMaster = \"spark://spark.master:7077\" # The IP of the driver/client program machine in client-mode, required for client mode sparkLocalIP = \"IP or hostname where DataIntegration runs\" # Jars containing the dependencies, required only for client and cluster modes # In client mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included sparkJars = \"eccenca-DataIntegration-assembly.jar\" } Execution in cluster mode \u00a4 DataIntegration application configuration \u00a4 Cluster mode is supported with Apache Yarn only at the moment. To run DataIntegration in cluster mode the following configuration can be used: 1 2 3 4 5 6 7 8 9 10 11 spark.interpreter.options = { # Specifies local or client-mode, required: local, cluster or client deploymentMode = \"cluster\" # URL of the Spark cluster master node sparkMaster = \"yarn-master-hostname\" # The IP of the driver/client program machine in client-mode, required for client mode sparkLocalIP = \"IP or hostname where DataIntegration runs\" # Jars containing the dependencies, required only for client and cluster modes # In cluster mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included sparkJars = \"eccenca-DataIntegration-assembly.jar\" } DataIntegration cluster deployment configuration \u00a4 In cluster mode one should keep in mind that, normally, DataIntegration will generate a jar and a project export. These artifacts can be copied or send to a cluster and will be executed there via the spark-submit command. That means the data processing is running in its own remote process separate from the DataIntegration application. An assembly jar and a workflow can be exported by an activity that belongs to each defined workflow. The activity can be configured in the dataintegration.conf . There are 3 phases of a deployment: staging, transform, loading and 3 types of artifact compositions as well as some other options deciding the target of the export. First the specified resource are copied to the configured resource folder of a project or a temp folder (staging) and then an action decides how the files are deployed. Artifacts (spark.deployment.options.artifact): \u2018jar\u2019: The assembly jar is deployed \u2018project\u2019: The exported project zip file is deployed (e.g. if the assembly was already globally deployed) \u2018project-jar\u2019: The jar and the project zip are deployed The artifacts are copied to the configured resource folder off the project the activity belongs to. Types ( spark.deployment.options.[phase].type, e.g. spark.deployment.options.staging.type=\"env-script\" ): \u2018script\u2019 A shell script is called to copy the files to the cluster (can be user supplied, contain auth, prepare a DataFactory activity etc.) \u2018copy\u2019 The resource are copied to a specified folder \u2018hdfs\u2019 The resource is imported to HDFS \u2018env-script\u2019 A shell script is loaded from a environment variable \u2018var-script\u2019 A shell script is loaded from a configuration variable Other options: spark.deployment.options.[phase].[typeName] Depending on the selected deployment type this contains one or more (separated by a comma) targeted local file system or HDFS paths or location of the scripts to run\\ e.g. spark.deployment.options.staging.type =\"script\" and spark.deployment.options.staging.script=\"/scripts/script.sh\" spark.deployment.options.overwriteExecution Boolean value that decides if the ExecuteSparkWorkflow action is overwritten by the deployment action and will run this instead. Example: 1 2 3 4 5 6 7 8 9 10 11 spark.deployment.options = { # Specifies artifacts: Stage the project export and the executable assembly jar artifact = \"project-jar\" # Type of the deployment: Copy project and jar to /data folder, then run a script to start processing the data staging.type = \"copy\" staging.copy = \"/data\" transform.type = \"script\" transform.script = \"conf/runWorkflow.sh\" # Bind the 2 actions to the \"run workflow\" button overwriteExecution = true } Activity parameters to skip deployment phases \u00a4 In some scenarios (especially deployments where a jar has to be copied to a remote location) it is required that a deployment phase can be skipped. E.g. the jar upload only has to be done once, the upload is defined in the \u201cstaging\u201d phase and the spark-submit call in the \u201ctransform\u201d phase. The parameter \u201cexecuteTransform\u201d (reachable via the activity tab) can\\ be set to false on the second run to avoid re-uploading artifacts. Configuration of the assembly jar \u00a4 In cluster mode, usually, we run a Spark Job by submitting an assembly jar to the cluster. This can be seen a command line version of DataIntegration and can also be used manually with \u2018spark-submit\u2019. In this case the configuration in the environment the jar runs in should look like this (options are set by the spark-submit configuration and parameters): 1 2 3 4 spark.interpreter.options = { # Specifies deployment mode, requires: local, cluster, client or submit deploymentMode = \"submit\" } Execution in local mode \u00a4 Local mode is mainly for testing, but can be used for deployment on a single server. HDFS and Hive are not required. The following configuration parameters have to be set: 1 2 3 4 5 6 spark.interpreter.options = { # Specifies deployment mode, requires: local, cluster, client or submit deploymentMode = \"local\" # URL of the Spark cluster master node, the [*] denotes the number of executors sparkMaster = \"local[4]\" } In this mode the parameters and Spark settings appended to the \u2018spark-submit\u2019 command will always be used and overwrite configuration settings in other sources. Configuration and usage of the SqlEndpoint dataset \u00a4 Server Side Configuration \u00a4 General Settings \u00a4 The SqlEndpoint dataset is a table or view behaving analog to a table in a relational database like MySQL. When data is written to an SqlEndpoint dataset, a JDBC server is started and can be queried with any JDBC client. In DataIntegration the SqlEndpoint dataset behaves like any other dataset. It can be used as a target for workflows, be profiled, used as a source for an operation or workflow etc. There are a two of configuration options that are relevant for the JDBC endpoints: 1 2 3 4 5 6 7 8 9 spark.sql.options = { # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. SqlEndpoint # datasets can still be started but can only be accessed internally if set to false. startThriftServer = true # Enable Hive integration # Sets Spark to use an infrastructure for meta data that is compatible with the hive metastore enableHiveSupport = true ... } The port on which the JDBC connections will be available is 10005 by default and can be changed in the hive-site.xml and spark-defaults.conf configuration files. Security Settings \u00a4 A secure connection can be configured with the authentification settings in the hive-site.xml , spark-defaults.conf and dataintegration.conf files. If Hive support is disabled ( enableHiveSupport = false ) or if the property hive.server2.authentication has the value None security can be disabled. There exist a number of option for secure JDBC connections via Thrift and Hive: Kerberos LDAP Custom authentication classes User impersonation Server and Client Certificates Eccenca provides a custom Authentification provider which allows to set 1 user/password combination for JDBC connections via: spark.sql.options = { endpointUser = \"user\" endpointPassword = \"password\" } The authentication provider class name is com.eccenca.di.sql.endpoint.security.SqlEndpointAuth . To use it the following configuration is needed: <configuration> <property> <name> hive.server2.authentication </name> <value> CUSTOM </value> </property> <property> <name> hive.server2.custom.authentication.class </name> <value> com.eccenca.di.sql.endpoint.security.SqlEndpointAuth </value> <description> Custom authentication class. Used when property 'hive.server2.authentication' is set to 'CUSTOM'. Provided class must be a proper implementation of the interface org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2 will call its Authenticate(user, password) method to authenticate requests. The implementation may optionally implement Hadoop's org.apache.hadoop.conf.Configurable class to grab Hive's Configuration object. </description> </property> ... </configuration> Check the Hive documentation for details: Hive admin manual or the documentation of a Hadoop Distribution (MapR, Hortenworks or AWS and Azure in he cloud etc.). Hadoop distributions usually provides instructions for configuring secure endpoints. Integration with various authentication providers can be configured and is mostly set up in hive-site.xml . SqlEndpoint Dataset Parameters \u00a4 The dataset only requires that the tableNamePrefix parameters is given. This will be used as the prefix for the names of the generated tables. When a set of Entities is written to the endpoint a view is generated for each entity type (defined by an rdf_type attribute). That means that the mapping or data source that are used as input for the SqlEndpoint need to have a type or require a user defined type mapping. The operator has a compatibility mode . Using it will avoid complex types such as Arrays. When arrays exit in the input they are converted to a String using the given arraySeperator . SqlEndpoint Activity \u00a4 The activity will start automatically, when the SqlEndpoint is used as a data sink and Dataintegration is configured to make the SqlEndpoint accessible remotely. When the activity is started and running it returns the server status and JDBC Url as its value. Stopping the activity will drop all views generated by the activity. It can be restarted by rerunning the workflow containing it as a sink. Remote Client Configuration (via JDBC and ODBC) \u00a4 Within Dataintegration the SqlEndpoint can be used as a source or sink like any other dataset. If the startThriftServer option is set to true access via JDBC or ODBC is possible. ODBC and JDBC drivers can be used to connect to relational databases. These drivers are used by clients like, Excel, PowerBI or other BI tools and transform standard SQL-queries to Hive-QL queries and handle the respective query results. Hive-QL support a subset of the SQL-92 standard. Depending on the complexity of the driver it \u2013 in case of a simple driver \u2013 supports the same subset or more modern standards. JDBC drivers are similar to ODBC ones, but serve as connectors for Java applications. When selecting a version of a driver the client operating system and its type (32bit/64 bit) are the most important factors. The version of the client drivers sometimes is the same as the servers. When no version of a driver is given the newest driver of the vendor should work, as it should be backwards compatible. Any JDBC or ODBC client can connect to a JDBC endpoint provided by an SqlEndpoint dataset. SqlEndpoint uses the same query processing as Hive, therefore the requirements for the client are: A JDBC driver compatible with Hive 1.2.1 (platform independent driver org.apache.hive.jdbc.HiveDriver is needed) or Hive 1.2.1 is ODPi runtime compliant A JDBC driver compatible with Spark 2.3.3 A Hive ODBC driver (ODBC driver for the client architecture and operating system needed) A detailed instruction to connect to a Hive or SqlEndpoint endpoint with various tools (e.g. SQuirreL, beeline, SQL Developer, \u2026) can be found at Apache HiveServer2 Clients .\\ The multi platform database client DBeaver can connect to the SQLEndpoint out of the box. Partitioning and merging of data sets \u00a4 The execution on Spark is possible independent of the used file system as long as it can be referenced/is accessible for all cluster nodes. HDFS is recommended and the default settings are recommended for the best performance on a small cluster. Especially when working in local mode on the local file systems some problems can occur with the parallelism settings of Spark and the resulting partitioned output resources. Problems can be avoided by changing the following are the default settings: 1 2 3 4 5 6 7 8 9 spark.interpreter.options = { # If true, data will be repartitioned before execution, # otherwise the existing partitioning or no partitioning will be used partitionOnImport = false # Number of partitions for repartitioning on import, default = 16 partitionNumber = 16 # Specifies if data is combined before output is written to disk combineOutput = false } When running only locally the configuration should be like the following example (especially combineOutput has to be true): 1 2 3 4 5 6 7 8 9 spark.interpreter.options = { # If true, data will be repartitioned before execution, # otherwise the existing partitioning or no partitioning will be used partitionOnImport = false # Number of partitions for repartitioning on import, default = 16 partitionNumber = 4 # Specifies if data is combined before output is written to disk combineOutput = true } Other options specific to Spark \u00a4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ################################################# # Spark # ################################################# spark.interpreter.options = { # the default name used when creating a SparkContext (will override spark.app.name in spark-defaults.conf) sparkAppName = \"eccenca DataIntegration Spark exe # Enables more detailed logging, counting of transformed records, etc. debugMode = false # Enable or disable a spark executor event log for debugging purposes eventLog = true # Folder for logs that people don't want in the log even though the information is necessary for debugging logFolder = ${elds.home}\"/var/dataintegration/logs/\" # Enable or disable an execution time log for benchmarking purposes timeLog = true # Enable or disable Sparks built-in log for debugging purposes (will override spark.eventLog.enabled in spark-defaults.conf) sparkLog = true # If true, data will be repartitioned before execution, otherwise the existing partitioning or no partitioning will be used partitionOnImport = false # Number of partitions for repartitioning on Import partitionNumber = 16 # Specifies number of Spark SQL shuffle partitions (will override spark.sql.shuffle.partitions in spark-defaults.conf) shufflePartitions = 32 # Minimum partition number for Spark execution defaultMinPartitions = 4 # Default parallelism partition number for Spark execution (will override spark.sql.shuffle.partitions in spark-defaults.conf) defaultParallelism = 4 # Specifies if data is combined before output is written to disk. If true the final output will be in a single file on a single partition combineOutput = true # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. Views/virtual datasets can still be started but can only be accessed internally if set to false. startThriftServer = false # Internal data model used in Spark Data Frames: 'sequence' or 'simple'. Sequence behaves like the entities used by the local executor of DataIntegration and is # sometimes be needed to work with non relational data (i.e. triple stores, dataplatform). The default value is 'simple' and casts most data objects to Strings # which is fast and works in most situations may lead to less clean data. columnType = sequence # General additional Java options that will be passed to the executors (worker nodes) in the cluster, default is \"\". sparkExecutorJavaOptions = \"\" # General additional Java options that will be passed to the driver application (DataIntegration), default is \"\". sparkDriverJavaOptions = \"\" # Enable or disable the Spark UI. This UI provides an overview of the Spark cluster and running jobs. It will start on port 4040 # and increase the port number by 1 if the port is already in use. The final port will be shown in the logs. False by default. enableSparkUI = true # This property decides if Hive integration is enabled orr not. # To use hive an external DB (such as MYSQL), the meta store, is needed. Please specify the necessary properties in the hive-site.xml. Note that Hive's default meta store (derby) should not be used in production and may lead to issues. enableHiveSupport = false }","title":"DataIntegration"},{"location":"deploy-and-configure/configuration/dataintegration/#dataintegration","text":"This section is intended to be a reference for all available eccenca DataIntegration configuration options.The configuration format is based on HOCON . The following sections introduce the most important configuration parameters. The entire list of available configuration parameters can be found in the dataintegration.conf file found in the release package.","title":"DataIntegration"},{"location":"deploy-and-configure/configuration/dataintegration/#oauth","text":"Authorization in eccenca DataIntegration is based on OAuth. Typically the eccenca DataPlatform is used as an OAuth endpoint, but external endpoints can be used as well. The Authorization Code Grant workflow is used for retrieving the OAuth token. The default configuration is the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # The URL of the eccenca DataPlatform. eccencaDataPlatform.url = \"http://localhost:9090\" # Use OAuth for authentification against DataPlatform eccencaDataPlatform.oauth = false # Enable if user information should be fetched via DP and OAuth. Only uncomment if OAuth is enabled and DP is configured. user.manager.web.plugin = oauthUserManager # The DataPlatform endpoint that is used for authentification eccencaDataPlatform.endpointId = \"default\" # Define the protocol used for accessing the workbench (http or https), defaults to http workbench.protocol = \"http\" # Optional parameter for specifying the host. workbench.host = \"localhost:9090\" # The URL to redirect to after logout. # If not set, the user will be redirected to the internal logout page. oauth.logoutRedirectUrl = \"http://localhost:9090/loggedOut\" # The OAuth client that will be used to load the workspace initially and run the schedulers. workbench.superuser.client = \"elds\" workbench.superuser.clientSecret = \"elds\" # Optional parameter for specifying an alternative OAuth authorization endpoint. # If not specified, the default OAuth authorization endpoint of the specified eccenca Platform URL is used. # Note that if the eccenca Platform URL is internal and not accessible for the user, a public authorization URL must be set here. oauth.authorizationUrl = \"http://localhost:9090/oauth/authorize\" # Optional parameter for specifying an alternative OAuth authorization endpoint. oauth.tokenUrl = \"http://localhost:9090/oauth/token\" # Optional parameter for specifying an alternative OAuth client ID. oauth.clientId = \"eldsClient\" # Optional parameter for specifying an alternative OAuth client secret. oauth.clientSecret = \"secret\" # Additional request parameters to append to all OAuth authentication requests. # oauth.requestParameters = \"&resource=value\"","title":"OAuth"},{"location":"deploy-and-configure/configuration/dataintegration/#dataplatform-configuration","text":"eccenca DataIntegration can only be run by OAuth users that are granted the urn:eccenca:di action by the DataPlatform. An example configuration that grants all users from a predefined group dataIntegrators access to DataIntegration is shown in the following: 1 2 3 4 5 6 7 8 @prefix eccauth: <https://vocab.eccenca.com/auth/> . @prefix : <http://eccenca.com/> . <urn:readAndWriteAllGraphs> a eccauth : AccessCondition ; eccauth : requiresGroup : dataIntegrators ; eccauth : allowedAction <urn:eccenca:di> ; eccauth : readGraph <urn:elds-backend-all-graphs> ; eccauth : writeGraph <urn:elds-backend-all-graphs> . In the shown configuration, the users also get access to all graphs in the RDF store. This is not a requirement for working with DataIntegration. Access may also be restricted to graphs that the data integration users are allowed to work with. Note The urn:elds-backend-all-actions action set also includes the urn:eccenca:di action, i.e., users that are granted urn:elds-backend-all-actions also get access to eccencca DataIntegration. In order to activate OAuth using the eccenca DataPlatform, the following minimal configuration is required: 1 2 3 4 eccencaDataPlatform.url = \"http://localhost:9090\" eccencaDataPlatform.oauth = true oauth.clientId = \"eldsClient\" oauth.clientSecret = \"secret\"","title":"DataPlatform configuration"},{"location":"deploy-and-configure/configuration/dataintegration/#super-user","text":"By default, the workspace is loaded the first time a user opens eccenca DataIntegration in the browser using their credentials. Any scheduler that is part of a project must be started manually. By configuring a super user, the workspace will be loaded at startup. After loading, all schedulers will be started using the credentials of the super user. In addition to the configuration of the eccencaDataPlatform according to the previous section, a super user is configured by specifying the following two parameters: 1 2 workbench.superuser.client = \"superUserClient\" workbench.superuser.clientSecret = \"superUserClientSecret\" Note The client credentials grant type is used to retrieve a token for the super user. Note that the schedulers are only started automatically when running in production mode.","title":"Super User"},{"location":"deploy-and-configure/configuration/dataintegration/#workspace-providers","text":"The backend that holds the workspace can be configured using the workspace.provider.plugin parameter in dataintegration.conf 1 workspace.provider.plugin = <workspace-provider-plugin-name> The following sections describe the available workspace provider plugins and how they are configured.","title":"Workspace Providers"},{"location":"deploy-and-configure/configuration/dataintegration/#rdf-store-workspace-backend","text":"When running in Corporate Memory, by default the workspace is held in the RDF store configured in the eccenca DataPlatform. The workspace is held using the eccenca DataPlatform, i.e., it requires the eccencaDataPlatform.url parameter to be configured. This workspace can be configured using the following parameter: Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ cacheDir String Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start. <empty> By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true. The corresponding configuration in your dataintegration.conf looks like the following: eccencaDataPlatform.url = <DATAPLATFORM_URL> ... workspace.provider.plugin = backend ... workspace.provider.backend = { # Load prefixes defined by all known vocabularies. loadAllVocabularyPrefixes = false # Load prefixes defined by vocabularies that are actually loaded in the RDF store. loadInstalledVocabularyPrefixes = true # The graph that contains the vocabulary meta data. vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\" # Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start. cacheDir = \"\" }","title":"RDF-store Workspace - backend"},{"location":"deploy-and-configure/configuration/dataintegration/#file-based-workspace-file","text":"The workspace can also be held on the filesystem. This workspace can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the workspace is persisted. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 workspace.provider.plugin = file ... workspace.provider.file = { # The directory to which the workspace is persisted. dir = ${user.home}\"/myWorkspace\" }","title":"File-based Workspace - file"},{"location":"deploy-and-configure/configuration/dataintegration/#hybrid-workspace-fileanddataplatform","text":"The so called hybrid workspace holds the workspace in the filesystem and in eccenca DataPlatform simultaneously. Each time a task is updated, it is written to both the filesystem and to the project RDF graph. In addition, on each (re)load of the Workspace, the contents of the file based workspace are pushed to the RDF store, to make sure that both workspace backends stay synchronized. Contents of the file system may be changed manually (reload the workspace afterwards). Contents of the RDF store are supposed to be read only and will be overwritten on reload. The workspace is held using the eccenca DataPlatform, i.e., it requires the eccencaDataPlatform.url parameter to be configured. This workspace can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the workspace is persisted. no default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ failOnDataPlatformError boolean If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning. false By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true. The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 eccencaDataPlatform.url = <DATAPLATFORM_URL> ... workspace.provider.plugin = fileAndDataPlatform ... workspace.provider.fileAndDataPlatform = { # The directory to which the workspace is persisted. dir = ${user.home}\"/myWorkspace\" # Load prefixes defined by all known vocabularies. loadAllVocabularyPrefixes = false # Load prefixes defined by vocabularies that are actually loaded in the RDF store. loadInstalledVocabularyPrefixes = true # The graph that contains the vocabulary meta data. vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\" # If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning. failOnDataPlatformError = false }","title":"Hybrid workspace - fileAndDataPlatform"},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-workspace-inmemory","text":"A workspace provider that holds all projects in memory. All contents will be gone on restart. The corresponding configuration in your dataintegration.conf looks like the following: 1 workspace.provider.plugin = inMemory","title":"In-memory Workspace - inMemory"},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-rdf-workspace-inmemoryrdfworkspace","text":"A workspace that is held in a in-memory RDF store and loses all its content on restart (mainly used for testing). Needed if operators are used that require an RDF store backend. This workspace can be configured using the following parameter: Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true. The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 9 10 workspace.provider.plugin = inMemoryRdfWorkspace ... workspace.provider.inMemoryRdfWorkspace = { # Load prefixes defined by all known vocabularies. loadAllVocabularyPrefixes = false # Load prefixes defined by vocabularies that are actually loaded in the RDF store. loadInstalledVocabularyPrefixes = true # The graph that contains the vocabulary meta data. vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\" }","title":"In-memory RDF Workspace - inMemoryRdfWorkspace"},{"location":"deploy-and-configure/configuration/dataintegration/#resource-repositories","text":"Project resources are held by a resource repository which is configured using the workspace.repository.plugin parameter in dataintegration.conf 1 workspace.repository.plugin = <resource-repository-plugin-name> The following sections describe the available resource repository plugins and how they are configured.","title":"Resource Repositories"},{"location":"deploy-and-configure/configuration/dataintegration/#project-specific-directories-projectfile","text":"By default, resources are held in project specific directories. This plugin can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the resources are persisted. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 workspace.repository.plugin = projectFile ... workspace.repository.projectFile = { dir = ${elds.home}\"/var/dataintegration/workspace/\" }","title":"Project Specific Directories - projectFile"},{"location":"deploy-and-configure/configuration/dataintegration/#shared-directory-file","text":"Alternatively, all resources across all DataIntegration projects can be held in a single directory on the file system. This plugin can be configured using the following parameter: Parameter Type Description Default dir String The directory to which the resources are persisted. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 workspace.repository.plugin = file ... workspace.repository.file = { dir = ${elds.home}\"/var/dataintegration/resources/\" }","title":"Shared Directory - file"},{"location":"deploy-and-configure/configuration/dataintegration/#hdfs-resources-hdfs","text":"Holds all resources on the HDFS file system. This plugin can be configured using the following parameter: Parameter Type Description Default path String The directory to which the resources are persisted. no default user String The hadoop user. hadoopuser The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 workspace.repository.plugin = hdfs ... workspace.repository.hdfs = { # The directory to which the resources are persisted. dir = \"/data/hdfs-datalake/\" # The hadoop user. user = \"hadoopuser\" }","title":"HDFS resources - hdfs"},{"location":"deploy-and-configure/configuration/dataintegration/#s3-bucket-project-specific-projects3","text":"In addition to storing files in the local filesystem an AWS S3 bucket can be used as the resource repository backend. To use resources stored on S3 the AWS keyID , secretKey need to be configured in the dataintegration.conf file. This and the region are used to connect to S3. Further, one bucket name has to be given. This is analog to the root folder of filesystem based resource repositories. This plugin can be configured using the following parameter: Parameter Type Description Default bucket String The S3 bucket name. no default accessKeyId String The S3 access key ID. no default secretKey String The S3 secret key. no default region String The AWS region the S3 bucket is located in. no default path String OPTIONAL. Path (absolute to the bucket (root)) that defines the folder that will be used to hold the workspace. no default The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 workspace.repository.plugin = projectS3 # project individual resources ... workspace.repository.projectS3 = { # The S3 bucket name. bucket = \"your-bucket-name\" # The S3 access key ID. accessKeyId = \"BUCKET-ACCESS-KEY\" # The S3 secret key. secretKey = \"BUCKET-SECRET-KEY\" # The AWS region the S3 bucket is located in. region = \"eu-central-1\" # OPTIONAL path in the bucket used to hold the DataIntegration workspace # /path/to/my-workspace/ }","title":"S3 Bucket, Project Specific -\u00a0projectS3"},{"location":"deploy-and-configure/configuration/dataintegration/#s3-bucket-shared-directory-s3","text":"Holds all resources shared across all your DataIntegration projects in a single S3 bucket. The available configuration options are the same as for projectS3 . The corresponding configuration in your dataintegration.conf looks like the following: 1 2 3 4 5 workspace.repository.plugin = s3 # resources shared across projects ... workspace.repository.s3 = { ... # same configuration as for projectS3 }","title":"S3 Bucket, Shared Directory - s3"},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-inmemory","text":"Resources can also be held in-memory. The corresponding configuration in your dataintegration.conf looks like the following: 1 workspace.repository.plugin = inMemory Note In-memory repositories will be emptied on restart.","title":"In-Memory -\u00a0inMemory"},{"location":"deploy-and-configure/configuration/dataintegration/#no-resources-empty","text":"In case you do not want to allow to store file resources you can define an empty repository. The empty resource repository that does not allow storing any resources. The resource repository can also be specified as empty. The corresponding configuration in your dataintegration.conf looks like the following: 1 workspace.repository.plugin = empty","title":"No resources -\u00a0empty"},{"location":"deploy-and-configure/configuration/dataintegration/#execution-report-manager","text":"The execution report manager is used to persist execution reports. It allows to retrieve previous reports. you can use it with file and in-memory models. In addition you can specify a retention time. Reports older than this time will be deleted, if a new report is added. The retention time is expressed as a Java Duration string, see https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence- for details.","title":"Execution Report Manager"},{"location":"deploy-and-configure/configuration/dataintegration/#disabled-none","text":"Discards execution reports and does not persist them. 1 workspace.reportManager.plugin = none # Discards execution reports and does not persist them.","title":"Disabled - None"},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-inmemory_1","text":"Holds the reports in memory. 1 2 3 4 5 workspace.reportManager.plugin = inMemory # Holds the reports in memory. workspace.reportManager.inMemory = { retentionTime = \"P30D\" # duration how long to keep - default is 30 Days }","title":"In-Memory - inMemory"},{"location":"deploy-and-configure/configuration/dataintegration/#file-based-file","text":"Holds the reports in a specified directory on the filesystem. 1 2 3 4 5 6 workspace.reportManager.plugin = file # Holds the reports in a specified directory on the filesystem. workspace.reportManager.file = { dir = \"/data/reports\" # directory where the reports will be stored retentionTime = \"P30D\" # duration how long to keep - default is 30 Days }","title":"File based - file"},{"location":"deploy-and-configure/configuration/dataintegration/#internal-datasets","text":"Internal datasets hold intermediate results during the execution of a DataIntegration Workflow. The type of internal dataset that is used can be configured. By default an in memory dataset is used for storing data between tasks: 1 dataset.internal.plugin = inMemory Alternatively, the internal data can also be held in the eccenca DataPlatform: 1 2 3 4 dataset.internal.plugin = eccencaDataPlatform dataset.internal.eccencaDataPlatform = { graph = \"https://ns.eccenca.com/dataintegration/internal\" } If the eccenca DataPlatform is not available, an external store may also be specified: 1 2 3 4 5 dataset.internal.plugin = sparqlEndpoint dataset.internal.sparqlEndpoint = { endpointURI = \"http://localhost:8890/sparql\" graph = \"https://ns.eccenca.com/dataintegration/internal\" } Warning If an RDF store based internal dataset is used, all internal data is stored in the same graph. For this reason, multiple different internal datasets cannot be used safely in that case.","title":"Internal Datasets"},{"location":"deploy-and-configure/configuration/dataintegration/#timeouts","text":"DataIntegration has a number of timeouts to maintain operation while connection issues or problems in other applications occur. The following sections list the most important global timeouts. Note In addition to these global timeouts, many datasets, such as the Knowledge Graph dataset, do provide additional timeout parameters. Refer to the documentation of datasets for individual timeout mechanisms of different datasets. Warning All timeouts set need to be lower than the gateway timeout in the network infrastructure.","title":"Timeouts"},{"location":"deploy-and-configure/configuration/dataintegration/#request-timeout","text":"The request timeout specifies how long a HTTP(S) request may take until it times out and is closed: 1 play.server.akka.requestTimeout = 10m This timeout mechanism can be disabled, by setting the timeout to \"infinite\" .","title":"Request timeout"},{"location":"deploy-and-configure/configuration/dataintegration/#idle-request-timeout","text":"The idle timeout specifies the maximum inactivity time of a HTTP(S) connection: 1 play.server.http.idleTimeout = 10m The connection will be closed after it has been open for the configured idle timeout, without any request or response being written. This timeout mechanism can be disabled, by setting the timeout to \"infinite\" .","title":"Idle request timeout"},{"location":"deploy-and-configure/configuration/dataintegration/#dataplatform-timeouts","text":"As DataIntegration depends on DataPlatform for managing the Knowledge Graph, it will query the health of DataPlatform as part of its own health check. The timeout to wait for DataPlatform\u2019s response to a health request can be configured: 1 healthCheck.dataplatform.timeout = 10000 # milliseconds In addition, there is a timeout when requesting authorization information from the eccenca DataPlatform, which is fixed to 61 seconds.","title":"DataPlatform timeouts"},{"location":"deploy-and-configure/configuration/dataintegration/#rdf-store-timeouts","text":"When reading and writing RDF there are a number of timeouts applied, depending on whether the Graph Store protocol or the SPARQL endpoint are used. For the Graph Store protocol, the following timeouts can be configured: 1 2 3 4 5 6 7 8 9 10 graphstore.default = { # Timeout in which a connection must be established connection.timeout.ms = 15000 # 15s # Timeout in which a response must be read read.timeout.ms = 150000 # 150s # Max request size of a single GraphStore request, larger data is split into multiple requests max.request.size = 300000000 # 300MB # Timeout in which a file upload of size max.request.size must be uploaded fileUpload.timeout.ms = 1800000 # half hour } For the SPARQL endpoint, the following parameters are applicable: 1 2 3 4 silk.remoteSparqlEndpoint.defaults = { connection.timeout.ms = 15000 # 15s read.timeout.ms = 180000 # 180s } Note The Knowledge Graph dataset provides additional timeout parameters for more fine-grained control.","title":"RDF store timeouts"},{"location":"deploy-and-configure/configuration/dataintegration/#linking-execution-timeout","text":"When executing linking rules there are a number of timeouts to prevent possibly erroneous linkage rules from generating too many links or staling the execution: 1 2 3 4 5 6 7 8 9 10 11 linking.execution = { # The maximum amount of links that are generated in the linking execution/evaluation linkLimit = { # The default value a link spec is initialized with, this can be changed for each link spec. default = 1000000 # 1 million # The absolute maximum of links that can be generated. This is necessary since the links are kept in-memory. max = 10000000 # 10 million } # The maximum time the matching task is allowed to run, this does not limit the loading time. matching.timeout.seconds = 3600 # 1 hour }","title":"Linking execution timeout"},{"location":"deploy-and-configure/configuration/dataintegration/#maximum-upload-size","text":"By default, the size of uploaded resources is limited to 10 MB. The upload limit can be increased: 1 play.http.parser.maxDiskBuffer = 100MB While uploading, resources are cached on the disk, i.e., the limit may exceed the size of the available memory.","title":"Maximum Upload Size"},{"location":"deploy-and-configure/configuration/dataintegration/#provenance","text":"By default, no provenance data is written to the RDF store. To enable writing provenance data to a named graph, a provenance plugin needs to be configured. Provenance output can be configured using the following parameter: Parameter Type Description Default provenance.graph String Set the graph where generated provenance will be written to in the RDF workspace provider. https://ns.eccenca.com/example/data/dataset/ provenance.persistWorkflowProvenancePlugin.plugin String Provenance plugin to set where provenance data should be written to. Possible options: - rdfWorkflowProvenance - writes provenance data to RDF backend - nopWorkflowProvenance - do NOT write provenance data (disable it) nopWorkflowProvenance To enable provenance output, the following lines can be added to dataintegration.conf : 1 2 provenance.graph = https://ns.eccenca.com/example/data/dataset/ provenance.persistWorkflowProvenancePlugin.plugin = rdfWorkflowProvenance","title":"Provenance"},{"location":"deploy-and-configure/configuration/dataintegration/#logging","text":"Logging for eccenca DataIntegration is based on the Logback logging framework. There are two ways to change the logging behavior from the default, the first is to provide a logback.xml file, the second is to set various logging properties in the dataintegration.conf file.","title":"Logging"},{"location":"deploy-and-configure/configuration/dataintegration/#logback-configuration-file","text":"The logback.xml file can be added to the ${ELDS_HOME}/etc/dataintegration/conf/ folder, from where it is read on application start-up and replaces the default logging config.","title":"Logback Configuration File"},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-example","text":"The following example logback.xml file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <appender name= \"TIME_BASED_FILE\" class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> /opt/elds/var/log/dataintegration.log </file> <rollingPolicy class= \"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\" > <!-- daily rollover, history for 1 week --> <fileNamePattern> /opt/elds/var/log/dataintegration.%d{yyyy-MM-dd}.log </fileNamePattern> <maxHistory> 7 </maxHistory> </rollingPolicy> <encoder> <pattern> %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n </pattern> </encoder> </appender> <logger name= \"com.eccenca\" level= \"INFO\" > <appender-ref ref= \"TIME_BASED_FILE\" /> </logger> </configuration>","title":"Configuration Example"},{"location":"deploy-and-configure/configuration/dataintegration/#logging-properties","text":"For debugging purposes and smaller adaptions it is possible to change log levels for any logger in the DataIntegration config file. There are following possibilities: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # The following log level properties will overwrite the config from the logback.xml file # Set the root logger level, valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL. # This affects any logger that is not explicitly defined in the logback.xml config file logging.root.level = DEBUG # Set the DI root log level. This affects all logger in DI packages that are not explicitly specified in the logback.xml logging.di.level = TRACE # Set the Silk root log level. This affects all logger in Silk packages that are not explicitly specified in the logback.xml logging.silk.level = WARN # Generic log level config: inside logging.level enter package path as key and log level as value. # Valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL # This can be used to override any log level, also these defined in the logback.xml file. logging.level { # Set log level of oauth package to TRACE, this overrides the config in the default logback config oauth=TRACE }","title":"Logging Properties"},{"location":"deploy-and-configure/configuration/dataintegration/#plugin-configuration","text":"The plugin architecture of eccenca DataIntegration allows to configure certain characteristics of the application, e.g. the persistence backend for the workspace.\\ A full list over all plugins are given in the eccenca DataIntegration user manual in the sections Plugin Reference as well as Activity Reference .","title":"Plugin Configuration"},{"location":"deploy-and-configure/configuration/dataintegration/#blacklisting-plugins","text":"In some cases the usage of specific plugins might pose a risk. In order to avoid to load a specific plugin, it can be blacklisted in the configuration file by setting the pluginRegistry.plugins.{pluginID}.enabled config parameter to false. The parameter takes a comma-separated list of plugin IDs. The corresponding plugins will not be loaded into the plugin registry and can thus not be selected or executed anymore. The plugin ID for each plugin can be found in the Plugin Reference and Activity Reference section of the eccenca DataIntegration user manual. Example config: 1 2 3 4 5 pluginRegistry { plugins { pluginToBeBlacklisted.enabled = false } }","title":"Blacklisting Plugins"},{"location":"deploy-and-configure/configuration/dataintegration/#spark-configuration","text":"The following chapters describe configuration options relevant for the execution of eccenca DataIntegration workflows on Spark. All options regarding the execution of DataIntegration on Spark are set in the dataintegration.conf file. The option to define SparkExecutor as the execution engine is execution.manager.plugin and needs to be changed from the default value: 1 execution.manager.plugin = LocalExecutionManager To the value that specifies the use of the SparkExecutor: 1 execution.manager.plugin = SparkExecutionManager","title":"Spark configuration"},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-spark-client-mode","text":"Spark provides a simple standalone cluster manager. You can launch a standalone cluster either manually, by starting a master and workers by hand, or by using the provided launch scripts. Spark can still run alongside Hive, Hadoop and other services in this mode. But in general this mode is preferred if only Spark applications are running in a cluster. When multiple cluster applications are running in parallel (e.g. different databases, interpreters or any software running on top of Yarn) or more advanced monitoring is needed the execution with Yarn is often recommended. For running DataIntegration in client mode the following configuration can be used 1 2 3 4 5 6 7 8 9 10 11 spark.interpreter.options = { # Specifies local or client-mode, required: local, cluster or client deploymentMode = \"client\" # URL of the Spark cluster master node sparkMaster = \"spark://spark.master:7077\" # The IP of the driver/client program machine in client-mode, required for client mode sparkLocalIP = \"IP or hostname where DataIntegration runs\" # Jars containing the dependencies, required only for client and cluster modes # In client mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included sparkJars = \"eccenca-DataIntegration-assembly.jar\" }","title":"Execution in Spark client Mode"},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-cluster-mode","text":"","title":"Execution in cluster mode"},{"location":"deploy-and-configure/configuration/dataintegration/#dataintegration-application-configuration","text":"Cluster mode is supported with Apache Yarn only at the moment. To run DataIntegration in cluster mode the following configuration can be used: 1 2 3 4 5 6 7 8 9 10 11 spark.interpreter.options = { # Specifies local or client-mode, required: local, cluster or client deploymentMode = \"cluster\" # URL of the Spark cluster master node sparkMaster = \"yarn-master-hostname\" # The IP of the driver/client program machine in client-mode, required for client mode sparkLocalIP = \"IP or hostname where DataIntegration runs\" # Jars containing the dependencies, required only for client and cluster modes # In cluster mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included sparkJars = \"eccenca-DataIntegration-assembly.jar\" }","title":"DataIntegration application configuration"},{"location":"deploy-and-configure/configuration/dataintegration/#dataintegration-cluster-deployment-configuration","text":"In cluster mode one should keep in mind that, normally, DataIntegration will generate a jar and a project export. These artifacts can be copied or send to a cluster and will be executed there via the spark-submit command. That means the data processing is running in its own remote process separate from the DataIntegration application. An assembly jar and a workflow can be exported by an activity that belongs to each defined workflow. The activity can be configured in the dataintegration.conf . There are 3 phases of a deployment: staging, transform, loading and 3 types of artifact compositions as well as some other options deciding the target of the export. First the specified resource are copied to the configured resource folder of a project or a temp folder (staging) and then an action decides how the files are deployed. Artifacts (spark.deployment.options.artifact): \u2018jar\u2019: The assembly jar is deployed \u2018project\u2019: The exported project zip file is deployed (e.g. if the assembly was already globally deployed) \u2018project-jar\u2019: The jar and the project zip are deployed The artifacts are copied to the configured resource folder off the project the activity belongs to. Types ( spark.deployment.options.[phase].type, e.g. spark.deployment.options.staging.type=\"env-script\" ): \u2018script\u2019 A shell script is called to copy the files to the cluster (can be user supplied, contain auth, prepare a DataFactory activity etc.) \u2018copy\u2019 The resource are copied to a specified folder \u2018hdfs\u2019 The resource is imported to HDFS \u2018env-script\u2019 A shell script is loaded from a environment variable \u2018var-script\u2019 A shell script is loaded from a configuration variable Other options: spark.deployment.options.[phase].[typeName] Depending on the selected deployment type this contains one or more (separated by a comma) targeted local file system or HDFS paths or location of the scripts to run\\ e.g. spark.deployment.options.staging.type =\"script\" and spark.deployment.options.staging.script=\"/scripts/script.sh\" spark.deployment.options.overwriteExecution Boolean value that decides if the ExecuteSparkWorkflow action is overwritten by the deployment action and will run this instead. Example: 1 2 3 4 5 6 7 8 9 10 11 spark.deployment.options = { # Specifies artifacts: Stage the project export and the executable assembly jar artifact = \"project-jar\" # Type of the deployment: Copy project and jar to /data folder, then run a script to start processing the data staging.type = \"copy\" staging.copy = \"/data\" transform.type = \"script\" transform.script = \"conf/runWorkflow.sh\" # Bind the 2 actions to the \"run workflow\" button overwriteExecution = true }","title":"DataIntegration cluster deployment configuration"},{"location":"deploy-and-configure/configuration/dataintegration/#activity-parameters-to-skip-deployment-phases","text":"In some scenarios (especially deployments where a jar has to be copied to a remote location) it is required that a deployment phase can be skipped. E.g. the jar upload only has to be done once, the upload is defined in the \u201cstaging\u201d phase and the spark-submit call in the \u201ctransform\u201d phase. The parameter \u201cexecuteTransform\u201d (reachable via the activity tab) can\\ be set to false on the second run to avoid re-uploading artifacts.","title":"Activity parameters to skip deployment phases"},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-of-the-assembly-jar","text":"In cluster mode, usually, we run a Spark Job by submitting an assembly jar to the cluster. This can be seen a command line version of DataIntegration and can also be used manually with \u2018spark-submit\u2019. In this case the configuration in the environment the jar runs in should look like this (options are set by the spark-submit configuration and parameters): 1 2 3 4 spark.interpreter.options = { # Specifies deployment mode, requires: local, cluster, client or submit deploymentMode = \"submit\" }","title":"Configuration of the assembly jar"},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-local-mode","text":"Local mode is mainly for testing, but can be used for deployment on a single server. HDFS and Hive are not required. The following configuration parameters have to be set: 1 2 3 4 5 6 spark.interpreter.options = { # Specifies deployment mode, requires: local, cluster, client or submit deploymentMode = \"local\" # URL of the Spark cluster master node, the [*] denotes the number of executors sparkMaster = \"local[4]\" } In this mode the parameters and Spark settings appended to the \u2018spark-submit\u2019 command will always be used and overwrite configuration settings in other sources.","title":"Execution in local mode"},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-and-usage-of-the-sqlendpoint-dataset","text":"","title":"Configuration and usage of the SqlEndpoint dataset"},{"location":"deploy-and-configure/configuration/dataintegration/#server-side-configuration","text":"","title":"Server Side Configuration"},{"location":"deploy-and-configure/configuration/dataintegration/#general-settings","text":"The SqlEndpoint dataset is a table or view behaving analog to a table in a relational database like MySQL. When data is written to an SqlEndpoint dataset, a JDBC server is started and can be queried with any JDBC client. In DataIntegration the SqlEndpoint dataset behaves like any other dataset. It can be used as a target for workflows, be profiled, used as a source for an operation or workflow etc. There are a two of configuration options that are relevant for the JDBC endpoints: 1 2 3 4 5 6 7 8 9 spark.sql.options = { # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. SqlEndpoint # datasets can still be started but can only be accessed internally if set to false. startThriftServer = true # Enable Hive integration # Sets Spark to use an infrastructure for meta data that is compatible with the hive metastore enableHiveSupport = true ... } The port on which the JDBC connections will be available is 10005 by default and can be changed in the hive-site.xml and spark-defaults.conf configuration files.","title":"General Settings"},{"location":"deploy-and-configure/configuration/dataintegration/#security-settings","text":"A secure connection can be configured with the authentification settings in the hive-site.xml , spark-defaults.conf and dataintegration.conf files. If Hive support is disabled ( enableHiveSupport = false ) or if the property hive.server2.authentication has the value None security can be disabled. There exist a number of option for secure JDBC connections via Thrift and Hive: Kerberos LDAP Custom authentication classes User impersonation Server and Client Certificates Eccenca provides a custom Authentification provider which allows to set 1 user/password combination for JDBC connections via: spark.sql.options = { endpointUser = \"user\" endpointPassword = \"password\" } The authentication provider class name is com.eccenca.di.sql.endpoint.security.SqlEndpointAuth . To use it the following configuration is needed: <configuration> <property> <name> hive.server2.authentication </name> <value> CUSTOM </value> </property> <property> <name> hive.server2.custom.authentication.class </name> <value> com.eccenca.di.sql.endpoint.security.SqlEndpointAuth </value> <description> Custom authentication class. Used when property 'hive.server2.authentication' is set to 'CUSTOM'. Provided class must be a proper implementation of the interface org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2 will call its Authenticate(user, password) method to authenticate requests. The implementation may optionally implement Hadoop's org.apache.hadoop.conf.Configurable class to grab Hive's Configuration object. </description> </property> ... </configuration> Check the Hive documentation for details: Hive admin manual or the documentation of a Hadoop Distribution (MapR, Hortenworks or AWS and Azure in he cloud etc.). Hadoop distributions usually provides instructions for configuring secure endpoints. Integration with various authentication providers can be configured and is mostly set up in hive-site.xml .","title":"Security Settings"},{"location":"deploy-and-configure/configuration/dataintegration/#sqlendpoint-dataset-parameters","text":"The dataset only requires that the tableNamePrefix parameters is given. This will be used as the prefix for the names of the generated tables. When a set of Entities is written to the endpoint a view is generated for each entity type (defined by an rdf_type attribute). That means that the mapping or data source that are used as input for the SqlEndpoint need to have a type or require a user defined type mapping. The operator has a compatibility mode . Using it will avoid complex types such as Arrays. When arrays exit in the input they are converted to a String using the given arraySeperator .","title":"SqlEndpoint Dataset Parameters"},{"location":"deploy-and-configure/configuration/dataintegration/#sqlendpoint-activity","text":"The activity will start automatically, when the SqlEndpoint is used as a data sink and Dataintegration is configured to make the SqlEndpoint accessible remotely. When the activity is started and running it returns the server status and JDBC Url as its value. Stopping the activity will drop all views generated by the activity. It can be restarted by rerunning the workflow containing it as a sink.","title":"SqlEndpoint Activity"},{"location":"deploy-and-configure/configuration/dataintegration/#remote-client-configuration-via-jdbc-and-odbc","text":"Within Dataintegration the SqlEndpoint can be used as a source or sink like any other dataset. If the startThriftServer option is set to true access via JDBC or ODBC is possible. ODBC and JDBC drivers can be used to connect to relational databases. These drivers are used by clients like, Excel, PowerBI or other BI tools and transform standard SQL-queries to Hive-QL queries and handle the respective query results. Hive-QL support a subset of the SQL-92 standard. Depending on the complexity of the driver it \u2013 in case of a simple driver \u2013 supports the same subset or more modern standards. JDBC drivers are similar to ODBC ones, but serve as connectors for Java applications. When selecting a version of a driver the client operating system and its type (32bit/64 bit) are the most important factors. The version of the client drivers sometimes is the same as the servers. When no version of a driver is given the newest driver of the vendor should work, as it should be backwards compatible. Any JDBC or ODBC client can connect to a JDBC endpoint provided by an SqlEndpoint dataset. SqlEndpoint uses the same query processing as Hive, therefore the requirements for the client are: A JDBC driver compatible with Hive 1.2.1 (platform independent driver org.apache.hive.jdbc.HiveDriver is needed) or Hive 1.2.1 is ODPi runtime compliant A JDBC driver compatible with Spark 2.3.3 A Hive ODBC driver (ODBC driver for the client architecture and operating system needed) A detailed instruction to connect to a Hive or SqlEndpoint endpoint with various tools (e.g. SQuirreL, beeline, SQL Developer, \u2026) can be found at Apache HiveServer2 Clients .\\ The multi platform database client DBeaver can connect to the SQLEndpoint out of the box.","title":"Remote Client Configuration (via JDBC and ODBC)"},{"location":"deploy-and-configure/configuration/dataintegration/#partitioning-and-merging-of-data-sets","text":"The execution on Spark is possible independent of the used file system as long as it can be referenced/is accessible for all cluster nodes. HDFS is recommended and the default settings are recommended for the best performance on a small cluster. Especially when working in local mode on the local file systems some problems can occur with the parallelism settings of Spark and the resulting partitioned output resources. Problems can be avoided by changing the following are the default settings: 1 2 3 4 5 6 7 8 9 spark.interpreter.options = { # If true, data will be repartitioned before execution, # otherwise the existing partitioning or no partitioning will be used partitionOnImport = false # Number of partitions for repartitioning on import, default = 16 partitionNumber = 16 # Specifies if data is combined before output is written to disk combineOutput = false } When running only locally the configuration should be like the following example (especially combineOutput has to be true): 1 2 3 4 5 6 7 8 9 spark.interpreter.options = { # If true, data will be repartitioned before execution, # otherwise the existing partitioning or no partitioning will be used partitionOnImport = false # Number of partitions for repartitioning on import, default = 16 partitionNumber = 4 # Specifies if data is combined before output is written to disk combineOutput = true }","title":"Partitioning and merging of data sets"},{"location":"deploy-and-configure/configuration/dataintegration/#other-options-specific-to-spark","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ################################################# # Spark # ################################################# spark.interpreter.options = { # the default name used when creating a SparkContext (will override spark.app.name in spark-defaults.conf) sparkAppName = \"eccenca DataIntegration Spark exe # Enables more detailed logging, counting of transformed records, etc. debugMode = false # Enable or disable a spark executor event log for debugging purposes eventLog = true # Folder for logs that people don't want in the log even though the information is necessary for debugging logFolder = ${elds.home}\"/var/dataintegration/logs/\" # Enable or disable an execution time log for benchmarking purposes timeLog = true # Enable or disable Sparks built-in log for debugging purposes (will override spark.eventLog.enabled in spark-defaults.conf) sparkLog = true # If true, data will be repartitioned before execution, otherwise the existing partitioning or no partitioning will be used partitionOnImport = false # Number of partitions for repartitioning on Import partitionNumber = 16 # Specifies number of Spark SQL shuffle partitions (will override spark.sql.shuffle.partitions in spark-defaults.conf) shufflePartitions = 32 # Minimum partition number for Spark execution defaultMinPartitions = 4 # Default parallelism partition number for Spark execution (will override spark.sql.shuffle.partitions in spark-defaults.conf) defaultParallelism = 4 # Specifies if data is combined before output is written to disk. If true the final output will be in a single file on a single partition combineOutput = true # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. Views/virtual datasets can still be started but can only be accessed internally if set to false. startThriftServer = false # Internal data model used in Spark Data Frames: 'sequence' or 'simple'. Sequence behaves like the entities used by the local executor of DataIntegration and is # sometimes be needed to work with non relational data (i.e. triple stores, dataplatform). The default value is 'simple' and casts most data objects to Strings # which is fast and works in most situations may lead to less clean data. columnType = sequence # General additional Java options that will be passed to the executors (worker nodes) in the cluster, default is \"\". sparkExecutorJavaOptions = \"\" # General additional Java options that will be passed to the driver application (DataIntegration), default is \"\". sparkDriverJavaOptions = \"\" # Enable or disable the Spark UI. This UI provides an overview of the Spark cluster and running jobs. It will start on port 4040 # and increase the port number by 1 if the port is already in use. The final port will be shown in the logs. False by default. enableSparkUI = true # This property decides if Hive integration is enabled orr not. # To use hive an external DB (such as MYSQL), the meta store, is needed. Please specify the necessary properties in the hive-site.xml. Note that Hive's default meta store (derby) should not be used in production and may lead to issues. enableHiveSupport = false }","title":"Other options specific to Spark"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/","text":"Activity Reference \u00a4 Project Activities \u00a4 The following activities are available for each project. Dataset matcher \u00a4 Generates matches between schema paths and datasets based on the schema discovery and profiling information of the datasets. Parameter Type Description Example datasetUri String If set, run dataset matching only for this particular dataset. The identifier for this plugin is DatasetMatcher . It can be found in the package com.eccenca.di.datamatching . Task Activities \u00a4 The following activities are available for different types of tasks. Custom \u00a4 Execute REST Task \u00a4 Executes the REST task. This plugin does not require any parameters. The identifier for this plugin is ExecuteRestTask . It can be found in the package com.eccenca.di.workflow.operators.rest . Dataset \u00a4 Dataset Profiler \u00a4 Generates profiling data of a dataset, e.g. data types, statistics etc. Parameter Type Description Example datasetUri String Optional URI of the datasetresource that should be profiled. If not specified an URI will be generated. uriPrefix String Optional URI prefix that is prepended to every generated URI, e.g. property URIs for every schema path. If not specified an URI prefix will be generated. entitySample-Limit String How many entities should be sampled for the profiling. If left blank, all entities will be considered. timeLimit String The time in milliseconds that each of the schema extraction step and profiling step should spend on. Leave blank for unlimited time. classProfiling-Limit int The maximum number of classes that are profiled from the extractedschema. schemaEntity-Limit int The maximum number of overal lschema entities (types, properties/attributes) that willbe extracted. executionType String The execution type to be used: SPARK, LEGACY. The legacy execution uses large in-memory maps and takes longer! The identifier for this plugin is DatasetProfiler . It can be found in the package com.eccenca.di.profiling . SQL endpoint status \u00a4 Shows the SQL endpoint status. This plugin does not require any parameters. The identifier for this plugin is SqlEndpointStatus . It can be found in the package com.eccenca.di.sql.endpoint.activity . Types Cache \u00a4 Holds the most frequent types in a dataset. This plugin does not require any parameters. The identifier for this plugin is TypesCache . It can be found in the package org.silkframework.workspace.activity.dataset . LinkSpecification \u00a4 Active Learning \u00a4 Executes an active learning iteration. Parameter Type Description Example fixedRandom-Seed boolean No description The identifier for this plugin is ActiveLearning . It can be found in the package org.silkframework.learning.active . Evaluate linking \u00a4 Evaluates the linking task by generating links. Parameter Type Description Example includeReference-Links boolean Do not generate a link for which there is a negative reference link while always generating positive reference links. useFileCache boolean Use a file cache. This avoids memory overflows for big files. partitionSize int The number of entities in a single partition in the cache. generateLinksWith-Entities boolean Generate detailed information about the matched entities. If set to false, the generated links won\u2019t be shown in the Workbench. writeOutputs boolean Write the generated links to the configured output of this task. linkLimit int If defined, the execution will stop after the configured number of links is reached. This is just a hint and the execution may produce slightly fewer or more links. timeout int Timeout in seconds after that the matching task of an evaluation should be aborted. Set to 0 or negative to disable the timeout. The identifier for this plugin is EvaluateLinking . It can be found in the package org.silkframework.workspace.activity.linking . Execute linking \u00a4 Executes the linking task using the configured execution. This plugin does not require any parameters. The identifier for this plugin is ExecuteLinking . It can be found in the package org.silkframework.workspace.activity.linking . Linking paths cache \u00a4 Holds the most frequent paths for the selected entities. This plugin does not require any parameters. The identifier for this plugin is LinkingPathsCache . It can be found in the package org.silkframework.workspace.activity.linking . Reference entities cache \u00a4 For each reference link, the reference entities cache holds all values of the linked entities. This plugin does not require any parameters. The identifier for this plugin is ReferenceEntitiesCache . It can be found in the package org.silkframework.workspace.activity.linking . Supervised learning \u00a4 Executes the supervised learning. This plugin does not require any parameters. The identifier for this plugin is SupervisedLearning . It can be found in the package org.silkframework.learning.active . Scheduler \u00a4 Activate \u00a4 Executes the scheduler This plugin does not require any parameters. The identifier for this plugin is ExecuteScheduler . It can be found in the package com.eccenca.di.scheduler . ScriptTask \u00a4 Execute Script \u00a4 Executes the script. This plugin does not require any parameters. The identifier for this plugin is ExecuteScript . It can be found in the package com.eccenca.di.scripttask . TransformSpecification \u00a4 Execute transform \u00a4 Executes the transformation. Parameter Type Description Example limit IntOptionParameter Limits the maximum number of entities that are transformed. The identifier for this plugin is ExecuteTransform . It can be found in the package org.silkframework.workspace.activity.transform . Transform paths cache \u00a4 Holds the most frequent paths for the selected entities. This plugin does not require any parameters. The identifier for this plugin is TransformPathsCache . It can be found in the package org.silkframework.workspace.activity.transform . Target vocabulary cache \u00a4 Holds the target vocabularies This plugin does not require any parameters. The identifier for this plugin is VocabularyCache . It can be found in the package org.silkframework.workspace.activity.transform . Workflow \u00a4 Execute locally \u00a4 Executes the workflow locally. This plugin does not require any parameters. The identifier for this plugin is ExecuteLocalWorkflow . It can be found in the package org.silkframework.workspace.activity.workflow . WorkflowExecution \u00a4 Generate Spark assembly \u00a4 Generate project and Spark assembly artifacts and deploy them using the specified configuration settings: type, artifact and options like destination in case of a simple copy Parameter Type Description Example executeStaging boolean Execute loading phase executeTransform boolean Execute transform phase executeLoading boolean Execute staging phase The identifier for this plugin is DeploySparkWorkflow . It can be found in the package com.eccenca.di.spark . Default execution \u00a4 Executes a workflow with the executor defined in the configuration This plugin does not require any parameters. The identifier for this plugin is ExecuteDefaultWorkflow . It can be found in the package com.eccenca.di.spark . Execute operator \u00a4 Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster. Parameter Type Description Example operator TaskReference The workflow to execute. The identifier for this plugin is ExecuteSparkOperator . It can be found in the package com.eccenca.di.spark . Execute on Spark \u00a4 Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster. This plugin does not require any parameters. The identifier for this plugin is ExecuteSparkWorkflow . It can be found in the package com.eccenca.di.spark . Execute with payload \u00a4 Executes a workflow with custom payload. Parameter Type Description Example configuration MultilineStringParameter No description configuration-Type String No description The identifier for this plugin is ExecuteWorkflowWithPayload . It can be found in the package org.silkframework.workbench.workflow . Generate view \u00a4 Generate and share a view on a workflow executed by the Spark executor. Executes a workflow on Spark and generates a SparkSQL temporary table instead of serializing the result. The table can be accessed via JDBC Parameter Type Description Example caching boolean Optional parameter that enables caching (default=false). userDefined-Name String Optional View name that is used when a view on a non virtual is generated (default =[TASK-ID]_generated_view). The identifier for this plugin is GenerateSparkView . It can be found in the package com.eccenca.di.sql.virtual . Hive 1.2.1 is ODPi runtime compliant \u21a9 ^/ \u21a9","title":"Activity Reference"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#activity-reference","text":"","title":"Activity Reference"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#project-activities","text":"The following activities are available for each project.","title":"Project Activities"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset-matcher","text":"Generates matches between schema paths and datasets based on the schema discovery and profiling information of the datasets. Parameter Type Description Example datasetUri String If set, run dataset matching only for this particular dataset. The identifier for this plugin is DatasetMatcher . It can be found in the package com.eccenca.di.datamatching .","title":"Dataset matcher"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#task-activities","text":"The following activities are available for different types of tasks.","title":"Task Activities"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#custom","text":"","title":"Custom"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-rest-task","text":"Executes the REST task. This plugin does not require any parameters. The identifier for this plugin is ExecuteRestTask . It can be found in the package com.eccenca.di.workflow.operators.rest .","title":"Execute REST Task"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset","text":"","title":"Dataset"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset-profiler","text":"Generates profiling data of a dataset, e.g. data types, statistics etc. Parameter Type Description Example datasetUri String Optional URI of the datasetresource that should be profiled. If not specified an URI will be generated. uriPrefix String Optional URI prefix that is prepended to every generated URI, e.g. property URIs for every schema path. If not specified an URI prefix will be generated. entitySample-Limit String How many entities should be sampled for the profiling. If left blank, all entities will be considered. timeLimit String The time in milliseconds that each of the schema extraction step and profiling step should spend on. Leave blank for unlimited time. classProfiling-Limit int The maximum number of classes that are profiled from the extractedschema. schemaEntity-Limit int The maximum number of overal lschema entities (types, properties/attributes) that willbe extracted. executionType String The execution type to be used: SPARK, LEGACY. The legacy execution uses large in-memory maps and takes longer! The identifier for this plugin is DatasetProfiler . It can be found in the package com.eccenca.di.profiling .","title":"Dataset Profiler"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#sql-endpoint-status","text":"Shows the SQL endpoint status. This plugin does not require any parameters. The identifier for this plugin is SqlEndpointStatus . It can be found in the package com.eccenca.di.sql.endpoint.activity .","title":"SQL endpoint status"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#types-cache","text":"Holds the most frequent types in a dataset. This plugin does not require any parameters. The identifier for this plugin is TypesCache . It can be found in the package org.silkframework.workspace.activity.dataset .","title":"Types Cache"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#linkspecification","text":"","title":"LinkSpecification"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#active-learning","text":"Executes an active learning iteration. Parameter Type Description Example fixedRandom-Seed boolean No description The identifier for this plugin is ActiveLearning . It can be found in the package org.silkframework.learning.active .","title":"Active Learning"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#evaluate-linking","text":"Evaluates the linking task by generating links. Parameter Type Description Example includeReference-Links boolean Do not generate a link for which there is a negative reference link while always generating positive reference links. useFileCache boolean Use a file cache. This avoids memory overflows for big files. partitionSize int The number of entities in a single partition in the cache. generateLinksWith-Entities boolean Generate detailed information about the matched entities. If set to false, the generated links won\u2019t be shown in the Workbench. writeOutputs boolean Write the generated links to the configured output of this task. linkLimit int If defined, the execution will stop after the configured number of links is reached. This is just a hint and the execution may produce slightly fewer or more links. timeout int Timeout in seconds after that the matching task of an evaluation should be aborted. Set to 0 or negative to disable the timeout. The identifier for this plugin is EvaluateLinking . It can be found in the package org.silkframework.workspace.activity.linking .","title":"Evaluate linking"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-linking","text":"Executes the linking task using the configured execution. This plugin does not require any parameters. The identifier for this plugin is ExecuteLinking . It can be found in the package org.silkframework.workspace.activity.linking .","title":"Execute linking"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#linking-paths-cache","text":"Holds the most frequent paths for the selected entities. This plugin does not require any parameters. The identifier for this plugin is LinkingPathsCache . It can be found in the package org.silkframework.workspace.activity.linking .","title":"Linking paths cache"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#reference-entities-cache","text":"For each reference link, the reference entities cache holds all values of the linked entities. This plugin does not require any parameters. The identifier for this plugin is ReferenceEntitiesCache . It can be found in the package org.silkframework.workspace.activity.linking .","title":"Reference entities cache"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#supervised-learning","text":"Executes the supervised learning. This plugin does not require any parameters. The identifier for this plugin is SupervisedLearning . It can be found in the package org.silkframework.learning.active .","title":"Supervised learning"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#scheduler","text":"","title":"Scheduler"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#activate","text":"Executes the scheduler This plugin does not require any parameters. The identifier for this plugin is ExecuteScheduler . It can be found in the package com.eccenca.di.scheduler .","title":"Activate"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#scripttask","text":"","title":"ScriptTask"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-script","text":"Executes the script. This plugin does not require any parameters. The identifier for this plugin is ExecuteScript . It can be found in the package com.eccenca.di.scripttask .","title":"Execute Script"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#transformspecification","text":"","title":"TransformSpecification"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-transform","text":"Executes the transformation. Parameter Type Description Example limit IntOptionParameter Limits the maximum number of entities that are transformed. The identifier for this plugin is ExecuteTransform . It can be found in the package org.silkframework.workspace.activity.transform .","title":"Execute transform"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#transform-paths-cache","text":"Holds the most frequent paths for the selected entities. This plugin does not require any parameters. The identifier for this plugin is TransformPathsCache . It can be found in the package org.silkframework.workspace.activity.transform .","title":"Transform paths cache"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#target-vocabulary-cache","text":"Holds the target vocabularies This plugin does not require any parameters. The identifier for this plugin is VocabularyCache . It can be found in the package org.silkframework.workspace.activity.transform .","title":"Target vocabulary cache"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#workflow","text":"","title":"Workflow"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-locally","text":"Executes the workflow locally. This plugin does not require any parameters. The identifier for this plugin is ExecuteLocalWorkflow . It can be found in the package org.silkframework.workspace.activity.workflow .","title":"Execute locally"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#workflowexecution","text":"","title":"WorkflowExecution"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#generate-spark-assembly","text":"Generate project and Spark assembly artifacts and deploy them using the specified configuration settings: type, artifact and options like destination in case of a simple copy Parameter Type Description Example executeStaging boolean Execute loading phase executeTransform boolean Execute transform phase executeLoading boolean Execute staging phase The identifier for this plugin is DeploySparkWorkflow . It can be found in the package com.eccenca.di.spark .","title":"Generate Spark assembly"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#default-execution","text":"Executes a workflow with the executor defined in the configuration This plugin does not require any parameters. The identifier for this plugin is ExecuteDefaultWorkflow . It can be found in the package com.eccenca.di.spark .","title":"Default execution"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-operator","text":"Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster. Parameter Type Description Example operator TaskReference The workflow to execute. The identifier for this plugin is ExecuteSparkOperator . It can be found in the package com.eccenca.di.spark .","title":"Execute operator"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-on-spark","text":"Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster. This plugin does not require any parameters. The identifier for this plugin is ExecuteSparkWorkflow . It can be found in the package com.eccenca.di.spark .","title":"Execute on Spark"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-with-payload","text":"Executes a workflow with custom payload. Parameter Type Description Example configuration MultilineStringParameter No description configuration-Type String No description The identifier for this plugin is ExecuteWorkflowWithPayload . It can be found in the package org.silkframework.workbench.workflow .","title":"Execute with payload"},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#generate-view","text":"Generate and share a view on a workflow executed by the Spark executor. Executes a workflow on Spark and generates a SparkSQL temporary table instead of serializing the result. The table can be accessed via JDBC Parameter Type Description Example caching boolean Optional parameter that enables caching (default=false). userDefined-Name String Optional View name that is used when a view on a non virtual is generated (default =[TASK-ID]_generated_view). The identifier for this plugin is GenerateSparkView . It can be found in the package com.eccenca.di.sql.virtual . Hive 1.2.1 is ODPi runtime compliant \u21a9 ^/ \u21a9","title":"Generate view"},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/datamanager/","text":"DataManager \u00a4 This page describes how to configure eccenca DataManager. It is intended for system administrators, who are responsible for installing, configuring, maintaining and supporting the deployment of DataManager. eccenca DataManager is a single-page JavaScript application, which means the application consists of a single HTML page which loads all needed web resources in the browser after loading the page itself. In the context of DataManager, these web resources are: The application including its configuration (app*.js, config.js) Styles (*.css) Web fonts for typography as well as for icons ( .woff, .ttf, *.eot) Images (e.g. logos) ( .png, .svg) DataManager communicates with different API endpoints in order to retrieve and manipulate data. The features of DataManager include: Dataset Manager to create and update datasets and its meta data Vocabulary Manager to install and remove Vocabulary descriptions Data browser to explore and manage graph-based data Taxonomy Editor to manage and create SKOS based taxonomies Query editor to query graph-based data via SPARQL queries Access control Compliance of W3C standards such as RDF , Linked Data and SPARQL","title":"DataManager"},{"location":"deploy-and-configure/configuration/datamanager/#datamanager","text":"This page describes how to configure eccenca DataManager. It is intended for system administrators, who are responsible for installing, configuring, maintaining and supporting the deployment of DataManager. eccenca DataManager is a single-page JavaScript application, which means the application consists of a single HTML page which loads all needed web resources in the browser after loading the page itself. In the context of DataManager, these web resources are: The application including its configuration (app*.js, config.js) Styles (*.css) Web fonts for typography as well as for icons ( .woff, .ttf, *.eot) Images (e.g. logos) ( .png, .svg) DataManager communicates with different API endpoints in order to retrieve and manipulate data. The features of DataManager include: Dataset Manager to create and update datasets and its meta data Vocabulary Manager to install and remove Vocabulary descriptions Data browser to explore and manage graph-based data Taxonomy Editor to manage and create SKOS based taxonomies Query editor to query graph-based data via SPARQL queries Access control Compliance of W3C standards such as RDF , Linked Data and SPARQL","title":"DataManager"},{"location":"deploy-and-configure/configuration/datamanager/account-settings-module/","text":"Account Settings module \u00a4 Configuration property: modules.accountSettings | Scope: app-wide and per workspace DataManager provides a menu point to edit the account settings. It also need to be enabled in keycloak. js.config.modules.accountSettings url enable Property Default Required Conflicts with Valid values js.config.modules.accountSettings.enable true no none boolean js.config.modules.accountSettings.url http://docker.local/auth/realms/cmem/account/?referer={{REFERRER}}&referrer_uri={{REFERRER_URI}} yes if enable is true none url string with placeholders: {{REFERRER}}, {{ REFERRER_URI}}","title":"Account Settings module"},{"location":"deploy-and-configure/configuration/datamanager/account-settings-module/#account-settings-module","text":"Configuration property: modules.accountSettings | Scope: app-wide and per workspace DataManager provides a menu point to edit the account settings. It also need to be enabled in keycloak. js.config.modules.accountSettings url enable Property Default Required Conflicts with Valid values js.config.modules.accountSettings.enable true no none boolean js.config.modules.accountSettings.url http://docker.local/auth/realms/cmem/account/?referer={{REFERRER}}&referrer_uri={{REFERRER_URI}} yes if enable is true none url string with placeholders: {{REFERRER}}, {{ REFERRER_URI}}","title":"Account Settings module"},{"location":"deploy-and-configure/configuration/datamanager/admin-module/","text":"Admin module \u00a4 Configuration property: modules.administration | Scope: app-wide and per workspace The Admin module of DataManager is used to manage user access rights. js.config.modules.administration enable accessConditions graph Property Default Required Conflicts with Valid values js.config.modules.administration.enable true no none boolean Set this property to true to enable the Admin module of DataManager. Note: If this property is set to false , all other settings of modules.administration are skipped. Property Default Required Conflicts with Valid values js.config.modules.administration.accessConditions.graph none yes none string (URI) Set this property to define the graph of saved access conditions. Configuration example \u00a4 js.config.modules : administration : enable : true accessConditions : graph : \"urn:elds-backend-access-conditions-graph\"","title":"Admin module"},{"location":"deploy-and-configure/configuration/datamanager/admin-module/#admin-module","text":"Configuration property: modules.administration | Scope: app-wide and per workspace The Admin module of DataManager is used to manage user access rights. js.config.modules.administration enable accessConditions graph Property Default Required Conflicts with Valid values js.config.modules.administration.enable true no none boolean Set this property to true to enable the Admin module of DataManager. Note: If this property is set to false , all other settings of modules.administration are skipped. Property Default Required Conflicts with Valid values js.config.modules.administration.accessConditions.graph none yes none string (URI) Set this property to define the graph of saved access conditions.","title":"Admin module"},{"location":"deploy-and-configure/configuration/datamanager/admin-module/#configuration-example","text":"js.config.modules : administration : enable : true accessConditions : graph : \"urn:elds-backend-access-conditions-graph\"","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/api-endpoints/","text":"Api endpoints \u00a4 Configuration property: api | Scope: app-wide and per workspace DataManager provides the option to define which endpoints should be used for SPARQL and SPARQL Update requests. js.config.api sparql sparqlUpdate defaultTimeout Property Default Required Conflicts with Valid values js.config.api.sparql \u2018/proxy/:endpointId/sparql\u2019 yes none string Use this property to define the default endpoint for all SPARQL requests. Note: When a relative path is set the base url will be added automatically. The placeholder :endpointId will be set according to the used workspace defined at js.config.workspaces[id].backend.endpointId . Property Default Required Conflicts with Valid values js.config.api.sparqlUpdate \u2018/proxy/:endpointId/update\u2019 yes none string Use this property to define the default endpoint for all SPARQL Update requests. Note: When a relative path is set the base url will be added automatically. The placeholder :endpointId will be set according to the used workspace defined at js.config.workspaces[id].backend.endpointId . Property Default Required Conflicts with Valid values js.config.api.defaultTimeout 60000 no none number Set this property to limit the timeout (in milliseconds) requesting data in the tables of DataManager. Configuration example \u00a4 js.config.api : sparql : /proxy/:endpointId/sparql sparqlUpdate : /proxy/:endpointId/update defaultTimeout : 60000","title":"Api endpoints"},{"location":"deploy-and-configure/configuration/datamanager/api-endpoints/#api-endpoints","text":"Configuration property: api | Scope: app-wide and per workspace DataManager provides the option to define which endpoints should be used for SPARQL and SPARQL Update requests. js.config.api sparql sparqlUpdate defaultTimeout Property Default Required Conflicts with Valid values js.config.api.sparql \u2018/proxy/:endpointId/sparql\u2019 yes none string Use this property to define the default endpoint for all SPARQL requests. Note: When a relative path is set the base url will be added automatically. The placeholder :endpointId will be set according to the used workspace defined at js.config.workspaces[id].backend.endpointId . Property Default Required Conflicts with Valid values js.config.api.sparqlUpdate \u2018/proxy/:endpointId/update\u2019 yes none string Use this property to define the default endpoint for all SPARQL Update requests. Note: When a relative path is set the base url will be added automatically. The placeholder :endpointId will be set according to the used workspace defined at js.config.workspaces[id].backend.endpointId . Property Default Required Conflicts with Valid values js.config.api.defaultTimeout 60000 no none number Set this property to limit the timeout (in milliseconds) requesting data in the tables of DataManager.","title":"Api endpoints"},{"location":"deploy-and-configure/configuration/datamanager/api-endpoints/#configuration-example","text":"js.config.api : sparql : /proxy/:endpointId/sparql sparqlUpdate : /proxy/:endpointId/update defaultTimeout : 60000","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/app-presentation/","text":"App presentation \u00a4 Configuration property: appPresentation | Scope: app-wide and per workspace DataManager provides the option to customize the visual presentation. js.config.appPresentation faviconUrl windowTitle logoUrl headerName Property Default Required Conflicts with Valid values js.config.appPresentation.faviconUrl none no none string (URL) Use this property to define a custom favicon that is displayed in browser tab. Pictures can be an URL or a Data URL . Property Default Required Conflicts with Valid values js.config.appPresentation.windowTitle \u2018eccenca DataManager\u2019 no none string Use this property to define a custom browser tab name. Property Default Required Conflicts with Valid values js.config.appPresentation.logoUrl none no none string (URL) Use this property to define a custom logo that is shown in the Module bar. Pictures can be a an URL or a Data URL . Property Default Required Conflicts with Valid values js.config.appPresentation.headerName Use this property to define a custom name that is shown in the Module bar. Configuration example \u00a4 js.config.appPresentation : faviconUrl : https://example.com/example/favicon.png windowTitle : Datamanager logoUrl : https://example.com/example/logo.png headerName : Datamanager","title":"App presentation"},{"location":"deploy-and-configure/configuration/datamanager/app-presentation/#app-presentation","text":"Configuration property: appPresentation | Scope: app-wide and per workspace DataManager provides the option to customize the visual presentation. js.config.appPresentation faviconUrl windowTitle logoUrl headerName Property Default Required Conflicts with Valid values js.config.appPresentation.faviconUrl none no none string (URL) Use this property to define a custom favicon that is displayed in browser tab. Pictures can be an URL or a Data URL . Property Default Required Conflicts with Valid values js.config.appPresentation.windowTitle \u2018eccenca DataManager\u2019 no none string Use this property to define a custom browser tab name. Property Default Required Conflicts with Valid values js.config.appPresentation.logoUrl none no none string (URL) Use this property to define a custom logo that is shown in the Module bar. Pictures can be a an URL or a Data URL . Property Default Required Conflicts with Valid values js.config.appPresentation.headerName Use this property to define a custom name that is shown in the Module bar.","title":"App presentation"},{"location":"deploy-and-configure/configuration/datamanager/app-presentation/#configuration-example","text":"js.config.appPresentation : faviconUrl : https://example.com/example/favicon.png windowTitle : Datamanager logoUrl : https://example.com/example/logo.png headerName : Datamanager","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/application-logging-with-logback/","text":"Application logging with Logback \u00a4 Logging for eccenca DataManager can also be configured with Logback , which, for example, allows a more granular control on file rolling strategies. For further information on configuration options, refer to the Logback\u2019s Configuration manual section and the Spring Boot\u2019s Configure Logback for logging manual section. Property Default Required Conflicts with Valid values logging.configuration none no none string (file path) Use this property to specify where the Logback configuration is located. Configuration example \u00a4 logging : configuration : \"${ELDS_HOME}/etc/datamanager/logback.xml\" The following example logback.xml file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <appender name=\"TIME_BASED_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <file>/opt/elds/var/log/datamanager.log</file> <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"> <!-- daily rollover, history for 1 week --> <fileNamePattern>/opt/elds/var/log/datamanger.%d{yyyy-MM-dd}.log</fileNamePattern> <maxHistory>7</maxHistory> </rollingPolicy> <encoder> <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern> </encoder> </appender> <logger name=\"com.eccenca\" level=\"INFO\"> <appender-ref ref=\"TIME_BASED_FILE\" /> </logger> </configuration>","title":"Application logging with Logback"},{"location":"deploy-and-configure/configuration/datamanager/application-logging-with-logback/#application-logging-with-logback","text":"Logging for eccenca DataManager can also be configured with Logback , which, for example, allows a more granular control on file rolling strategies. For further information on configuration options, refer to the Logback\u2019s Configuration manual section and the Spring Boot\u2019s Configure Logback for logging manual section. Property Default Required Conflicts with Valid values logging.configuration none no none string (file path) Use this property to specify where the Logback configuration is located.","title":"Application logging with Logback"},{"location":"deploy-and-configure/configuration/datamanager/application-logging-with-logback/#configuration-example","text":"logging : configuration : \"${ELDS_HOME}/etc/datamanager/logback.xml\" The following example logback.xml file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <appender name=\"TIME_BASED_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <file>/opt/elds/var/log/datamanager.log</file> <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"> <!-- daily rollover, history for 1 week --> <fileNamePattern>/opt/elds/var/log/datamanger.%d{yyyy-MM-dd}.log</fileNamePattern> <maxHistory>7</maxHistory> </rollingPolicy> <encoder> <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern> </encoder> </appender> <logger name=\"com.eccenca\" level=\"INFO\"> <appender-ref ref=\"TIME_BASED_FILE\" /> </logger> </configuration>","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/dataset-module/","text":"Dataset module \u00a4 Configuration property: modules.datasets | Scope: app-wide and per workspace The Dataset module of DataManager is used to manage datasets and their attached resources. js.config.modules.datasets enable startWith graphUrl dataintegration url project Property Default Required Conflicts with Valid values js.config.modules.datasets.enable false no none boolean Set this property to true to enable the Dataset module of DataManager. Note: If this property is set to false , all other settings of modules.datasets are skipped. To use the module you also need to have read access to the graphs specified in js.config.modules.vocabulary.graphUrl and js.config.shacl.shapesGraph as well as the access control action urn:eccenca:di . Property Default Required Conflicts with Valid values js.config.modules.datasets.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.datasets.graphUrl none yes none string (URI) Use this property to define the target graph for read and write operations. Property Default Required Conflicts with Valid values js.config.modules.datasets.dataintegration.url none yes none string (URL) Use this property to define the URL of DataIntegration that is needed for dataset workflows. Property Default Required Conflicts with Valid values js.config.modules.datasets.dataintegration.project none yes none string Use this property to define the name of the DataIntegration project. Configuration example \u00a4 js.config.modules : datasets : enable : true startWith : false graphUrl : https://example.com/example/datasets/ dataintegration : url : https://example.com/dataintegration/ project : your_project_name","title":"Dataset module"},{"location":"deploy-and-configure/configuration/datamanager/dataset-module/#dataset-module","text":"Configuration property: modules.datasets | Scope: app-wide and per workspace The Dataset module of DataManager is used to manage datasets and their attached resources. js.config.modules.datasets enable startWith graphUrl dataintegration url project Property Default Required Conflicts with Valid values js.config.modules.datasets.enable false no none boolean Set this property to true to enable the Dataset module of DataManager. Note: If this property is set to false , all other settings of modules.datasets are skipped. To use the module you also need to have read access to the graphs specified in js.config.modules.vocabulary.graphUrl and js.config.shacl.shapesGraph as well as the access control action urn:eccenca:di . Property Default Required Conflicts with Valid values js.config.modules.datasets.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.datasets.graphUrl none yes none string (URI) Use this property to define the target graph for read and write operations. Property Default Required Conflicts with Valid values js.config.modules.datasets.dataintegration.url none yes none string (URL) Use this property to define the URL of DataIntegration that is needed for dataset workflows. Property Default Required Conflicts with Valid values js.config.modules.datasets.dataintegration.project none yes none string Use this property to define the name of the DataIntegration project.","title":"Dataset module"},{"location":"deploy-and-configure/configuration/datamanager/dataset-module/#configuration-example","text":"js.config.modules : datasets : enable : true startWith : false graphUrl : https://example.com/example/datasets/ dataintegration : url : https://example.com/dataintegration/ project : your_project_name","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/default-configuration/","text":"Default Configuration \u00a4 The following configuration shows the default configuration of DataManager. js.config.workspaces : default : name : Eccenca Vocabulary Service authorization : type : anonymous backend : type : dataplatform url : https://vocab.eccenca.com/ endpointId : default js.config.appPresentation : windowTitle : eccenca DataManager headerName : DataManager js.config.shacl : shapesGraph : https://vocab.eccenca.com/shacl/ js.config.resourceTable : timeoutDownload : 600000 js.config.api : sparql : /proxy/:endpointId/sparql sparqlQueryBase64Encoded : false sparqlUpdate : /proxy/:endpointId/update defaultTimeout : 60000 js.config.errorPages : graphAccess : title : Unauthorized User message : You are not authorized to use this workspace. moduleAccess : title : No module accessible message : You have no access to any module. workspaceAccess : title : Workspace access problem message : You are logged in successfully but you do not have enough permissions. Please contact your administrator. js.config.titleHelper : properties : - http://www.w3.org/ns/shacl#name - http://www.w3.org/2004/02/skos/core#prefLabel - http://xmlns.com/foaf/0.1/name - http://purl.org/dc/elements/1.1/title - http://purl.org/dc/terms/title - http://www.w3.org/2000/01/rdf-schema#label languages : - en - '' js.config.userPermissions : allowCreateWorkspace : true allowSelectWorkspace : true js.config.modules.task : enable : true js.config.modules.administration : enable : true accessConditions : graph : urn:elds-backend-access-conditions-graph js.config.modules.datasets : enable : false js.config.modules.explore : enable : true shacl : allowCopy : false showGraphInfo : false useSaveApi : false graphlist : defaultGraph : '' hideSearch : false navigation : defaultClass : '' itemsPerPage : 10 topQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf owl:Thing . } UNION { ?r a ?resource . FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } UNION { ?resource a owl:Class FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) } subQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { ?resource rdfs:subClassOf {{RESOURCE}} . OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) } searchQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX skos: <http://www.w3.org/2004/02/skos/core#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf+ owl:Thing } UNION { ?r a ?resource . } UNION { ?resource a owl:Class } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } OPTIONAL { ?resource rdfs:label ?label1 . } OPTIONAL { ?resource skos:prefLabel ?label2 . } BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) FILTER(isIRI(?resource)) . FILTER( regex(str(?resource),\"{{QUERY}}\",\"i\") || regex(str(?label1),\"{{QUERY}}\",\"i\") || regex(str(?label2),\"{{QUERY}}\",\"i\") ) } listQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?instance {{FROM}} WHERE { { ?instance a {{RESOURCE}} . FILTER isIRI(?instance) . } UNION { ?class rdfs:subClassOf+ {{RESOURCE}} . ?instance a ?class . FILTER isIRI(?instance). } } overallSearchQuery : | SELECT ?resource {{FROM}} WHERE { ?resource ?p0 ?label. FILTER (!isBLANK(?resource)). FILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))). } details : properties : enable : true usage : enable : true references : enable : true turtle : enable : true statistics : enable : true sunburst : enable : true visualization : enable : true webvowlConfig : filter : literals : true relations : true solitarySubclasses : true classDisjointness : true setOperators : true degreeOfCollapsing : true mode : dynamicLabelWidth : true pickAndPin : true nodeScaling : true compactNotation : true colorExternals : true export : json : true svg : true gravity : classDistance : true dataTypeDistance : true reset : true pause : true search : true js.config.modules.thesaurus : enable : true js.config.modules.query : enable : true graph : https://ns.eccenca.com/data/queries/ timeout : 600000 js.config.modules.vocabulary : enable : false js.config.modules.gdprsearch : enable : false js.config.modules.linkrules : enable : false js.config.modules.annotation : enable : false js.config.modules.search : enable : false js.config.modules.tracking : enable : false js.config.modules.reports : enable : false","title":"Default Configuration"},{"location":"deploy-and-configure/configuration/datamanager/default-configuration/#default-configuration","text":"The following configuration shows the default configuration of DataManager. js.config.workspaces : default : name : Eccenca Vocabulary Service authorization : type : anonymous backend : type : dataplatform url : https://vocab.eccenca.com/ endpointId : default js.config.appPresentation : windowTitle : eccenca DataManager headerName : DataManager js.config.shacl : shapesGraph : https://vocab.eccenca.com/shacl/ js.config.resourceTable : timeoutDownload : 600000 js.config.api : sparql : /proxy/:endpointId/sparql sparqlQueryBase64Encoded : false sparqlUpdate : /proxy/:endpointId/update defaultTimeout : 60000 js.config.errorPages : graphAccess : title : Unauthorized User message : You are not authorized to use this workspace. moduleAccess : title : No module accessible message : You have no access to any module. workspaceAccess : title : Workspace access problem message : You are logged in successfully but you do not have enough permissions. Please contact your administrator. js.config.titleHelper : properties : - http://www.w3.org/ns/shacl#name - http://www.w3.org/2004/02/skos/core#prefLabel - http://xmlns.com/foaf/0.1/name - http://purl.org/dc/elements/1.1/title - http://purl.org/dc/terms/title - http://www.w3.org/2000/01/rdf-schema#label languages : - en - '' js.config.userPermissions : allowCreateWorkspace : true allowSelectWorkspace : true js.config.modules.task : enable : true js.config.modules.administration : enable : true accessConditions : graph : urn:elds-backend-access-conditions-graph js.config.modules.datasets : enable : false js.config.modules.explore : enable : true shacl : allowCopy : false showGraphInfo : false useSaveApi : false graphlist : defaultGraph : '' hideSearch : false navigation : defaultClass : '' itemsPerPage : 10 topQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf owl:Thing . } UNION { ?r a ?resource . FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } UNION { ?resource a owl:Class FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) } subQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { ?resource rdfs:subClassOf {{RESOURCE}} . OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) } searchQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX skos: <http://www.w3.org/2004/02/skos/core#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf+ owl:Thing } UNION { ?r a ?resource . } UNION { ?resource a owl:Class } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } OPTIONAL { ?resource rdfs:label ?label1 . } OPTIONAL { ?resource skos:prefLabel ?label2 . } BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) FILTER(isIRI(?resource)) . FILTER( regex(str(?resource),\"{{QUERY}}\",\"i\") || regex(str(?label1),\"{{QUERY}}\",\"i\") || regex(str(?label2),\"{{QUERY}}\",\"i\") ) } listQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?instance {{FROM}} WHERE { { ?instance a {{RESOURCE}} . FILTER isIRI(?instance) . } UNION { ?class rdfs:subClassOf+ {{RESOURCE}} . ?instance a ?class . FILTER isIRI(?instance). } } overallSearchQuery : | SELECT ?resource {{FROM}} WHERE { ?resource ?p0 ?label. FILTER (!isBLANK(?resource)). FILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))). } details : properties : enable : true usage : enable : true references : enable : true turtle : enable : true statistics : enable : true sunburst : enable : true visualization : enable : true webvowlConfig : filter : literals : true relations : true solitarySubclasses : true classDisjointness : true setOperators : true degreeOfCollapsing : true mode : dynamicLabelWidth : true pickAndPin : true nodeScaling : true compactNotation : true colorExternals : true export : json : true svg : true gravity : classDistance : true dataTypeDistance : true reset : true pause : true search : true js.config.modules.thesaurus : enable : true js.config.modules.query : enable : true graph : https://ns.eccenca.com/data/queries/ timeout : 600000 js.config.modules.vocabulary : enable : false js.config.modules.gdprsearch : enable : false js.config.modules.linkrules : enable : false js.config.modules.annotation : enable : false js.config.modules.search : enable : false js.config.modules.tracking : enable : false js.config.modules.reports : enable : false","title":"Default Configuration"},{"location":"deploy-and-configure/configuration/datamanager/error-pages/","text":"Error pages \u00a4 Configuration property: errorPages | Scope: app-wide and per workspace DataManager provides the option to customize special user errors. js.config.errorPages graphAccess title message moduleAccess title message workspaceAccess title message Property Default Required Conflicts with Valid values js.config.errorPages.graphAccess.title \u2018Unauthorized User\u2019 no none string Use this property to define a custom title that is displayed if a user has no read access to any graph of the selected workspace. Property Default Required Conflicts with Valid values js.config.errorPages.graphAccess.message \u2018You are not authorized to use this workspace.\u2019 no none string Use this property to define a custom message that is displayed if a user has no read access to any graph of the selected workspace. Property Default Required Conflicts with Valid values js.config.errorPages.moduleAccess.title \u2018No module accessible\u2019 no none string Use this property to define a custom title that is displayed if a user has no permission to any module of DataManager. Property Default Required Conflicts with Valid values js.config.errorPages.moduleAccess.message \u2018You have no access to any module.\u2019 no none string Use this property to define a custom message that is displayed if a user has no permission to any module of DataManager. Property Default Required Conflicts with Valid values js.config.errorPages.workspaceAccess.title \u2018Workspace access problem\u2019 no none string Use this property to define a custom title that is displayed if the workspace is not accessible. Property Default Required Conflicts with Valid values js.config.errorPages.workspaceAccess.message \u2018The configured workspace is not accessible.\u2019 no none string Use this property to define a custom message that is displayed if the workspace is not accessible. Configuration example \u00a4 js.config.errorPages : graphAccess : title : Unauthorized User message : You are not authorized to use this workspace. moduleAccess : title : No module accessible message : You have no access to any module. workspaceAccess : title : Workspace access problem message: : The configured workspace is not accessible.","title":"Error pages"},{"location":"deploy-and-configure/configuration/datamanager/error-pages/#error-pages","text":"Configuration property: errorPages | Scope: app-wide and per workspace DataManager provides the option to customize special user errors. js.config.errorPages graphAccess title message moduleAccess title message workspaceAccess title message Property Default Required Conflicts with Valid values js.config.errorPages.graphAccess.title \u2018Unauthorized User\u2019 no none string Use this property to define a custom title that is displayed if a user has no read access to any graph of the selected workspace. Property Default Required Conflicts with Valid values js.config.errorPages.graphAccess.message \u2018You are not authorized to use this workspace.\u2019 no none string Use this property to define a custom message that is displayed if a user has no read access to any graph of the selected workspace. Property Default Required Conflicts with Valid values js.config.errorPages.moduleAccess.title \u2018No module accessible\u2019 no none string Use this property to define a custom title that is displayed if a user has no permission to any module of DataManager. Property Default Required Conflicts with Valid values js.config.errorPages.moduleAccess.message \u2018You have no access to any module.\u2019 no none string Use this property to define a custom message that is displayed if a user has no permission to any module of DataManager. Property Default Required Conflicts with Valid values js.config.errorPages.workspaceAccess.title \u2018Workspace access problem\u2019 no none string Use this property to define a custom title that is displayed if the workspace is not accessible. Property Default Required Conflicts with Valid values js.config.errorPages.workspaceAccess.message \u2018The configured workspace is not accessible.\u2019 no none string Use this property to define a custom message that is displayed if the workspace is not accessible.","title":"Error pages"},{"location":"deploy-and-configure/configuration/datamanager/error-pages/#configuration-example","text":"js.config.errorPages : graphAccess : title : Unauthorized User message : You are not authorized to use this workspace. moduleAccess : title : No module accessible message : You have no access to any module. workspaceAccess : title : Workspace access problem message: : The configured workspace is not accessible.","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/explore-module/","text":"Explore module \u00a4 Configuration property: modules.explore | Scope: app-wide and per workspace The Explore module of DataManager is used for graph data exploration. js.config.modules.explore enable startWith overallSearchQuery mapServer url ext graphlist defaultGraph hideSearch whiteList navigation defaultClass topQuery subQuery searchQuery listQuery itemsPerPage details properties enable usage enable references enable turtle enable history enable statistics enable sunburst enable visualization enable webvowlConfig filter literals relations solitarySubclasses classDisjointness setOperators degreeOfCollapsing mode dynamicLabelWidth pickAndPin nodeScaling compactNotation colorExternals export json svg gravity classDistance dataTypeDistance reset pause search externalTools toolX enable tabname iframeUrlTemplate Property Default Required Conflicts with Valid values js.config.modules.explore.enable true no none boolean Use this property to enable the Explore module of DataManager. Note: If this property is set to false , all other settings of modules.explore are skipped. Property Default Required Conflicts with Valid values js.config.modules.explore.startWith false no none boolean Use this property to load the Explore module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.explore.overallSearchQuery see below no none string (query Use this property to define a custom query for the search field provided in the Module bar. # default query js.config.modules.explore.overallSearchQuery : | SELECT DISTINCT ?resource ?_resource {{FROM}} WHERE { ?resource ?p0 ?label. FILTER (!isBLANK(?resource)). BIND (?resource as ?_resource) . FILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))). } Note: The placeholder {{QUERY}} is replaced with the search string entered by the user. A placeholder {{FROM}} can be used to insert the currently selected graph URI. Property Default Required Conflicts with Valid values js.config.modules.explore.mapServer.url none no none string (URI) Extension of the tiles as provided by OpenMapTiles Map Server Note: It works together with js.config.modules.explore.mapServer.ext , and only if it is set. Property Default Required Conflicts with Valid values js.config.modules.explore.mapServer.ext none no none string (URI) The service url as provided by OpenMapTiles Map Server. If not defined, the wikimedia server is used. Note: It works together with js.config.modules.explore.mapServer.url , and only if it is set. This is how mapServer.ext and mapServer.url are used in your configuration: js.config.modules.explore : mapServer : url : 'https://osm.your-host.com/styles/osm-bright' ext : 'png' Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.defaultGraph none no none string (file extension) Use this property to define a graph URI the user is allowed to work on. Note: If the property is set the Graph box is hidden. Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.hideSearch true no none boolean Set this property to true to hide the Search field in the Graph box Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.whiteList none no none list of strings (query) Use this property to specify a list of graphs the user can see. Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.defaultClass none no none string (URI) Use this property to setup a default class. Note: It works together with defaultGraph , and only if it is set. Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.topQuery see below no none string (query) Use this property to specify a custom query that defines which top level classes of a graph are displayed. # default query js.config.modules.explore.navigation.topQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf owl:Thing . } UNION { ?r a ?resource . FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } UNION { ?resource a owl:Class FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) FILTER (!regex( STR(?resource), \"^http://(www.w3.org/(2002/07/owl|2000/01/rdf-schema|1999/02/22-rdf-syntax-ns))#\", \"i\" )) } Note: A placeholder {{FROM}} can be used to insert the currently selected graph URI. The {{FROM}} placeholder will be resolved to FROM <graphUri> . Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.subQuery see below no none string (query) Use this property to specify a custom query that defines which subclasses of top level classes of a graph are displayed. # default query js.config.modules.explore.navigation.subQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { ?resource rdfs:subClassOf {{RESOURCE}} . OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) } Note: The placeholder {{RESOURCE}} is replaced with the selected parent class. A placeholder {{FROM}} can be used to insert the currently selected graph URI. The {{FROM}} placeholder will be resolved to FROM <graphUri> . Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.searchQuery see below no none string (query) Use this property to specify a custom query that defines which classes of a graph are displayed when the user uses the Search field in the Navigation box. # default query js.config.modules.explore.navigation.searchQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX skos: <http://www.w3.org/2004/02/skos/core#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf+ owl:Thing } UNION { ?r a ?resource . } UNION { ?resource a owl:Class } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } OPTIONAL { ?resource rdfs:label ?label1 . } OPTIONAL { ?resource skos:prefLabel ?label2 . } BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) FILTER(isIRI(?resource)) . FILTER( regex(str(?resource),\"{{QUERY}}\",\"i\") || regex(str(?label1),\"{{QUERY}}\",\"i\") || regex(str(?label2),\"{{QUERY}}\",\"i\") ) } Note: The placeholder {{QUERY}} is replaced with the search string. A placeholder {{GRAPH}} can be used for insert currently selected graph URI (will be resolved to <graphUri> ). Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.listQuery see below no none string (query) Use this property to specify a custom query that defines which resources are displayed that are type of a selected class of a graph. # default query js.config.modules.explore.navigation.listQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?instance {{FROM}} WHERE { { ?instance a {{RESOURCE}} . FILTER isIRI(?instance) . } UNION { ?class rdfs:subClassOf+ {{RESOURCE}} . ?instance a ?class . FILTER isIRI(?instance). } } Note: The placeholder {{RESOURCE}} is replaced by the selected resource URI. A placeholder {{GRAPH}} can be used for insert currently selected graph URI (will be resolved to <graphUri> ). Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.itemsPerPage 15 no none number Use this property to specify the number of items shown per page of navigation. Property Default Required Conflicts with Valid values js.config.modules.explore.details.properties.enable true no none boolean Use this property to enable the properties tab of DataManager Property Default Required Conflicts with Valid values js.config.modules.explore.details.usage.enable true no none boolean Use this property to enable the usage tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.references.enable true no none boolean Use this property to enable the references tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.turtle.enable true no none boolean Use this property to enable the turtle tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.history.enable true no none boolean Use this property to enable the History tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.statistics.enable true no none boolean Use this property to enable the statistic tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.statistics.sunburst.enable true no none boolean Sunburst is the visualization element in statistic tab. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.enable true no none boolean Use this property to enable the visualization tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.literals true no none boolean Use this property to enable the literals filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.relations true no none boolean Use this property to enable the relations filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.solitarySubclasses true no none boolean Use this property to enable the solitary subclasses filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.classDisjointness true no none boolean Use this property to enable the class disjointness filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.setOperators true no none boolean Use this property to enable the set operators filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.degreeOfCollapsing true no none boolean Use this property to enable the degree of collapsing function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.dynamicLabelWidth true no none boolean Use this property to enable the dynamic label width mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.pickAndPin true no none boolean Use this property to enable the pick and pin mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.nodeScaling true no none boolean Use this property to enable the node scaling mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.compactNotation true no none boolean Use this property to enable the compact notation mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.colorExternals true no none boolean Use this property to enable the color externals mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.export.json true no none boolean Use this property to enable the feature export as json in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.export.svg true no none boolean Use this property to enable the feature export as svg in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.gravity.classDistance true no none boolean Use this property to enable the class distance option in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.gravity.dataTypeDistance true no none boolean Use this property to enable the dataType distance option in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.reset true no none boolean Use this property to enable the reset function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.pause true no none boolean Use this property to enable the pause function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.search true no none boolean Use this property to enable the search function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.enable false no none boolean The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame. Use this property to enable or disable one specific external tool configuration. js.config.modules.explore.externalTools.toolX.enable : true Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.tabname none yes none string The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame. Use this property to name the tab for the external tool. Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.iframeUrlTemplate none yes none string (URL) The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame. Use this property to specify the URL which will be loaded in the iFrame inside of the new application tab. js.config.modules.explore.externalTools.toolX.iframeUrlTemplate : \"http://example.org/app/{{RESOURCE}}\" Note: The placeholder {{RESOURCE}} is replaced by the selected resource URI. The placeholder {{RESOURCELABEL}} is replaced with the titleHelper generated label of the resource. Configuration example \u00a4 js.config.modules.explore : enable : true startWith : true overallSearchQuery : | SELECT DISTINCT ?resource ?_resource {{FROM}} WHERE { ?resource ?p0 ?label. FILTER (!isBLANK(?resource)). BIND (?resource as ?_resource) . FILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))). } graphlist : defaultGraph : '' hideSearch : true navigation : topQuery : | SELECT DISTINCT ?resource WHERE { ?r a ?resource . FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } subQuery : | SELECT DISTINCT ?resource WHERE { ?r a ?resource . ?resource rdfs:subClassOf {{RESOURCE}} . } searchQuery : | SELECT DISTINCT ?resource WHERE { ?r a ?resource . ?resource rdfs:label ?label . FILTER(contains(?label, \"{{QUERY}}\")) . } listQuery : | SELECT DISTINCT ?resource WHERE { { ?resource a {{RESOURCE}} . } UNION { ?class rdfs:subClassOf+ {{RESOURCE}} . ?resource a ?class . } } details : properties : enable : true usage : enable : true references : enable : true turtle : enable : true statistics : enable : true sunburst : enable : true visualization : enable : true webvowlConfig : filter : literals : true relations : true solitarySubclasses : true classDisjointness : true setOperators : true degreeOfCollapsing : true mode : dynamicLabelWidth : true pickAndPin : true nodeScaling : true compactNotation : true colorExternals : true export : json : true svg : true gravity : classDistance : true dataTypeDistance : true reset : true pause : true search : true","title":"Explore module"},{"location":"deploy-and-configure/configuration/datamanager/explore-module/#explore-module","text":"Configuration property: modules.explore | Scope: app-wide and per workspace The Explore module of DataManager is used for graph data exploration. js.config.modules.explore enable startWith overallSearchQuery mapServer url ext graphlist defaultGraph hideSearch whiteList navigation defaultClass topQuery subQuery searchQuery listQuery itemsPerPage details properties enable usage enable references enable turtle enable history enable statistics enable sunburst enable visualization enable webvowlConfig filter literals relations solitarySubclasses classDisjointness setOperators degreeOfCollapsing mode dynamicLabelWidth pickAndPin nodeScaling compactNotation colorExternals export json svg gravity classDistance dataTypeDistance reset pause search externalTools toolX enable tabname iframeUrlTemplate Property Default Required Conflicts with Valid values js.config.modules.explore.enable true no none boolean Use this property to enable the Explore module of DataManager. Note: If this property is set to false , all other settings of modules.explore are skipped. Property Default Required Conflicts with Valid values js.config.modules.explore.startWith false no none boolean Use this property to load the Explore module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.explore.overallSearchQuery see below no none string (query Use this property to define a custom query for the search field provided in the Module bar. # default query js.config.modules.explore.overallSearchQuery : | SELECT DISTINCT ?resource ?_resource {{FROM}} WHERE { ?resource ?p0 ?label. FILTER (!isBLANK(?resource)). BIND (?resource as ?_resource) . FILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))). } Note: The placeholder {{QUERY}} is replaced with the search string entered by the user. A placeholder {{FROM}} can be used to insert the currently selected graph URI. Property Default Required Conflicts with Valid values js.config.modules.explore.mapServer.url none no none string (URI) Extension of the tiles as provided by OpenMapTiles Map Server Note: It works together with js.config.modules.explore.mapServer.ext , and only if it is set. Property Default Required Conflicts with Valid values js.config.modules.explore.mapServer.ext none no none string (URI) The service url as provided by OpenMapTiles Map Server. If not defined, the wikimedia server is used. Note: It works together with js.config.modules.explore.mapServer.url , and only if it is set. This is how mapServer.ext and mapServer.url are used in your configuration: js.config.modules.explore : mapServer : url : 'https://osm.your-host.com/styles/osm-bright' ext : 'png' Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.defaultGraph none no none string (file extension) Use this property to define a graph URI the user is allowed to work on. Note: If the property is set the Graph box is hidden. Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.hideSearch true no none boolean Set this property to true to hide the Search field in the Graph box Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.whiteList none no none list of strings (query) Use this property to specify a list of graphs the user can see. Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.defaultClass none no none string (URI) Use this property to setup a default class. Note: It works together with defaultGraph , and only if it is set. Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.topQuery see below no none string (query) Use this property to specify a custom query that defines which top level classes of a graph are displayed. # default query js.config.modules.explore.navigation.topQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf owl:Thing . } UNION { ?r a ?resource . FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } UNION { ?resource a owl:Class FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) FILTER (!regex( STR(?resource), \"^http://(www.w3.org/(2002/07/owl|2000/01/rdf-schema|1999/02/22-rdf-syntax-ns))#\", \"i\" )) } Note: A placeholder {{FROM}} can be used to insert the currently selected graph URI. The {{FROM}} placeholder will be resolved to FROM <graphUri> . Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.subQuery see below no none string (query) Use this property to specify a custom query that defines which subclasses of top level classes of a graph are displayed. # default query js.config.modules.explore.navigation.subQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?resource ?hasChildren {{FROM}} WHERE { ?resource rdfs:subClassOf {{RESOURCE}} . OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } FILTER(isIRI(?resource)) . BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) } Note: The placeholder {{RESOURCE}} is replaced with the selected parent class. A placeholder {{FROM}} can be used to insert the currently selected graph URI. The {{FROM}} placeholder will be resolved to FROM <graphUri> . Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.searchQuery see below no none string (query) Use this property to specify a custom query that defines which classes of a graph are displayed when the user uses the Search field in the Navigation box. # default query js.config.modules.explore.navigation.searchQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX owl: <http://www.w3.org/2002/07/owl#> PREFIX skos: <http://www.w3.org/2004/02/skos/core#> SELECT ?resource ?hasChildren {{FROM}} WHERE { { ?resource rdfs:subClassOf+ owl:Thing } UNION { ?r a ?resource . } UNION { ?resource a owl:Class } OPTIONAL{ ?child rdfs:subClassOf ?resource . FILTER(isIRI(?child)) . } OPTIONAL { ?resource rdfs:label ?label1 . } OPTIONAL { ?resource skos:prefLabel ?label2 . } BIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren) FILTER(isIRI(?resource)) . FILTER( regex(str(?resource),\"{{QUERY}}\",\"i\") || regex(str(?label1),\"{{QUERY}}\",\"i\") || regex(str(?label2),\"{{QUERY}}\",\"i\") ) } Note: The placeholder {{QUERY}} is replaced with the search string. A placeholder {{GRAPH}} can be used for insert currently selected graph URI (will be resolved to <graphUri> ). Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.listQuery see below no none string (query) Use this property to specify a custom query that defines which resources are displayed that are type of a selected class of a graph. # default query js.config.modules.explore.navigation.listQuery : | PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?instance {{FROM}} WHERE { { ?instance a {{RESOURCE}} . FILTER isIRI(?instance) . } UNION { ?class rdfs:subClassOf+ {{RESOURCE}} . ?instance a ?class . FILTER isIRI(?instance). } } Note: The placeholder {{RESOURCE}} is replaced by the selected resource URI. A placeholder {{GRAPH}} can be used for insert currently selected graph URI (will be resolved to <graphUri> ). Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.itemsPerPage 15 no none number Use this property to specify the number of items shown per page of navigation. Property Default Required Conflicts with Valid values js.config.modules.explore.details.properties.enable true no none boolean Use this property to enable the properties tab of DataManager Property Default Required Conflicts with Valid values js.config.modules.explore.details.usage.enable true no none boolean Use this property to enable the usage tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.references.enable true no none boolean Use this property to enable the references tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.turtle.enable true no none boolean Use this property to enable the turtle tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.history.enable true no none boolean Use this property to enable the History tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.statistics.enable true no none boolean Use this property to enable the statistic tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.statistics.sunburst.enable true no none boolean Sunburst is the visualization element in statistic tab. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.enable true no none boolean Use this property to enable the visualization tab of DataManager. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.literals true no none boolean Use this property to enable the literals filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.relations true no none boolean Use this property to enable the relations filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.solitarySubclasses true no none boolean Use this property to enable the solitary subclasses filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.classDisjointness true no none boolean Use this property to enable the class disjointness filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.setOperators true no none boolean Use this property to enable the set operators filter in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.degreeOfCollapsing true no none boolean Use this property to enable the degree of collapsing function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.dynamicLabelWidth true no none boolean Use this property to enable the dynamic label width mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.pickAndPin true no none boolean Use this property to enable the pick and pin mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.nodeScaling true no none boolean Use this property to enable the node scaling mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.compactNotation true no none boolean Use this property to enable the compact notation mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.colorExternals true no none boolean Use this property to enable the color externals mode in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.export.json true no none boolean Use this property to enable the feature export as json in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.export.svg true no none boolean Use this property to enable the feature export as svg in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.gravity.classDistance true no none boolean Use this property to enable the class distance option in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.gravity.dataTypeDistance true no none boolean Use this property to enable the dataType distance option in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.reset true no none boolean Use this property to enable the reset function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.pause true no none boolean Use this property to enable the pause function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.search true no none boolean Use this property to enable the search function in the OWL viewer. Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.enable false no none boolean The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame. Use this property to enable or disable one specific external tool configuration. js.config.modules.explore.externalTools.toolX.enable : true Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.tabname none yes none string The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame. Use this property to name the tab for the external tool. Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.iframeUrlTemplate none yes none string (URL) The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame. Use this property to specify the URL which will be loaded in the iFrame inside of the new application tab. js.config.modules.explore.externalTools.toolX.iframeUrlTemplate : \"http://example.org/app/{{RESOURCE}}\" Note: The placeholder {{RESOURCE}} is replaced by the selected resource URI. The placeholder {{RESOURCELABEL}} is replaced with the titleHelper generated label of the resource.","title":"Explore module"},{"location":"deploy-and-configure/configuration/datamanager/explore-module/#configuration-example","text":"js.config.modules.explore : enable : true startWith : true overallSearchQuery : | SELECT DISTINCT ?resource ?_resource {{FROM}} WHERE { ?resource ?p0 ?label. FILTER (!isBLANK(?resource)). BIND (?resource as ?_resource) . FILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))). } graphlist : defaultGraph : '' hideSearch : true navigation : topQuery : | SELECT DISTINCT ?resource WHERE { ?r a ?resource . FILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } . } subQuery : | SELECT DISTINCT ?resource WHERE { ?r a ?resource . ?resource rdfs:subClassOf {{RESOURCE}} . } searchQuery : | SELECT DISTINCT ?resource WHERE { ?r a ?resource . ?resource rdfs:label ?label . FILTER(contains(?label, \"{{QUERY}}\")) . } listQuery : | SELECT DISTINCT ?resource WHERE { { ?resource a {{RESOURCE}} . } UNION { ?class rdfs:subClassOf+ {{RESOURCE}} . ?resource a ?class . } } details : properties : enable : true usage : enable : true references : enable : true turtle : enable : true statistics : enable : true sunburst : enable : true visualization : enable : true webvowlConfig : filter : literals : true relations : true solitarySubclasses : true classDisjointness : true setOperators : true degreeOfCollapsing : true mode : dynamicLabelWidth : true pickAndPin : true nodeScaling : true compactNotation : true colorExternals : true export : json : true svg : true gravity : classDistance : true dataTypeDistance : true reset : true pause : true search : true","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/query-module/","text":"Query module \u00a4 Configuration property: modules.query | Scope: app-wide and per workspace The Query module of DataManager is used to query rdf data directly from store. js.config.modules.query enable graph startWith timeout Property Default Required Conflicts with Valid values js.config.modules.query.enable true no none boolean Set this property to true to enable the Query module of DataManager. Note: If this property is set to false , all other settings of modules.query are skipped. To use the module you also need to have read access to the graphs specified in js.config.modules.vocabulary.graph and js.config.shacl.shapesGraph as well as the access control action urn:eccenca:QueryUserInterface . Property Default Required Conflicts with Valid values js.config.modules.query.graph none yes none string (URI) Use this property to define the target graph for read and write operations. Property Default Required Conflicts with Valid values js.config.modules.query.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.query.timeout 600000 no none number Set this property to limit the timeout (in milliseconds) requesting manual queries in Query Module. Configuration example \u00a4 js.config.modules : query : enable : true startWith : false graph : \"https://ns.eccenca.com/data/queries/\" timeout : 600000","title":"Query module"},{"location":"deploy-and-configure/configuration/datamanager/query-module/#query-module","text":"Configuration property: modules.query | Scope: app-wide and per workspace The Query module of DataManager is used to query rdf data directly from store. js.config.modules.query enable graph startWith timeout Property Default Required Conflicts with Valid values js.config.modules.query.enable true no none boolean Set this property to true to enable the Query module of DataManager. Note: If this property is set to false , all other settings of modules.query are skipped. To use the module you also need to have read access to the graphs specified in js.config.modules.vocabulary.graph and js.config.shacl.shapesGraph as well as the access control action urn:eccenca:QueryUserInterface . Property Default Required Conflicts with Valid values js.config.modules.query.graph none yes none string (URI) Use this property to define the target graph for read and write operations. Property Default Required Conflicts with Valid values js.config.modules.query.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.query.timeout 600000 no none number Set this property to limit the timeout (in milliseconds) requesting manual queries in Query Module.","title":"Query module"},{"location":"deploy-and-configure/configuration/datamanager/query-module/#configuration-example","text":"js.config.modules : query : enable : true startWith : false graph : \"https://ns.eccenca.com/data/queries/\" timeout : 600000","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/resource-table-configuration/","text":"Resource Table configuration \u00a4 Configuration property: resourceTable DataManager provides the option to configure the Resource Tables. js.config.resourceTable timeoutDownload pagination limit Property Default Required Conflicts with Valid values js.config.resourceTable.timeoutDownload 600000 no none number Set this property to limit the timeout (in milliseconds) requesting a file to download in the tables of Query Module. Property Default Required Conflicts with Valid values js.config.resourceTable. pagination.limit 25 no none number Set this property to limit the default pagination limit on any Resource Table. Configuration example \u00a4 js.config.shacl : shapesGraph : \"https://vocab.eccenca.com/shacl/\"","title":"Resource Table configuration"},{"location":"deploy-and-configure/configuration/datamanager/resource-table-configuration/#resource-table-configuration","text":"Configuration property: resourceTable DataManager provides the option to configure the Resource Tables. js.config.resourceTable timeoutDownload pagination limit Property Default Required Conflicts with Valid values js.config.resourceTable.timeoutDownload 600000 no none number Set this property to limit the timeout (in milliseconds) requesting a file to download in the tables of Query Module. Property Default Required Conflicts with Valid values js.config.resourceTable. pagination.limit 25 no none number Set this property to limit the default pagination limit on any Resource Table.","title":"Resource Table configuration"},{"location":"deploy-and-configure/configuration/datamanager/resource-table-configuration/#configuration-example","text":"js.config.shacl : shapesGraph : \"https://vocab.eccenca.com/shacl/\"","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/shacl-configuration/","text":"Shacl configuration \u00a4 Configuration property: shacl DataManager provides the option to configure shacl. js.config.shacl shapesGraph Property Default Required Conflicts with Valid values js.config.shacl.shapesGraph none yes none string (URL) Define in which graph shacl shapes exists. Configuration example \u00a4 js.config.shacl : shapesGraph : \"https://vocab.eccenca.com/shacl/\"","title":"Shacl configuration"},{"location":"deploy-and-configure/configuration/datamanager/shacl-configuration/#shacl-configuration","text":"Configuration property: shacl DataManager provides the option to configure shacl. js.config.shacl shapesGraph Property Default Required Conflicts with Valid values js.config.shacl.shapesGraph none yes none string (URL) Define in which graph shacl shapes exists.","title":"Shacl configuration"},{"location":"deploy-and-configure/configuration/datamanager/shacl-configuration/#configuration-example","text":"js.config.shacl : shapesGraph : \"https://vocab.eccenca.com/shacl/\"","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/task-module/","text":"Task module \u00a4 Configuration property: modules.task | Scope: app-wide and per workspace The Task module of DataManager is used to offer direct user action interfaces to view or manipulate specific data. js.config.modules.task enable Property Default Required Conflicts with Valid values js.config.modules.task.enable true no none boolean Set this property to true to enable the Task module of DataManager. Note: If this property is set to false , all other settings of modules.task are skipped. Configuration example \u00a4 js.config.modules: task: enable: true","title":"Task module"},{"location":"deploy-and-configure/configuration/datamanager/task-module/#task-module","text":"Configuration property: modules.task | Scope: app-wide and per workspace The Task module of DataManager is used to offer direct user action interfaces to view or manipulate specific data. js.config.modules.task enable Property Default Required Conflicts with Valid values js.config.modules.task.enable true no none boolean Set this property to true to enable the Task module of DataManager. Note: If this property is set to false , all other settings of modules.task are skipped.","title":"Task module"},{"location":"deploy-and-configure/configuration/datamanager/task-module/#configuration-example","text":"js.config.modules: task: enable: true","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/thesaurus-module/","text":"Thesaurus module \u00a4 Configuration property: modules.thesaurus | Scope: app-wide and per workspace The Thesaurus module of DataManager is used to manage thesauri or taxonomies with SKOS vocabularies. js.config.modules.thesaurus enable startWith Property Default Required Conflicts with Valid values js.config.modules.thesaurus.enable false no none boolean Set this property to true to enable the Thesaurus module of DataManager. Note: If this property is set to false , all other settings of modules.thesaurus are skipped. To use the module you also need to have read access to the graph js.config.shacl.shapesGraph as well as the access control action urn:eccenca:ThesaurusUserInterface . Property Default Required Conflicts with Valid values js.config.modules.thesaurus.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Configuration example \u00a4 js.config.modules : thesaurus : enable : true startWith : false","title":"Thesaurus module"},{"location":"deploy-and-configure/configuration/datamanager/thesaurus-module/#thesaurus-module","text":"Configuration property: modules.thesaurus | Scope: app-wide and per workspace The Thesaurus module of DataManager is used to manage thesauri or taxonomies with SKOS vocabularies. js.config.modules.thesaurus enable startWith Property Default Required Conflicts with Valid values js.config.modules.thesaurus.enable false no none boolean Set this property to true to enable the Thesaurus module of DataManager. Note: If this property is set to false , all other settings of modules.thesaurus are skipped. To use the module you also need to have read access to the graph js.config.shacl.shapesGraph as well as the access control action urn:eccenca:ThesaurusUserInterface . Property Default Required Conflicts with Valid values js.config.modules.thesaurus.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default.","title":"Thesaurus module"},{"location":"deploy-and-configure/configuration/datamanager/thesaurus-module/#configuration-example","text":"js.config.modules : thesaurus : enable : true startWith : false","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/title-helper/","text":"Title helper \u00a4 Configuration property: titleHelper | Scope: app-wide and per workspace DataManager provides the option to define which labels of properties are displayed. js.config.titleHelper properties languages Property Default Required Conflicts with Valid values js.config.titleHelper.properties see below yes, if titleHelper.languages is set none list of strings Use this property to define an array of properties used for getting titles. The default value is: [ 'h tt p : //www.w3.org/2004/02/skos/core#prefLabel', 'h tt p : //xmlns.com/foaf/0.1/name', 'h tt p : //purl.org/dc/elements/1.1/title', 'h tt p : //purl.org/dc/terms/title', 'h tt p : //www.w3.org/2000/01/rdf-schema#label' ] Property Default Required Conflicts with Valid values js.config.titleHelper.languages [\u2018en\u2019] no none list of strings Use this property to define an array of languages used for getting titles. Configuration example \u00a4 js.config.titleHelper : properties : - \"http://xmlns.com/foaf/0.1/name\" - \"http://www.w3.org/2000/01/rdf-schema#label\" languages : - en - de Note: The order how labels in different languages are displayed is determined by the rank of properties combined with all given languages , default language english (if not already set) and the path property without any language tag. In this example ordering is set as following: - http://xmlns.com/foaf/0.1/name@en - http://xmlns.com/foaf/0.1/name@de - http://xmlns.com/foaf/0.1/name - http://www.w3.org/2000/01/rdf-schema#label@en - http://www.w3.org/2000/01/rdf-schema#label@de - http://www.w3.org/2000/01/rdf-schema#label.","title":"Title helper"},{"location":"deploy-and-configure/configuration/datamanager/title-helper/#title-helper","text":"Configuration property: titleHelper | Scope: app-wide and per workspace DataManager provides the option to define which labels of properties are displayed. js.config.titleHelper properties languages Property Default Required Conflicts with Valid values js.config.titleHelper.properties see below yes, if titleHelper.languages is set none list of strings Use this property to define an array of properties used for getting titles. The default value is: [ 'h tt p : //www.w3.org/2004/02/skos/core#prefLabel', 'h tt p : //xmlns.com/foaf/0.1/name', 'h tt p : //purl.org/dc/elements/1.1/title', 'h tt p : //purl.org/dc/terms/title', 'h tt p : //www.w3.org/2000/01/rdf-schema#label' ] Property Default Required Conflicts with Valid values js.config.titleHelper.languages [\u2018en\u2019] no none list of strings Use this property to define an array of languages used for getting titles.","title":"Title helper"},{"location":"deploy-and-configure/configuration/datamanager/title-helper/#configuration-example","text":"js.config.titleHelper : properties : - \"http://xmlns.com/foaf/0.1/name\" - \"http://www.w3.org/2000/01/rdf-schema#label\" languages : - en - de Note: The order how labels in different languages are displayed is determined by the rank of properties combined with all given languages , default language english (if not already set) and the path property without any language tag. In this example ordering is set as following: - http://xmlns.com/foaf/0.1/name@en - http://xmlns.com/foaf/0.1/name@de - http://xmlns.com/foaf/0.1/name - http://www.w3.org/2000/01/rdf-schema#label@en - http://www.w3.org/2000/01/rdf-schema#label@de - http://www.w3.org/2000/01/rdf-schema#label.","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/user-permissions/","text":"User permissions \u00a4 Configuration property: userPermissions DataManager provides the option to define user rights on the Workspace page. js.config.userPermissions allowCreateWorkspace allowSelectWorkspace Property Default Required Conflicts with Valid values js.config.userPermissions.allowCreateWorkspace true no none boolean When this property is set to true the user has the right to manually create new workspaces on the Workspace page. Property Default Required Conflicts with Valid values js.config.userPermissions.allowSelectWorkspace true no none boolean When this property is set to true the user has the right to manually select an existing workspace. This property can only be used when the js.config.userPermissions.allowCreateWorkspace property is set true . Configuration example \u00a4 js.config.userPermissions : allowCreateWorkspace : true allowSelectWorkspace : true","title":"User permissions"},{"location":"deploy-and-configure/configuration/datamanager/user-permissions/#user-permissions","text":"Configuration property: userPermissions DataManager provides the option to define user rights on the Workspace page. js.config.userPermissions allowCreateWorkspace allowSelectWorkspace Property Default Required Conflicts with Valid values js.config.userPermissions.allowCreateWorkspace true no none boolean When this property is set to true the user has the right to manually create new workspaces on the Workspace page. Property Default Required Conflicts with Valid values js.config.userPermissions.allowSelectWorkspace true no none boolean When this property is set to true the user has the right to manually select an existing workspace. This property can only be used when the js.config.userPermissions.allowCreateWorkspace property is set true .","title":"User permissions"},{"location":"deploy-and-configure/configuration/datamanager/user-permissions/#configuration-example","text":"js.config.userPermissions : allowCreateWorkspace : true allowSelectWorkspace : true","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/vocabs-module/","text":"Vocabs module \u00a4 Configuration property: modules.vocabulary | Scope: app-wide and per workspace The Vocabs module of DataManager is used to manage available vocabularies. js.config.modules.vocabulary enable startWith graphUrl Property Default Required Conflicts with Valid values js.config.modules.vocabulary.enable false no none boolean Set this property to true to enable the Vocabs module of DataManager. Note: If this property is set to false , all other settings of modules.vocabulary are skipped. To use the module you also need to have read access to the graph specified in js.config.modules.vocabulary.graphUrl as well as the access control action urn:eccenca:VocabularyUserInterface . Property Default Required Conflicts with Valid values js.config.modules.vocabulary.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.vocabulary.graphUrl none yes none string (URI) Use this property to define the target graph for read and write operations. Configuration example \u00a4 js.config.modules : vocabulary : enable : true startWith : false graphUrl : https://example.com/example/vocabs/","title":"Vocabs module"},{"location":"deploy-and-configure/configuration/datamanager/vocabs-module/#vocabs-module","text":"Configuration property: modules.vocabulary | Scope: app-wide and per workspace The Vocabs module of DataManager is used to manage available vocabularies. js.config.modules.vocabulary enable startWith graphUrl Property Default Required Conflicts with Valid values js.config.modules.vocabulary.enable false no none boolean Set this property to true to enable the Vocabs module of DataManager. Note: If this property is set to false , all other settings of modules.vocabulary are skipped. To use the module you also need to have read access to the graph specified in js.config.modules.vocabulary.graphUrl as well as the access control action urn:eccenca:VocabularyUserInterface . Property Default Required Conflicts with Valid values js.config.modules.vocabulary.startWith false no none boolean Set this property to true to load this module as default one after login. Note: If more than one module has defined startWith: true the most left module in Module bar will be set as default. Property Default Required Conflicts with Valid values js.config.modules.vocabulary.graphUrl none yes none string (URI) Use this property to define the target graph for read and write operations.","title":"Vocabs module"},{"location":"deploy-and-configure/configuration/datamanager/vocabs-module/#configuration-example","text":"js.config.modules : vocabulary : enable : true startWith : false graphUrl : https://example.com/example/vocabs/","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/workspaces/","text":"Workspaces \u00a4 Configuration property: workspaces | Scope: app-wide only DataManager provides the option to define pre-configured workspaces a user can select for login. js.config.workspaces id name authorization type logoutRedirectUrl - backend type url endpointId - DIWorkspace enable url Property Default Required Conflicts with Valid values js.config.workspaces[id].name none yes none string Use this property to define a descriptive name for the workspace that the user sees when selecting a workspace. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.type none yes none string Use this property to define the type of authorization. Note: Currently, possible values are anonymous and oauth2 . If oauth2 is selected, you need to configure more options as described in section Authorization . Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.logoutRedirectUrl none no none string (URL) Use this property to define a page other than the DataManager page that a user is redirected to after logout. Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.type none yes none string Use this property to define the type of the data backend. Note: Currently, the only possible value is dataplatform . Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.url none yes none string (URL) Use this property to define the base URL of the data backend. Note: For js.config.workspaces[id].backend.type: dataplatform the base URL must not contain \u201c / \u201d at the end. Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.endpointId none yes none string Use this property to define the identifier of a specific SPARQL endpoint at the data backend. Property Default Required Conflicts with Valid values js.config.workspaces[id].DIWorkspace.enable none yes none true / false Use this property to enable/disable Data Integration menu item on navigation menu. Property Default Required Conflicts with Valid values js.config.workspaces[id].DIWorkspace.url none yes none string Use this property to define the url where Data Integration is accessible. Configuration example \u00a4 js.config.workspaces : # definition for workspace 1 default : name : 'Default Workspace' authorization : type : 'oauth2' oauth2 : clientId : 'eldsClient' grantType : 'implicit' authorizeUrl : 'https://<dataplatform_uri>/oauth/authorize' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' DIWorkspace : enable : true url : /dataintegration/workspace-beta # definition for workspace 2 otherSpace : name : 'Another Workspace' authorization : type : 'anonymous' backend : type : 'dataplatform' url : '<dataplatform_uri>' endpointId : 'default' DIWorkspace : enable : true url : /dataintegration/workspace-beta Most configuration options that are app-wide applied can be overruled by specific workspace configurations. To overrule an app-wide configuration include the specific configuration option in the workspace definition as shown in the example below: js.config.workspaces : # definition for workspace 1 default : name : 'Default Workspace' authorization : type : 'anonymous' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' # overwrites app-wide appPresentation configuration for this workspace refer to chapter App presentation appPresentation : windowTitle : 'Example Name' js.config.appPresentation : faviconUrl : https://example.com/example/favicon.png windowTitle : DataManager logoUrl : https://example.com/example/logo.png headerName : Datamanager In the example above, the properties js.config.workspaces and js.config.appPresentation are configured. The property js.config.appPresentation.windowTitle in js.config.workspaces changes the title of DataManager from \u2018 DataManager \u2018 to \u2018 Example Name \u2018 for this specific workspace. Users logged in to this workspace see Example Name as window title, because the workspace configuration overrules the app-wide configuration.","title":"Workspaces"},{"location":"deploy-and-configure/configuration/datamanager/workspaces/#workspaces","text":"Configuration property: workspaces | Scope: app-wide only DataManager provides the option to define pre-configured workspaces a user can select for login. js.config.workspaces id name authorization type logoutRedirectUrl - backend type url endpointId - DIWorkspace enable url Property Default Required Conflicts with Valid values js.config.workspaces[id].name none yes none string Use this property to define a descriptive name for the workspace that the user sees when selecting a workspace. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.type none yes none string Use this property to define the type of authorization. Note: Currently, possible values are anonymous and oauth2 . If oauth2 is selected, you need to configure more options as described in section Authorization . Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.logoutRedirectUrl none no none string (URL) Use this property to define a page other than the DataManager page that a user is redirected to after logout. Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.type none yes none string Use this property to define the type of the data backend. Note: Currently, the only possible value is dataplatform . Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.url none yes none string (URL) Use this property to define the base URL of the data backend. Note: For js.config.workspaces[id].backend.type: dataplatform the base URL must not contain \u201c / \u201d at the end. Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.endpointId none yes none string Use this property to define the identifier of a specific SPARQL endpoint at the data backend. Property Default Required Conflicts with Valid values js.config.workspaces[id].DIWorkspace.enable none yes none true / false Use this property to enable/disable Data Integration menu item on navigation menu. Property Default Required Conflicts with Valid values js.config.workspaces[id].DIWorkspace.url none yes none string Use this property to define the url where Data Integration is accessible.","title":"Workspaces"},{"location":"deploy-and-configure/configuration/datamanager/workspaces/#configuration-example","text":"js.config.workspaces : # definition for workspace 1 default : name : 'Default Workspace' authorization : type : 'oauth2' oauth2 : clientId : 'eldsClient' grantType : 'implicit' authorizeUrl : 'https://<dataplatform_uri>/oauth/authorize' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' DIWorkspace : enable : true url : /dataintegration/workspace-beta # definition for workspace 2 otherSpace : name : 'Another Workspace' authorization : type : 'anonymous' backend : type : 'dataplatform' url : '<dataplatform_uri>' endpointId : 'default' DIWorkspace : enable : true url : /dataintegration/workspace-beta Most configuration options that are app-wide applied can be overruled by specific workspace configurations. To overrule an app-wide configuration include the specific configuration option in the workspace definition as shown in the example below: js.config.workspaces : # definition for workspace 1 default : name : 'Default Workspace' authorization : type : 'anonymous' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' # overwrites app-wide appPresentation configuration for this workspace refer to chapter App presentation appPresentation : windowTitle : 'Example Name' js.config.appPresentation : faviconUrl : https://example.com/example/favicon.png windowTitle : DataManager logoUrl : https://example.com/example/logo.png headerName : Datamanager In the example above, the properties js.config.workspaces and js.config.appPresentation are configured. The property js.config.appPresentation.windowTitle in js.config.workspaces changes the title of DataManager from \u2018 DataManager \u2018 to \u2018 Example Name \u2018 for this specific workspace. Users logged in to this workspace see Example Name as window title, because the workspace configuration overrules the app-wide configuration.","title":"Configuration example"},{"location":"deploy-and-configure/configuration/datamanager/workspaces-authorization/","text":"Workspaces Authorization \u00a4 DataManager provides several types of authorization. js.config.workspaces id authorization type oauth2 grantType authorizeUrl clientId tokenUrl clientSecret If you want to use OAuth2, you need to define authorization.type to oauth2 as well as specify further configuration parameters within the oauth2 parameter: Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.grantType none yes, if authorization.type is oauth2 none implicit or authorization_code Use this property to define the OAuth2 workflow. Depending on what value you choose you have to configure different properties: implicit : authorizeURL and clientId authorization_code : authorizeUrl , clientId , tokenUrl , and as optional property clientSecret Note: It is recommended to use the implicit workflow, as DataManager is a client application. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.authorizeUrl none yes, if authorization.type is oauth2 none string (URL) Use this property to define the authorization endpoint URL of the OAuth2 authorization server. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.clientId none yes, if authorization.type is oauth2 none string Use this property to define the client identifier issued by the OAuth2 authorization server. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.tokenUrl none yes, if authorization.oauth2.grantType is \u2018 authorization_code \u2018 none string (URL) Use this property to define the authorization token endpoint URL of the OAuth2 authorization server. Note: This property is only needed for js.config.workspaces[id].authorization.oauth2.grantType=authorization_code . Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.clientSecret none yes, if authorization.oauth2.grantType is \u2018 authorization_code \u2018 none string Use this property to define a passphrase for OAuth2 client authorization. Usually, this property is not required. It must only be set if the authorization server expects a client secret. Configuration example \u00a4 The two examples below show how a backend configuration can look like as for example for an eccenca DataPlatform using different authorization options. DataPlatform configuration with anonymous access: js.config.workspaces : default : name : 'Default Workspace (anonymous)' authorization : type : 'anonymous' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' DataPlatform configuration with implicit OAuth2 workflow: js.config.workspaces : default : name : 'Default endpoint (oauth-implicit)' authorization : type : 'oauth2' oauth2 : clientId : 'eldsClient' grantType : 'implicit' authorizeUrl : 'https://<dataplatform_uri>/oauth/authorize' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' DataPlatform with authorization code OAuth2 workflow: js.config.workspaces : default : name : 'Default endpoint (oauth-implicit)' authorization : type : 'oauth2' oauth2 : clientId : 'eldsClient' grantType : 'authorization_code' authorizeUrl : 'https://<dataplatform_uri>/oauth/authorize' tokenUrl : 'https://<dataplatform_uri>/oauth/token' clientSecret : 'secret' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default'","title":"Workspaces Authorization"},{"location":"deploy-and-configure/configuration/datamanager/workspaces-authorization/#workspaces-authorization","text":"DataManager provides several types of authorization. js.config.workspaces id authorization type oauth2 grantType authorizeUrl clientId tokenUrl clientSecret If you want to use OAuth2, you need to define authorization.type to oauth2 as well as specify further configuration parameters within the oauth2 parameter: Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.grantType none yes, if authorization.type is oauth2 none implicit or authorization_code Use this property to define the OAuth2 workflow. Depending on what value you choose you have to configure different properties: implicit : authorizeURL and clientId authorization_code : authorizeUrl , clientId , tokenUrl , and as optional property clientSecret Note: It is recommended to use the implicit workflow, as DataManager is a client application. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.authorizeUrl none yes, if authorization.type is oauth2 none string (URL) Use this property to define the authorization endpoint URL of the OAuth2 authorization server. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.clientId none yes, if authorization.type is oauth2 none string Use this property to define the client identifier issued by the OAuth2 authorization server. Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.tokenUrl none yes, if authorization.oauth2.grantType is \u2018 authorization_code \u2018 none string (URL) Use this property to define the authorization token endpoint URL of the OAuth2 authorization server. Note: This property is only needed for js.config.workspaces[id].authorization.oauth2.grantType=authorization_code . Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.clientSecret none yes, if authorization.oauth2.grantType is \u2018 authorization_code \u2018 none string Use this property to define a passphrase for OAuth2 client authorization. Usually, this property is not required. It must only be set if the authorization server expects a client secret.","title":"Workspaces Authorization"},{"location":"deploy-and-configure/configuration/datamanager/workspaces-authorization/#configuration-example","text":"The two examples below show how a backend configuration can look like as for example for an eccenca DataPlatform using different authorization options. DataPlatform configuration with anonymous access: js.config.workspaces : default : name : 'Default Workspace (anonymous)' authorization : type : 'anonymous' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' DataPlatform configuration with implicit OAuth2 workflow: js.config.workspaces : default : name : 'Default endpoint (oauth-implicit)' authorization : type : 'oauth2' oauth2 : clientId : 'eldsClient' grantType : 'implicit' authorizeUrl : 'https://<dataplatform_uri>/oauth/authorize' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default' DataPlatform with authorization code OAuth2 workflow: js.config.workspaces : default : name : 'Default endpoint (oauth-implicit)' authorization : type : 'oauth2' oauth2 : clientId : 'eldsClient' grantType : 'authorization_code' authorizeUrl : 'https://<dataplatform_uri>/oauth/authorize' tokenUrl : 'https://<dataplatform_uri>/oauth/token' clientSecret : 'secret' backend : type : 'dataplatform' url : 'https://<dataplatform_uri>' endpointId : 'default'","title":"Configuration example"},{"location":"deploy-and-configure/configuration/dataplatform/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/docker-orchestration/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/keycloak/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/production-ready-settings/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/quad-store-configuration/","text":"","title":"Index"},{"location":"deploy-and-configure/configuration/reverse-proxy/","text":"","title":"Index"},{"location":"deploy-and-configure/installation/","text":"Installation \u00a4 This page describes proven deployment scenarios for eccenca Corporate Memory. All Corporate Memory components are distributed as Docker images and can be obtained from eccenca\u2019s Artifactory service. To run them you need a Docker enabled Linux server. In addition to that, eccenca provides distribution archives for all components which contain configuration examples (YAML) as well as JAR/WAR artifacts. Operating Systems (OS) \u00a4 Corporate Memory is tested on Ubuntu 18.04 (backward compatible with 16.04 and 14.04) and RHEL 7.7. Special note on RHEL SELinux Support: there is no limitation for RedHat SELinux. We recommend to keep the SELinux in enforced mode. You can keep the default setting of the /etc/selinux/config file. sample config /etc/selinux/config 1 2 3 4 5 6 7 8 9 10 11 # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX = enforcing # SELINUXTYPE= can take one of three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE = targeted Docker-compose based Orchestration deployment \u00a4 Docker Compose is a convenient way to provision several Docker containers locally for development setups or on remote servers for single node setups. eccenca is heavily using Docker Compose for all kinds of internal and customer deployments. For more details on how to use docker-compose based orchestration refer to Scenario: Local Installation and Scenario: Single Node Cloud Installation . DataIntegration \u00a4 Running on a Spark Cluster \u00a4 eccenca DataIntegration supports the execution of DataIntegration workflows in a cluster environment with Apache Spark. Prerequisites \u00a4 For the execution of DataIntegration in a Spark cluster the following software components from the Hadoop eco-system are recommended: Scala 2.11 or 2.10 Apache Spark 2.1.2 (compiled for Scala 2.11) Apache Hadoop 2.7 (HDFS) Apache Hive 1.2, with a relational data bases as meta store (e.g. Derby) Recent versions of the following Hadoop distributions are generally supported as well: Hortonworks (HDP 2.5) Cloudera (CDH 5.8) Oracle Big Data Lite (4.6) Microsoft HDInsight (based on HDP) Installation \u00a4 A Spark application can run in three different modes: local mode client mode cluster mode The local mode is for running Spark applications on one local machine. In the client mode the DataIntegration application will run outside of the cluster and create Spark Jobs to be executed in the cluster at run time. The cluster mode requires that the application using Spark runs completely in the cluster and is managed by the software running on the cluster (e.g. Spark, Apache Yarn, Mesos). DataIntegration supports local mode (for testing), client mode (for production, only with clusters managed by Spark) or cluster mode on Yarn (for production, integrates best with other distributed applications). When running DataIntegration in a cluster, the same installation procedure and prerequisites apply as for the local installation. The application can be installed outside the cluster or on any cluster node. A number of configuration options have to be set to be able to connect to and use a Spark cluster. The necessary configuration options are described in DataIntegration . DataPlatform \u00a4 Scaling \u00a4 Run multiple DataPlatform instances with the same configuration to enable high-availability and/or high-performance setups. Prerequisites \u00a4 For running multiple DataPlatform instances the following prerequisites apply: The same application configuration properties must be used by all scaled instances. If access control for any SPARQL endpoint is active, a shared Redis cache used by all DataPlatform instances is required. Limitations \u00a4 When running multiple DataPlatform instances it is not possible to use a shared Virtuoso backend with provisioned access control active. Troubleshooting \u00a4 In case DataPlatform failed to start, check the logs for error messages pointing to faulty parameters in the configuration. Since not every faulty behavior is apparent from reading the logs, the following checks can help you to verify the configuration: Check the http(s)://<servername:port>/actuator/health/ endpoint to verify if the SPARQL proxy service endpoints are configured properly. Note: Refer to the Spring documentation on how to set active profiles. Plugins \u00a4 In some cases DataPlatform needs to be extended with plugins. Extensions are necessary when drivers cannot be included due to licensing restrictions or when plugins are delivered separately. In this case, you have to update the .war file of DataPlatform by placing the plugin .jar files in the same directory, or by stating the path via the configuration option. To include plugins that are located in the same directory as the eccenca-DataPlatform.war file, execute the .war file with the option -u or --update-war : 1 2 # with plugins located in the same folder as the WAR file java -jar ${ JAVA_TOOL_OPTIONS } eccenca-DataPlatform.war --update-war If the plugins to be included are not located in the same folder as the .war file, you can specify a directory containing the plugins as the argument of the -u or --update-war option. 1 java -jar ${ JAVA_TOOL_OPTIONS } eccenca-DataPlatform.war -u /data/plugins The last command repackages the eccenca-DataPlatform.war by including all plugins (.jar) located in the specified directory. Note: Make sure that only the eccenca-DataPlatform.war file is in the directory since multiple .war files can cause problems. Note: During the update procedure, the directory WEB-INF is created. Due to security concerns the update mechanism does not delete this directory. You can delete it after the update process is finished.","title":"Installation"},{"location":"deploy-and-configure/installation/#installation","text":"This page describes proven deployment scenarios for eccenca Corporate Memory. All Corporate Memory components are distributed as Docker images and can be obtained from eccenca\u2019s Artifactory service. To run them you need a Docker enabled Linux server. In addition to that, eccenca provides distribution archives for all components which contain configuration examples (YAML) as well as JAR/WAR artifacts.","title":"Installation"},{"location":"deploy-and-configure/installation/#operating-systems-os","text":"Corporate Memory is tested on Ubuntu 18.04 (backward compatible with 16.04 and 14.04) and RHEL 7.7. Special note on RHEL SELinux Support: there is no limitation for RedHat SELinux. We recommend to keep the SELinux in enforced mode. You can keep the default setting of the /etc/selinux/config file. sample config /etc/selinux/config 1 2 3 4 5 6 7 8 9 10 11 # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX = enforcing # SELINUXTYPE= can take one of three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE = targeted","title":"Operating Systems (OS)"},{"location":"deploy-and-configure/installation/#docker-compose-based-orchestration-deployment","text":"Docker Compose is a convenient way to provision several Docker containers locally for development setups or on remote servers for single node setups. eccenca is heavily using Docker Compose for all kinds of internal and customer deployments. For more details on how to use docker-compose based orchestration refer to Scenario: Local Installation and Scenario: Single Node Cloud Installation .","title":"Docker-compose based Orchestration deployment"},{"location":"deploy-and-configure/installation/#dataintegration","text":"","title":"DataIntegration"},{"location":"deploy-and-configure/installation/#running-on-a-spark-cluster","text":"eccenca DataIntegration supports the execution of DataIntegration workflows in a cluster environment with Apache Spark.","title":"Running on a Spark Cluster"},{"location":"deploy-and-configure/installation/#prerequisites","text":"For the execution of DataIntegration in a Spark cluster the following software components from the Hadoop eco-system are recommended: Scala 2.11 or 2.10 Apache Spark 2.1.2 (compiled for Scala 2.11) Apache Hadoop 2.7 (HDFS) Apache Hive 1.2, with a relational data bases as meta store (e.g. Derby) Recent versions of the following Hadoop distributions are generally supported as well: Hortonworks (HDP 2.5) Cloudera (CDH 5.8) Oracle Big Data Lite (4.6) Microsoft HDInsight (based on HDP)","title":"Prerequisites"},{"location":"deploy-and-configure/installation/#installation_1","text":"A Spark application can run in three different modes: local mode client mode cluster mode The local mode is for running Spark applications on one local machine. In the client mode the DataIntegration application will run outside of the cluster and create Spark Jobs to be executed in the cluster at run time. The cluster mode requires that the application using Spark runs completely in the cluster and is managed by the software running on the cluster (e.g. Spark, Apache Yarn, Mesos). DataIntegration supports local mode (for testing), client mode (for production, only with clusters managed by Spark) or cluster mode on Yarn (for production, integrates best with other distributed applications). When running DataIntegration in a cluster, the same installation procedure and prerequisites apply as for the local installation. The application can be installed outside the cluster or on any cluster node. A number of configuration options have to be set to be able to connect to and use a Spark cluster. The necessary configuration options are described in DataIntegration .","title":"Installation "},{"location":"deploy-and-configure/installation/#dataplatform","text":"","title":"DataPlatform"},{"location":"deploy-and-configure/installation/#scaling","text":"Run multiple DataPlatform instances with the same configuration to enable high-availability and/or high-performance setups.","title":"Scaling"},{"location":"deploy-and-configure/installation/#prerequisites_1","text":"For running multiple DataPlatform instances the following prerequisites apply: The same application configuration properties must be used by all scaled instances. If access control for any SPARQL endpoint is active, a shared Redis cache used by all DataPlatform instances is required.","title":"Prerequisites "},{"location":"deploy-and-configure/installation/#limitations","text":"When running multiple DataPlatform instances it is not possible to use a shared Virtuoso backend with provisioned access control active.","title":"Limitations"},{"location":"deploy-and-configure/installation/#troubleshooting","text":"In case DataPlatform failed to start, check the logs for error messages pointing to faulty parameters in the configuration. Since not every faulty behavior is apparent from reading the logs, the following checks can help you to verify the configuration: Check the http(s)://<servername:port>/actuator/health/ endpoint to verify if the SPARQL proxy service endpoints are configured properly. Note: Refer to the Spring documentation on how to set active profiles.","title":"Troubleshooting"},{"location":"deploy-and-configure/installation/#plugins","text":"In some cases DataPlatform needs to be extended with plugins. Extensions are necessary when drivers cannot be included due to licensing restrictions or when plugins are delivered separately. In this case, you have to update the .war file of DataPlatform by placing the plugin .jar files in the same directory, or by stating the path via the configuration option. To include plugins that are located in the same directory as the eccenca-DataPlatform.war file, execute the .war file with the option -u or --update-war : 1 2 # with plugins located in the same folder as the WAR file java -jar ${ JAVA_TOOL_OPTIONS } eccenca-DataPlatform.war --update-war If the plugins to be included are not located in the same folder as the .war file, you can specify a directory containing the plugins as the argument of the -u or --update-war option. 1 java -jar ${ JAVA_TOOL_OPTIONS } eccenca-DataPlatform.war -u /data/plugins The last command repackages the eccenca-DataPlatform.war by including all plugins (.jar) located in the specified directory. Note: Make sure that only the eccenca-DataPlatform.war file is in the directory since multiple .war files can cause problems. Note: During the update procedure, the directory WEB-INF is created. Due to security concerns the update mechanism does not delete this directory. You can delete it after the update process is finished.","title":"Plugins"},{"location":"deploy-and-configure/installation/scenario-local-installation/","text":"Introduction \u00a4 This page describes a docker-compose based orchestration running on your local machine and accessible via browser. The code examples in this section assumes that you have POSIX-compliant shell (linux, macOS or WSL for Windows). Requirements \u00a4 Access credentials to eccenca Artifactory and eccenca Docker Registry \u2192 contact us to get yours docker and docker-compose installed locally git installed locally At least 4 CPUs and 12GB of RAM (recommended: 16GB) dedicated to docker Setup & Check Installation Environment \u00a4 Download the [Corporate Memory docker orchestration] https://releases.eccenca.com/docker-orchestration/ from eccenca Artifactory. Open a terminal window, create a directory, copy and extract docker orchestration there. # create eccenca-corporate-memory directory in your ${HOME} and set as a working dir $ mkdir ${ HOME } /eccenca-corporate-memory && cd ${ HOME } /eccenca-corporate-memory # cp Corporate Memory docker orchestration distribution in the local directory # Change VERSION to the version you have downloaded e.g. v20.03 $ cp ${ HOME } /Downloads/cmem-orchestration-VERSION.zip ./ $ unzip cmem-orchestration-VERSION.zip $ rm cmem-orchestration-VERSION.zip $ git init && git add . && git commit -m \"stub\" Check your local environment: # run the following command (without $) to check your docker server version, should be at least 19.03 # to have the current security patches, always update your docker version to the latest one $ docker info | grep -i version Server Version: 19 .03.8 # check docker-compose version, should be at least 1.25.0 # update to the latest version if necessary $ docker-compose --version docker-compose version 1 .25.4, build 8d51620a # login into eccenca docker registry $ docker login docker-registry.eccenca.com Username: yourusername Password: Login Succeeded Installation \u00a4 To install Corporate Memory, you need to modify your local hosts file (located in /etc/hosts), minimal configuration is as follows: ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127 .0.0.1 localhost 127 .0.0.1 docker.localhost 127 .0.0.1 corporate.memory Corporate Memory uses stardog triple store as a backend. Stardog requires a license: # if you have a license copy it to conf/stardog/stardog-license-key.bin # for example $ cp ~/Downloads/stardog-Eccenca-Developer-license-key.bin conf/stardog/stardog-license-key.bin Check validity of your stardog license with make stardog-license-check command: $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin Licensee: Eccenca User ( noreply@stardog.com ) , Eccenca Type: Subscription Issued: Mon Jan 20 19 :39:39 GMT 2020 Expiration: 286 days Support: 286 days Quantity: 1 In case you do not have stardog license or your license has expired, request a trial license using make stardog-license-request command: # if stardog-license-check is failing with invalid or expired license $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin The license is invalid: java.io.EOFException make: *** [ stardog-license-check ] Error 1 # request stardog trial license $ make stardog-license-request docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license request --force --output /data/stardog-license-key.bin Thank you for downloading Stardog. A valid license was not found in /data. Would you like to download a trial license from Stardog ( y/N ) ? y Contacting Stardog............... Please provide a valid email address to start your 60 -day trial ( we may occasionally contact you with Stardog news ) : you@your-domain.com Contacting license server..................... Email validated. You now have a 60 -day Stardog trial license. Starting Stardog... %\u2584, \u2591\u2591\u0393\u256c\u2580\u2580\u2588\u2593\u2563\u2310 \u2584\u2593\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2568\u2593 .\u2310\u2310. .\u00bd\u2593\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2584 \u2310\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393\u00ab\u2310 \u2264\u2591\u2593\u2588\u2588\u2588\u2593\u2593\u258c\u2584\u2591\u2591\u2591\u2591\u2591\u2593\u2592\u2588\u0393\u2310 .\u00bb\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2265\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2590\u2588\u2584\u2559\u2591\u2591\u2265\u2591\u2591\u2265 [ \u00bb. \u250c\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2593\u2593\u2593\u258c\u258c\u258c\u258c\u2593\u2593\u2588\u2593\u2310 .\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2592\u2592\u2593\u2593\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u255f\u2588\u2588\u2588\u2588\u2588\u2588\u2559 \u2514\u2588b \u2588\u2588\u2588\u2588\u2580\u2580\u2592\u2588\u2588\u2588\u2588\u2588\u258c \u0393 .\u2229\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 ` \u2559 \u255f\u2588\u2593\u2229 \u2588\u2588\u2588\u2580\u2588\u2588\u258c \u251c\u2591, .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588 \u2590\u2588 \u2588\u2588 \u2559 \u251c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588 , ' \u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2555 \u2590\u2588\u2584 \u2588\u2588 .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2584\u2591\u2591\u2591\u2591\u2591\u2584\u2563\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2588\u2588\u2591\u2591\u2563 \u2588\u2588\u2584\u2584\u2563\u2592\u2592\u258c\u2584\u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591, \u2514\u2559\u2580\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2566 \u2588\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u258c \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2559\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2580\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u256c\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 ' \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584 ` \u2514\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2551\u2588\u2588\u2593\u2584\u2591\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393 \"\u2559\u2591\u2591\u2591\u2591\u2580\u2580 \u2559\u2591\u2591\u2584\u2563\u2593\u2593\u2588\u2588\u2580\u2580 \u2559\u2580\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2310 \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2514\u2563\u2588\u2588\u2588\u2580\u2580\u2514 \u2559\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2588\u2580 '\"\"` .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2560\u2593\u2588\u2588\u2588\u2580\u00b2 \u00ab\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2580 \u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2534\u2580\u2580\u2559 .\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229` \u00e1\u2580\u2580\u2555\u2584#\u258c\u2580\u2580\u2591\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2559\u2229\" \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229 ` \u2514\u2591\u2591\u2591\u2591\u2559\u2229 ` Thank you! # check that you have a valid license now $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin Licensee: Stardog Trial User ( you@your-domain.com ) , Stardog Union Version: Stardog * Type: Trial Issued: Sun Mar 29 12 :03:25 GMT 2020 Expiration: 59 days Support: The license does not include maintenance. Quantity: 3 Run the command to clean workspace, pull the images, start the Corporate Memory instance and load initial data: $ cd ${ HOME } /eccenca-corporate-memory # Pulling the images will take time $ make clean-pull-start-bootstrap You should see the output as follows: /usr/local/bin/docker-compose kill /usr/local/bin/docker-compose stop /usr/local/bin/docker-compose down --volumes --remove-orphans Removing network dockerlocal_default Removing volume dockerlocal_stardog /usr/local/bin/docker-compose rm -v --force No stopped containers Removing data/dataplatform/shui-vocab/includes/widoco/ Pulling apache2 ... done Pulling datamanager ... done Pulling dataintegration ... done Pulling stardog ... done Pulling dataplatform ... done Pulling postgres ... done Pulling keycloak ... done Creating network \"dockerlocal_default\" with the default driver Creating volume \"dockerlocal_stardog\" with default driver Creating dockerlocal_apache2_1 ... done Creating dockerlocal_stardog_1 ... done Creating dockerlocal_postgres_1 ... done Creating dockerlocal_datamanager_1 ... done Creating dockerlocal_dataintegration_1 ... done Creating dockerlocal_keycloak_1 ... done Creating dockerlocal_dataplatform_1 ... done /Users/ivanermilov/eccenca-corporate-memory//scripts/waitForSuccessfulStart.sh Waiting for healthy orchestration...................... done CMEM-Orchestration successfully started. Run make logs to see log output Initial Login / Test \u00a4 Open your browser and navigate to http://docker.localhost Click CONTINUE WITH LOGIN and use one of these default accounts: account password description admin admin Is member of the global admin group (can see and do anything) user user Is member of the local user group (can not change access conditions or see internal graphs) After successful login, you will see Corporate Memory interface. You can now proceed to the Getting Started section.","title":"Introduction"},{"location":"deploy-and-configure/installation/scenario-local-installation/#introduction","text":"This page describes a docker-compose based orchestration running on your local machine and accessible via browser. The code examples in this section assumes that you have POSIX-compliant shell (linux, macOS or WSL for Windows).","title":"Introduction"},{"location":"deploy-and-configure/installation/scenario-local-installation/#requirements","text":"Access credentials to eccenca Artifactory and eccenca Docker Registry \u2192 contact us to get yours docker and docker-compose installed locally git installed locally At least 4 CPUs and 12GB of RAM (recommended: 16GB) dedicated to docker","title":"Requirements"},{"location":"deploy-and-configure/installation/scenario-local-installation/#setup-check-installation-environment","text":"Download the [Corporate Memory docker orchestration] https://releases.eccenca.com/docker-orchestration/ from eccenca Artifactory. Open a terminal window, create a directory, copy and extract docker orchestration there. # create eccenca-corporate-memory directory in your ${HOME} and set as a working dir $ mkdir ${ HOME } /eccenca-corporate-memory && cd ${ HOME } /eccenca-corporate-memory # cp Corporate Memory docker orchestration distribution in the local directory # Change VERSION to the version you have downloaded e.g. v20.03 $ cp ${ HOME } /Downloads/cmem-orchestration-VERSION.zip ./ $ unzip cmem-orchestration-VERSION.zip $ rm cmem-orchestration-VERSION.zip $ git init && git add . && git commit -m \"stub\" Check your local environment: # run the following command (without $) to check your docker server version, should be at least 19.03 # to have the current security patches, always update your docker version to the latest one $ docker info | grep -i version Server Version: 19 .03.8 # check docker-compose version, should be at least 1.25.0 # update to the latest version if necessary $ docker-compose --version docker-compose version 1 .25.4, build 8d51620a # login into eccenca docker registry $ docker login docker-registry.eccenca.com Username: yourusername Password: Login Succeeded","title":"Setup &amp; Check Installation Environment"},{"location":"deploy-and-configure/installation/scenario-local-installation/#installation","text":"To install Corporate Memory, you need to modify your local hosts file (located in /etc/hosts), minimal configuration is as follows: ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127 .0.0.1 localhost 127 .0.0.1 docker.localhost 127 .0.0.1 corporate.memory Corporate Memory uses stardog triple store as a backend. Stardog requires a license: # if you have a license copy it to conf/stardog/stardog-license-key.bin # for example $ cp ~/Downloads/stardog-Eccenca-Developer-license-key.bin conf/stardog/stardog-license-key.bin Check validity of your stardog license with make stardog-license-check command: $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin Licensee: Eccenca User ( noreply@stardog.com ) , Eccenca Type: Subscription Issued: Mon Jan 20 19 :39:39 GMT 2020 Expiration: 286 days Support: 286 days Quantity: 1 In case you do not have stardog license or your license has expired, request a trial license using make stardog-license-request command: # if stardog-license-check is failing with invalid or expired license $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin The license is invalid: java.io.EOFException make: *** [ stardog-license-check ] Error 1 # request stardog trial license $ make stardog-license-request docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license request --force --output /data/stardog-license-key.bin Thank you for downloading Stardog. A valid license was not found in /data. Would you like to download a trial license from Stardog ( y/N ) ? y Contacting Stardog............... Please provide a valid email address to start your 60 -day trial ( we may occasionally contact you with Stardog news ) : you@your-domain.com Contacting license server..................... Email validated. You now have a 60 -day Stardog trial license. Starting Stardog... %\u2584, \u2591\u2591\u0393\u256c\u2580\u2580\u2588\u2593\u2563\u2310 \u2584\u2593\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2568\u2593 .\u2310\u2310. .\u00bd\u2593\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2584 \u2310\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393\u00ab\u2310 \u2264\u2591\u2593\u2588\u2588\u2588\u2593\u2593\u258c\u2584\u2591\u2591\u2591\u2591\u2591\u2593\u2592\u2588\u0393\u2310 .\u00bb\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2265\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2590\u2588\u2584\u2559\u2591\u2591\u2265\u2591\u2591\u2265 [ \u00bb. \u250c\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2593\u2593\u2593\u258c\u258c\u258c\u258c\u2593\u2593\u2588\u2593\u2310 .\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2592\u2592\u2593\u2593\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u255f\u2588\u2588\u2588\u2588\u2588\u2588\u2559 \u2514\u2588b \u2588\u2588\u2588\u2588\u2580\u2580\u2592\u2588\u2588\u2588\u2588\u2588\u258c \u0393 .\u2229\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 ` \u2559 \u255f\u2588\u2593\u2229 \u2588\u2588\u2588\u2580\u2588\u2588\u258c \u251c\u2591, .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588 \u2590\u2588 \u2588\u2588 \u2559 \u251c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588 , ' \u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2555 \u2590\u2588\u2584 \u2588\u2588 .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2584\u2591\u2591\u2591\u2591\u2591\u2584\u2563\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2588\u2588\u2591\u2591\u2563 \u2588\u2588\u2584\u2584\u2563\u2592\u2592\u258c\u2584\u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591, \u2514\u2559\u2580\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2566 \u2588\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u258c \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2559\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2580\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u256c\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 ' \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584 ` \u2514\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2551\u2588\u2588\u2593\u2584\u2591\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393 \"\u2559\u2591\u2591\u2591\u2591\u2580\u2580 \u2559\u2591\u2591\u2584\u2563\u2593\u2593\u2588\u2588\u2580\u2580 \u2559\u2580\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2310 \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2514\u2563\u2588\u2588\u2588\u2580\u2580\u2514 \u2559\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2588\u2580 '\"\"` .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2560\u2593\u2588\u2588\u2588\u2580\u00b2 \u00ab\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2580 \u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2534\u2580\u2580\u2559 .\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229` \u00e1\u2580\u2580\u2555\u2584#\u258c\u2580\u2580\u2591\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2559\u2229\" \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229 ` \u2514\u2591\u2591\u2591\u2591\u2559\u2229 ` Thank you! # check that you have a valid license now $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /Users/ivanermilov/eccenca-corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin Licensee: Stardog Trial User ( you@your-domain.com ) , Stardog Union Version: Stardog * Type: Trial Issued: Sun Mar 29 12 :03:25 GMT 2020 Expiration: 59 days Support: The license does not include maintenance. Quantity: 3 Run the command to clean workspace, pull the images, start the Corporate Memory instance and load initial data: $ cd ${ HOME } /eccenca-corporate-memory # Pulling the images will take time $ make clean-pull-start-bootstrap You should see the output as follows: /usr/local/bin/docker-compose kill /usr/local/bin/docker-compose stop /usr/local/bin/docker-compose down --volumes --remove-orphans Removing network dockerlocal_default Removing volume dockerlocal_stardog /usr/local/bin/docker-compose rm -v --force No stopped containers Removing data/dataplatform/shui-vocab/includes/widoco/ Pulling apache2 ... done Pulling datamanager ... done Pulling dataintegration ... done Pulling stardog ... done Pulling dataplatform ... done Pulling postgres ... done Pulling keycloak ... done Creating network \"dockerlocal_default\" with the default driver Creating volume \"dockerlocal_stardog\" with default driver Creating dockerlocal_apache2_1 ... done Creating dockerlocal_stardog_1 ... done Creating dockerlocal_postgres_1 ... done Creating dockerlocal_datamanager_1 ... done Creating dockerlocal_dataintegration_1 ... done Creating dockerlocal_keycloak_1 ... done Creating dockerlocal_dataplatform_1 ... done /Users/ivanermilov/eccenca-corporate-memory//scripts/waitForSuccessfulStart.sh Waiting for healthy orchestration...................... done CMEM-Orchestration successfully started. Run make logs to see log output","title":"Installation"},{"location":"deploy-and-configure/installation/scenario-local-installation/#initial-login-test","text":"Open your browser and navigate to http://docker.localhost Click CONTINUE WITH LOGIN and use one of these default accounts: account password description admin admin Is member of the global admin group (can see and do anything) user user Is member of the local user group (can not change access conditions or see internal graphs) After successful login, you will see Corporate Memory interface. You can now proceed to the Getting Started section.","title":"Initial Login / Test"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/","text":"Scenario: RedHat Enterprise Linux 7 \u00a4 Introduction \u00a4 This page describes a docker-compose based orchestration running on RedHat Enterprise Linux 7 (RHEL 7) inside a VirtualBox virtual machine. Requirements \u00a4 Virtualbox and vagrant installed locally Terminal with ssh client installed locally POSIX-compatible command line interface (Linux, macOS or WSL for Windows) Provisioning \u00a4 Create a working directory for this scenario and inside the working directory Vagrantfile with the following contents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \"2\" in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don't change it unless you know what # you're doing. Vagrant.configure ( \"2\" ) do | config | config.vbguest.auto_update = false config.vbguest.no_remote = true config.vm.box = \"generic/rhel7\" config.ssh.private_key_path = File.expand_path ( '~/.vagrant.d/insecure_private_key' ) config.ssh.insert_key = false config.vm.define \"rhel7\" do | rhel7 | rhel7.vm.network \"private_network\" , ip: \"10.10.10.10\" rhel7.vm.hostname = \"rhel7.eccenca.local\" rhel7.vm.provider \"virtualbox\" do | dpvm | dpvm.memory = 10240 dpvm.cpus = 4 end end end Spin up the virtual machine: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ vagrant up Bringing machine 'rhel7' up with 'virtualbox' provider... == > rhel7: Importing base box 'generic/rhel7' ... == > rhel7: Matching MAC address for NAT networking... == > rhel7: Checking if box 'generic/rhel7' is up to date... == > rhel7: A newer version of the box 'generic/rhel7' for provider 'virtualbox' is == > rhel7: available! You currently have version '1.9.18' . The latest is version == > rhel7: '2.0.6' . Run ` vagrant box update ` to update. == > rhel7: Setting the name of the VM: rhel7_rhel7_1587731923819_11065 == > rhel7: Clearing any previously set network interfaces... == > rhel7: Preparing network interfaces based on configuration... rhel7: Adapter 1 : nat rhel7: Adapter 2 : hostonly == > rhel7: Forwarding ports... rhel7: 22 ( guest ) = > 2222 ( host ) ( adapter 1 ) == > rhel7: Running 'pre-boot' VM customizations... == > rhel7: Booting VM... == > rhel7: Waiting for machine to boot. This may take a few minutes... rhel7: SSH address: 127 .0.0.1:2222 rhel7: SSH username: vagrant rhel7: SSH auth method: private key == > rhel7: Machine booted and ready! == > rhel7: Checking for guest additions in VM... rhel7: The guest additions on this VM do not match the installed version of rhel7: VirtualBox! In most cases this is fine, but in rare cases it can rhel7: prevent things such as shared folders from working properly. If you see rhel7: shared folder errors, please make sure the guest additions within the rhel7: virtual machine match the version of VirtualBox you have installed on rhel7: your host and reload your VM. rhel7: rhel7: Guest Additions Version: 5 .2.30 r130521 rhel7: VirtualBox Version: 6 .0 == > rhel7: Setting hostname... == > rhel7: Configuring and enabling network interfaces... Now you can connect to the virtual machine using ~/.vagrant.d/insecure_private_key ssh key: 1 2 3 4 5 # add vagrant ssh key to your keychain ssh-add ~/.vagrant.d/insecure_private_key # connect to the VM ssh vagrant@10.10.10.10 Info For username:password in curl command use the credentials to access eccenca Artifactory and docker registry. Install the necessary software Inside the virtual machine and download the Corporate Memory orchestration from releases.eccenca.com : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # switch to superuser sudo su # Register your RHEL instance subscription-manager register export POOL_ID = $( subscription-manager list --available | grep \"Pool ID:\" | cut -d ':' -f 2 | tr -d '[:space:]' ) subscription-manager attach --pool = ${ POOL_ID } # enable RHEL repositories subscription-manager repos --enable = rhel-7-server-rpms subscription-manager repos --enable = rhel-7-server-extras-rpms subscription-manager repos --enable = rhel-7-server-optional-rpms # install and start docker yum install docker device-mapper-libs device-mapper-event-libs systemctl start docker.service systemctl enable docker.service # install docker-compose curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /bin/docker-compose chmod +x /bin/docker-compose # Install necessary system utilities yum install unzip git jq # get corporate memory orchestration package curl -u username https://releases.eccenca.com/docker-orchestration/cmem-orchestration-v21.11.5.zip > cmem-orchestration.zip unzip cmem-orchestration.zip rm cmem-orchestration.zip mv cmem-orchestration-v* /opt/corporate-memory cd /opt/corporate-memory git init && git add README.md && git commit -m \"init\" # give docker daemon access to /opt/corporate-memory directory chcon -Rt svirt_sandbox_file_t /opt/corporate-memory Create /opt/corporate-memory/environments/prod.env file with the following contents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash linenums=\"1\" CMEM_SERVICE_ACCOUNT_CLIENT_SECRET = c8c12828-000c-467b-9b6d-2d6b5e16df4a STARDOG_PASSWORD = admin TRUSTSTOREPASS = Aimeik5Ocho5riuC DEPLOYHOST = corporate.memory DI_VERSION = v20.03 DP_VERSION = v20.03 DM_VERSION = v20.03 APACHE2_VERSION = v2.6.0 KEYCLOAK_VERSION = v6.0.1-2 POSTGRES_VERSION = 11 .5-alpine STARDOG_VERSION = v7.2.0-1 DATAINTEGRATION_JAVA_TOOL_OPTIONS = -Xmx2g DATAPLATFORM_JAVA_TOOL_OPTIONS = -Xms1g -Xmx2g STARDOG_SERVER_JAVA_ARGS = -Xms1g -Xmx1g -XX:MaxDirectMemorySize = 2g DEPLOYPROTOCOL = https PORT = 443 APACHE_BASE_FILE = docker-compose.apache2-ssl.yml DATAINTEGRATION_BASE_FILE = docker-compose.dataintegration-ssl.yml APACHE_CONFIG = default.ssl.conf PROXY_ADDRESS_FORWARDING = true Login into eccenca docker registry: 1 docker login docker-registry.eccenca.com Provide a stardog license or request a trial license: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # check validity of your license $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin The license is invalid: java.io.EOFException make: *** [ custom.dist.Makefile:5: stardog-license-check ] Error 1 # request stardog trial license $ make stardog-license-request docker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license request --force --output /data/stardog-license-key.bin Thank you for downloading Stardog. A valid license was not found in /data. Would you like to download a trial license from Stardog ( y/N ) ? y Contacting Stardog.............. Please provide a valid email address to start your 60 -day trial ( we may occasionally contact you with Stardog news ) : ivan.ermilov@eccenca.com Contacting license server................... Email validated. You now have a 60 -day Stardog trial license. Starting Stardog... %\u2584, \u2591\u2591\u0393\u256c\u2580\u2580\u2588\u2593\u2563\u2310 \u2584\u2593\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2568\u2593 .\u2310\u2310. .\u00bd\u2593\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2584 \u2310\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393\u00ab\u2310 \u2264\u2591\u2593\u2588\u2588\u2588\u2593\u2593\u258c\u2584\u2591\u2591\u2591\u2591\u2591\u2593\u2592\u2588\u0393\u2310 .\u00bb\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2265\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2590\u2588\u2584\u2559\u2591\u2591\u2265\u2591\u2591\u2265 [ \u00bb. \u250c\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2593\u2593\u2593\u258c\u258c\u258c\u258c\u2593\u2593\u2588\u2593\u2310 .\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2592\u2592\u2593\u2593\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u255f\u2588\u2588\u2588\u2588\u2588\u2588\u2559 \u2514\u2588b \u2588\u2588\u2588\u2588\u2580\u2580\u2592\u2588\u2588\u2588\u2588\u2588\u258c \u0393 .\u2229\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 ` \u2559 \u255f\u2588\u2593\u2229 \u2588\u2588\u2588\u2580\u2588\u2588\u258c \u251c\u2591, .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588 \u2590\u2588 \u2588\u2588 \u2559 \u251c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588 , ' \u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2555 \u2590\u2588\u2584 \u2588\u2588 .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2584\u2591\u2591\u2591\u2591\u2591\u2584\u2563\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2588\u2588\u2591\u2591\u2563 \u2588\u2588\u2584\u2584\u2563\u2592\u2592\u258c\u2584\u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591, \u2514\u2559\u2580\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2566 \u2588\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u258c \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2559\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2580\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u256c\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 ' \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584 ` \u2514\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2551\u2588\u2588\u2593\u2584\u2591\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393 \"\u2559\u2591\u2591\u2591\u2591\u2580\u2580 \u2559\u2591\u2591\u2584\u2563\u2593\u2593\u2588\u2588\u2580\u2580 \u2559\u2580\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2310 \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2514\u2563\u2588\u2588\u2588\u2580\u2580\u2514 \u2559\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2588\u2580 '\"\"` .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2560\u2593\u2588\u2588\u2588\u2580\u00b2 \u00ab\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2580 \u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2534\u2580\u2580\u2559 .\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229` \u00e1\u2580\u2580\u2555\u2584#\u258c\u2580\u2580\u2591\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2559\u2229\" \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229 ` \u2514\u2591\u2591\u2591\u2591\u2559\u2229 ` Thank you! # check the license again $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin Licensee: Stardog Trial User ( ivan.ermilov@eccenca.com ) , Stardog Union Version: Stardog * Type: Trial Issued: Mon Mar 30 10 :47:17 GMT 2020 Expiration: 59 days Support: The license does not include maintenance. Quantity: 3 Finally deploy the Corporate Memory instance: 1 2 3 4 5 # create local truststore CONFIGFILE = environments/prod.env make buildTrustStore # start and bootstrap Corporate Memory CONFIGFILE = environments/prod.env make clean-pull-start-bootstrap You have successfully deployed a Corporate Memory instance. Access Corporate Memory Instance \u00a4 On your localhost where you are running VirtualBox, modify /etc/hosts file: 1 echo \"10.10.10.10 corporate.memory\" >> /etc/hosts Open your browser and navigate to [https://corporate.memory] https://corporate.memory Click CONTINUE WITH LOGIN and use one of these default accounts: account password description admin admin Is member of the global admin group (can see and do anything) user user Is member of the local user group (can not change access conditions or see internal graphs) After successful login, you will see Corporate Memory interface. You can now proceed to the Getting Started section.","title":"Scenario: RedHat Enterprise Linux 7"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#scenario-redhat-enterprise-linux-7","text":"","title":"Scenario: RedHat Enterprise Linux 7"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#introduction","text":"This page describes a docker-compose based orchestration running on RedHat Enterprise Linux 7 (RHEL 7) inside a VirtualBox virtual machine.","title":"Introduction"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#requirements","text":"Virtualbox and vagrant installed locally Terminal with ssh client installed locally POSIX-compatible command line interface (Linux, macOS or WSL for Windows)","title":"Requirements"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#provisioning","text":"Create a working directory for this scenario and inside the working directory Vagrantfile with the following contents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \"2\" in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don't change it unless you know what # you're doing. Vagrant.configure ( \"2\" ) do | config | config.vbguest.auto_update = false config.vbguest.no_remote = true config.vm.box = \"generic/rhel7\" config.ssh.private_key_path = File.expand_path ( '~/.vagrant.d/insecure_private_key' ) config.ssh.insert_key = false config.vm.define \"rhel7\" do | rhel7 | rhel7.vm.network \"private_network\" , ip: \"10.10.10.10\" rhel7.vm.hostname = \"rhel7.eccenca.local\" rhel7.vm.provider \"virtualbox\" do | dpvm | dpvm.memory = 10240 dpvm.cpus = 4 end end end Spin up the virtual machine: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ vagrant up Bringing machine 'rhel7' up with 'virtualbox' provider... == > rhel7: Importing base box 'generic/rhel7' ... == > rhel7: Matching MAC address for NAT networking... == > rhel7: Checking if box 'generic/rhel7' is up to date... == > rhel7: A newer version of the box 'generic/rhel7' for provider 'virtualbox' is == > rhel7: available! You currently have version '1.9.18' . The latest is version == > rhel7: '2.0.6' . Run ` vagrant box update ` to update. == > rhel7: Setting the name of the VM: rhel7_rhel7_1587731923819_11065 == > rhel7: Clearing any previously set network interfaces... == > rhel7: Preparing network interfaces based on configuration... rhel7: Adapter 1 : nat rhel7: Adapter 2 : hostonly == > rhel7: Forwarding ports... rhel7: 22 ( guest ) = > 2222 ( host ) ( adapter 1 ) == > rhel7: Running 'pre-boot' VM customizations... == > rhel7: Booting VM... == > rhel7: Waiting for machine to boot. This may take a few minutes... rhel7: SSH address: 127 .0.0.1:2222 rhel7: SSH username: vagrant rhel7: SSH auth method: private key == > rhel7: Machine booted and ready! == > rhel7: Checking for guest additions in VM... rhel7: The guest additions on this VM do not match the installed version of rhel7: VirtualBox! In most cases this is fine, but in rare cases it can rhel7: prevent things such as shared folders from working properly. If you see rhel7: shared folder errors, please make sure the guest additions within the rhel7: virtual machine match the version of VirtualBox you have installed on rhel7: your host and reload your VM. rhel7: rhel7: Guest Additions Version: 5 .2.30 r130521 rhel7: VirtualBox Version: 6 .0 == > rhel7: Setting hostname... == > rhel7: Configuring and enabling network interfaces... Now you can connect to the virtual machine using ~/.vagrant.d/insecure_private_key ssh key: 1 2 3 4 5 # add vagrant ssh key to your keychain ssh-add ~/.vagrant.d/insecure_private_key # connect to the VM ssh vagrant@10.10.10.10 Info For username:password in curl command use the credentials to access eccenca Artifactory and docker registry. Install the necessary software Inside the virtual machine and download the Corporate Memory orchestration from releases.eccenca.com : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # switch to superuser sudo su # Register your RHEL instance subscription-manager register export POOL_ID = $( subscription-manager list --available | grep \"Pool ID:\" | cut -d ':' -f 2 | tr -d '[:space:]' ) subscription-manager attach --pool = ${ POOL_ID } # enable RHEL repositories subscription-manager repos --enable = rhel-7-server-rpms subscription-manager repos --enable = rhel-7-server-extras-rpms subscription-manager repos --enable = rhel-7-server-optional-rpms # install and start docker yum install docker device-mapper-libs device-mapper-event-libs systemctl start docker.service systemctl enable docker.service # install docker-compose curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose- $( uname -s ) - $( uname -m ) \" -o /bin/docker-compose chmod +x /bin/docker-compose # Install necessary system utilities yum install unzip git jq # get corporate memory orchestration package curl -u username https://releases.eccenca.com/docker-orchestration/cmem-orchestration-v21.11.5.zip > cmem-orchestration.zip unzip cmem-orchestration.zip rm cmem-orchestration.zip mv cmem-orchestration-v* /opt/corporate-memory cd /opt/corporate-memory git init && git add README.md && git commit -m \"init\" # give docker daemon access to /opt/corporate-memory directory chcon -Rt svirt_sandbox_file_t /opt/corporate-memory Create /opt/corporate-memory/environments/prod.env file with the following contents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash linenums=\"1\" CMEM_SERVICE_ACCOUNT_CLIENT_SECRET = c8c12828-000c-467b-9b6d-2d6b5e16df4a STARDOG_PASSWORD = admin TRUSTSTOREPASS = Aimeik5Ocho5riuC DEPLOYHOST = corporate.memory DI_VERSION = v20.03 DP_VERSION = v20.03 DM_VERSION = v20.03 APACHE2_VERSION = v2.6.0 KEYCLOAK_VERSION = v6.0.1-2 POSTGRES_VERSION = 11 .5-alpine STARDOG_VERSION = v7.2.0-1 DATAINTEGRATION_JAVA_TOOL_OPTIONS = -Xmx2g DATAPLATFORM_JAVA_TOOL_OPTIONS = -Xms1g -Xmx2g STARDOG_SERVER_JAVA_ARGS = -Xms1g -Xmx1g -XX:MaxDirectMemorySize = 2g DEPLOYPROTOCOL = https PORT = 443 APACHE_BASE_FILE = docker-compose.apache2-ssl.yml DATAINTEGRATION_BASE_FILE = docker-compose.dataintegration-ssl.yml APACHE_CONFIG = default.ssl.conf PROXY_ADDRESS_FORWARDING = true Login into eccenca docker registry: 1 docker login docker-registry.eccenca.com Provide a stardog license or request a trial license: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # check validity of your license $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin The license is invalid: java.io.EOFException make: *** [ custom.dist.Makefile:5: stardog-license-check ] Error 1 # request stardog trial license $ make stardog-license-request docker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license request --force --output /data/stardog-license-key.bin Thank you for downloading Stardog. A valid license was not found in /data. Would you like to download a trial license from Stardog ( y/N ) ? y Contacting Stardog.............. Please provide a valid email address to start your 60 -day trial ( we may occasionally contact you with Stardog news ) : ivan.ermilov@eccenca.com Contacting license server................... Email validated. You now have a 60 -day Stardog trial license. Starting Stardog... %\u2584, \u2591\u2591\u0393\u256c\u2580\u2580\u2588\u2593\u2563\u2310 \u2584\u2593\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2568\u2593 .\u2310\u2310. .\u00bd\u2593\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2584 \u2310\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393\u00ab\u2310 \u2264\u2591\u2593\u2588\u2588\u2588\u2593\u2593\u258c\u2584\u2591\u2591\u2591\u2591\u2591\u2593\u2592\u2588\u0393\u2310 .\u00bb\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2265\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2590\u2588\u2584\u2559\u2591\u2591\u2265\u2591\u2591\u2265 [ \u00bb. \u250c\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2593\u2593\u2593\u258c\u258c\u258c\u258c\u2593\u2593\u2588\u2593\u2310 .\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2592\u2592\u2593\u2593\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u255f\u2588\u2588\u2588\u2588\u2588\u2588\u2559 \u2514\u2588b \u2588\u2588\u2588\u2588\u2580\u2580\u2592\u2588\u2588\u2588\u2588\u2588\u258c \u0393 .\u2229\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 ` \u2559 \u255f\u2588\u2593\u2229 \u2588\u2588\u2588\u2580\u2588\u2588\u258c \u251c\u2591, .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588 \u2590\u2588 \u2588\u2588 \u2559 \u251c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588 , ' \u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2555 \u2590\u2588\u2584 \u2588\u2588 .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2584\u2591\u2591\u2591\u2591\u2591\u2584\u2563\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2588\u2588\u2591\u2591\u2563 \u2588\u2588\u2584\u2584\u2563\u2592\u2592\u258c\u2584\u2584 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591, \u2514\u2559\u2580\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2566 \u2588\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u258c \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2559\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2580\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u256c\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 ' \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584 ` \u2514\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2551\u2588\u2588\u2593\u2584\u2591\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393 \"\u2559\u2591\u2591\u2591\u2591\u2580\u2580 \u2559\u2591\u2591\u2584\u2563\u2593\u2593\u2588\u2588\u2580\u2580 \u2559\u2580\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2310 \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2514\u2563\u2588\u2588\u2588\u2580\u2580\u2514 \u2559\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2588\u2580 '\"\"` .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2560\u2593\u2588\u2588\u2588\u2580\u00b2 \u00ab\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2580 \u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2534\u2580\u2580\u2559 .\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229` \u00e1\u2580\u2580\u2555\u2584#\u258c\u2580\u2580\u2591\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2559\u2229\" \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229 ` \u2514\u2591\u2591\u2591\u2591\u2559\u2229 ` Thank you! # check the license again $ make stardog-license-check docker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin Licensee: Stardog Trial User ( ivan.ermilov@eccenca.com ) , Stardog Union Version: Stardog * Type: Trial Issued: Mon Mar 30 10 :47:17 GMT 2020 Expiration: 59 days Support: The license does not include maintenance. Quantity: 3 Finally deploy the Corporate Memory instance: 1 2 3 4 5 # create local truststore CONFIGFILE = environments/prod.env make buildTrustStore # start and bootstrap Corporate Memory CONFIGFILE = environments/prod.env make clean-pull-start-bootstrap You have successfully deployed a Corporate Memory instance.","title":"Provisioning"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#access-corporate-memory-instance","text":"On your localhost where you are running VirtualBox, modify /etc/hosts file: 1 echo \"10.10.10.10 corporate.memory\" >> /etc/hosts Open your browser and navigate to [https://corporate.memory] https://corporate.memory Click CONTINUE WITH LOGIN and use one of these default accounts: account password description admin admin Is member of the global admin group (can see and do anything) user user Is member of the local user group (can not change access conditions or see internal graphs) After successful login, you will see Corporate Memory interface. You can now proceed to the Getting Started section.","title":"Access Corporate Memory Instance"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/","text":"Scenario: Single Node Cloud Installation \u00a4 Introduction \u00a4 This page describes a docker-compose based orchestration running on a server instance accessible publicly via browser (SSL enabled via letsencrypt). Requirements \u00a4 ssh access to a server instance (Debian 10) with a public IP address A resolvable domain name to this server Terminal with ssh client installed locally An eccenca partner account for the docker registry as well as the release artifact area Server Provisioning \u00a4 In this step, you install necessary software on the server and execute the following commands as root: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apt-get update # install ntp and set timezone apt-get install -y ntp timedatectl set-timezone Europe/Berlin # install needed packages apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common gnupg lsb-release gettext zip unzip git make vim # install docker curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install docker-ce docker-ce-cli containerd.io # (optional) add a user to docker group # usermod -a -G docker admin # install docker-compose curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose Installation \u00a4 Info For username and password in curl command use the credentials to access eccenca Artifactory and docker registry. Connect to the server and navigate to the directory with the Corporate Memory docker orchestration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # login to the eccenca docker registry docker login docker-registry.eccenca.com # download the Corporate Memory orchestration distribution cd /opt curl -u username https://releases.eccenca.com/docker-orchestration/latest.zip > cmem-orchestration.zip # unzip the orchestration and move the unzipped directory to /opt/cmem-orchestration unzip cmem-orchestration.zip rm cmem-orchestration.zip mv cmem-orchestration-v* /opt/cmem-orchestration # configure git in order to commit changes to the orchestration cd /opt/cmem-orchestration git config --global user.email \"you@example.com\" && git init && git add . && git commit -m \"stub\" The Corporate Memory docker orchestration is configured with environment files. You will need to create an environment file at /opt/cmem-orchestration/environments/prod.env - for now, you can use the provide file config.ssl-letsencrypt.env as a template. Warning You need to change the lines with DEPLOYHOST and LETSENCRYPT_MAIL to you actual values. 1 2 3 4 5 cd /opt/cmem-orchestration/environments cp config.ssl-letsencrypt.env prod.env # change DEPLOYHOST and LETSENCRYPT_MAIL values vi prod.env In addition that, you need to remove the default config and link it to your prod.env 1 2 3 4 cd /opt/cmem-orchestration/environments rm config.env ln -s prod.env config.env To see all available configuration options refer to Docker Orchestration configuration page. Next, request SSL certificates from letsencrypt service: 1 2 cd /opt/cmem-orchestration make letsencrypt-create Change CMEM_BASE_URI according to your DEPLOYHOST. 1 2 3 4 5 6 7 8 9 # update cmemc configuration rm conf/cmemc/cmemc.ini cat <<EOF > conf/cmemc/cmemc.ini [cmem] CMEM_BASE_URI=https://corporate-memory.eccenca.dev/ OAUTH_GRANT_TYPE=client_credentials OAUTH_CLIENT_ID=cmem-service-account OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a EOF Finally deploy the Corporate Memory instance: 1 2 make clean-pull-start-bootstrap make tutorials-import Optional: you can install cmem as a systemd service for this use these commands as root oder sudo: 1 2 3 cp /opt/cmem-orchestration/conf/systemd/cmem-orchestration.service /etc/systemd/system systemctl enable cmem-orchestration systemctl start cmem-orchestration Validation and Finalisation \u00a4 Open your browser and navigate to the host you have created in DNS server, e.g. [https://corporate-memory.eccenca.dev] https://corporate-memory.eccenca.dev/ Click CONTINUE WITH LOGIN and use one of these default accounts: account password description admin admin Is member of the global admin group (can see and do anything) user user Is member of the local user group (can not change access conditions or see internal graphs) After successful login, you will see Corporate Memory interface. You can now proceed to the Getting Started section. Do not forget to change the passwords of your deployment, especially if it is available from the public internet. For this, take a look at Change Passwords and Keys . Change the passwords for your needs To login in to keycloak and change the passwords To change keycloak admin To change cmem admin To change cmem user To change OAUTH_CLIENT_SECRET","title":"Scenario: Single Node Cloud Installation"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#scenario-single-node-cloud-installation","text":"","title":"Scenario: Single Node Cloud Installation"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#introduction","text":"This page describes a docker-compose based orchestration running on a server instance accessible publicly via browser (SSL enabled via letsencrypt).","title":"Introduction"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#requirements","text":"ssh access to a server instance (Debian 10) with a public IP address A resolvable domain name to this server Terminal with ssh client installed locally An eccenca partner account for the docker registry as well as the release artifact area","title":"Requirements"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#server-provisioning","text":"In this step, you install necessary software on the server and execute the following commands as root: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apt-get update # install ntp and set timezone apt-get install -y ntp timedatectl set-timezone Europe/Berlin # install needed packages apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common gnupg lsb-release gettext zip unzip git make vim # install docker curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install docker-ce docker-ce-cli containerd.io # (optional) add a user to docker group # usermod -a -G docker admin # install docker-compose curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose","title":"Server Provisioning"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#installation","text":"Info For username and password in curl command use the credentials to access eccenca Artifactory and docker registry. Connect to the server and navigate to the directory with the Corporate Memory docker orchestration: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # login to the eccenca docker registry docker login docker-registry.eccenca.com # download the Corporate Memory orchestration distribution cd /opt curl -u username https://releases.eccenca.com/docker-orchestration/latest.zip > cmem-orchestration.zip # unzip the orchestration and move the unzipped directory to /opt/cmem-orchestration unzip cmem-orchestration.zip rm cmem-orchestration.zip mv cmem-orchestration-v* /opt/cmem-orchestration # configure git in order to commit changes to the orchestration cd /opt/cmem-orchestration git config --global user.email \"you@example.com\" && git init && git add . && git commit -m \"stub\" The Corporate Memory docker orchestration is configured with environment files. You will need to create an environment file at /opt/cmem-orchestration/environments/prod.env - for now, you can use the provide file config.ssl-letsencrypt.env as a template. Warning You need to change the lines with DEPLOYHOST and LETSENCRYPT_MAIL to you actual values. 1 2 3 4 5 cd /opt/cmem-orchestration/environments cp config.ssl-letsencrypt.env prod.env # change DEPLOYHOST and LETSENCRYPT_MAIL values vi prod.env In addition that, you need to remove the default config and link it to your prod.env 1 2 3 4 cd /opt/cmem-orchestration/environments rm config.env ln -s prod.env config.env To see all available configuration options refer to Docker Orchestration configuration page. Next, request SSL certificates from letsencrypt service: 1 2 cd /opt/cmem-orchestration make letsencrypt-create Change CMEM_BASE_URI according to your DEPLOYHOST. 1 2 3 4 5 6 7 8 9 # update cmemc configuration rm conf/cmemc/cmemc.ini cat <<EOF > conf/cmemc/cmemc.ini [cmem] CMEM_BASE_URI=https://corporate-memory.eccenca.dev/ OAUTH_GRANT_TYPE=client_credentials OAUTH_CLIENT_ID=cmem-service-account OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a EOF Finally deploy the Corporate Memory instance: 1 2 make clean-pull-start-bootstrap make tutorials-import Optional: you can install cmem as a systemd service for this use these commands as root oder sudo: 1 2 3 cp /opt/cmem-orchestration/conf/systemd/cmem-orchestration.service /etc/systemd/system systemctl enable cmem-orchestration systemctl start cmem-orchestration","title":"Installation"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#validation-and-finalisation","text":"Open your browser and navigate to the host you have created in DNS server, e.g. [https://corporate-memory.eccenca.dev] https://corporate-memory.eccenca.dev/ Click CONTINUE WITH LOGIN and use one of these default accounts: account password description admin admin Is member of the global admin group (can see and do anything) user user Is member of the local user group (can not change access conditions or see internal graphs) After successful login, you will see Corporate Memory interface. You can now proceed to the Getting Started section. Do not forget to change the passwords of your deployment, especially if it is available from the public internet. For this, take a look at Change Passwords and Keys . Change the passwords for your needs To login in to keycloak and change the passwords To change keycloak admin To change cmem admin To change cmem user To change OAUTH_CLIENT_SECRET","title":"Validation and Finalisation"},{"location":"deploy-and-configure/migrating-stores/","text":"Migrating Stores \u00a4 Sizing and Deployment \u00a4 size and deploy of the new store (refer to the capacity planning / sizing considerations, refer to the docker container / orchestration) store specific config (e.g. search-all-graphs in SD) Transferring Data and Configuration \u00a4 backing-up / exporting and restore / import of graphs, DI-projects, configuration (if any) graphs blacklisting the DI projects graphs config DM stardog text match support (this is a DM parameter!) search queries navigation DP configure the resp. store DI nothing to do \u2026 just duplicate / copy the configuration as-is cmemc admin workspace export / import Test and Validation \u00a4 best practice: run all (SELECT) queries in the query catalog and compare results (e.g. with cmemc) theoretically this could also be applied to INSERT queries (by re-writing into SELECTS in case you want / need to omit altering your graphs) count all triples in all graphs on both instances before/after export/import (cmemc graph count \u2013all) Optimizing Your Setup \u00a4 optimizing customization (e.g. queries in SHAPES; DI; DM-config) \u201ctextmatch\u201d / \u201clucene\u201d queries need to be migrated (a query can be helpful to find these queries\u2026) performance comparisons could be automated via \u201ccmemc query replay\u201d identify query that won\u2019t run or run slow general query best practices \u2192 query optimization guide use VALUE instead of FILTER (?x IN (\u2026)) (esp. on GDB)","title":"Migrating Stores"},{"location":"deploy-and-configure/migrating-stores/#migrating-stores","text":"","title":"Migrating Stores"},{"location":"deploy-and-configure/migrating-stores/#sizing-and-deployment","text":"size and deploy of the new store (refer to the capacity planning / sizing considerations, refer to the docker container / orchestration) store specific config (e.g. search-all-graphs in SD)","title":"Sizing and Deployment"},{"location":"deploy-and-configure/migrating-stores/#transferring-data-and-configuration","text":"backing-up / exporting and restore / import of graphs, DI-projects, configuration (if any) graphs blacklisting the DI projects graphs config DM stardog text match support (this is a DM parameter!) search queries navigation DP configure the resp. store DI nothing to do \u2026 just duplicate / copy the configuration as-is cmemc admin workspace export / import","title":"Transferring Data and Configuration"},{"location":"deploy-and-configure/migrating-stores/#test-and-validation","text":"best practice: run all (SELECT) queries in the query catalog and compare results (e.g. with cmemc) theoretically this could also be applied to INSERT queries (by re-writing into SELECTS in case you want / need to omit altering your graphs) count all triples in all graphs on both instances before/after export/import (cmemc graph count \u2013all)","title":"Test and Validation"},{"location":"deploy-and-configure/migrating-stores/#optimizing-your-setup","text":"optimizing customization (e.g. queries in SHAPES; DI; DM-config) \u201ctextmatch\u201d / \u201clucene\u201d queries need to be migrated (a query can be helpful to find these queries\u2026) performance comparisons could be automated via \u201ccmemc query replay\u201d identify query that won\u2019t run or run slow general query best practices \u2192 query optimization guide use VALUE instead of FILTER (?x IN (\u2026)) (esp. on GDB)","title":"Optimizing Your Setup"},{"location":"deploy-and-configure/requirements/","text":"Requirements \u00a4 This page lists software and hardware requirements for eccenca Corporate Memory deployments. For a general overview of a deployment setup please refer to the System Architecture . Corporate Memory \u00a4 Minimal Setup \u00a4 A minimal single-node deployment for testing/evaluation purposes means: no memory consuming linking and transformation workflows, nearly no concurrent users (< 5). Depending on how much RAM is dedicated to the triple store, Knowledge Graphs up to several million triples can be built and served. Operating System / Hardware Bare metal server or VM with Ubuntu or RHEL linux OS (see Installation for details) 16 GB RAM 100 GB free disk space (10 GB for docker images + data + logs over time) docker and docker-compose (we deliver an orchestration including all needed components) Triple / Quad Store Stardog 7.4.x Ontotext GraphDB OpenLink Virtuoso Open Source Identity Management (delivered with orchestration) Keycloak >= v6.0.1 PostgreSQL >= v11.5 Proxy Server (delivered with orchestration) Apache >=2.4 (incl. rewrite and other modules) For an example of a single-node installation refer to the following scenarios: Scenario: Local Installation Scenario: RedHat Enterprise Linux 7 Scenario: Single Node Cloud Installation Typical Setup \u00a4 In a typical deployment all components are installed on separate VMs (nodes). Therefore, six separate VMs are required. The following numbers are based on existing customer deployments running Knowledge Graphs up to 300 million triples with 40 concurrent users. eccenca DataPlatform >= 4 cores >= 8 GB RAM eccenca DataIntegration >= 4 cores >= 8 GB RAM eccenca DataManager >= 2 GB RAM Triple / Quad Store >= 8 GB RAM >= 4 cores KeyCloak incl. PostgeSQL >= 4 GB RAM Proxy Server >= 2 cores >= 2 GB RAM Info needs to be scaled with concurrent users depends on the DataIntegration workflows needs to be scaled with the amount of triples in cloud deployments, this could / will be a cloud service Clients \u00a4 Browser / Web Client \u00a4 We support all (LTS/ESR) versions of the below listed browsers that are actively supported be the respective publishers Microsoft Edge > v88.0 Google Chrome or Chromium > v92.0 Firefox > v78.0 Info Internet Explorer 11 as well as Safari Browser are not officially supported. IE11 is reported not to work. Command Line Client (cmemc) \u00a4 Python 3.7 Installation options: pip-based installation single binary / executable available for Ubuntu, RHEL and Microsoft Windows docker image based on the official debian slim image","title":"Requirements"},{"location":"deploy-and-configure/requirements/#requirements","text":"This page lists software and hardware requirements for eccenca Corporate Memory deployments. For a general overview of a deployment setup please refer to the System Architecture .","title":"Requirements"},{"location":"deploy-and-configure/requirements/#corporate-memory","text":"","title":"Corporate Memory"},{"location":"deploy-and-configure/requirements/#minimal-setup","text":"A minimal single-node deployment for testing/evaluation purposes means: no memory consuming linking and transformation workflows, nearly no concurrent users (< 5). Depending on how much RAM is dedicated to the triple store, Knowledge Graphs up to several million triples can be built and served. Operating System / Hardware Bare metal server or VM with Ubuntu or RHEL linux OS (see Installation for details) 16 GB RAM 100 GB free disk space (10 GB for docker images + data + logs over time) docker and docker-compose (we deliver an orchestration including all needed components) Triple / Quad Store Stardog 7.4.x Ontotext GraphDB OpenLink Virtuoso Open Source Identity Management (delivered with orchestration) Keycloak >= v6.0.1 PostgreSQL >= v11.5 Proxy Server (delivered with orchestration) Apache >=2.4 (incl. rewrite and other modules) For an example of a single-node installation refer to the following scenarios: Scenario: Local Installation Scenario: RedHat Enterprise Linux 7 Scenario: Single Node Cloud Installation","title":"Minimal Setup"},{"location":"deploy-and-configure/requirements/#typical-setup","text":"In a typical deployment all components are installed on separate VMs (nodes). Therefore, six separate VMs are required. The following numbers are based on existing customer deployments running Knowledge Graphs up to 300 million triples with 40 concurrent users. eccenca DataPlatform >= 4 cores >= 8 GB RAM eccenca DataIntegration >= 4 cores >= 8 GB RAM eccenca DataManager >= 2 GB RAM Triple / Quad Store >= 8 GB RAM >= 4 cores KeyCloak incl. PostgeSQL >= 4 GB RAM Proxy Server >= 2 cores >= 2 GB RAM Info needs to be scaled with concurrent users depends on the DataIntegration workflows needs to be scaled with the amount of triples in cloud deployments, this could / will be a cloud service","title":"Typical Setup"},{"location":"deploy-and-configure/requirements/#clients","text":"","title":"Clients"},{"location":"deploy-and-configure/requirements/#browser-web-client","text":"We support all (LTS/ESR) versions of the below listed browsers that are actively supported be the respective publishers Microsoft Edge > v88.0 Google Chrome or Chromium > v92.0 Firefox > v78.0 Info Internet Explorer 11 as well as Safari Browser are not officially supported. IE11 is reported not to work.","title":"Browser / Web Client"},{"location":"deploy-and-configure/requirements/#command-line-client-cmemc","text":"Python 3.7 Installation options: pip-based installation single binary / executable available for Ubuntu, RHEL and Microsoft Windows docker image based on the official debian slim image","title":"Command Line Client (cmemc)"},{"location":"deploy-and-configure/system-architecture/","text":"System Architecture \u00a4 This section describes the general system architecture of eccenca Corporate Memory and its components. eccenca Corporate Memory consists of five core components: (2) eccenca DataIntegration (3) eccenca DataManager , (4) eccenca DataPlatform , (8) Keycloak , and (12) cmemc (Corporate Memory Control) DataIntegration (2) is a Corporate Memory component which enables integration of multiple databases into a single consistent knowledge graph. Datasets in their original format are mapped and linked to RDF schemata and then linked to and persisted into a knowledge graph. The data integration is performed semi-automatically based on domain-specific integration rules and vocabularies (OWL ontologies). Corporate Memory supports multiple kinds of source integration data sources (6) such as SQL databases or files of different formats. These files can be processed with DataIntegration either locally or on a remote Spark cluster (7). DataManager (3) is a single-page JavaScript application which enables creating and managing knowledge graphs based on established W3C standards. It is a generic data browser suitable to edit, explore and query the created knowledge graph. DataManager provides convenient options to create specific data views by using Shapes Constraint Language (SHACL). DataPlatform (4) is a semantic middleware application which provides a unified access to semantic graph data. Additionally, DataPlatform manages authorization of the users according to the access control lists defined in the Triple Store. The knowledge graph is stored in a triple store (5) connected to DataPlatform. This can either be a physical store like Complexible Stardog , GraphDB , Virtuoso or a remotely accessible SPARQL 1.1 compliant HTTP endpoint. Keycloak (8) provides authentication. Keycloak can act as an authentication broker for already existing, external OpenId Connect or SAML infrastructures (9). In addition to that, Keycloak supports a wide variety of internal user management configuration scenarios and the option to connect to an external LDAP server for user and group synchronization (10). Keycloak uses the embedded Java-based relational database H2 as a default to store its configuration data. However, it is highly recommended to [use a relational database] https://www.keycloak.org/docs/6.0/server_installation/#_database (11) for production use instead. Refer to the [Keycloak manual] https://www.keycloak.org/docs/6.0/server_installation/ for further information on possible setups. cmemc (12) (Corporate Memory Control) is the eccenca Corporate Memory Command Line Interface (CLI). cmemc is intended for system administrators and Linked Data Experts who wants to automate and remote control activities on Corporate Memory.","title":"System Architecture"},{"location":"deploy-and-configure/system-architecture/#system-architecture","text":"This section describes the general system architecture of eccenca Corporate Memory and its components. eccenca Corporate Memory consists of five core components: (2) eccenca DataIntegration (3) eccenca DataManager , (4) eccenca DataPlatform , (8) Keycloak , and (12) cmemc (Corporate Memory Control) DataIntegration (2) is a Corporate Memory component which enables integration of multiple databases into a single consistent knowledge graph. Datasets in their original format are mapped and linked to RDF schemata and then linked to and persisted into a knowledge graph. The data integration is performed semi-automatically based on domain-specific integration rules and vocabularies (OWL ontologies). Corporate Memory supports multiple kinds of source integration data sources (6) such as SQL databases or files of different formats. These files can be processed with DataIntegration either locally or on a remote Spark cluster (7). DataManager (3) is a single-page JavaScript application which enables creating and managing knowledge graphs based on established W3C standards. It is a generic data browser suitable to edit, explore and query the created knowledge graph. DataManager provides convenient options to create specific data views by using Shapes Constraint Language (SHACL). DataPlatform (4) is a semantic middleware application which provides a unified access to semantic graph data. Additionally, DataPlatform manages authorization of the users according to the access control lists defined in the Triple Store. The knowledge graph is stored in a triple store (5) connected to DataPlatform. This can either be a physical store like Complexible Stardog , GraphDB , Virtuoso or a remotely accessible SPARQL 1.1 compliant HTTP endpoint. Keycloak (8) provides authentication. Keycloak can act as an authentication broker for already existing, external OpenId Connect or SAML infrastructures (9). In addition to that, Keycloak supports a wide variety of internal user management configuration scenarios and the option to connect to an external LDAP server for user and group synchronization (10). Keycloak uses the embedded Java-based relational database H2 as a default to store its configuration data. However, it is highly recommended to [use a relational database] https://www.keycloak.org/docs/6.0/server_installation/#_database (11) for production use instead. Refer to the [Keycloak manual] https://www.keycloak.org/docs/6.0/server_installation/ for further information on possible setups. cmemc (12) (Corporate Memory Control) is the eccenca Corporate Memory Command Line Interface (CLI). cmemc is intended for system administrators and Linked Data Experts who wants to automate and remote control activities on Corporate Memory.","title":"System Architecture"},{"location":"deploy-and-configure/troubleshooting/","text":"","title":"Index"},{"location":"develop/","text":"\u2605 Develop \u00a4 API documentation and programming recipes. Base URL \u00a4 All relative API URLs are prefixed by a base URL of the form https://{domain}:{port}/{context} with the following parameters: domain : The hostname or domain, where DataPlatform is installed. (required: true, type: string) port : The port on which the application server is available. (required: false, type: integer) context : The application context where DataPlatform is available (can be empty). (required: false, type: string) HTTP error responses \u00a4 The default format for HTTP error responses is compliant with RFC 7807 Problem Details for HTTP APIs . An HTTP error response contains a JSON object that provides at least two fields: title : A short, human-readable summary of the problem type. detail : A human-readable explanation specific to this occurrence of the problem. The following optional non-standard fields may also be set: status : The HTTP status code for this occurrence of the problem. cause : The cause for this occurrence of the problem. It contains at least the same elements as specified previously, such as title and detail . The following example shows an HTTP response containing JSON problem details using the application/problem+json media type: HTTP/ 1.1 500 Co ntent - Type : applica t io n /problem+jso n { \"title\" : \"Internal Server Error\" , \"status\" : 500 , \"detail\" : \"Database server 'Stardog' unavailable\" , \"cause\" : { \"title\" : \"Internal Server Error\" , \"status\" : 500 , \"detail\" : \"Connection refused (Connection refused)\" } } Available APIs and Recipes \u00a4 cmempy - Python API \u2014 cmempy is a Python API wrapper around the eccenca Corporate Memory HTTP APIs which can be used to rapidly script processes which interact with Corporate Memory. DataIntegration APIs \u2014 eccenca DataIntegration APIs can be used to control, initiate and setup all task and activities related to the \u2605 Build step (such as datasets, transformations, linking tasks etc.). ( DataIntegration OpenAPI Reference ) DataPlatform APIs \u2014 eccenca DataPlatform APIs can be used to import, export, query and extract information from graphs as well as to check access conditions. DataPlatform OpenAPI Reference )","title":"\u2606 Develop"},{"location":"develop/#develop","text":"API documentation and programming recipes.","title":"\u2605 Develop"},{"location":"develop/#base-url","text":"All relative API URLs are prefixed by a base URL of the form https://{domain}:{port}/{context} with the following parameters: domain : The hostname or domain, where DataPlatform is installed. (required: true, type: string) port : The port on which the application server is available. (required: false, type: integer) context : The application context where DataPlatform is available (can be empty). (required: false, type: string)","title":"Base URL"},{"location":"develop/#http-error-responses","text":"The default format for HTTP error responses is compliant with RFC 7807 Problem Details for HTTP APIs . An HTTP error response contains a JSON object that provides at least two fields: title : A short, human-readable summary of the problem type. detail : A human-readable explanation specific to this occurrence of the problem. The following optional non-standard fields may also be set: status : The HTTP status code for this occurrence of the problem. cause : The cause for this occurrence of the problem. It contains at least the same elements as specified previously, such as title and detail . The following example shows an HTTP response containing JSON problem details using the application/problem+json media type: HTTP/ 1.1 500 Co ntent - Type : applica t io n /problem+jso n { \"title\" : \"Internal Server Error\" , \"status\" : 500 , \"detail\" : \"Database server 'Stardog' unavailable\" , \"cause\" : { \"title\" : \"Internal Server Error\" , \"status\" : 500 , \"detail\" : \"Connection refused (Connection refused)\" } }","title":"HTTP error responses"},{"location":"develop/#available-apis-and-recipes","text":"cmempy - Python API \u2014 cmempy is a Python API wrapper around the eccenca Corporate Memory HTTP APIs which can be used to rapidly script processes which interact with Corporate Memory. DataIntegration APIs \u2014 eccenca DataIntegration APIs can be used to control, initiate and setup all task and activities related to the \u2605 Build step (such as datasets, transformations, linking tasks etc.). ( DataIntegration OpenAPI Reference ) DataPlatform APIs \u2014 eccenca DataPlatform APIs can be used to import, export, query and extract information from graphs as well as to check access conditions. DataPlatform OpenAPI Reference )","title":"Available APIs and Recipes"},{"location":"develop/accessing-graphs-with-java-applications/","text":"Accessing Graphs with Java Applications \u00a4 Introduction \u00a4 This short recipe covers how to connect to Corporate Memory using a Java program.Such program can connect to Corporate Memory at any time autonomously, independently of whether a user is logged in or not. Java example \u00a4 This example assumes that there is a Corporate Memory instance runnning at http://docker.localhost , and the programmer has access to its files. The process is very simple: Obtain a Bearer token. Go to the file cmem-orchestration/environments/config.env , and get the client secret from variable CMEM_SERVICE_ACCOUNT_CLIENT_SECRET . With the client secret, connect to to the OpenID endpoint to obtain the Bearer token. Use the Bearer token to connect to Corporate Memory, and, for example, execute a query. The following code provides a simple implementation of the process: JavaCMEMHTTPClient.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package com.eccenca.cmem.client ; import java.io.IOException ; import java.util.ArrayList ; import java.util.List ; import org.apache.http.HttpEntity ; import org.apache.http.HttpResponse ; import org.apache.http.NameValuePair ; import org.apache.http.client.ClientProtocolException ; import org.apache.http.client.entity.UrlEncodedFormEntity ; import org.apache.http.client.methods.HttpPost ; import org.apache.http.impl.client.CloseableHttpClient ; import org.apache.http.impl.client.HttpClientBuilder ; import org.apache.http.message.BasicNameValuePair ; import org.apache.http.util.EntityUtils ; import org.json.JSONObject ; public class HTTPClient { public static void main ( String [] args ) throws ClientProtocolException , IOException { // We assume that the Corporate Memory instance is running at docker.localhost String openidConnectEndpoint = \"http://docker.localhost/auth/realms/cmem/protocol/openid-connect/token\" ; // Get the client secret to obtain the bearer token from file cmem-orchestration/environments/config.env, // variable CMEM_SERVICE_ACCOUNT_CLIENT_SECRET String clientSecret = \"...\" ; // Create an HTTP Client CloseableHttpClient client = HttpClientBuilder . create (). build (); // POST request to obtain the bearer token for later authorization HttpPost httpPostToken = new HttpPost ( openidConnectEndpoint ); httpPostToken . setHeader ( \"Content-type\" , \"application/x-www-form-urlencoded\" ); List < NameValuePair > params = new ArrayList < NameValuePair > (); params . add ( new BasicNameValuePair ( \"grant_type\" , \"client_credentials\" )); params . add ( new BasicNameValuePair ( \"client_id\" , \"cmem-service-account\" )); params . add ( new BasicNameValuePair ( \"client_secret\" , clientSecret )); httpPostToken . setEntity ( new UrlEncodedFormEntity ( params )); // Parse the JSON response to obtain the bearer token HttpResponse httpResponseToken = client . execute ( httpPostToken ); HttpEntity httpEntity = httpResponseToken . getEntity (); String responseBody = EntityUtils . toString ( httpEntity ); JSONObject obj = new JSONObject ( responseBody ); String bearerToken = \"Bearer \" + obj . getString ( \"access_token\" ); // POST request to query the default SPARQL endpoint with the bearer token obtained above HttpPost httpPostQuery = new HttpPost ( \"http://docker.localhost/dataplatform/proxy/default/sparql\" ); httpPostQuery . setHeader ( \"Accept\" , \"application/sparql-results+json\" ); httpPostQuery . setHeader ( \"Content-type\" , \"application/x-www-form-urlencoded\" ); httpPostQuery . setHeader ( \"Authorization\" , bearerToken ); final ArrayList < NameValuePair > postParameters = new ArrayList < NameValuePair > (); postParameters . add ( new BasicNameValuePair ( \"query\" , \"SELECT * WHERE {?s ?p ?o} LIMIT 10\" )); httpPostQuery . setEntity ( new UrlEncodedFormEntity ( postParameters )); // The response (variable responseBodyQuery bellow) should have some bindings: // { // \"head\": { // \"vars\": [ \"s\" , \"p\" , \"o\" ] // } , // \"results\": { // \"bindings\": [ HttpResponse httpResponseQuery = client . execute ( httpPostQuery ); HttpEntity httpEntityQuery = httpResponseQuery . getEntity (); String responseBodyQuery = EntityUtils . toString ( httpEntityQuery ); } }","title":"Accessing Graphs with Java Applications"},{"location":"develop/accessing-graphs-with-java-applications/#accessing-graphs-with-java-applications","text":"","title":"Accessing Graphs with Java Applications"},{"location":"develop/accessing-graphs-with-java-applications/#introduction","text":"This short recipe covers how to connect to Corporate Memory using a Java program.Such program can connect to Corporate Memory at any time autonomously, independently of whether a user is logged in or not.","title":"Introduction"},{"location":"develop/accessing-graphs-with-java-applications/#java-example","text":"This example assumes that there is a Corporate Memory instance runnning at http://docker.localhost , and the programmer has access to its files. The process is very simple: Obtain a Bearer token. Go to the file cmem-orchestration/environments/config.env , and get the client secret from variable CMEM_SERVICE_ACCOUNT_CLIENT_SECRET . With the client secret, connect to to the OpenID endpoint to obtain the Bearer token. Use the Bearer token to connect to Corporate Memory, and, for example, execute a query. The following code provides a simple implementation of the process: JavaCMEMHTTPClient.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package com.eccenca.cmem.client ; import java.io.IOException ; import java.util.ArrayList ; import java.util.List ; import org.apache.http.HttpEntity ; import org.apache.http.HttpResponse ; import org.apache.http.NameValuePair ; import org.apache.http.client.ClientProtocolException ; import org.apache.http.client.entity.UrlEncodedFormEntity ; import org.apache.http.client.methods.HttpPost ; import org.apache.http.impl.client.CloseableHttpClient ; import org.apache.http.impl.client.HttpClientBuilder ; import org.apache.http.message.BasicNameValuePair ; import org.apache.http.util.EntityUtils ; import org.json.JSONObject ; public class HTTPClient { public static void main ( String [] args ) throws ClientProtocolException , IOException { // We assume that the Corporate Memory instance is running at docker.localhost String openidConnectEndpoint = \"http://docker.localhost/auth/realms/cmem/protocol/openid-connect/token\" ; // Get the client secret to obtain the bearer token from file cmem-orchestration/environments/config.env, // variable CMEM_SERVICE_ACCOUNT_CLIENT_SECRET String clientSecret = \"...\" ; // Create an HTTP Client CloseableHttpClient client = HttpClientBuilder . create (). build (); // POST request to obtain the bearer token for later authorization HttpPost httpPostToken = new HttpPost ( openidConnectEndpoint ); httpPostToken . setHeader ( \"Content-type\" , \"application/x-www-form-urlencoded\" ); List < NameValuePair > params = new ArrayList < NameValuePair > (); params . add ( new BasicNameValuePair ( \"grant_type\" , \"client_credentials\" )); params . add ( new BasicNameValuePair ( \"client_id\" , \"cmem-service-account\" )); params . add ( new BasicNameValuePair ( \"client_secret\" , clientSecret )); httpPostToken . setEntity ( new UrlEncodedFormEntity ( params )); // Parse the JSON response to obtain the bearer token HttpResponse httpResponseToken = client . execute ( httpPostToken ); HttpEntity httpEntity = httpResponseToken . getEntity (); String responseBody = EntityUtils . toString ( httpEntity ); JSONObject obj = new JSONObject ( responseBody ); String bearerToken = \"Bearer \" + obj . getString ( \"access_token\" ); // POST request to query the default SPARQL endpoint with the bearer token obtained above HttpPost httpPostQuery = new HttpPost ( \"http://docker.localhost/dataplatform/proxy/default/sparql\" ); httpPostQuery . setHeader ( \"Accept\" , \"application/sparql-results+json\" ); httpPostQuery . setHeader ( \"Content-type\" , \"application/x-www-form-urlencoded\" ); httpPostQuery . setHeader ( \"Authorization\" , bearerToken ); final ArrayList < NameValuePair > postParameters = new ArrayList < NameValuePair > (); postParameters . add ( new BasicNameValuePair ( \"query\" , \"SELECT * WHERE {?s ?p ?o} LIMIT 10\" )); httpPostQuery . setEntity ( new UrlEncodedFormEntity ( postParameters )); // The response (variable responseBodyQuery bellow) should have some bindings: // { // \"head\": { // \"vars\": [ \"s\" , \"p\" , \"o\" ] // } , // \"results\": { // \"bindings\": [ HttpResponse httpResponseQuery = client . execute ( httpPostQuery ); HttpEntity httpEntityQuery = httpResponseQuery . getEntity (); String responseBodyQuery = EntityUtils . toString ( httpEntityQuery ); } }","title":"Java example"},{"location":"develop/cmempy-python-api/","tags":["API"],"text":"cmempy - Python API \u00a4 Introduction \u00a4 cmempy is a Python API wrapper around the eccenca Corporate Memory HTTP APIs which can be used to rapidly script processes which interact with Corporate Memory. cmempy is also the underlying Python module which powers the cmemc - Command Line Interface . Installation \u00a4 cmempy is published as an Apache 2 licensed open source python package at pypi.org , hence you are able to install it with a simple pip command: pip install cmem-cmempy Configure a Connection \u00a4 The used Corporate Memory connection is configured by providing environment variables similar to cmemc ( Environment based Configuration ). These environment variables can be created and changed in your code or used from the process which executes your python code (e.g. your shell). If you have a working cmemc file based configuration setup already you can export the environment to your shell using cmemc config eval . The following table lists all processed environment variables: Variable Description Default Value CMEM_BASE_URI Base URL of your Corporate Memory http://docker.localhost DI_API_ENDPOINT Data Integration API endpoint CMEM_BASE_URI/dataintegration DP_API_ENDPOINT Data Platform API endpoint CMEM_BASE_URI/dataplatform OAUTH_TOKEN_URI OAuth 2.0 Token endpoint CMEM_BASE_URI/auth/realms/cmem/protocol/openid-connect/token OAUTH_GRANT_TYPE OAuth 2.0 grant type (password or client_credentials) client_credentials OAUTH_USER Username to retrieve the token admin OAUTH_PASSWORD Password to retrieve the token secret OAUTH_CLIENT_ID OAuth 2.0 client id cmem-service-account OAUTH_CLIENT_SECRET OAuth 2.0 client secret secret SSL_VERIFY Verify SSL certs for API requestsv True REQUESTS_CA_BUNDLE Path to the CA Bundle file (.pem) Internal path to included CA bundle Commented Example \u00a4 Here is a commented code example how to configure and use cmempy. The example demonstrates, how to execute SPARQL queries on DataPlatform, as well as how to work with the DataIntegration workspace and retrieve workflow status information: example_usage.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \"\"\"Basic example, how to use cmempy\"\"\" from os import environ from cmem.cmempy.workspace.projects.project import get_projects from cmem.cmempy.workflow import get_workflows from cmem.cmempy.workspace.activities.taskactivity import get_activity_status from cmem.cmempy.queries import SparqlQuery # setup the environment for the connection to Corporate Memory environ [ \"CMEM_BASE_URI\" ] = \"http://docker.local\" environ [ \"OAUTH_GRANT_TYPE\" ] = \"client_credentials\" environ [ \"OAUTH_CLIENT_ID\" ] = \"cmem-service-account\" environ [ \"OAUTH_CLIENT_SECRET\" ] = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" # this query simply lists 5 resource subjects from the triple store QUERY_TEXT = \"SELECT DISTINCT ?s WHERE {?s ?p ?o} LIMIT 5\" # the default result is a JSON structure according to the W3C standard # seeAlso: https://www.w3.org/TR/sparql11-results-json/ results = SparqlQuery ( QUERY_TEXT ) . get_results () print ( results ) # loop over project descriptions for project in get_projects (): project_id = project [ \"name\" ] print ( \"Project: {} :\" . format ( project_id )) # loop over workflow ids for a project for workflow_id in get_workflows ( project_id ): # get the status object of a specific workflow status = get_activity_status ( project_id , workflow_id ) message = status [ \"message\" ] print ( \"- Workflow: {} ( {} ):\" . format ( workflow_id , message )) Starting this script should result in an output similar to this: $ python example_usage.py { \"head\" : { \"vars\" : [ \"s\" ] } , \"results\" : { \"bindings\" : [ { \"s\" : { \"type\" : \"uri\" , \"value\" : \"https://vocab.eccenca.com/dsm/\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"https://vocab.eccenca.com/dsm/ThesaurusProject\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://www.w3.org/2002/07/owl#\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://www.w3.org/2000/01/rdf-schema#\" } } ] } } Project: cmem: - Workflow: my-workflow ( Idle ) :","title":"cmempy - Python API"},{"location":"develop/cmempy-python-api/#cmempy-python-api","text":"","title":"cmempy - Python API"},{"location":"develop/cmempy-python-api/#introduction","text":"cmempy is a Python API wrapper around the eccenca Corporate Memory HTTP APIs which can be used to rapidly script processes which interact with Corporate Memory. cmempy is also the underlying Python module which powers the cmemc - Command Line Interface .","title":"Introduction"},{"location":"develop/cmempy-python-api/#installation","text":"cmempy is published as an Apache 2 licensed open source python package at pypi.org , hence you are able to install it with a simple pip command: pip install cmem-cmempy","title":"Installation"},{"location":"develop/cmempy-python-api/#configure-a-connection","text":"The used Corporate Memory connection is configured by providing environment variables similar to cmemc ( Environment based Configuration ). These environment variables can be created and changed in your code or used from the process which executes your python code (e.g. your shell). If you have a working cmemc file based configuration setup already you can export the environment to your shell using cmemc config eval . The following table lists all processed environment variables: Variable Description Default Value CMEM_BASE_URI Base URL of your Corporate Memory http://docker.localhost DI_API_ENDPOINT Data Integration API endpoint CMEM_BASE_URI/dataintegration DP_API_ENDPOINT Data Platform API endpoint CMEM_BASE_URI/dataplatform OAUTH_TOKEN_URI OAuth 2.0 Token endpoint CMEM_BASE_URI/auth/realms/cmem/protocol/openid-connect/token OAUTH_GRANT_TYPE OAuth 2.0 grant type (password or client_credentials) client_credentials OAUTH_USER Username to retrieve the token admin OAUTH_PASSWORD Password to retrieve the token secret OAUTH_CLIENT_ID OAuth 2.0 client id cmem-service-account OAUTH_CLIENT_SECRET OAuth 2.0 client secret secret SSL_VERIFY Verify SSL certs for API requestsv True REQUESTS_CA_BUNDLE Path to the CA Bundle file (.pem) Internal path to included CA bundle","title":"Configure a Connection"},{"location":"develop/cmempy-python-api/#commented-example","text":"Here is a commented code example how to configure and use cmempy. The example demonstrates, how to execute SPARQL queries on DataPlatform, as well as how to work with the DataIntegration workspace and retrieve workflow status information: example_usage.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \"\"\"Basic example, how to use cmempy\"\"\" from os import environ from cmem.cmempy.workspace.projects.project import get_projects from cmem.cmempy.workflow import get_workflows from cmem.cmempy.workspace.activities.taskactivity import get_activity_status from cmem.cmempy.queries import SparqlQuery # setup the environment for the connection to Corporate Memory environ [ \"CMEM_BASE_URI\" ] = \"http://docker.local\" environ [ \"OAUTH_GRANT_TYPE\" ] = \"client_credentials\" environ [ \"OAUTH_CLIENT_ID\" ] = \"cmem-service-account\" environ [ \"OAUTH_CLIENT_SECRET\" ] = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" # this query simply lists 5 resource subjects from the triple store QUERY_TEXT = \"SELECT DISTINCT ?s WHERE {?s ?p ?o} LIMIT 5\" # the default result is a JSON structure according to the W3C standard # seeAlso: https://www.w3.org/TR/sparql11-results-json/ results = SparqlQuery ( QUERY_TEXT ) . get_results () print ( results ) # loop over project descriptions for project in get_projects (): project_id = project [ \"name\" ] print ( \"Project: {} :\" . format ( project_id )) # loop over workflow ids for a project for workflow_id in get_workflows ( project_id ): # get the status object of a specific workflow status = get_activity_status ( project_id , workflow_id ) message = status [ \"message\" ] print ( \"- Workflow: {} ( {} ):\" . format ( workflow_id , message )) Starting this script should result in an output similar to this: $ python example_usage.py { \"head\" : { \"vars\" : [ \"s\" ] } , \"results\" : { \"bindings\" : [ { \"s\" : { \"type\" : \"uri\" , \"value\" : \"https://vocab.eccenca.com/dsm/\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"https://vocab.eccenca.com/dsm/ThesaurusProject\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://www.w3.org/2002/07/owl#\" } } , { \"s\" : { \"type\" : \"uri\" , \"value\" : \"http://www.w3.org/2000/01/rdf-schema#\" } } ] } } Project: cmem: - Workflow: my-workflow ( Idle ) :","title":"Commented Example"},{"location":"develop/create-custom-transformations-with-python/","text":"Create Custom Transformations with Python \u00a4 Caution This section describes the obsolete Python 2 plugin system. We recommend to migrate to the new Python 3 plugin system: Python Plugins Introduction \u00a4 Beside the fact that there are over 180 built-in operators available for your data transformations, there will be the moment where you need a new special operator to solve a specific problem which can\u2019t be solved with the built-ins or which is just easier to solve when you simply program it.This page gives an overview on the Script Transform Operator and how to use it to create Python based custom transformations. General Working Model \u00a4 The python script operator has two parameters: A multi-text field for the script and a function field for the name of the function to be executed. The operator performs the following two steps: first, it loads the script then, it executes the function for each \u201crow\u201d of the transformation. Each input value is always given as an array of strings (such as [\u201cEve\u201d, \u201cAlice\u201d, \u201cBob\u201d], more specifically - as an instance of org.python.core.PyArray ). If there are no values for the current iteration, an empty array is given. The number of input arrays (of strings) depends on the number of incoming connections in the transformation operator. These connections are ordered, means the first connected building block delivers parameter one (as an array of strings), the second building block delivers parameter two (as an array of strings), etc. In the same way as the input parameters, the result value should be a list of strings. The operator will try its best to map whatever is returned to a proper list of strings but this could fail, so don\u2019t try it too hard \u2026 Preliminaries \u00a4 Enabling the script operators \u00a4 Because the script operators allow potentially unsafe operations (such as writing to the file system), they are disabled by default. In order to use those plugins, they need to be enabled explicitly in the config: pluginRegistry.plugins.python2Script.enabled = true The following script operators are available: python2Script : Python 2 transform operator. scalaScript : Scala transform operator. script : Scala script operator to be used in workflows. Using Python libraries \u00a4 External python libraries can be configured and will be loaded from the the following folder by default: com.eccenca.di.scripting.transformer.Python2ScriptTransformer = { modulePath = ${elds.home}\"/etc/dataintegration/pythonModules/\" } The configured modulePath will be added to the Python sys.path . Parameter Validation \u00a4 Some parameter value validation should be done inside the defined function. This includes test how many strings are in the list test if these strings have a specific format Error Handling and Logging \u00a4 Syntax errors are instantly shown in the transformation editor while execution errors or exceptions are shown in the evaluation and execution report. Both type of errors are also logged. In addition to error logging, the function can create print-output which is added to the logging as well. Example: Days between Two Dates \u00a4 The following well commented and very verbose code example calculates the difference between two dates and returns the number of dates as a result: dates_difference.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 \"\"\"Example transform operators for data integration.\"\"\" # imports can be done normally in the script, but only from the standard lib import array from datetime import datetime def days_between_dates ( first_input , second_input ): \"\"\"Compare two dates and return number of days between them. The date values need to be in the form YYYY-MM-DD \"\"\" # check for arrays (actually not needed since DI will always give arrays) # more specifically: the type of inputs is org.python.core.PyArray if not isinstance ( first_input , array . array ): raise ValueError ( \"First input should be a list of strings, but was {} .\" . format ( type ( first_input )) ) if not isinstance ( second_input , array . array ): raise ValueError ( \"First input should be a list of strings, but was {} .\" . format ( type ( second_input )) ) # check list element count if len ( first_input ) != 1 : raise ValueError ( \"First input should be exactly one value (but had {} values)\" . format ( len ( first_input )) ) if len ( second_input ) != 1 : raise ValueError ( \"Second input should be exactly one value (but had {} values)\" . format ( len ( second_input )) ) first_input = str ( first_input [ 0 ]) second_input = str ( second_input [ 0 ]) date_format = \"%Y-%m- %d \" # create dates from strings try : first_date = datetime . strptime ( first_input , date_format ) except ValueError : raise ValueError ( \"Error: First input date has wrong format. \" \"Must be 'yyyy-mm-dd' (ex: '2016-10-01').\" ) try : second_date = datetime . strptime ( second_input , date_format ) except ValueError : raise ValueError ( \"Error: First input date has wrong format. \" \"Must be 'yyyy-mm-dd' (ex: '2016-10-01').\" ) # calculate and return delta - casted to a list of one string return [ str (( second_date - first_date ) . days )] Based on this example the following pytest test suite is \u201cgreen\u201d (given here for clarification of the operator behaviour): test_dates_difference.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import pytest from transform_operators import days_between_dates def test_valid_inputs (): \"\"\"test valid inputs\"\"\" assert days_between_dates ([ \"2022-05-02\" ], [ \"2022-05-01\" ]) == [ \"-1\" ] assert days_between_dates ([ \"2022-05-01\" ], [ \"2022-05-22\" ]) == [ \"21\" ] assert days_between_dates ([ \"2022-01-01\" ], [ \"2021-12-31\" ]) == [ \"-1\" ] def test_missing_inputs (): \"\"\"test missing inputs\"\"\" with pytest . raises ( TypeError ): days_between_dates ([ \"2022-05-02\" ]) with pytest . raises ( ValueError ): days_between_dates ([], [ \"2022-05-02\" ]) with pytest . raises ( ValueError ): days_between_dates ([ \"2022-05-02\" ], []) def test_invalid_inputs (): \"\"\"invalid test inputs\"\"\" with pytest . raises ( ValueError ): days_between_dates ([ \"2022-05-02\" ], [ \"...\" ]) with pytest . raises ( ValueError ): days_between_dates ([ \"2022-05-02\" ], \"...\" ) with pytest . raises ( ValueError ): days_between_dates ([ \"...\" ], [ \"2022-05-02\" ]) with pytest . raises ( ValueError ): days_between_dates ( \"2022-05-02\" , \"...\" ) with pytest . raises ( TypeError ): days_between_dates ( 10 , 20 ) This well tested operator can now be used in your transformation (left: the transformation flow, right: the evaluation report) Special environment variables \u00a4 A number of useful variables are injected and can be accessed from the Python script as follows: example_inject_env.py 1 2 from os import environ as env env [ 'VARIABLE_NAME' ] The following variables are available: CMEM_BASE_URI : The base URI of the current CorporateMemory deployment. OAUTH_ACCESS_TOKEN : The current super user token. Note that this is only available, if a super user is configured. OAUTH_GRANT_TYPE : The corresponding OAuth grant type. Set to: prefetched_token","title":"Create Custom Transformations with Python"},{"location":"develop/create-custom-transformations-with-python/#create-custom-transformations-with-python","text":"Caution This section describes the obsolete Python 2 plugin system. We recommend to migrate to the new Python 3 plugin system: Python Plugins","title":"Create Custom Transformations with Python"},{"location":"develop/create-custom-transformations-with-python/#introduction","text":"Beside the fact that there are over 180 built-in operators available for your data transformations, there will be the moment where you need a new special operator to solve a specific problem which can\u2019t be solved with the built-ins or which is just easier to solve when you simply program it.This page gives an overview on the Script Transform Operator and how to use it to create Python based custom transformations.","title":"Introduction"},{"location":"develop/create-custom-transformations-with-python/#general-working-model","text":"The python script operator has two parameters: A multi-text field for the script and a function field for the name of the function to be executed. The operator performs the following two steps: first, it loads the script then, it executes the function for each \u201crow\u201d of the transformation. Each input value is always given as an array of strings (such as [\u201cEve\u201d, \u201cAlice\u201d, \u201cBob\u201d], more specifically - as an instance of org.python.core.PyArray ). If there are no values for the current iteration, an empty array is given. The number of input arrays (of strings) depends on the number of incoming connections in the transformation operator. These connections are ordered, means the first connected building block delivers parameter one (as an array of strings), the second building block delivers parameter two (as an array of strings), etc. In the same way as the input parameters, the result value should be a list of strings. The operator will try its best to map whatever is returned to a proper list of strings but this could fail, so don\u2019t try it too hard \u2026","title":"General Working Model"},{"location":"develop/create-custom-transformations-with-python/#preliminaries","text":"","title":"Preliminaries"},{"location":"develop/create-custom-transformations-with-python/#enabling-the-script-operators","text":"Because the script operators allow potentially unsafe operations (such as writing to the file system), they are disabled by default. In order to use those plugins, they need to be enabled explicitly in the config: pluginRegistry.plugins.python2Script.enabled = true The following script operators are available: python2Script : Python 2 transform operator. scalaScript : Scala transform operator. script : Scala script operator to be used in workflows.","title":"Enabling the script operators"},{"location":"develop/create-custom-transformations-with-python/#using-python-libraries","text":"External python libraries can be configured and will be loaded from the the following folder by default: com.eccenca.di.scripting.transformer.Python2ScriptTransformer = { modulePath = ${elds.home}\"/etc/dataintegration/pythonModules/\" } The configured modulePath will be added to the Python sys.path .","title":"Using Python libraries"},{"location":"develop/create-custom-transformations-with-python/#parameter-validation","text":"Some parameter value validation should be done inside the defined function. This includes test how many strings are in the list test if these strings have a specific format","title":"Parameter Validation"},{"location":"develop/create-custom-transformations-with-python/#error-handling-and-logging","text":"Syntax errors are instantly shown in the transformation editor while execution errors or exceptions are shown in the evaluation and execution report. Both type of errors are also logged. In addition to error logging, the function can create print-output which is added to the logging as well.","title":"Error Handling and Logging"},{"location":"develop/create-custom-transformations-with-python/#example-days-between-two-dates","text":"The following well commented and very verbose code example calculates the difference between two dates and returns the number of dates as a result: dates_difference.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 \"\"\"Example transform operators for data integration.\"\"\" # imports can be done normally in the script, but only from the standard lib import array from datetime import datetime def days_between_dates ( first_input , second_input ): \"\"\"Compare two dates and return number of days between them. The date values need to be in the form YYYY-MM-DD \"\"\" # check for arrays (actually not needed since DI will always give arrays) # more specifically: the type of inputs is org.python.core.PyArray if not isinstance ( first_input , array . array ): raise ValueError ( \"First input should be a list of strings, but was {} .\" . format ( type ( first_input )) ) if not isinstance ( second_input , array . array ): raise ValueError ( \"First input should be a list of strings, but was {} .\" . format ( type ( second_input )) ) # check list element count if len ( first_input ) != 1 : raise ValueError ( \"First input should be exactly one value (but had {} values)\" . format ( len ( first_input )) ) if len ( second_input ) != 1 : raise ValueError ( \"Second input should be exactly one value (but had {} values)\" . format ( len ( second_input )) ) first_input = str ( first_input [ 0 ]) second_input = str ( second_input [ 0 ]) date_format = \"%Y-%m- %d \" # create dates from strings try : first_date = datetime . strptime ( first_input , date_format ) except ValueError : raise ValueError ( \"Error: First input date has wrong format. \" \"Must be 'yyyy-mm-dd' (ex: '2016-10-01').\" ) try : second_date = datetime . strptime ( second_input , date_format ) except ValueError : raise ValueError ( \"Error: First input date has wrong format. \" \"Must be 'yyyy-mm-dd' (ex: '2016-10-01').\" ) # calculate and return delta - casted to a list of one string return [ str (( second_date - first_date ) . days )] Based on this example the following pytest test suite is \u201cgreen\u201d (given here for clarification of the operator behaviour): test_dates_difference.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import pytest from transform_operators import days_between_dates def test_valid_inputs (): \"\"\"test valid inputs\"\"\" assert days_between_dates ([ \"2022-05-02\" ], [ \"2022-05-01\" ]) == [ \"-1\" ] assert days_between_dates ([ \"2022-05-01\" ], [ \"2022-05-22\" ]) == [ \"21\" ] assert days_between_dates ([ \"2022-01-01\" ], [ \"2021-12-31\" ]) == [ \"-1\" ] def test_missing_inputs (): \"\"\"test missing inputs\"\"\" with pytest . raises ( TypeError ): days_between_dates ([ \"2022-05-02\" ]) with pytest . raises ( ValueError ): days_between_dates ([], [ \"2022-05-02\" ]) with pytest . raises ( ValueError ): days_between_dates ([ \"2022-05-02\" ], []) def test_invalid_inputs (): \"\"\"invalid test inputs\"\"\" with pytest . raises ( ValueError ): days_between_dates ([ \"2022-05-02\" ], [ \"...\" ]) with pytest . raises ( ValueError ): days_between_dates ([ \"2022-05-02\" ], \"...\" ) with pytest . raises ( ValueError ): days_between_dates ([ \"...\" ], [ \"2022-05-02\" ]) with pytest . raises ( ValueError ): days_between_dates ( \"2022-05-02\" , \"...\" ) with pytest . raises ( TypeError ): days_between_dates ( 10 , 20 ) This well tested operator can now be used in your transformation (left: the transformation flow, right: the evaluation report)","title":"Example: Days between Two Dates"},{"location":"develop/create-custom-transformations-with-python/#special-environment-variables","text":"A number of useful variables are injected and can be accessed from the Python script as follows: example_inject_env.py 1 2 from os import environ as env env [ 'VARIABLE_NAME' ] The following variables are available: CMEM_BASE_URI : The base URI of the current CorporateMemory deployment. OAUTH_ACCESS_TOKEN : The current super user token. Note that this is only available, if a super user is configured. OAUTH_GRANT_TYPE : The corresponding OAuth grant type. Set to: prefetched_token","title":"Special environment variables"},{"location":"develop/dataintegration-apis/","tags":["API"],"text":"DataIntegration APIs \u00a4 Introduction \u00a4 eccenca DataIntegration APIs can be used to control, initiate and setup all task and activities related to the \u2605 Build step (such as datasets, transformations, linking tasks etc.). Media Types \u00a4 The default media type of most responses is application/json. Other possible response media types can be reached by changing the Accept header of the request. Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method. Dependent on the specific API, eccenca DataIntegration works with the following application media types which correspond to the following specification documents: Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/xml XML Media Types application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/problem+json Problem Details for HTTP APIs (Open) API Reference \u00a4 The latest Open API specification: Alternatively, you can (re)view it redoc web UI .","title":"DataIntegration APIs"},{"location":"develop/dataintegration-apis/#dataintegration-apis","text":"","title":"DataIntegration APIs"},{"location":"develop/dataintegration-apis/#introduction","text":"eccenca DataIntegration APIs can be used to control, initiate and setup all task and activities related to the \u2605 Build step (such as datasets, transformations, linking tasks etc.).","title":"Introduction"},{"location":"develop/dataintegration-apis/#media-types","text":"The default media type of most responses is application/json. Other possible response media types can be reached by changing the Accept header of the request. Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method. Dependent on the specific API, eccenca DataIntegration works with the following application media types which correspond to the following specification documents: Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/xml XML Media Types application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/problem+json Problem Details for HTTP APIs","title":"Media Types"},{"location":"develop/dataintegration-apis/#open-api-reference","text":"The latest Open API specification: Alternatively, you can (re)view it redoc web UI .","title":"(Open) API Reference"},{"location":"develop/dataplatform-apis/","tags":["API"],"text":"DataPlatform APIs \u00a4 Introduction \u00a4 eccenca DataPlatform APIs can be used to import, export, query and extract information from graphs as well as to check access conditions. This section describes common characteristics and features of all provided APIs. Media Types \u00a4 The default media type of most responses is application/json . Other possible response media types can be reached by changing the Accept header of the request. Alternatively, the desired response media type can be expressed in the request URI. Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method. Dependent on the specific API, eccenca DataPlatform works with the following application media types which correspond to the following specification documents: Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/ld+json JSON-LD 1.0 text/turtle RDF 1.1 Turtle - Terse RDF Triple Language application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/rdf+xml RDF 1.1 XML Syntax application/n-quads RDF 1.1 N-Quads application/trig RDF 1.1 TriG application/sparql-query SPARQL 1.1 Query Language application/sparql-update SPARQL 1.1 Update application/sparql-results+json SPARQL 1.1 Query Results JSON Format application/sparql-results+xml SPARQL Query Results XML Format (Second Edition) text/csv SPARQL 1.1 Query Results CSV and TSV Formats text/tab-separated-values SPARQL 1.1 Query Results CSV and TSV Formats application/vnd.openxmlformats-officedocument.spreadsheetml.sheet Microsoft Office Excel (.xlsx) format application/problem+json Problem Details for HTTP APIs Media type request by URI \u00a4 The desired response media type can be requested by adding a format query parameter. The parameter value is interpreted as a media type abbreviation that is expanded to a proper media type string. eccenca DataPlatform maps the following abbreviations to media types it supports: Abbreviation Media Type rdf application/rdf+xml ttl text/turtle jsonld application/ld+json nt application/n-triples trig application/trig nq application/n-quads srj application/sparql-results+json srx application/sparql-results+xml csv text/csv tsv text/tab-separated-values Thus, for example, a request to /proxy/default/graph with the Accept header value text/turtle and a request to /proxy/default/graph?format=ttl express the same intent for the media type of the response. If both this format query parameter and an Accept header is present in a request, the parameter value takes precedence. Usage of media type request by URI can be useful to create browser links that will express an intent for the media type of the response. Security Schemes \u00a4 The default security scheme is OAuth 2.0. However, this can be changed in the configuration. SPARQL result set streaming \u00a4 The SPARQL proxy pipes the results of SPARQL queries directly from the underlying data endpoint to the request client. This however does not always apply for CONSTRUCT queries. The result of a CONSTRUCT query is a set of statements. RDF graph serialization formats tend to group the information for compactness - e.g. in Turtle , all statements for a subject are written together - avoiding subject and subject-predicate repetition, for which it is necessary to have the complete result set at disposal. Therefore, before sending the result to the request client, the complete result is loaded and then serialized. This creates a potential danger whenever a large result set is build and could lead to overload of the server. There is however one serialization format (the N-Triples format ) which is streaming friendly and that should always be used whenever large result sets are expected. SPARQL default graph & RDF dataset \u00a4 Default graph \u00a4 The definition of the RDF dataset of a query in the SPARQL 1.1 specification leads to problems regarding the default graph of a SPARQL service. On one hand it is defined that: A SPARQL query is executed against an RDF dataset which represents a collection of graphs. An RDF dataset comprises one graph, the default graph, which does not have a name, and zero or more named graphs, where each named graph is identified by an IRI. Furthermore, it says: A SPARQL query may specify the dataset to be used for matching by using the FROM clause and the FROM NAMED clause to describe the RDF dataset. If a query provides such a dataset description, then it is used in place of any dataset that the query service would use if no dataset description is provided in a query. The RDF dataset may also be specified in a SPARQL protocol request, in which case the protocol description overrides any description in the query itself. A query service may refuse a query request if the dataset description is not acceptable to the service. The FROM and FROM NAMED keywords allow a query to specify an RDF dataset by reference; they indicate that the dataset should include graphs that are obtained from representations of the resources identified by the given IRIs (i.e. the absolute form of the given IRI references). The dataset resulting from a number of FROM and FROM NAMED clauses is: a default graph consisting of the RDF merge of the graphs referred to in the FROM clauses, and a set of (IRI, graph) pairs, one from each FROM NAMED clause. If there is no FROM clause, but there is one or more FROM NAMED clauses, then the dataset includes an empty graph for the default graph. That means the default graph of a SPARQL service cannot be explicitly referenced in the RDF dataset of a SPARQL query using FROM / FROM NAMED. For this reason, DataPlatform does not allow the manipulation of the service\u2019s default graph. To enforce this policy, the following restriction applies to incoming SPARQL 1.1 Update queries: Update queries (INSERT DATA, DELETE DATA and DELETE/INSERT) targeted against the service\u2019s default graph will not be accepted by returning an HTTP 400 Bad Request status code. Default RDF dataset \u00a4 The interpretation of the RDF dataset of a query differs between various SPARQL service implementations (as shown here ). In the case a query declares no RDF dataset, DataPlatform uses the following default RDF dataset declaration to provide a uniform behavior for all supported SPARQL services: The default graph is the union ( RDF Merge graph ) of all named graphs the user is allowed to access. The set of named graphs contains all named graphs the user is allowed to access. (Open) API Reference \u00a4 The latest Open API specification: Alternatively, you can (re)view it redoc web UI","title":"DataPlatform APIs"},{"location":"develop/dataplatform-apis/#dataplatform-apis","text":"","title":"DataPlatform APIs"},{"location":"develop/dataplatform-apis/#introduction","text":"eccenca DataPlatform APIs can be used to import, export, query and extract information from graphs as well as to check access conditions. This section describes common characteristics and features of all provided APIs.","title":"Introduction"},{"location":"develop/dataplatform-apis/#media-types","text":"The default media type of most responses is application/json . Other possible response media types can be reached by changing the Accept header of the request. Alternatively, the desired response media type can be expressed in the request URI. Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method. Dependent on the specific API, eccenca DataPlatform works with the following application media types which correspond to the following specification documents: Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/ld+json JSON-LD 1.0 text/turtle RDF 1.1 Turtle - Terse RDF Triple Language application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/rdf+xml RDF 1.1 XML Syntax application/n-quads RDF 1.1 N-Quads application/trig RDF 1.1 TriG application/sparql-query SPARQL 1.1 Query Language application/sparql-update SPARQL 1.1 Update application/sparql-results+json SPARQL 1.1 Query Results JSON Format application/sparql-results+xml SPARQL Query Results XML Format (Second Edition) text/csv SPARQL 1.1 Query Results CSV and TSV Formats text/tab-separated-values SPARQL 1.1 Query Results CSV and TSV Formats application/vnd.openxmlformats-officedocument.spreadsheetml.sheet Microsoft Office Excel (.xlsx) format application/problem+json Problem Details for HTTP APIs","title":"Media Types"},{"location":"develop/dataplatform-apis/#media-type-request-by-uri","text":"The desired response media type can be requested by adding a format query parameter. The parameter value is interpreted as a media type abbreviation that is expanded to a proper media type string. eccenca DataPlatform maps the following abbreviations to media types it supports: Abbreviation Media Type rdf application/rdf+xml ttl text/turtle jsonld application/ld+json nt application/n-triples trig application/trig nq application/n-quads srj application/sparql-results+json srx application/sparql-results+xml csv text/csv tsv text/tab-separated-values Thus, for example, a request to /proxy/default/graph with the Accept header value text/turtle and a request to /proxy/default/graph?format=ttl express the same intent for the media type of the response. If both this format query parameter and an Accept header is present in a request, the parameter value takes precedence. Usage of media type request by URI can be useful to create browser links that will express an intent for the media type of the response.","title":"Media type request by URI"},{"location":"develop/dataplatform-apis/#security-schemes","text":"The default security scheme is OAuth 2.0. However, this can be changed in the configuration.","title":"Security Schemes"},{"location":"develop/dataplatform-apis/#sparql-result-set-streaming","text":"The SPARQL proxy pipes the results of SPARQL queries directly from the underlying data endpoint to the request client. This however does not always apply for CONSTRUCT queries. The result of a CONSTRUCT query is a set of statements. RDF graph serialization formats tend to group the information for compactness - e.g. in Turtle , all statements for a subject are written together - avoiding subject and subject-predicate repetition, for which it is necessary to have the complete result set at disposal. Therefore, before sending the result to the request client, the complete result is loaded and then serialized. This creates a potential danger whenever a large result set is build and could lead to overload of the server. There is however one serialization format (the N-Triples format ) which is streaming friendly and that should always be used whenever large result sets are expected.","title":"SPARQL result set streaming"},{"location":"develop/dataplatform-apis/#sparql-default-graph-rdf-dataset","text":"","title":"SPARQL default graph &amp; RDF dataset"},{"location":"develop/dataplatform-apis/#default-graph","text":"The definition of the RDF dataset of a query in the SPARQL 1.1 specification leads to problems regarding the default graph of a SPARQL service. On one hand it is defined that: A SPARQL query is executed against an RDF dataset which represents a collection of graphs. An RDF dataset comprises one graph, the default graph, which does not have a name, and zero or more named graphs, where each named graph is identified by an IRI. Furthermore, it says: A SPARQL query may specify the dataset to be used for matching by using the FROM clause and the FROM NAMED clause to describe the RDF dataset. If a query provides such a dataset description, then it is used in place of any dataset that the query service would use if no dataset description is provided in a query. The RDF dataset may also be specified in a SPARQL protocol request, in which case the protocol description overrides any description in the query itself. A query service may refuse a query request if the dataset description is not acceptable to the service. The FROM and FROM NAMED keywords allow a query to specify an RDF dataset by reference; they indicate that the dataset should include graphs that are obtained from representations of the resources identified by the given IRIs (i.e. the absolute form of the given IRI references). The dataset resulting from a number of FROM and FROM NAMED clauses is: a default graph consisting of the RDF merge of the graphs referred to in the FROM clauses, and a set of (IRI, graph) pairs, one from each FROM NAMED clause. If there is no FROM clause, but there is one or more FROM NAMED clauses, then the dataset includes an empty graph for the default graph. That means the default graph of a SPARQL service cannot be explicitly referenced in the RDF dataset of a SPARQL query using FROM / FROM NAMED. For this reason, DataPlatform does not allow the manipulation of the service\u2019s default graph. To enforce this policy, the following restriction applies to incoming SPARQL 1.1 Update queries: Update queries (INSERT DATA, DELETE DATA and DELETE/INSERT) targeted against the service\u2019s default graph will not be accepted by returning an HTTP 400 Bad Request status code.","title":"Default graph"},{"location":"develop/dataplatform-apis/#default-rdf-dataset","text":"The interpretation of the RDF dataset of a query differs between various SPARQL service implementations (as shown here ). In the case a query declares no RDF dataset, DataPlatform uses the following default RDF dataset declaration to provide a uniform behavior for all supported SPARQL services: The default graph is the union ( RDF Merge graph ) of all named graphs the user is allowed to access. The set of named graphs contains all named graphs the user is allowed to access.","title":"Default RDF dataset"},{"location":"develop/dataplatform-apis/#open-api-reference","text":"The latest Open API specification: Alternatively, you can (re)view it redoc web UI","title":"(Open) API Reference"},{"location":"develop/python-plugins/","tags":["python-plugins"],"text":"Python Plugins \u00a4 Introduction \u00a4 The Python plugin system allows to extend eccenca DataIntegration with custom operators. Install and Updating Plugins \u00a4 Plugins are a released as parts of Python packages. The can but do not need to be open source and published on pypi.org (a widely used Python Package Index). If you want to install a python plugin package, you can do this by using cmemc\u2019s admin workspace python command group. The following shell commands demonstrate the basic workflow: # list all installed python packages # Note: the list contains plugin packages as well all dependencies which they are using $ cmemc admin workspace python list Name Version ------------------ ----------- certifi 2022 .5.18.1 charset-normalizer 2 .0.12 cmem-cmempy 22 .1.1 cmem-plugin-base 1 .2.0 idna 3 .3 isodate 0 .6.1 jep 4 .0.2 pip 20 .3.4 pyparsing 3 .0.9 rdflib 6 .1.1 requests 2 .27.1 requests-toolbelt 0 .9.1 setuptools 52 .0.0 six 1 .16.0 urllib3 1 .26.9 wheel 0 .34.2 # Install a plugin package from pypi.org $ cmemc admin workspace python install cmem-plugin-graphql Install package cmem-plugin-graphql ... done # list available plugins $ cmemc admin workspace python list-plugins ID Type Label --------------------------------- -------------- ------------- cmem_plugin_graphql-GraphQLPlugin WorkflowPlugin GraphQL query # uninstall the plugin package $ cmemc admin workspace python uninstall cmem-plugin-graphql Uninstall package cmem-plugin-graphql ... done # validate that no plugins are installed $ cmemc admin workspace python list-plugins ID Type Label ---- ------ ------- You can also install specific versions of a package by using version qualifier $ cmemc admin workspace python install cmem-plugin-graphql == 1 .0.0 Install package cmem-plugin-graphql ... done And you can also install a package from a source distribution file $ cmemc admin workspace python install cmem-plugin-graphql-1.0.0.tar.gz Install package cmem-plugin-graphql ... done Developing Plugins \u00a4 We recommend to start developing a plugin by creating a new project with our official python project template (cmem-plugin-template) . This template will generate a fully configured Python poetry source repository together with build plans for gitlab and github. Based on the template, you will be able to develop your own plugins. In the following, we will introduce some basic concepts. Workflow plugins \u00a4 A workflow plugin implements a new operator (task) that can be used within a workflow. A workflow plugin may accept an arbitrary list of inputs and optionally returns a single output. A minimal plugin that just outputs the first input looks like this: workflow.py 1 2 3 4 5 6 7 8 9 10 from typing import Sequence from cmem_plugin_base.dataintegration.entity import Entities from cmem_plugin_base.dataintegration.description import PluginParameter , Plugin from cmem_plugin_base.dataintegration.plugins import WorkflowPlugin @Plugin ( label = \"My Workflow Plugin\" ) class MyWorkflowPlugin ( WorkflowPlugin ): def execute ( self , inputs : Sequence [ Entities ]) -> Entities : return inputs [ 0 ] The lifecycle of a plugin is as follows: The plugin will be instantiated once the workflow execution reaches the respective plugin. The execute function is called with the results of the connected input operators. The output is forwarded to the next subsequent operator. Because the returned Entities object can only be iterated once, the above process has to be repeated each time the output is iterated over. Multiple iterations happen if the output of the workflow plugin is connected to multiple operators. Transform plugins \u00a4 A transform plugin can be used in transform and linking rules. It accepts an arbitrary number of inputs and returns an output. Each input as well as the output consists of a sequence of values. A minimal plugin that just outputs the first input looks like this: transform.py 1 2 3 4 5 6 7 8 9 from typing import Sequence from cmem_plugin_base.dataintegration.description import PluginParameter , Plugin from cmem_plugin_base.dataintegration.plugins import TransformPlugin @Plugin ( label = \"My Transform Plugin\" ) class MyTransformPlugin ( TransformPlugin ): def transform ( self , inputs : Sequence [ Sequence [ str ]]) -> Sequence [ str ]: return inputs [ 0 ] Logging \u00a4 The Python standard output is redirected to the DataIntegration standard output. By default, println and logging statements will therefore be printed to the standard output. The default Python logging configuration applies, so logs can be redirected to files or other outputs as well. Preliminaries \u00a4 This section describes which backend components are needed on the DataIntegration server. When using our official docker images, these components are already installed and enabled. Python \u00a4 An installation of the CPython distribution (at least version 3.3) is required. While other distributions, such as Anaconda, should be working as well, only CPython is officially supported. Java Embedded Python (Jep) \u00a4 The Jep module needs to be installed. The easiest way is to execute: pip install jep The libraries contained in the Jep module need to be accessible from the Java Virtual Machine running DataIntegration. This can be achieved by setting an environment variable to the directory path where the Jep module is located: Linux : set LD_LIBRARY_PATH . OS X : set DYLD_LIBRARY_PATH . Windows : set PATH . For alternative installation methods, visit","title":"Python Plugins"},{"location":"develop/python-plugins/#python-plugins","text":"","title":"Python Plugins"},{"location":"develop/python-plugins/#introduction","text":"The Python plugin system allows to extend eccenca DataIntegration with custom operators.","title":"Introduction"},{"location":"develop/python-plugins/#install-and-updating-plugins","text":"Plugins are a released as parts of Python packages. The can but do not need to be open source and published on pypi.org (a widely used Python Package Index). If you want to install a python plugin package, you can do this by using cmemc\u2019s admin workspace python command group. The following shell commands demonstrate the basic workflow: # list all installed python packages # Note: the list contains plugin packages as well all dependencies which they are using $ cmemc admin workspace python list Name Version ------------------ ----------- certifi 2022 .5.18.1 charset-normalizer 2 .0.12 cmem-cmempy 22 .1.1 cmem-plugin-base 1 .2.0 idna 3 .3 isodate 0 .6.1 jep 4 .0.2 pip 20 .3.4 pyparsing 3 .0.9 rdflib 6 .1.1 requests 2 .27.1 requests-toolbelt 0 .9.1 setuptools 52 .0.0 six 1 .16.0 urllib3 1 .26.9 wheel 0 .34.2 # Install a plugin package from pypi.org $ cmemc admin workspace python install cmem-plugin-graphql Install package cmem-plugin-graphql ... done # list available plugins $ cmemc admin workspace python list-plugins ID Type Label --------------------------------- -------------- ------------- cmem_plugin_graphql-GraphQLPlugin WorkflowPlugin GraphQL query # uninstall the plugin package $ cmemc admin workspace python uninstall cmem-plugin-graphql Uninstall package cmem-plugin-graphql ... done # validate that no plugins are installed $ cmemc admin workspace python list-plugins ID Type Label ---- ------ ------- You can also install specific versions of a package by using version qualifier $ cmemc admin workspace python install cmem-plugin-graphql == 1 .0.0 Install package cmem-plugin-graphql ... done And you can also install a package from a source distribution file $ cmemc admin workspace python install cmem-plugin-graphql-1.0.0.tar.gz Install package cmem-plugin-graphql ... done","title":"Install and Updating Plugins"},{"location":"develop/python-plugins/#developing-plugins","text":"We recommend to start developing a plugin by creating a new project with our official python project template (cmem-plugin-template) . This template will generate a fully configured Python poetry source repository together with build plans for gitlab and github. Based on the template, you will be able to develop your own plugins. In the following, we will introduce some basic concepts.","title":"Developing Plugins"},{"location":"develop/python-plugins/#workflow-plugins","text":"A workflow plugin implements a new operator (task) that can be used within a workflow. A workflow plugin may accept an arbitrary list of inputs and optionally returns a single output. A minimal plugin that just outputs the first input looks like this: workflow.py 1 2 3 4 5 6 7 8 9 10 from typing import Sequence from cmem_plugin_base.dataintegration.entity import Entities from cmem_plugin_base.dataintegration.description import PluginParameter , Plugin from cmem_plugin_base.dataintegration.plugins import WorkflowPlugin @Plugin ( label = \"My Workflow Plugin\" ) class MyWorkflowPlugin ( WorkflowPlugin ): def execute ( self , inputs : Sequence [ Entities ]) -> Entities : return inputs [ 0 ] The lifecycle of a plugin is as follows: The plugin will be instantiated once the workflow execution reaches the respective plugin. The execute function is called with the results of the connected input operators. The output is forwarded to the next subsequent operator. Because the returned Entities object can only be iterated once, the above process has to be repeated each time the output is iterated over. Multiple iterations happen if the output of the workflow plugin is connected to multiple operators.","title":"Workflow plugins"},{"location":"develop/python-plugins/#transform-plugins","text":"A transform plugin can be used in transform and linking rules. It accepts an arbitrary number of inputs and returns an output. Each input as well as the output consists of a sequence of values. A minimal plugin that just outputs the first input looks like this: transform.py 1 2 3 4 5 6 7 8 9 from typing import Sequence from cmem_plugin_base.dataintegration.description import PluginParameter , Plugin from cmem_plugin_base.dataintegration.plugins import TransformPlugin @Plugin ( label = \"My Transform Plugin\" ) class MyTransformPlugin ( TransformPlugin ): def transform ( self , inputs : Sequence [ Sequence [ str ]]) -> Sequence [ str ]: return inputs [ 0 ]","title":"Transform plugins"},{"location":"develop/python-plugins/#logging","text":"The Python standard output is redirected to the DataIntegration standard output. By default, println and logging statements will therefore be printed to the standard output. The default Python logging configuration applies, so logs can be redirected to files or other outputs as well.","title":"Logging"},{"location":"develop/python-plugins/#preliminaries","text":"This section describes which backend components are needed on the DataIntegration server. When using our official docker images, these components are already installed and enabled.","title":"Preliminaries"},{"location":"develop/python-plugins/#python","text":"An installation of the CPython distribution (at least version 3.3) is required. While other distributions, such as Anaconda, should be working as well, only CPython is officially supported.","title":"Python"},{"location":"develop/python-plugins/#java-embedded-python-jep","text":"The Jep module needs to be installed. The easiest way is to execute: pip install jep The libraries contained in the Jep module need to be accessible from the Java Virtual Machine running DataIntegration. This can be achieved by setting an environment variable to the directory path where the Jep module is located: Linux : set LD_LIBRARY_PATH . OS X : set DYLD_LIBRARY_PATH . Windows : set PATH . For alternative installation methods, visit","title":"Java Embedded Python (Jep)"},{"location":"explore-and-author/","text":"\u2605 Explore and Author \u00a4 Explore, author and interact with your Knowledge Graph. In the Explore section you will learn how Corporate Memory allows you to interact with your Enterprise Knowledge Graph. All relevant modules and functionalities are described. You will also learn how we make use of SHACL Shapes in order to customize the way how you can interact with your data in DataManager. Building a customized User Interface \u2014 Working with shapes allows for creation of a customized Linked Data user interface. Graph Exploration \u2014 The Explore module provides a generic and extensible RDF data browser and editor. Query Module \u2014 The Query module provides a user interface to store, describe, search and edit SPARQL queries.","title":"Explore and Author"},{"location":"explore-and-author/#explore-and-author","text":"Explore, author and interact with your Knowledge Graph. In the Explore section you will learn how Corporate Memory allows you to interact with your Enterprise Knowledge Graph. All relevant modules and functionalities are described. You will also learn how we make use of SHACL Shapes in order to customize the way how you can interact with your data in DataManager. Building a customized User Interface \u2014 Working with shapes allows for creation of a customized Linked Data user interface. Graph Exploration \u2014 The Explore module provides a generic and extensible RDF data browser and editor. Query Module \u2014 The Query module provides a user interface to store, describe, search and edit SPARQL queries.","title":"\u2605 Explore and Author"},{"location":"explore-and-author/building-a-customized-user-interface/","text":"Building a customized User Interface \u00a4 Introduction \u00a4 Working with shapes allows for creation of a customized Linked Data user interface.In addition to the standard PROPERTIES tab that shows all properties of a data resource, you can create custom \u201cform\u201d-like data interfaces. These configurable forms allow for a cleaner interface to view and author data resources. In addition, they enable integration of data from other resources that are linked to the current resource, creating a more concise view on your data. Defining forms \u00a4 You can define forms using SHACL rules. The rules state: What types of resources the form definition applies to. This is based on the rdf:type of a resource. What fields are shown in the form in which order. Field contents are retrieved from properties connected to the resource. Which other, linked resources are shown in the form. Linked resources can either be shown as links or as their full form. Which texts are used to name and describe fields, as well as the tab in the user interface. Forms are defined in the CMEM Shapes Catalog graph. The graph URI is https://vocab.eccenca.com/shacl/ . Form definitions are twofold: The form itself is defined as so called NodeShape . NodeShapes define which types of resources the form applies to (the target class), and which fields are shown in the form (the Properties). The individual fields are defined as so called PropertyShape . PropertyShapes define which property is used to retrieve data for the field (the Path), the name of the field, a description, its cardinality (min and max count), its position in the form (the Order), and if it should always be shown. In case of object properties, it also defines the type of the linked resource (the class). The full list of features is described in Building a customized User Interface . To define a new form, for example for foaf:Person resources, navigate to the CMEM Shapes Catalog graph and select NodeShape in Navigation. The list of existing NodeShapes is shown. Click \u201cCreate a new SHACL Node shape\u201d in the upper right to create a new NodeShape. Enter a name of the resource. An empty NodeShape resource is created and shown. To create the initial definition, click (Edit). A form is shown to you with input fields Name, Property Shapes, Vocabulary, Target class and Stateent Annotation. The initial definition requires the name, and the target class. Fields themselves are attached to the form later. Target class in particular binds the form to the resources it should cover. The Target class field features an auto-complete that displays all classes stored in Corporate Memory. The example form should cover resources of type foaf:Person resources, so enter foaf:Person in the Target class field. Click SAVE to save the NodeShape. You have now created an \u201cempty\u201d form that covers foaf:Person resources with tab name \u201cPerson\u201d. Navigating to a foaf:Person resource, you see a new tab as defined. You can still see all properties of the resource in the PROPERTIES tab. To define new fields, for example showing the email address of the person (defined as foaf:mbox ), navigate to the CMEM Shapes Catalog graph and select PropertyShape in Navigation. The list of existing PropertyShapes is shown. Click CREATE NEW PROPERTYSHAPE in the upper right to create a new PropertyShape. Enter a name of the resource. An empty PropertyShape resource is created and shown. Edit the form using . A form is shown with all relevant properties of a field definition. Required in this step are: The name of the field, which will be displayed left of the data content or input field in the form. The description, which will be displayed as tooltip on the question mark to the right of the name. The path, which states which property the field represents. In this example, it is foaf:mbox . The form the field should be shown in (Property of). The field provides an auto-complete, so just enter \u201cPerson\u201d and select the NodeShape resource you defined in the previous step. Click SAVE after filling out the required fields. NodeShapes \u00a4 Node Shapes are resources of type shacl:NodeShape . They are used to define custom forms attached to resources of a specific type. The following NodeShape properties are supported: In addition to these properties, the following non-standard properties from the eccenca SHACL UI extension are supported on Node Shapes: Naming and Presentation \u00a4 In this group, presentation and naming properties are collected. Most of the properties are straight forward to use. Name \u00a4 The name of the node is presented to the user only when he needs to distinguish between different shapes for the same resource. Used Path: shacl:name Description \u00a4 The node description should provide context information for the user when creating a new resource based on this node. Used Path: rdfs:comment Tab Name (deprecated) \u00a4 Name of the tab (deprecated, only interpreted until 20.06) Used Path: shui:tabName Navigation list query \u00a4 This property links the node shape to a SPARQL 1.1 Select Query in order to provide a sophisticated user navigation list query e.g. to add specific additional columns. Used Path: shui:navigationListQuery Vocabulary \u00a4 In this group, the affected vocabulary classes as well as the used property shapes are managed. Property \u00a4 Properties of this node Used Path: shacl:property Target class \u00a4 Class this NodeShape applies to. Used Path: shacl:targetClass Processing \u00a4 In this group, all shape properties are managed, have an effect on how new or existing resources are processed or created. URI template \u00a4 A compact sequence of characters for describing a range of URIs through variable expansion. Used Path: shui:uriTemplate On update update \u00a4 A query executed when any value of the resource is added, changed or removed. Used Path: shui:onUpdateUpdate Target Graph Template \u00a4 Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect). Used Path: shui:targetGraphTemplate Statement Annotation \u00a4 Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature. Enable \u00a4 A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape. Used Path: shui:enableStatementLevelMetadata Provide as Shape \u00a4 A value of true enables this node shape to be applied as statement annotation (reification). Used Path: shui:isApplicableAsStatementLevelMetadata PropertyShapes \u00a4 Property Shapes are resources of type [shacl:PropertyShape](http://www.w3.org/ns/shacl#PropertyShape) . They are used to specify constraints and UI options that need to be met in the context of a Node Shape. The following Property Shape properties of SHACL are supported: Info Name and Description are displayed using the configuration of titleHelper. See Deploy and Configure \u2192 Configuration \u2192 DataManager for more details. Naming and Presentation \u00a4 In this group, presentation and naming properties are collected. Most of the properties are straight forward to use, other properties provide more complex features, such as table reports. Name \u00a4 This name will be shown to the user. Used Path: shacl:name Description \u00a4 This text will be shown to the user in a tooltip. You can use new and blank lines for basic text structuring. Used Path: shacl:description Query: Table Report \u00a4 Use this property to provide a tabular read-only report of a custom SPARQL query at the place where this property shape is used in the user interface. The following placeholder can be used in the query text of the sparql query: {{shuiMainResource}} - refers to the main resource rendered in the starte node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ; {{shuiResource}} - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ; {{shuiGraph}} - the currently used graph. Used Path: shui:valueQuery Query: Table Report (hide header) \u00a4 If set to true, the report table will be rendered without header (in case you expect only a single value). Used Path: shui:valueQueryHideHeader Query: Table Report (hide footer) \u00a4 If set to true, the report table will be rendered without footer (in case you expect only a single value or row). Used Path: shui:valueQueryHideFooter Order \u00a4 Specifies the order of the property in the UI. Ordering is separate for each group. Used Path: shacl:order Group \u00a4 Group to which the property belongs to. Used Path: shacl:group Show always \u00a4 Default is false. A value of true let optional properties (min count = 0) show up by default. Used Path: shui:showAlways Read only \u00a4 Default is false. A value of true means the properties are not editable by the user. Useful for displaying system properties. Used Path: shui:readOnly Vocabulary \u00a4 In this group, property paths as well cardinality restrictions are managed. Property of \u00a4 The node shape this property shape belongs to. Used Path: shacl:property Path \u00a4 The datatype or object property used in this shape. Used Path: shacl:path Node kind \u00a4 Type of the node. Used Path: shacl:nodeKind Min count \u00a4 Min cardinality, 0 will show this property under optionals unless \u2018Show always = true\u2019 Used Path: shacl:minCount Max count \u00a4 Max cardinality Used Path: shacl:maxCount Datatype Property Specific \u00a4 In this group, all shape properties are managed, which only have effects on datatype properties. Datatype \u00a4 The datatype of the property. Used Path: shacl:datatype Use textarea \u00a4 Default is false. A value of true enables multiline editing capabilities for Literals via a textarea widget. Used Path: shui:textarea Regex Pattern \u00a4 A XPath regular expression (Perl like) that all literal strings need to match. Used Path: shacl:pattern Regex Flags \u00a4 An optional string of flags for the regular expression pattern (e.g. \u2018i\u2019 for case-insensitive mode) Used Path: shacl:flags Languages allowed \u00a4 This limits the given Literals to a list of languages. This property works only in combination with the datatype rdf:langString. Note that the expression for this property only allows for \u20182 Char ISO-639-1-Codes\u2019 only (no sub-tags). Used Path: shui:languageIn Languages Unique \u00a4 Default is false. A value of true enforces that no pair of Literals may use the same language tag. Used Path: shacl:uniqueLang Object Property Specific \u00a4 In this group, all shape properties are managed, which only have effects on object properties. Class \u00a4 Class of the connected IRI if nodeKind == sh:IRI. Used Path: shacl:class Query: Selectable Resources \u00a4 This query allows for listing selectable resources in the dropdown list for this property shape. Used Path: shui:uiQuery Inverse Path \u00a4 Default is false. A value of true inverts the expected / created direction of a relation. Used Path: shui:inversePath Deny new resources \u00a4 A value of true disables the option to create new resources. Used Path: shui:denyNewResources Node shape \u00a4 The shape of the linked resource. Used Path: shacl:node Processing \u00a4 In this group, all shape properties are managed, have an effect on how new or existing resources are processed or created. URI template \u00a4 A compact sequence of characters for describing a range of URIs through variable expansion. Used Path: shui:uriTemplate Ignore on copy \u00a4 Disables reusing the value(s) when creating a copy of the resource. Used Path: shui:ignoreOnCopy Query: On insert update \u00a4 This query is executed when a property value is added or changed. The following placeholder can be used in the query text of the sparql query: {{shuiMainResource}} - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ; {{shuiResource}} - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ; {{shuiGraph}} - the currently used graph. Used Path: shui:onInsertUpdate Query: On delete update \u00a4 This query is executed when a value is changed or removed. The following placeholder can be used in the query text of the sparql query: {{shuiMainResource}} - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ; {{shuiResource}} - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ; {{shuiGraph}} - the currently used graph. Used Path: shui:onDeleteUpdate Target Graph Template \u00a4 Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect). Used Path: shui:targetGraphTemplate Statement Annotation \u00a4 Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature. Enable \u00a4 A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape. Used Path: shui:enableStatementLevelMetadata Provided Shapes \u00a4 Instead of providing all possible statement annotation node shapes for the creation of new statement annotations, this property will limit the list to the selected shapes only. Used Path: shui:provideStatementLevelMetadataShapes Datatypes \u00a4 This is a list of supported data types in shapes. Not all datatypes result in specific widgets. anyURI \u00a4 The -lexical space- of anyURI is finite-length character sequences which, when the algorithm defined in Section 5.4 of [XML Linking Language] is applied to them, result in strings which are legal URIs according to [RFC 2396], as amended by [RFC 2732]. Note: Spaces are, in principle, allowed in the -lexical space- of anyURI, however, their use is highly discouraged (unless they are encoded by %20). IRI: http://www.w3.org/2001/XMLSchema#anyURI base64Binary \u00a4 The lexical forms of base64Binary values are limited to the 65 characters of the Base64 Alphabet defined in [RFC 2045], i.e., a-z, A-Z, 0-9, the plus sign (+), the forward slash (/) and the equal sign (=), together with the characters defined in [XML 1.0 (Second Edition)] as white space. No other characters are allowed. IRI: http://www.w3.org/2001/XMLSchema#base64Binary boolean \u00a4 An instance of a datatype that is defined as -boolean- can have the following legal literals {true, false, 1, 0}. IRI: http://www.w3.org/2001/XMLSchema#boolean byte \u00a4 byte is -derived- from short by setting the value of -maxInclusive- to be 127 and -minInclusive- to be -128. byte has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126, +100. IRI: http://www.w3.org/2001/XMLSchema#byte date \u00a4 The -lexical space- of date consists of finite-length sequences of characters of the form: \u2018-\u2018? yyyy \u2018-\u2018 mm \u2018-\u2018 dd zzzzzz? where the date and optional timezone are represented exactly the same way as they are for dateTime. The first moment of the interval is that represented by: \u2018-\u2018 yyyy \u2018-\u2018 mm \u2018-\u2018 dd \u2018T00:00:00\u2019 zzzzzz? and the least upper bound of the interval is the timeline point represented (noncanonically) by: \u2018-\u2018 yyyy \u2018-\u2018 mm \u2018-\u2018 dd \u2018T24:00:00\u2019 zzzzzz?. IRI: http://www.w3.org/2001/XMLSchema#date dateTime \u00a4 The -lexical space- of dateTime consists of finite-length sequences of characters of the form: \u2018-\u2018? yyyy \u2018-\u2018 mm \u2018-\u2018 dd \u2018T\u2019 hh \u2018:\u2019 mm \u2018:\u2019 ss (\u2018.\u2019 s+)? (zzzzzz)? For example, 2002-10-10T12:00:00-05:00 (noon on 10 October 2002, Central Daylight Savings Time as well as Eastern Standard Time in the U.S.) is 2002-10-10T17:00:00Z, five hours later than 2002-10-10T12:00:00Z. IRI: http://www.w3.org/2001/XMLSchema#dateTime dateTimeStamp \u00a4 The lexical space of dateTimeStamp consists of strings which are in the -lexical space- of dateTime and which also match the regular expression \u2018.*(Z|(+|-)[0-9][0-9]:[0-9][0-9])\u2019 IRI: http://www.w3.org/2001/XMLSchema#dateTimeStamp decimal \u00a4 decimal has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) separated by a period as a decimal indicator. An optional leading sign is allowed. If the sign is omitted, \u2018+\u2019 is assumed. Leading and trailing zeroes are optional. If the fractional part is zero, the period and following zeroes can be omitted. For example: -1.23, 12678967.543233, +100000.00, 210. IRI: http://www.w3.org/2001/XMLSchema#decimal double \u00a4 double values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent -must- be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for double. IRI: http://www.w3.org/2001/XMLSchema#double duration \u00a4 The lexical representation for duration is the [ISO 8601] extended format PnYn MnDTnH nMnS, where nY represents the number of years, nM the number of months, nD the number of days, \u2018T\u2019 is the date/time separator, nH the number of hours, nM the number of minutes and nS the number of seconds. The number of seconds can include decimal digits to arbitrary precision. IRI: http://www.w3.org/2001/XMLSchema#duration float \u00a4 float values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent -must- be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for float. IRI: http://www.w3.org/2001/XMLSchema#float gDay \u00a4 The lexical representation for gDay is the left truncated lexical representation for date: \u2014DD . An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gDay gMonth \u00a4 The lexical representation for gMonth is the left and right truncated lexical representation for date: \u2013MM. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gMonth gMonthDay \u00a4 The lexical representation for gMonthDay is the left truncated lexical representation for date: \u2013MM-DD. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats (-D). This datatype can be used to represent a specific day in a month. To say, for example, that my birthday occurs on the 14th of September ever year. IRI: http://www.w3.org/2001/XMLSchema#gMonthDay gYear \u00a4 The lexical representation for gYear is the reduced (right truncated) lexical representation for dateTime: CCYY. No left truncation is allowed. An optional following time zone qualifier is allowed as for dateTime. To accommodate year values outside the range from 0001 to 9999, additional digits can be added to the left of this representation and a preceding \u2018-\u2018 sign is allowed. For example, to indicate 1999, one would write: 1999. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gYear gYearMonth \u00a4 The lexical representation for gYearMonth is the reduced (right truncated) lexical representation for dateTime: CCYY-MM. No left truncation is allowed. An optional following time zone qualifier is allowed. To accommodate year values outside the range from 0001 to 9999, additional digits can be added to the left of this representation and a preceding \u2018-\u2018 sign is allowed. For example, to indicate the month of May 1999, one would write: 1999-05. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gYearMonth hexBinary \u00a4 hexBinary has a lexical representation where each binary octet is encoded as a character tuple, consisting of two hexadecimal digits ([0-9a-fA-F]) representing the octet code. For example, \u20180FB7\u2019 is a hex encoding for the 16-bit integer 4023 (whose binary representation is 111110110111). IRI: http://www.w3.org/2001/XMLSchema#hexBinary HTML \u00a4 The datatype of RDF literals storing fragments of HTML content IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML int \u00a4 int is -derived- from long by setting the value of -maxInclusive- to be 2147483647 and -minInclusive- to be -2147483648. int has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126789675, +100000. IRI: http://www.w3.org/2001/XMLSchema#int integer \u00a4 integer has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) with an optional leading sign. If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#integer Jinja Template String (datatype) \u00a4 Jinja is a modern and designer-friendly templating language for Python and other languages. IRI: https://vocab.eccenca.com/shui/jinja langString \u00a4 The datatype of language-tagged string values IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#langString language \u00a4 language represents natural language identifiers as defined by by [RFC 3066] . The -value space- of language is the set of all strings that are valid language identifiers as defined [RFC 3066] . The -lexical space- of language is the set of all strings that conform to the pattern [a-zA-Z]{1,8}(-[a-zA-Z0-9]{1,8})* . The -base type- of language is token. IRI: http://www.w3.org/2001/XMLSchema#language long \u00a4 long is -derived- from integer by setting the value of -maxInclusive- to be 9223372036854775807 and -minInclusive- to be -9223372036854775808. long has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#long Markdown \u00a4 In addition to rdf:HTML, this is the datatype RDF literals storing fragments of markdown content. IRI: http://ns.ontowiki.net/SysOnt/Markdown Name \u00a4 Name represents XML Names. The -value space- of Name is the set of all strings which -match- the Name production of [XML 1.0 (Second Edition)]. The -lexical space- of Name is the set of all strings which -match- the Name production of [XML 1.0 (Second Edition)]. The -base type- of Name is token. IRI: http://www.w3.org/2001/XMLSchema#Name NCName \u00a4 NCName represents XML \u2018non-colonized\u2019 Names. The -value space- of NCName is the set of all strings which -match- the NCName production of [Namespaces in XML]. The -lexical space- of NCName is the set of all strings which -match- the NCName production of [Namespaces in XML]. The -base type- of NCName is Name. IRI: http://www.w3.org/2001/XMLSchema#NCName negativeInteger \u00a4 negativeInteger has a lexical representation consisting of a negative sign (\u2018-\u2018) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: -1, -12678967543233, -100000. IRI: http://www.w3.org/2001/XMLSchema#negativeInteger NMTOKEN \u00a4 NMTOKEN represents the NMTOKEN attribute type from [XML 1.0 (Second Edition)]. The -value space- of NMTOKEN is the set of tokens that -match- the Nmtoken production in [XML 1.0 (Second Edition)]. The -lexical space- of NMTOKEN is the set of strings that -match- the Nmtoken production in [XML 1.0 (Second Edition)]. The -base type- of NMTOKEN is token. IRI: http://www.w3.org/2001/XMLSchema#NMTOKEN nonNegativeInteger \u00a4 nonNegativeInteger has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, the positive sign (\u2018+\u2019) is assumed. If the sign is present, it must be \u2018+\u2019 except for lexical forms denoting zero, which may be preceded by a positive (\u2018+\u2019) or a negative (\u2018-\u2018) sign. For example: 1, 0, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#nonNegativeInteger nonPositiveInteger \u00a4 nonPositiveInteger has a lexical representation consisting of an optional preceding sign followed by a finite-length sequence of decimal digits (#x30-#x39). The sign may be \u2018+\u2019 or may be omitted only for lexical forms denoting zero, in all other lexical forms, the negative sign (\u2018-\u2018) must be present. For example: -1, 0, -12678967543233, -100000. IRI: http://www.w3.org/2001/XMLSchema#nonPositiveInteger normalizedString \u00a4 normalizedString represents white space normalized strings. The -value space- of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The -lexical space- of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The -base type- of normalizedString is string. IRI: http://www.w3.org/2001/XMLSchema#normalizedString PlainLiteral \u00a4 The class of plain (i.e. untyped) literal values, as used in RIF and OWL 2 IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#PlainLiteral positiveInteger \u00a4 positiveInteger has a lexical representation consisting of an optional positive sign (\u2018+\u2019) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: 1, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#positiveInteger short \u00a4 short is -derived- from int by setting the value of -maxInclusive- to be 32767 and -minInclusive- to be -32768. short has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678, +10000. IRI: http://www.w3.org/2001/XMLSchema#short sparqlOperation \u00a4 sparql operation datatype (query or update) IRI: https://vocab.eccenca.com/shui/sparqlOperation sparqlQuery \u00a4 SPARQL 1.1 Query IRI: https://vocab.eccenca.com/shui/sparqlQuery sparqlUpdate \u00a4 SPARQL 1.1 Update IRI: https://vocab.eccenca.com/shui/sparqlUpdate string \u00a4 The string datatype represents character strings in XML. The -value space- of string is the set of finite-length sequences of characters (as defined in [XML 1.0 (Second Edition)]) that -match- the Char production from [XML 1.0 (Second Edition)]. A character is an atomic unit of communication, it is not further specified except to note that every character has a corresponding Universal Character Set code point, which is an integer. IRI: http://www.w3.org/2001/XMLSchema#string time \u00a4 The lexical representation for time is the left truncated lexical representation for dateTime: hh ss.sss with optional following time zone indicator. For example, to indicate 1:20 pm for Eastern Standard Time which is 5 hours behind Coordinated Universal Time (UTC), one would write: 13:20:00-05:00. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#time token \u00a4 token represents tokenized strings. The -value space- of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The -lexical space- of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The -base type- of token is normalizedString. IRI: http://www.w3.org/2001/XMLSchema#token unsignedByte \u00a4 unsignedByte is -derived- from unsignedShort by setting the value of -maxInclusive- to be 255. unsignedByte has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 126, 100. IRI: http://www.w3.org/2001/XMLSchema#unsignedByte unsignedInt \u00a4 unsignedInt is -derived- from unsignedLong by setting the value of -maxInclusive- to be 4294967295. unsignedInt has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 1267896754, 100000. IRI: http://www.w3.org/2001/XMLSchema#unsignedInt unsignedLong \u00a4 unsignedLong is -derived- from nonNegativeInteger by setting the value of -maxInclusive- to be 18446744073709551615. unsignedLong has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678967543233, 100000. IRI: http://www.w3.org/2001/XMLSchema#unsignedLong unsignedShort \u00a4 unsignedShort is -derived- from unsignedInt by setting the value of -maxInclusive- to be 65535. unsignedShort has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678, 10000. IRI: http://www.w3.org/2001/XMLSchema#unsignedShort XMLLiteral \u00a4 The datatype of XML literal values. IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral Using forms \u00a4 Once a Node Shape is created for a specific class, you are able to use the specified entry form in the Explore component of Corporate Memory. Editing existing resources \u00a4 While browsing your knowledge graph, you will always see your shape in action, when you click on a resource which is an instance of the class which is linked with shacl:targetClass from your Node Shape. The next images demonstrate this behavior : Creating new resources \u00a4 You can also create new resources by using a shaped form. One way to achieve this, is to select the class in the navigation tree on the lower left part in the Explore component and then click the Floating Action Button at the bottom or use the context menu on upper right side. The next images demonstrate this behaviour:","title":"Building a customized User Interface"},{"location":"explore-and-author/building-a-customized-user-interface/#building-a-customized-user-interface","text":"","title":"Building a customized User Interface"},{"location":"explore-and-author/building-a-customized-user-interface/#introduction","text":"Working with shapes allows for creation of a customized Linked Data user interface.In addition to the standard PROPERTIES tab that shows all properties of a data resource, you can create custom \u201cform\u201d-like data interfaces. These configurable forms allow for a cleaner interface to view and author data resources. In addition, they enable integration of data from other resources that are linked to the current resource, creating a more concise view on your data.","title":"Introduction"},{"location":"explore-and-author/building-a-customized-user-interface/#defining-forms","text":"You can define forms using SHACL rules. The rules state: What types of resources the form definition applies to. This is based on the rdf:type of a resource. What fields are shown in the form in which order. Field contents are retrieved from properties connected to the resource. Which other, linked resources are shown in the form. Linked resources can either be shown as links or as their full form. Which texts are used to name and describe fields, as well as the tab in the user interface. Forms are defined in the CMEM Shapes Catalog graph. The graph URI is https://vocab.eccenca.com/shacl/ . Form definitions are twofold: The form itself is defined as so called NodeShape . NodeShapes define which types of resources the form applies to (the target class), and which fields are shown in the form (the Properties). The individual fields are defined as so called PropertyShape . PropertyShapes define which property is used to retrieve data for the field (the Path), the name of the field, a description, its cardinality (min and max count), its position in the form (the Order), and if it should always be shown. In case of object properties, it also defines the type of the linked resource (the class). The full list of features is described in Building a customized User Interface . To define a new form, for example for foaf:Person resources, navigate to the CMEM Shapes Catalog graph and select NodeShape in Navigation. The list of existing NodeShapes is shown. Click \u201cCreate a new SHACL Node shape\u201d in the upper right to create a new NodeShape. Enter a name of the resource. An empty NodeShape resource is created and shown. To create the initial definition, click (Edit). A form is shown to you with input fields Name, Property Shapes, Vocabulary, Target class and Stateent Annotation. The initial definition requires the name, and the target class. Fields themselves are attached to the form later. Target class in particular binds the form to the resources it should cover. The Target class field features an auto-complete that displays all classes stored in Corporate Memory. The example form should cover resources of type foaf:Person resources, so enter foaf:Person in the Target class field. Click SAVE to save the NodeShape. You have now created an \u201cempty\u201d form that covers foaf:Person resources with tab name \u201cPerson\u201d. Navigating to a foaf:Person resource, you see a new tab as defined. You can still see all properties of the resource in the PROPERTIES tab. To define new fields, for example showing the email address of the person (defined as foaf:mbox ), navigate to the CMEM Shapes Catalog graph and select PropertyShape in Navigation. The list of existing PropertyShapes is shown. Click CREATE NEW PROPERTYSHAPE in the upper right to create a new PropertyShape. Enter a name of the resource. An empty PropertyShape resource is created and shown. Edit the form using . A form is shown with all relevant properties of a field definition. Required in this step are: The name of the field, which will be displayed left of the data content or input field in the form. The description, which will be displayed as tooltip on the question mark to the right of the name. The path, which states which property the field represents. In this example, it is foaf:mbox . The form the field should be shown in (Property of). The field provides an auto-complete, so just enter \u201cPerson\u201d and select the NodeShape resource you defined in the previous step. Click SAVE after filling out the required fields.","title":"Defining forms"},{"location":"explore-and-author/building-a-customized-user-interface/#nodeshapes","text":"Node Shapes are resources of type shacl:NodeShape . They are used to define custom forms attached to resources of a specific type. The following NodeShape properties are supported: In addition to these properties, the following non-standard properties from the eccenca SHACL UI extension are supported on Node Shapes:","title":"NodeShapes"},{"location":"explore-and-author/building-a-customized-user-interface/#naming-and-presentation","text":"In this group, presentation and naming properties are collected. Most of the properties are straight forward to use.","title":"Naming and Presentation"},{"location":"explore-and-author/building-a-customized-user-interface/#name","text":"The name of the node is presented to the user only when he needs to distinguish between different shapes for the same resource. Used Path: shacl:name","title":"Name"},{"location":"explore-and-author/building-a-customized-user-interface/#description","text":"The node description should provide context information for the user when creating a new resource based on this node. Used Path: rdfs:comment","title":"Description"},{"location":"explore-and-author/building-a-customized-user-interface/#tab-name-deprecated","text":"Name of the tab (deprecated, only interpreted until 20.06) Used Path: shui:tabName","title":"Tab Name (deprecated)"},{"location":"explore-and-author/building-a-customized-user-interface/#navigation-list-query","text":"This property links the node shape to a SPARQL 1.1 Select Query in order to provide a sophisticated user navigation list query e.g. to add specific additional columns. Used Path: shui:navigationListQuery","title":"Navigation list query"},{"location":"explore-and-author/building-a-customized-user-interface/#vocabulary","text":"In this group, the affected vocabulary classes as well as the used property shapes are managed.","title":"Vocabulary"},{"location":"explore-and-author/building-a-customized-user-interface/#property","text":"Properties of this node Used Path: shacl:property","title":"Property"},{"location":"explore-and-author/building-a-customized-user-interface/#target-class","text":"Class this NodeShape applies to. Used Path: shacl:targetClass","title":"Target class"},{"location":"explore-and-author/building-a-customized-user-interface/#processing","text":"In this group, all shape properties are managed, have an effect on how new or existing resources are processed or created.","title":"Processing"},{"location":"explore-and-author/building-a-customized-user-interface/#uri-template","text":"A compact sequence of characters for describing a range of URIs through variable expansion. Used Path: shui:uriTemplate","title":"URI template"},{"location":"explore-and-author/building-a-customized-user-interface/#on-update-update","text":"A query executed when any value of the resource is added, changed or removed. Used Path: shui:onUpdateUpdate","title":"On update update"},{"location":"explore-and-author/building-a-customized-user-interface/#target-graph-template","text":"Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect). Used Path: shui:targetGraphTemplate","title":"Target Graph Template"},{"location":"explore-and-author/building-a-customized-user-interface/#statement-annotation","text":"Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature.","title":"Statement Annotation"},{"location":"explore-and-author/building-a-customized-user-interface/#enable","text":"A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape. Used Path: shui:enableStatementLevelMetadata","title":"Enable"},{"location":"explore-and-author/building-a-customized-user-interface/#provide-as-shape","text":"A value of true enables this node shape to be applied as statement annotation (reification). Used Path: shui:isApplicableAsStatementLevelMetadata","title":"Provide as Shape"},{"location":"explore-and-author/building-a-customized-user-interface/#propertyshapes","text":"Property Shapes are resources of type [shacl:PropertyShape](http://www.w3.org/ns/shacl#PropertyShape) . They are used to specify constraints and UI options that need to be met in the context of a Node Shape. The following Property Shape properties of SHACL are supported: Info Name and Description are displayed using the configuration of titleHelper. See Deploy and Configure \u2192 Configuration \u2192 DataManager for more details.","title":"PropertyShapes"},{"location":"explore-and-author/building-a-customized-user-interface/#naming-and-presentation_1","text":"In this group, presentation and naming properties are collected. Most of the properties are straight forward to use, other properties provide more complex features, such as table reports.","title":"Naming and Presentation"},{"location":"explore-and-author/building-a-customized-user-interface/#name_1","text":"This name will be shown to the user. Used Path: shacl:name","title":"Name"},{"location":"explore-and-author/building-a-customized-user-interface/#description_1","text":"This text will be shown to the user in a tooltip. You can use new and blank lines for basic text structuring. Used Path: shacl:description","title":"Description"},{"location":"explore-and-author/building-a-customized-user-interface/#query-table-report","text":"Use this property to provide a tabular read-only report of a custom SPARQL query at the place where this property shape is used in the user interface. The following placeholder can be used in the query text of the sparql query: {{shuiMainResource}} - refers to the main resource rendered in the starte node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ; {{shuiResource}} - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ; {{shuiGraph}} - the currently used graph. Used Path: shui:valueQuery","title":"Query: Table Report"},{"location":"explore-and-author/building-a-customized-user-interface/#query-table-report-hide-header","text":"If set to true, the report table will be rendered without header (in case you expect only a single value). Used Path: shui:valueQueryHideHeader","title":"Query: Table Report (hide header)"},{"location":"explore-and-author/building-a-customized-user-interface/#query-table-report-hide-footer","text":"If set to true, the report table will be rendered without footer (in case you expect only a single value or row). Used Path: shui:valueQueryHideFooter","title":"Query: Table Report (hide footer)"},{"location":"explore-and-author/building-a-customized-user-interface/#order","text":"Specifies the order of the property in the UI. Ordering is separate for each group. Used Path: shacl:order","title":"Order"},{"location":"explore-and-author/building-a-customized-user-interface/#group","text":"Group to which the property belongs to. Used Path: shacl:group","title":"Group"},{"location":"explore-and-author/building-a-customized-user-interface/#show-always","text":"Default is false. A value of true let optional properties (min count = 0) show up by default. Used Path: shui:showAlways","title":"Show always"},{"location":"explore-and-author/building-a-customized-user-interface/#read-only","text":"Default is false. A value of true means the properties are not editable by the user. Useful for displaying system properties. Used Path: shui:readOnly","title":"Read only"},{"location":"explore-and-author/building-a-customized-user-interface/#vocabulary_1","text":"In this group, property paths as well cardinality restrictions are managed.","title":"Vocabulary"},{"location":"explore-and-author/building-a-customized-user-interface/#property-of","text":"The node shape this property shape belongs to. Used Path: shacl:property","title":"Property of"},{"location":"explore-and-author/building-a-customized-user-interface/#path","text":"The datatype or object property used in this shape. Used Path: shacl:path","title":"Path"},{"location":"explore-and-author/building-a-customized-user-interface/#node-kind","text":"Type of the node. Used Path: shacl:nodeKind","title":"Node kind"},{"location":"explore-and-author/building-a-customized-user-interface/#min-count","text":"Min cardinality, 0 will show this property under optionals unless \u2018Show always = true\u2019 Used Path: shacl:minCount","title":"Min count"},{"location":"explore-and-author/building-a-customized-user-interface/#max-count","text":"Max cardinality Used Path: shacl:maxCount","title":"Max count"},{"location":"explore-and-author/building-a-customized-user-interface/#datatype-property-specific","text":"In this group, all shape properties are managed, which only have effects on datatype properties.","title":"Datatype Property Specific"},{"location":"explore-and-author/building-a-customized-user-interface/#datatype","text":"The datatype of the property. Used Path: shacl:datatype","title":"Datatype"},{"location":"explore-and-author/building-a-customized-user-interface/#use-textarea","text":"Default is false. A value of true enables multiline editing capabilities for Literals via a textarea widget. Used Path: shui:textarea","title":"Use textarea"},{"location":"explore-and-author/building-a-customized-user-interface/#regex-pattern","text":"A XPath regular expression (Perl like) that all literal strings need to match. Used Path: shacl:pattern","title":"Regex Pattern"},{"location":"explore-and-author/building-a-customized-user-interface/#regex-flags","text":"An optional string of flags for the regular expression pattern (e.g. \u2018i\u2019 for case-insensitive mode) Used Path: shacl:flags","title":"Regex Flags"},{"location":"explore-and-author/building-a-customized-user-interface/#languages-allowed","text":"This limits the given Literals to a list of languages. This property works only in combination with the datatype rdf:langString. Note that the expression for this property only allows for \u20182 Char ISO-639-1-Codes\u2019 only (no sub-tags). Used Path: shui:languageIn","title":"Languages allowed"},{"location":"explore-and-author/building-a-customized-user-interface/#languages-unique","text":"Default is false. A value of true enforces that no pair of Literals may use the same language tag. Used Path: shacl:uniqueLang","title":"Languages Unique"},{"location":"explore-and-author/building-a-customized-user-interface/#object-property-specific","text":"In this group, all shape properties are managed, which only have effects on object properties.","title":"Object Property Specific"},{"location":"explore-and-author/building-a-customized-user-interface/#class","text":"Class of the connected IRI if nodeKind == sh:IRI. Used Path: shacl:class","title":"Class"},{"location":"explore-and-author/building-a-customized-user-interface/#query-selectable-resources","text":"This query allows for listing selectable resources in the dropdown list for this property shape. Used Path: shui:uiQuery","title":"Query: Selectable Resources"},{"location":"explore-and-author/building-a-customized-user-interface/#inverse-path","text":"Default is false. A value of true inverts the expected / created direction of a relation. Used Path: shui:inversePath","title":"Inverse Path"},{"location":"explore-and-author/building-a-customized-user-interface/#deny-new-resources","text":"A value of true disables the option to create new resources. Used Path: shui:denyNewResources","title":"Deny new resources"},{"location":"explore-and-author/building-a-customized-user-interface/#node-shape","text":"The shape of the linked resource. Used Path: shacl:node","title":"Node shape"},{"location":"explore-and-author/building-a-customized-user-interface/#processing_1","text":"In this group, all shape properties are managed, have an effect on how new or existing resources are processed or created.","title":"Processing"},{"location":"explore-and-author/building-a-customized-user-interface/#uri-template_1","text":"A compact sequence of characters for describing a range of URIs through variable expansion. Used Path: shui:uriTemplate","title":"URI template"},{"location":"explore-and-author/building-a-customized-user-interface/#ignore-on-copy","text":"Disables reusing the value(s) when creating a copy of the resource. Used Path: shui:ignoreOnCopy","title":"Ignore on copy"},{"location":"explore-and-author/building-a-customized-user-interface/#query-on-insert-update","text":"This query is executed when a property value is added or changed. The following placeholder can be used in the query text of the sparql query: {{shuiMainResource}} - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ; {{shuiResource}} - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ; {{shuiGraph}} - the currently used graph. Used Path: shui:onInsertUpdate","title":"Query: On insert update"},{"location":"explore-and-author/building-a-customized-user-interface/#query-on-delete-update","text":"This query is executed when a value is changed or removed. The following placeholder can be used in the query text of the sparql query: {{shuiMainResource}} - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ; {{shuiResource}} - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ; {{shuiGraph}} - the currently used graph. Used Path: shui:onDeleteUpdate","title":"Query: On delete update"},{"location":"explore-and-author/building-a-customized-user-interface/#target-graph-template_1","text":"Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect). Used Path: shui:targetGraphTemplate","title":"Target Graph Template"},{"location":"explore-and-author/building-a-customized-user-interface/#statement-annotation_1","text":"Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature.","title":"Statement Annotation"},{"location":"explore-and-author/building-a-customized-user-interface/#enable_1","text":"A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape. Used Path: shui:enableStatementLevelMetadata","title":"Enable"},{"location":"explore-and-author/building-a-customized-user-interface/#provided-shapes","text":"Instead of providing all possible statement annotation node shapes for the creation of new statement annotations, this property will limit the list to the selected shapes only. Used Path: shui:provideStatementLevelMetadataShapes","title":"Provided Shapes"},{"location":"explore-and-author/building-a-customized-user-interface/#datatypes","text":"This is a list of supported data types in shapes. Not all datatypes result in specific widgets.","title":"Datatypes"},{"location":"explore-and-author/building-a-customized-user-interface/#anyuri","text":"The -lexical space- of anyURI is finite-length character sequences which, when the algorithm defined in Section 5.4 of [XML Linking Language] is applied to them, result in strings which are legal URIs according to [RFC 2396], as amended by [RFC 2732]. Note: Spaces are, in principle, allowed in the -lexical space- of anyURI, however, their use is highly discouraged (unless they are encoded by %20). IRI: http://www.w3.org/2001/XMLSchema#anyURI","title":"anyURI"},{"location":"explore-and-author/building-a-customized-user-interface/#base64binary","text":"The lexical forms of base64Binary values are limited to the 65 characters of the Base64 Alphabet defined in [RFC 2045], i.e., a-z, A-Z, 0-9, the plus sign (+), the forward slash (/) and the equal sign (=), together with the characters defined in [XML 1.0 (Second Edition)] as white space. No other characters are allowed. IRI: http://www.w3.org/2001/XMLSchema#base64Binary","title":"base64Binary"},{"location":"explore-and-author/building-a-customized-user-interface/#boolean","text":"An instance of a datatype that is defined as -boolean- can have the following legal literals {true, false, 1, 0}. IRI: http://www.w3.org/2001/XMLSchema#boolean","title":"boolean"},{"location":"explore-and-author/building-a-customized-user-interface/#byte","text":"byte is -derived- from short by setting the value of -maxInclusive- to be 127 and -minInclusive- to be -128. byte has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126, +100. IRI: http://www.w3.org/2001/XMLSchema#byte","title":"byte"},{"location":"explore-and-author/building-a-customized-user-interface/#date","text":"The -lexical space- of date consists of finite-length sequences of characters of the form: \u2018-\u2018? yyyy \u2018-\u2018 mm \u2018-\u2018 dd zzzzzz? where the date and optional timezone are represented exactly the same way as they are for dateTime. The first moment of the interval is that represented by: \u2018-\u2018 yyyy \u2018-\u2018 mm \u2018-\u2018 dd \u2018T00:00:00\u2019 zzzzzz? and the least upper bound of the interval is the timeline point represented (noncanonically) by: \u2018-\u2018 yyyy \u2018-\u2018 mm \u2018-\u2018 dd \u2018T24:00:00\u2019 zzzzzz?. IRI: http://www.w3.org/2001/XMLSchema#date","title":"date"},{"location":"explore-and-author/building-a-customized-user-interface/#datetime","text":"The -lexical space- of dateTime consists of finite-length sequences of characters of the form: \u2018-\u2018? yyyy \u2018-\u2018 mm \u2018-\u2018 dd \u2018T\u2019 hh \u2018:\u2019 mm \u2018:\u2019 ss (\u2018.\u2019 s+)? (zzzzzz)? For example, 2002-10-10T12:00:00-05:00 (noon on 10 October 2002, Central Daylight Savings Time as well as Eastern Standard Time in the U.S.) is 2002-10-10T17:00:00Z, five hours later than 2002-10-10T12:00:00Z. IRI: http://www.w3.org/2001/XMLSchema#dateTime","title":"dateTime"},{"location":"explore-and-author/building-a-customized-user-interface/#datetimestamp","text":"The lexical space of dateTimeStamp consists of strings which are in the -lexical space- of dateTime and which also match the regular expression \u2018.*(Z|(+|-)[0-9][0-9]:[0-9][0-9])\u2019 IRI: http://www.w3.org/2001/XMLSchema#dateTimeStamp","title":"dateTimeStamp"},{"location":"explore-and-author/building-a-customized-user-interface/#decimal","text":"decimal has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) separated by a period as a decimal indicator. An optional leading sign is allowed. If the sign is omitted, \u2018+\u2019 is assumed. Leading and trailing zeroes are optional. If the fractional part is zero, the period and following zeroes can be omitted. For example: -1.23, 12678967.543233, +100000.00, 210. IRI: http://www.w3.org/2001/XMLSchema#decimal","title":"decimal"},{"location":"explore-and-author/building-a-customized-user-interface/#double","text":"double values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent -must- be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for double. IRI: http://www.w3.org/2001/XMLSchema#double","title":"double"},{"location":"explore-and-author/building-a-customized-user-interface/#duration","text":"The lexical representation for duration is the [ISO 8601] extended format PnYn MnDTnH nMnS, where nY represents the number of years, nM the number of months, nD the number of days, \u2018T\u2019 is the date/time separator, nH the number of hours, nM the number of minutes and nS the number of seconds. The number of seconds can include decimal digits to arbitrary precision. IRI: http://www.w3.org/2001/XMLSchema#duration","title":"duration"},{"location":"explore-and-author/building-a-customized-user-interface/#float","text":"float values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent -must- be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for float. IRI: http://www.w3.org/2001/XMLSchema#float","title":"float"},{"location":"explore-and-author/building-a-customized-user-interface/#gday","text":"The lexical representation for gDay is the left truncated lexical representation for date: \u2014DD . An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gDay","title":"gDay"},{"location":"explore-and-author/building-a-customized-user-interface/#gmonth","text":"The lexical representation for gMonth is the left and right truncated lexical representation for date: \u2013MM. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gMonth","title":"gMonth"},{"location":"explore-and-author/building-a-customized-user-interface/#gmonthday","text":"The lexical representation for gMonthDay is the left truncated lexical representation for date: \u2013MM-DD. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats (-D). This datatype can be used to represent a specific day in a month. To say, for example, that my birthday occurs on the 14th of September ever year. IRI: http://www.w3.org/2001/XMLSchema#gMonthDay","title":"gMonthDay"},{"location":"explore-and-author/building-a-customized-user-interface/#gyear","text":"The lexical representation for gYear is the reduced (right truncated) lexical representation for dateTime: CCYY. No left truncation is allowed. An optional following time zone qualifier is allowed as for dateTime. To accommodate year values outside the range from 0001 to 9999, additional digits can be added to the left of this representation and a preceding \u2018-\u2018 sign is allowed. For example, to indicate 1999, one would write: 1999. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gYear","title":"gYear"},{"location":"explore-and-author/building-a-customized-user-interface/#gyearmonth","text":"The lexical representation for gYearMonth is the reduced (right truncated) lexical representation for dateTime: CCYY-MM. No left truncation is allowed. An optional following time zone qualifier is allowed. To accommodate year values outside the range from 0001 to 9999, additional digits can be added to the left of this representation and a preceding \u2018-\u2018 sign is allowed. For example, to indicate the month of May 1999, one would write: 1999-05. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#gYearMonth","title":"gYearMonth"},{"location":"explore-and-author/building-a-customized-user-interface/#hexbinary","text":"hexBinary has a lexical representation where each binary octet is encoded as a character tuple, consisting of two hexadecimal digits ([0-9a-fA-F]) representing the octet code. For example, \u20180FB7\u2019 is a hex encoding for the 16-bit integer 4023 (whose binary representation is 111110110111). IRI: http://www.w3.org/2001/XMLSchema#hexBinary","title":"hexBinary"},{"location":"explore-and-author/building-a-customized-user-interface/#html","text":"The datatype of RDF literals storing fragments of HTML content IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML","title":"HTML"},{"location":"explore-and-author/building-a-customized-user-interface/#int","text":"int is -derived- from long by setting the value of -maxInclusive- to be 2147483647 and -minInclusive- to be -2147483648. int has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126789675, +100000. IRI: http://www.w3.org/2001/XMLSchema#int","title":"int"},{"location":"explore-and-author/building-a-customized-user-interface/#integer","text":"integer has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) with an optional leading sign. If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#integer","title":"integer"},{"location":"explore-and-author/building-a-customized-user-interface/#jinja-template-string-datatype","text":"Jinja is a modern and designer-friendly templating language for Python and other languages. IRI: https://vocab.eccenca.com/shui/jinja","title":"Jinja Template String (datatype)"},{"location":"explore-and-author/building-a-customized-user-interface/#langstring","text":"The datatype of language-tagged string values IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#langString","title":"langString"},{"location":"explore-and-author/building-a-customized-user-interface/#language","text":"language represents natural language identifiers as defined by by [RFC 3066] . The -value space- of language is the set of all strings that are valid language identifiers as defined [RFC 3066] . The -lexical space- of language is the set of all strings that conform to the pattern [a-zA-Z]{1,8}(-[a-zA-Z0-9]{1,8})* . The -base type- of language is token. IRI: http://www.w3.org/2001/XMLSchema#language","title":"language"},{"location":"explore-and-author/building-a-customized-user-interface/#long","text":"long is -derived- from integer by setting the value of -maxInclusive- to be 9223372036854775807 and -minInclusive- to be -9223372036854775808. long has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#long","title":"long"},{"location":"explore-and-author/building-a-customized-user-interface/#markdown","text":"In addition to rdf:HTML, this is the datatype RDF literals storing fragments of markdown content. IRI: http://ns.ontowiki.net/SysOnt/Markdown","title":"Markdown"},{"location":"explore-and-author/building-a-customized-user-interface/#name_2","text":"Name represents XML Names. The -value space- of Name is the set of all strings which -match- the Name production of [XML 1.0 (Second Edition)]. The -lexical space- of Name is the set of all strings which -match- the Name production of [XML 1.0 (Second Edition)]. The -base type- of Name is token. IRI: http://www.w3.org/2001/XMLSchema#Name","title":"Name"},{"location":"explore-and-author/building-a-customized-user-interface/#ncname","text":"NCName represents XML \u2018non-colonized\u2019 Names. The -value space- of NCName is the set of all strings which -match- the NCName production of [Namespaces in XML]. The -lexical space- of NCName is the set of all strings which -match- the NCName production of [Namespaces in XML]. The -base type- of NCName is Name. IRI: http://www.w3.org/2001/XMLSchema#NCName","title":"NCName"},{"location":"explore-and-author/building-a-customized-user-interface/#negativeinteger","text":"negativeInteger has a lexical representation consisting of a negative sign (\u2018-\u2018) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: -1, -12678967543233, -100000. IRI: http://www.w3.org/2001/XMLSchema#negativeInteger","title":"negativeInteger"},{"location":"explore-and-author/building-a-customized-user-interface/#nmtoken","text":"NMTOKEN represents the NMTOKEN attribute type from [XML 1.0 (Second Edition)]. The -value space- of NMTOKEN is the set of tokens that -match- the Nmtoken production in [XML 1.0 (Second Edition)]. The -lexical space- of NMTOKEN is the set of strings that -match- the Nmtoken production in [XML 1.0 (Second Edition)]. The -base type- of NMTOKEN is token. IRI: http://www.w3.org/2001/XMLSchema#NMTOKEN","title":"NMTOKEN"},{"location":"explore-and-author/building-a-customized-user-interface/#nonnegativeinteger","text":"nonNegativeInteger has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, the positive sign (\u2018+\u2019) is assumed. If the sign is present, it must be \u2018+\u2019 except for lexical forms denoting zero, which may be preceded by a positive (\u2018+\u2019) or a negative (\u2018-\u2018) sign. For example: 1, 0, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#nonNegativeInteger","title":"nonNegativeInteger"},{"location":"explore-and-author/building-a-customized-user-interface/#nonpositiveinteger","text":"nonPositiveInteger has a lexical representation consisting of an optional preceding sign followed by a finite-length sequence of decimal digits (#x30-#x39). The sign may be \u2018+\u2019 or may be omitted only for lexical forms denoting zero, in all other lexical forms, the negative sign (\u2018-\u2018) must be present. For example: -1, 0, -12678967543233, -100000. IRI: http://www.w3.org/2001/XMLSchema#nonPositiveInteger","title":"nonPositiveInteger"},{"location":"explore-and-author/building-a-customized-user-interface/#normalizedstring","text":"normalizedString represents white space normalized strings. The -value space- of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The -lexical space- of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The -base type- of normalizedString is string. IRI: http://www.w3.org/2001/XMLSchema#normalizedString","title":"normalizedString"},{"location":"explore-and-author/building-a-customized-user-interface/#plainliteral","text":"The class of plain (i.e. untyped) literal values, as used in RIF and OWL 2 IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#PlainLiteral","title":"PlainLiteral"},{"location":"explore-and-author/building-a-customized-user-interface/#positiveinteger","text":"positiveInteger has a lexical representation consisting of an optional positive sign (\u2018+\u2019) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: 1, 12678967543233, +100000. IRI: http://www.w3.org/2001/XMLSchema#positiveInteger","title":"positiveInteger"},{"location":"explore-and-author/building-a-customized-user-interface/#short","text":"short is -derived- from int by setting the value of -maxInclusive- to be 32767 and -minInclusive- to be -32768. short has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678, +10000. IRI: http://www.w3.org/2001/XMLSchema#short","title":"short"},{"location":"explore-and-author/building-a-customized-user-interface/#sparqloperation","text":"sparql operation datatype (query or update) IRI: https://vocab.eccenca.com/shui/sparqlOperation","title":"sparqlOperation"},{"location":"explore-and-author/building-a-customized-user-interface/#sparqlquery","text":"SPARQL 1.1 Query IRI: https://vocab.eccenca.com/shui/sparqlQuery","title":"sparqlQuery"},{"location":"explore-and-author/building-a-customized-user-interface/#sparqlupdate","text":"SPARQL 1.1 Update IRI: https://vocab.eccenca.com/shui/sparqlUpdate","title":"sparqlUpdate"},{"location":"explore-and-author/building-a-customized-user-interface/#string","text":"The string datatype represents character strings in XML. The -value space- of string is the set of finite-length sequences of characters (as defined in [XML 1.0 (Second Edition)]) that -match- the Char production from [XML 1.0 (Second Edition)]. A character is an atomic unit of communication, it is not further specified except to note that every character has a corresponding Universal Character Set code point, which is an integer. IRI: http://www.w3.org/2001/XMLSchema#string","title":"string"},{"location":"explore-and-author/building-a-customized-user-interface/#time","text":"The lexical representation for time is the left truncated lexical representation for dateTime: hh ss.sss with optional following time zone indicator. For example, to indicate 1:20 pm for Eastern Standard Time which is 5 hours behind Coordinated Universal Time (UTC), one would write: 13:20:00-05:00. See also ISO 8601 Date and Time Formats (-D). IRI: http://www.w3.org/2001/XMLSchema#time","title":"time"},{"location":"explore-and-author/building-a-customized-user-interface/#token","text":"token represents tokenized strings. The -value space- of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The -lexical space- of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The -base type- of token is normalizedString. IRI: http://www.w3.org/2001/XMLSchema#token","title":"token"},{"location":"explore-and-author/building-a-customized-user-interface/#unsignedbyte","text":"unsignedByte is -derived- from unsignedShort by setting the value of -maxInclusive- to be 255. unsignedByte has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 126, 100. IRI: http://www.w3.org/2001/XMLSchema#unsignedByte","title":"unsignedByte"},{"location":"explore-and-author/building-a-customized-user-interface/#unsignedint","text":"unsignedInt is -derived- from unsignedLong by setting the value of -maxInclusive- to be 4294967295. unsignedInt has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 1267896754, 100000. IRI: http://www.w3.org/2001/XMLSchema#unsignedInt","title":"unsignedInt"},{"location":"explore-and-author/building-a-customized-user-interface/#unsignedlong","text":"unsignedLong is -derived- from nonNegativeInteger by setting the value of -maxInclusive- to be 18446744073709551615. unsignedLong has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678967543233, 100000. IRI: http://www.w3.org/2001/XMLSchema#unsignedLong","title":"unsignedLong"},{"location":"explore-and-author/building-a-customized-user-interface/#unsignedshort","text":"unsignedShort is -derived- from unsignedInt by setting the value of -maxInclusive- to be 65535. unsignedShort has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678, 10000. IRI: http://www.w3.org/2001/XMLSchema#unsignedShort","title":"unsignedShort"},{"location":"explore-and-author/building-a-customized-user-interface/#xmlliteral","text":"The datatype of XML literal values. IRI: http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral","title":"XMLLiteral"},{"location":"explore-and-author/building-a-customized-user-interface/#using-forms","text":"Once a Node Shape is created for a specific class, you are able to use the specified entry form in the Explore component of Corporate Memory.","title":"Using forms"},{"location":"explore-and-author/building-a-customized-user-interface/#editing-existing-resources","text":"While browsing your knowledge graph, you will always see your shape in action, when you click on a resource which is an instance of the class which is linked with shacl:targetClass from your Node Shape. The next images demonstrate this behavior :","title":"Editing existing resources"},{"location":"explore-and-author/building-a-customized-user-interface/#creating-new-resources","text":"You can also create new resources by using a shaped form. One way to achieve this, is to select the class in the navigation tree on the lower left part in the Explore component and then click the Floating Action Button at the bottom or use the context menu on upper right side. The next images demonstrate this behaviour:","title":"Creating new resources"},{"location":"explore-and-author/embedding-services-via-the-integrations-module/","text":"Embedding Services via the Integrations Module \u00a4 A DataManager module is available that can be used to embed / integrate other web-services in Corporate Memory. The module can be used and configured globally or individually per workspace configuration. Activation and configuration in DataManager \u00a4 In order to use it you need to add respective configuration section(s) into the DataManager application.yml configuration file. This is a sample for a global configuration that enables the module and shows two services: js.config.modules.integrations : name : \"INTEGRATIONS\" enable : true tabs : - name : \"Service One\" url : \"https://one.eccenca.com/service-one\" - name : \"Service Two\" url : \"https://two.eccenca.com/service-two\" The name properties can be customized. Important is that the module is enabled (\u201c enable: true \u201d) in at least one place (globally or in a certain workspace) in order to be shown. Warning In case your Corporate Memory Instance is served via HTTPS, no HTTP services can be used due to browser security limitations. You can redefine all or parts of the configuration per workspace, e.g. in order to disable the module in a specific workspace add \u201c modules.integrations.enable: false \u201d to the configuration of the respective workspace. A restart of DataManager will be required in order for the configuration change to become effective. Link Configuration in DataIntegration \u00a4 The (module) link configuration in DataIntegration is managed in its own configuration. Thus, the following snippet from a dataintegration.conf shows how to add the \u201cINTEGRATINOS\u201d link to the DataIntegrations menu: eccencaDataManager . moduleLinks = [ { path = \"explore\" defaultLabel = \"Knowledge Graphs\" icon = \"application-explore\" }, { path = \"vocab\" defaultLabel = \"Vocabularies\" icon = \"application-vocabularies\" }, { path = \"thesaurus\" defaultLabel = \"Thesauri\" icon = \"module-thesauri\" }, { path = \"query\" defaultLabel = \"Queries\" icon = \"application-queries\" }, { path = \"integrations\" defaultLabel = \"INTEGRATIONS\" icon = \"module-integrations\" } ] Warning The \u201c name\" and \u201c defaultLabel \u201d property should be aligned in the DataManager and DataIntegration configuration for consistency. A restart of DataIntegration will be required in order for the configuration change to become effective. (Redash) Dashboard Integration \u00a4 A typical (eccenca) use case for the Integrations Module is to embed redash dashboards. In order show a dashboard in a Corporate Memory make sure your redash instance use the same protocol as your Corporate Memory instance (typically https). Then open the dashboard that should be embedded and click the sharing button . In the dialog make sure \u201c Allow public access \u201d is enabled. Copy the \u201c Secret address \u201d and paste this address into the \u201c url \u201d property of a tab configuration, as shown above.","title":"Embedding Services via the Integrations Module"},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#embedding-services-via-the-integrations-module","text":"A DataManager module is available that can be used to embed / integrate other web-services in Corporate Memory. The module can be used and configured globally or individually per workspace configuration.","title":"Embedding Services via the Integrations Module"},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#activation-and-configuration-in-datamanager","text":"In order to use it you need to add respective configuration section(s) into the DataManager application.yml configuration file. This is a sample for a global configuration that enables the module and shows two services: js.config.modules.integrations : name : \"INTEGRATIONS\" enable : true tabs : - name : \"Service One\" url : \"https://one.eccenca.com/service-one\" - name : \"Service Two\" url : \"https://two.eccenca.com/service-two\" The name properties can be customized. Important is that the module is enabled (\u201c enable: true \u201d) in at least one place (globally or in a certain workspace) in order to be shown. Warning In case your Corporate Memory Instance is served via HTTPS, no HTTP services can be used due to browser security limitations. You can redefine all or parts of the configuration per workspace, e.g. in order to disable the module in a specific workspace add \u201c modules.integrations.enable: false \u201d to the configuration of the respective workspace. A restart of DataManager will be required in order for the configuration change to become effective.","title":"Activation and configuration in DataManager"},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#link-configuration-in-dataintegration","text":"The (module) link configuration in DataIntegration is managed in its own configuration. Thus, the following snippet from a dataintegration.conf shows how to add the \u201cINTEGRATINOS\u201d link to the DataIntegrations menu: eccencaDataManager . moduleLinks = [ { path = \"explore\" defaultLabel = \"Knowledge Graphs\" icon = \"application-explore\" }, { path = \"vocab\" defaultLabel = \"Vocabularies\" icon = \"application-vocabularies\" }, { path = \"thesaurus\" defaultLabel = \"Thesauri\" icon = \"module-thesauri\" }, { path = \"query\" defaultLabel = \"Queries\" icon = \"application-queries\" }, { path = \"integrations\" defaultLabel = \"INTEGRATIONS\" icon = \"module-integrations\" } ] Warning The \u201c name\" and \u201c defaultLabel \u201d property should be aligned in the DataManager and DataIntegration configuration for consistency. A restart of DataIntegration will be required in order for the configuration change to become effective.","title":"Link Configuration in DataIntegration"},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#redash-dashboard-integration","text":"A typical (eccenca) use case for the Integrations Module is to embed redash dashboards. In order show a dashboard in a Corporate Memory make sure your redash instance use the same protocol as your Corporate Memory instance (typically https). Then open the dashboard that should be embedded and click the sharing button . In the dialog make sure \u201c Allow public access \u201d is enabled. Copy the \u201c Secret address \u201d and paste this address into the \u201c url \u201d property of a tab configuration, as shown above.","title":"(Redash) Dashboard Integration"},{"location":"explore-and-author/graph-exploration/","text":"Graph Exploration \u00a4 Introduction \u00a4 The Explore module provides a generic and extensible RDF data browser and editor. Use the Explore module to browse through your resources, to change between list and detail views and to edit resources. To open the Explore module, click EXPLORE in the Module bar. The user interface of the Explore module shows the following elements: Graphs box Navigation box Main window The main window provides multiple views depending on what resource has been selected. If necessary, you can hide the Graphs and Navigation boxes: To hide the boxes, click in the upper left corner. To show the boxes again, click . Graphs \u00a4 The Graphs box shows the list of graphs you have access to. A Lock icon indicates that you have only read access to this graph and are not allowed to edit data of this graph. To select a graph click the graph name in the Graphs box. The structure of the selected graph is displayed in the Navigation box below. On the right window, the Metadata view of the selected graph appears showing several tabs with metadata information. The Graphs are categorized into groups as follows: System : List all the system graphs that are available by default. In this you can search only system specific graphs. User: List all the graphs created by users. Vocabularies : List all the graph that belongs Vocabularies All Adding a new graph \u00a4 To add a new graph to the Graphs list: In the Graphs box, click Add graph Click Add new graph . A dialog box appears. Enter the graph URI (e.g. https://ns.eccenca.com ) for the new graph. Optional : Click Choose file to upload a file containing the graph data. You can upload one of the following file formats: Turtle, N-Triples, RDF/XML, or JSON-LD. Click ADD GRAPH to save the new graph. Note: When adding a new graph you can enter only a URI and skip the upload step. You can upload a file at a later date using the Managing a graph option. Downloading a graph \u00a4 To download a graph from the Graphs list: In the Graphs box, select the graph you want to download. In the Graphs box, click Download graph icon. Click Download graph . A message box appears, stating that downloading can take a long time. Click DOWNLOAD . Managing a graph \u00a4 Use this function to add updated data to a graph, to replace data or to delete a graph. To update or replace data of a graph: In the Graphs box, select the graph you want to update or replace. In the Graphs box, click Manage graph icon. Click Manage graph . A dialog box appears. Click Choose file to upload a file containing the new or updated data. You can upload one of the following file formats: Turtle, N-Triples, RDF/XML, or JSON-LD. Choose one of the following options: Update : add uploaded data to Graph. Replace : clear Graph and add uploaded data. Click UPDATE to save your changes. To delete the graph click the Delete icon and confirm or cancel the deletion process. Navigation \u00a4 When a graph is selected in the Graphs box the structure of the graph is displayed in the Navigation box. By default, only the top classes of the graph are listed. An arrow indicates that a class has subclasses. Click the arrow to show the subclasses. Use the Search field of the Navigation box to search for a keyword in the navigation structure of the graph. Enter a keyword and press Enter to start the search. To reset the results delete the keyword and press Enter. Instance List of a class \u00a4 Select a class in the Navigation box to show all instances of this class in the Instance List on the right window. Click to change the shown resources via the defined SHACL-shape form. Alternatively, click Properties to add or modify the properties. Custom Instance List \u00a4 The table uses a default query to list all resources with a given class. For more complex representations, it is possible to customize the view adding a triple of the form < ShaclShapeURI > < https :// vocab.eccenca.com / shui / navigationListQuery > < sparqlQueryURI > . The sparqlQueryURI has a queryText which can contain the following placeholders: {{FROM}}. For example, having the following shape associated to PropertyShape: < https :// vocab.eccenca.com / shacl / ShaclPropertyShapeNodeShape > < http :// www.w3.org / ns / shacl #targetClass> < http :// www.w3.org / ns / shacl #PropertyShape> . and adding the following triples: < https :// vocab.eccenca.com / shacl / ShaclPropertyShapeNodeShape > < https :// vocab.eccenca.com / shui / navigationListQuery > < https :// vocab.eccenca.com / shacl / shaclPropertyShapeListQuery > . < https :// vocab.eccenca.com / shacl / shaclPropertyShapeListQuery > a < https :// vocab.eccenca.com / shui / SparqlQuery > ; < http :// www.w3.org / 2000 / 01 / rdf - schema #label> \"Shape Shapes: UI query for listing property shapes\" ; < https :// vocab.eccenca.com / shui / queryText > \"\"\" PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX shacl: <http://www.w3.org/ns/shacl#> PREFIX shui: <https://vocab.eccenca.com/shui/> SELECT DISTINCT ?propertyShape ?path ?description {{FROM}} WHERE { ?propertyShape a shacl:PropertyShape . OPTIONAL { ?propertyShape shacl:path ?path } . OPTIONAL { ?propertyShape shacl:description ?description } . FILTER isIRI(?propertyShape) }\"\"\" ; < https :// vocab.eccenca.com / shui / queryType > \"SELECT\" . will result in an custom Instance List for the class <http://www.w3.org/ns/shacl#PropertyShape> : Instance Details \u00a4 To open the Instance Details of a resource click on that resource in the Instance List. Resources are shown as grey buttons. Use on the right upper corner to remove the resource. A dialog box appears where you are asked to confirm the operation. Info When you remove the resource all triples related to that resource are deleted too. The instance data is provided on the following tabs: Metadata of graphs \u00a4 To display metadata of a graph, select a graph in the Graph box on the left side. In the main window you see the metadata arranged in the following tabs: Resource Properties Statistics Graph Vocab References Turtle Usage Info The Usage tab is only displayed when it contain data. Resource \u00a4 The Resource tab provides the following metadata of a graph: Label: Name of the graph Abstract: Description of the graph Type: Allows you to set type of graph Authors: Lists the authors of the graph Publisher: Shows the publisher of the graph Current Version: Current version number of the graph Date of Last Modification: Date when the graph was last modified Preferred Namespace: Preferred namespace and URI, like https://vocab.eccenca.com/dsm/ Preferred namespace prefix: is used as a (short)reference in SPARQL queries Namespaces: Lists the namespaces used by the graph External Links: Links to internet resources: homepages, pages, SPARQL endpoints or data dumps Creation Date: Creation date of the graph Issued: Publication date of the graph Publisher: Publisher of a graph The property Namespaces shows only those namespaces that are specified by the respective property. But it is possible that the graph actually uses more namespaces than indicated. Note: The descriptions given comply with the intended usage of the respective properties. But in practice these properties are often used with a wrong content. Therefore in some cases the description given may not correspond to the actually displayed metadata of a graph. Properties \u00a4 This tab shows all properties and objects of the selected resource. Use the icons on the right side to edit or delete properties. Use SHOW IN LIST to display objects in a list view. Click ADD to add a new value as an object to a property. In the dialog box, select the type from the drop-down list and enter the value. Click SAVE to save your changes. To add a new property click . In the dialog box, enter a property, select the value type from the drop-down list and enter a value. Click SAVE to save your changes. Statistics \u00a4 The Statistics tab indicates the number of classes, properties, entities and triples of the graph. To show these values the application uses by default the VoID Statistics contained in the graph. Use Update VoID statistics to update the values with a new query. Depending on the size of the graph this operation can take some time. Use Save to save updated VoID statistics in the graph. The Save button can only be used when an update query was executed before or when the option auto fetch VoID Statistics is activated. Saving VoID statistics does not modify the property Date of Last Modification. In order to display the metadata of a graph various properties are analysed that are contained in the graph itself. If these properties are missing in the graph, some metadata properties may remain empty. These empty properties are hidden by default. Info If you leave this module or even the view, this option is reset to the default setting. Info The values given in VoID Statistics can be obsolete. If the graph does not contain the required VoID Statistics no value is displayed. Graph \u00a4 The Graph tab shows a visual graph representation of the ontology concepts in a graph selected in the Graphs box. It displays ontology classes as circle-shaped nodes, which are connected by directed, labelled edges (arrows) representing properties. Classes defined by the ontology are colored light blue. External, referenced classes are colored dark blue and have the additional label \u201c(external)\u201d. You can zoom in and out of the graph using your mouse wheel or \u201cpinching\u201d, if you have a touch screen or touch pad. You can drag the canvas by left-clicking the canvas, holding the click and moving the mouse. Vocab \u00a4 This tab displays graph visualization of installed vocabularies. It displays all classes showing the class-subclass. You can open the class details and view list of instances related to that class. It also allows you to copy resource IRI. References \u00a4 This tab shows all properties that use the selected resource as an object. Turtle \u00a4 This tab shows the turtle RDF representation of the raw data representing the resource. You can use this tab to edit the selected resource: Enter your changes in turtle. Add a changelog comment (optional). Click UPDATE to save your changes. Deleting the entire turtle representation deletes the resource.","title":"Graph Exploration"},{"location":"explore-and-author/graph-exploration/#graph-exploration","text":"","title":"Graph Exploration"},{"location":"explore-and-author/graph-exploration/#introduction","text":"The Explore module provides a generic and extensible RDF data browser and editor. Use the Explore module to browse through your resources, to change between list and detail views and to edit resources. To open the Explore module, click EXPLORE in the Module bar. The user interface of the Explore module shows the following elements: Graphs box Navigation box Main window The main window provides multiple views depending on what resource has been selected. If necessary, you can hide the Graphs and Navigation boxes: To hide the boxes, click in the upper left corner. To show the boxes again, click .","title":"Introduction"},{"location":"explore-and-author/graph-exploration/#graphs","text":"The Graphs box shows the list of graphs you have access to. A Lock icon indicates that you have only read access to this graph and are not allowed to edit data of this graph. To select a graph click the graph name in the Graphs box. The structure of the selected graph is displayed in the Navigation box below. On the right window, the Metadata view of the selected graph appears showing several tabs with metadata information. The Graphs are categorized into groups as follows: System : List all the system graphs that are available by default. In this you can search only system specific graphs. User: List all the graphs created by users. Vocabularies : List all the graph that belongs Vocabularies All","title":"Graphs"},{"location":"explore-and-author/graph-exploration/#adding-a-new-graph","text":"To add a new graph to the Graphs list: In the Graphs box, click Add graph Click Add new graph . A dialog box appears. Enter the graph URI (e.g. https://ns.eccenca.com ) for the new graph. Optional : Click Choose file to upload a file containing the graph data. You can upload one of the following file formats: Turtle, N-Triples, RDF/XML, or JSON-LD. Click ADD GRAPH to save the new graph. Note: When adding a new graph you can enter only a URI and skip the upload step. You can upload a file at a later date using the Managing a graph option.","title":"Adding a new graph"},{"location":"explore-and-author/graph-exploration/#downloading-a-graph","text":"To download a graph from the Graphs list: In the Graphs box, select the graph you want to download. In the Graphs box, click Download graph icon. Click Download graph . A message box appears, stating that downloading can take a long time. Click DOWNLOAD .","title":"Downloading a graph"},{"location":"explore-and-author/graph-exploration/#managing-a-graph","text":"Use this function to add updated data to a graph, to replace data or to delete a graph. To update or replace data of a graph: In the Graphs box, select the graph you want to update or replace. In the Graphs box, click Manage graph icon. Click Manage graph . A dialog box appears. Click Choose file to upload a file containing the new or updated data. You can upload one of the following file formats: Turtle, N-Triples, RDF/XML, or JSON-LD. Choose one of the following options: Update : add uploaded data to Graph. Replace : clear Graph and add uploaded data. Click UPDATE to save your changes. To delete the graph click the Delete icon and confirm or cancel the deletion process.","title":"Managing a graph"},{"location":"explore-and-author/graph-exploration/#navigation","text":"When a graph is selected in the Graphs box the structure of the graph is displayed in the Navigation box. By default, only the top classes of the graph are listed. An arrow indicates that a class has subclasses. Click the arrow to show the subclasses. Use the Search field of the Navigation box to search for a keyword in the navigation structure of the graph. Enter a keyword and press Enter to start the search. To reset the results delete the keyword and press Enter.","title":"Navigation"},{"location":"explore-and-author/graph-exploration/#instance-list-of-a-class","text":"Select a class in the Navigation box to show all instances of this class in the Instance List on the right window. Click to change the shown resources via the defined SHACL-shape form. Alternatively, click Properties to add or modify the properties.","title":"Instance List of a class"},{"location":"explore-and-author/graph-exploration/#custom-instance-list","text":"The table uses a default query to list all resources with a given class. For more complex representations, it is possible to customize the view adding a triple of the form < ShaclShapeURI > < https :// vocab.eccenca.com / shui / navigationListQuery > < sparqlQueryURI > . The sparqlQueryURI has a queryText which can contain the following placeholders: {{FROM}}. For example, having the following shape associated to PropertyShape: < https :// vocab.eccenca.com / shacl / ShaclPropertyShapeNodeShape > < http :// www.w3.org / ns / shacl #targetClass> < http :// www.w3.org / ns / shacl #PropertyShape> . and adding the following triples: < https :// vocab.eccenca.com / shacl / ShaclPropertyShapeNodeShape > < https :// vocab.eccenca.com / shui / navigationListQuery > < https :// vocab.eccenca.com / shacl / shaclPropertyShapeListQuery > . < https :// vocab.eccenca.com / shacl / shaclPropertyShapeListQuery > a < https :// vocab.eccenca.com / shui / SparqlQuery > ; < http :// www.w3.org / 2000 / 01 / rdf - schema #label> \"Shape Shapes: UI query for listing property shapes\" ; < https :// vocab.eccenca.com / shui / queryText > \"\"\" PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX shacl: <http://www.w3.org/ns/shacl#> PREFIX shui: <https://vocab.eccenca.com/shui/> SELECT DISTINCT ?propertyShape ?path ?description {{FROM}} WHERE { ?propertyShape a shacl:PropertyShape . OPTIONAL { ?propertyShape shacl:path ?path } . OPTIONAL { ?propertyShape shacl:description ?description } . FILTER isIRI(?propertyShape) }\"\"\" ; < https :// vocab.eccenca.com / shui / queryType > \"SELECT\" . will result in an custom Instance List for the class <http://www.w3.org/ns/shacl#PropertyShape> :","title":"Custom Instance List"},{"location":"explore-and-author/graph-exploration/#instance-details","text":"To open the Instance Details of a resource click on that resource in the Instance List. Resources are shown as grey buttons. Use on the right upper corner to remove the resource. A dialog box appears where you are asked to confirm the operation. Info When you remove the resource all triples related to that resource are deleted too. The instance data is provided on the following tabs:","title":"Instance Details"},{"location":"explore-and-author/graph-exploration/#metadata-of-graphs","text":"To display metadata of a graph, select a graph in the Graph box on the left side. In the main window you see the metadata arranged in the following tabs: Resource Properties Statistics Graph Vocab References Turtle Usage Info The Usage tab is only displayed when it contain data.","title":"Metadata of graphs"},{"location":"explore-and-author/graph-exploration/#resource","text":"The Resource tab provides the following metadata of a graph: Label: Name of the graph Abstract: Description of the graph Type: Allows you to set type of graph Authors: Lists the authors of the graph Publisher: Shows the publisher of the graph Current Version: Current version number of the graph Date of Last Modification: Date when the graph was last modified Preferred Namespace: Preferred namespace and URI, like https://vocab.eccenca.com/dsm/ Preferred namespace prefix: is used as a (short)reference in SPARQL queries Namespaces: Lists the namespaces used by the graph External Links: Links to internet resources: homepages, pages, SPARQL endpoints or data dumps Creation Date: Creation date of the graph Issued: Publication date of the graph Publisher: Publisher of a graph The property Namespaces shows only those namespaces that are specified by the respective property. But it is possible that the graph actually uses more namespaces than indicated. Note: The descriptions given comply with the intended usage of the respective properties. But in practice these properties are often used with a wrong content. Therefore in some cases the description given may not correspond to the actually displayed metadata of a graph.","title":"Resource"},{"location":"explore-and-author/graph-exploration/#properties","text":"This tab shows all properties and objects of the selected resource. Use the icons on the right side to edit or delete properties. Use SHOW IN LIST to display objects in a list view. Click ADD to add a new value as an object to a property. In the dialog box, select the type from the drop-down list and enter the value. Click SAVE to save your changes. To add a new property click . In the dialog box, enter a property, select the value type from the drop-down list and enter a value. Click SAVE to save your changes.","title":"Properties"},{"location":"explore-and-author/graph-exploration/#statistics","text":"The Statistics tab indicates the number of classes, properties, entities and triples of the graph. To show these values the application uses by default the VoID Statistics contained in the graph. Use Update VoID statistics to update the values with a new query. Depending on the size of the graph this operation can take some time. Use Save to save updated VoID statistics in the graph. The Save button can only be used when an update query was executed before or when the option auto fetch VoID Statistics is activated. Saving VoID statistics does not modify the property Date of Last Modification. In order to display the metadata of a graph various properties are analysed that are contained in the graph itself. If these properties are missing in the graph, some metadata properties may remain empty. These empty properties are hidden by default. Info If you leave this module or even the view, this option is reset to the default setting. Info The values given in VoID Statistics can be obsolete. If the graph does not contain the required VoID Statistics no value is displayed.","title":"Statistics"},{"location":"explore-and-author/graph-exploration/#graph","text":"The Graph tab shows a visual graph representation of the ontology concepts in a graph selected in the Graphs box. It displays ontology classes as circle-shaped nodes, which are connected by directed, labelled edges (arrows) representing properties. Classes defined by the ontology are colored light blue. External, referenced classes are colored dark blue and have the additional label \u201c(external)\u201d. You can zoom in and out of the graph using your mouse wheel or \u201cpinching\u201d, if you have a touch screen or touch pad. You can drag the canvas by left-clicking the canvas, holding the click and moving the mouse.","title":"Graph"},{"location":"explore-and-author/graph-exploration/#vocab","text":"This tab displays graph visualization of installed vocabularies. It displays all classes showing the class-subclass. You can open the class details and view list of instances related to that class. It also allows you to copy resource IRI.","title":"Vocab"},{"location":"explore-and-author/graph-exploration/#references","text":"This tab shows all properties that use the selected resource as an object.","title":"References"},{"location":"explore-and-author/graph-exploration/#turtle","text":"This tab shows the turtle RDF representation of the raw data representing the resource. You can use this tab to edit the selected resource: Enter your changes in turtle. Add a changelog comment (optional). Click UPDATE to save your changes. Deleting the entire turtle representation deletes the resource.","title":"Turtle"},{"location":"explore-and-author/query-module/","text":"Query Module \u00a4 Introduction \u00a4 The Query module provides a user interface to store, describe, search and edit SPARQL queries.The queries are evaluated on the Knowledge Graph (quad store) provide a way to granularly aggregate semantic data as tables. These tables can then be exported as CSV, Excel or JSON. The Query module features three tabs Catalog , Query and Info . To access the module click QUERY in the Module bar. Query catalog \u00a4 The Catalog tab is the default view of the Query module. It lists all existing SPARQL queries including their metadata, such as name, type, a human readable description as well as creation and modification dates. To create a new query in the catalog, use the button on the lower right. Click this button to create a new query in the Query tab using SPARQL (SPARQL Protocol And RDF Query Language - see https://www.w3.org/TR/rdf-sparql-query/ for further information). Use the Search bar in order to filter for specific queries. Select the query from the Queries catalog, to open and load the query. To delete an existing query, click . Info Deleting a query cannot be undone. Query tab \u00a4 Use the Query tab to edit and execute SPARQL queries. In this view you can also browse query result previews and download full query results as CSV files. The main feature of the Query tab is the query editor, which provides an interface where you can write and edit your SPARQL queries. The query editor features SPARQL syntax highlighting and SPARQL validation, allowing only syntactically correct SPARQL queries to be executed. The Query editor allows to Run query, Download Results, Delete, Save and Save as Queries. Running a query and exporting results \u00a4 Click RUN QUERY to execute the query and to display a preview of the query results under the query editor. The results are presented as a table with pagination. To export the full set of results without any limits in form of a CSV file click Download as\u2026 on the top right. Info The preview result ordering has no impact on the result ordering in the exported file. If you want to export some ordered query results, you need to use the ORDER BY construct in the SPARQL query itself. Saving a query \u00a4 To save a query in the Query catalog click SAVE QUERY. If you have loaded the current query from the Query catalog and edited it, clicking SAVE QUERY opens a dialog that allows you to either overwrite the existing query or store it as a new query. If you have created the current query from scratch, the query will immediately be saved to the Query catalog. After saving a query, the Info tab is opened in edit mode. When you cancel the operation at this point only editing the metadata is canceled, not saving the query to the Query catalog. Using placeholders in queries \u00a4 In addition to the standard SPARQL syntax, placeholders can be used to parametrize a query. Placeholders are indicated in the query using a string of the form {{placeholdername}} . Multiple placeholders can be defined by changing the name inside the brackets. When a query contains a placeholder, the placeholder list to the right of the query editor shows a field with its name. When running a query that contains placeholders, the query editor replaces the {{placeholdername}} string in the query with the respective string entered into the placeholder list. This is a direct string replacement, so placeholders can contain simple strings and literal values, URIs, variables or even sub queries. Running a query with a placeholder is only possible when all placeholder fields in the placeholder list have been filled. A typical use case is restricting a query to a specific class of objects stated by a placeholder: SELECT * WHERE { ? classInstance a < http : // dbpedia . org / ontology / {{ class }} > . } This query selects all instances of a specific DBpedia Ontology class. When you enter Person into the class placeholder field in the placeholder list the following query is executed: SELECT * WHERE { ? classInstance a < http : // dbpedia . org / ontology / Person > . } SQL This feature allows for easy parametrization without having to know correct SPARQL syntax or URIs.","title":"Query Module"},{"location":"explore-and-author/query-module/#query-module","text":"","title":"Query Module"},{"location":"explore-and-author/query-module/#introduction","text":"The Query module provides a user interface to store, describe, search and edit SPARQL queries.The queries are evaluated on the Knowledge Graph (quad store) provide a way to granularly aggregate semantic data as tables. These tables can then be exported as CSV, Excel or JSON. The Query module features three tabs Catalog , Query and Info . To access the module click QUERY in the Module bar.","title":"Introduction"},{"location":"explore-and-author/query-module/#query-catalog","text":"The Catalog tab is the default view of the Query module. It lists all existing SPARQL queries including their metadata, such as name, type, a human readable description as well as creation and modification dates. To create a new query in the catalog, use the button on the lower right. Click this button to create a new query in the Query tab using SPARQL (SPARQL Protocol And RDF Query Language - see https://www.w3.org/TR/rdf-sparql-query/ for further information). Use the Search bar in order to filter for specific queries. Select the query from the Queries catalog, to open and load the query. To delete an existing query, click . Info Deleting a query cannot be undone.","title":"Query catalog"},{"location":"explore-and-author/query-module/#query-tab","text":"Use the Query tab to edit and execute SPARQL queries. In this view you can also browse query result previews and download full query results as CSV files. The main feature of the Query tab is the query editor, which provides an interface where you can write and edit your SPARQL queries. The query editor features SPARQL syntax highlighting and SPARQL validation, allowing only syntactically correct SPARQL queries to be executed. The Query editor allows to Run query, Download Results, Delete, Save and Save as Queries.","title":"Query tab"},{"location":"explore-and-author/query-module/#running-a-query-and-exporting-results","text":"Click RUN QUERY to execute the query and to display a preview of the query results under the query editor. The results are presented as a table with pagination. To export the full set of results without any limits in form of a CSV file click Download as\u2026 on the top right. Info The preview result ordering has no impact on the result ordering in the exported file. If you want to export some ordered query results, you need to use the ORDER BY construct in the SPARQL query itself.","title":"Running a query and exporting results"},{"location":"explore-and-author/query-module/#saving-a-query","text":"To save a query in the Query catalog click SAVE QUERY. If you have loaded the current query from the Query catalog and edited it, clicking SAVE QUERY opens a dialog that allows you to either overwrite the existing query or store it as a new query. If you have created the current query from scratch, the query will immediately be saved to the Query catalog. After saving a query, the Info tab is opened in edit mode. When you cancel the operation at this point only editing the metadata is canceled, not saving the query to the Query catalog.","title":"Saving a query"},{"location":"explore-and-author/query-module/#using-placeholders-in-queries","text":"In addition to the standard SPARQL syntax, placeholders can be used to parametrize a query. Placeholders are indicated in the query using a string of the form {{placeholdername}} . Multiple placeholders can be defined by changing the name inside the brackets. When a query contains a placeholder, the placeholder list to the right of the query editor shows a field with its name. When running a query that contains placeholders, the query editor replaces the {{placeholdername}} string in the query with the respective string entered into the placeholder list. This is a direct string replacement, so placeholders can contain simple strings and literal values, URIs, variables or even sub queries. Running a query with a placeholder is only possible when all placeholder fields in the placeholder list have been filled. A typical use case is restricting a query to a specific class of objects stated by a placeholder: SELECT * WHERE { ? classInstance a < http : // dbpedia . org / ontology / {{ class }} > . } This query selects all instances of a specific DBpedia Ontology class. When you enter Person into the class placeholder field in the placeholder list the following query is executed: SELECT * WHERE { ? classInstance a < http : // dbpedia . org / ontology / Person > . } SQL This feature allows for easy parametrization without having to know correct SPARQL syntax or URIs.","title":"Using placeholders in queries"},{"location":"explore-and-author/statement-annotations/","text":"Statement Annotations \u00a4 Introduction \u00a4 Statement Annotations provide a way to express knowledge about statements. Typical use cases for Statement Annotations include: the temporal validity of information, the origin of information, or just a way to annotate a specific statement with a human readable comment. Usage \u00a4 If enabled on a specific type of statement or type of resource, you see a Statement Annotation text bubble beside every annotatable statement: This bubble has different statuses: A empty text bubble indicates, that there is no annotation on the statement, but the annotation feature is enabled for this statement. A filled text bubble indicates, that there is at least one annotation on the statement. No bubble indicates, that the annotation feature is NOT enabled on this type of statement. Clicking on one of the flags opens the Statement Annotation dialog for this specific statement: In the Statement Annotation dialog, you can select the Statement Annotation Template and click Create. Setup In order to have a working Statement Annotation setup, the following steps need to be done: 1. Create a Statement Annotation Graph Create a new Graph, edit its metadata and change the type to Statement Annotation Graph. 2. Setup and import the Statement Annotation Graph in your data graph In your data graph, where the resources exist which you want to annotate, import the Statement Annotation Graph and select it as an Annotation Graph. 3. Create a shaped form which will be used to annotate statements In your Shape Catalog, select a Node Shape (or create one) which you want to use for statement annotations, and Enable Statement Annotation to true. 4. Allow statement annotations in your shaped forms on specific Classes or Properties Finally, select the Node Shape or Property Shape from your Shape Catalog, and enable annotations by setting the Enable option in the Statement Annotations group to true. This will enable the feature on the statements of all resources shown with this Node Shape or on all statements shown with this Property Shape. Technical Background \u00a4 From the technical point of view, the Statement Annotation feature uses RDF Reification to annotate Statements (Triples) with additional background information. Statement resources can be annotated with custom Annotation Resources. These Annotation Resources are based on specific Shapes which are enabled as Statement Annotation shapes. Reification Resources as well as Annotation Resources are managed in a Statement Annotation Graph, which need to be configured on a Graph as well as imported to this Graph. The following illustration depicts this schema with boxes and arrows: Some notes on this: There is one Statement Reification Resource per Statement Annotation. Removing the Statement Annotation also removes the Statement Reification Resource. All annotation triples (8 triples in the image) are created in the Statement Annotation Graph, so Step 2 of the setup procedure is important. Querying Statement Annotations \u00a4 In order to automate access to Statement Annotations, you can query them with SPARQL e.g. via cmemc or the API endpoint. Here is a query example to start with: # Request SPO of all Statement Annotations which annotate a triple of my ResourceIRI (parameter) PREFIX rdf : < http :// www.w3.org / 1999 / 02 / 22 - rdf - syntax - ns #> SELECT DISTINCT ? StatementAnnotationGraph ? AnnotationResource ? p ? o WHERE { GRAPH ? StatementAnnotationGraph { ? StatementResource a rdf : Statement . ? StatementResource rdf : subject | rdf : predicate | rdf : object < {{ ResourceIRI }} > . ? StatementResource rdf : value ? AnnotationResource . ? AnnotationResource ? p ? o } }","title":"Statement Annotations"},{"location":"explore-and-author/statement-annotations/#statement-annotations","text":"","title":"Statement Annotations"},{"location":"explore-and-author/statement-annotations/#introduction","text":"Statement Annotations provide a way to express knowledge about statements. Typical use cases for Statement Annotations include: the temporal validity of information, the origin of information, or just a way to annotate a specific statement with a human readable comment.","title":"Introduction"},{"location":"explore-and-author/statement-annotations/#usage","text":"If enabled on a specific type of statement or type of resource, you see a Statement Annotation text bubble beside every annotatable statement: This bubble has different statuses: A empty text bubble indicates, that there is no annotation on the statement, but the annotation feature is enabled for this statement. A filled text bubble indicates, that there is at least one annotation on the statement. No bubble indicates, that the annotation feature is NOT enabled on this type of statement. Clicking on one of the flags opens the Statement Annotation dialog for this specific statement: In the Statement Annotation dialog, you can select the Statement Annotation Template and click Create. Setup In order to have a working Statement Annotation setup, the following steps need to be done: 1. Create a Statement Annotation Graph Create a new Graph, edit its metadata and change the type to Statement Annotation Graph. 2. Setup and import the Statement Annotation Graph in your data graph In your data graph, where the resources exist which you want to annotate, import the Statement Annotation Graph and select it as an Annotation Graph. 3. Create a shaped form which will be used to annotate statements In your Shape Catalog, select a Node Shape (or create one) which you want to use for statement annotations, and Enable Statement Annotation to true. 4. Allow statement annotations in your shaped forms on specific Classes or Properties Finally, select the Node Shape or Property Shape from your Shape Catalog, and enable annotations by setting the Enable option in the Statement Annotations group to true. This will enable the feature on the statements of all resources shown with this Node Shape or on all statements shown with this Property Shape.","title":"Usage"},{"location":"explore-and-author/statement-annotations/#technical-background","text":"From the technical point of view, the Statement Annotation feature uses RDF Reification to annotate Statements (Triples) with additional background information. Statement resources can be annotated with custom Annotation Resources. These Annotation Resources are based on specific Shapes which are enabled as Statement Annotation shapes. Reification Resources as well as Annotation Resources are managed in a Statement Annotation Graph, which need to be configured on a Graph as well as imported to this Graph. The following illustration depicts this schema with boxes and arrows: Some notes on this: There is one Statement Reification Resource per Statement Annotation. Removing the Statement Annotation also removes the Statement Reification Resource. All annotation triples (8 triples in the image) are created in the Statement Annotation Graph, so Step 2 of the setup procedure is important.","title":"Technical Background"},{"location":"explore-and-author/statement-annotations/#querying-statement-annotations","text":"In order to automate access to Statement Annotations, you can query them with SPARQL e.g. via cmemc or the API endpoint. Here is a query example to start with: # Request SPO of all Statement Annotations which annotate a triple of my ResourceIRI (parameter) PREFIX rdf : < http :// www.w3.org / 1999 / 02 / 22 - rdf - syntax - ns #> SELECT DISTINCT ? StatementAnnotationGraph ? AnnotationResource ? p ? o WHERE { GRAPH ? StatementAnnotationGraph { ? StatementResource a rdf : Statement . ? StatementResource rdf : subject | rdf : predicate | rdf : object < {{ ResourceIRI }} > . ? StatementResource rdf : value ? AnnotationResource . ? AnnotationResource ? p ? o } }","title":"Querying Statement Annotations"},{"location":"explore-and-author/thesauri-management/","text":"Thesauri Management \u00a4 Introduction \u00a4 The Thesaurus module provides a user interface to create, browse and edit thesaurus resources and general taxonomical data modeled in the Simple Knowledge Organization System (SKOS) .A thesaurus is a reference work that lists concepts with similar meaning, containing for example synonyms, often including taxonomical relations between these concepts. Taxonomies describe classifications of concepts into categories sharing particular features and their relations to broader (parent) and narrower (child) concepts. An example for a taxonomy or classification is how companies can be categorized into industries, industry groups and sectors. An airport belongs to the sector Airport Services, the broader category Transportation Infrastructure, the broader category Transportation and the broader category Industrials. You can think of these relations as a hierarchical tree representing the relations as individual branches like shown in the navigation tree on the left side of the Thesaurus view. In a concept scheme Industries, a top branch in this tree, as for example the sub-industry Industrials or Health Care, is called a top concept. All branches together belong to the concept scheme Industries. Info In order to correctly build thesauri or taxonomies with the Thesaurus module you should be familiar with SKOS structures. SKOS is a convenient way to model taxonomical data. The SKOS Reference provides detailed documentation on the usage of SKOS. The Thesaurus module allows to create, browse and edit such structures, providing a way to structure your hierarchical data in a simple interface and make it accessible for use cases like documentation and master data management. Info Before you start working with the Thesaurus module ensure that the vocabulary Simple Knowledge Organization System is installed in the Vocabulary catalog (see section Vocabulary Catalog ). Click THESAURUS in the Module bar to open the Thesaurus project catalog. Thesaurus project catalog \u00a4 The Thesaurus project catalog lists thesaurus projects with relevant metadata in a searchable, sortable table. In order to get more information on a thesaurus project and edit its metadata, click in the table row. The view expands showing the project metadata. Click on the right side of the row to open the edit mode, enter your changes and click SAVE. To open the detail view of a thesaurus project, click the project name in the catalog. Creating a new thesaurus project \u00a4 In order to create a new thesaurus project, click on the lower right of the thesaurus project catalog or click and then Create new thesaurus project. Enter a name for your thesaurus project and more metadata if required and click SAVE. The thesaurus detail view is shown, displaying an empty thesaurus project. Importing an existing thesaurus \u00a4 You can import existing thesaurus data in a thesaurus project. To import a thesaurus make sure that the data is in \u2018.ttl\u2019 format and you are on project level in the thesaurus detail view. This means that the project name must be selected in the top left. Click the context menu in the upper right of the thesaurus detail view, then select Import data. Upload the file containing the thesaurus. Click SAVE to import the data. Exporting a thesaurus project \u00a4 To export a thesaurus project, go to the project level, click in the upper right of the thesaurus detail view, then select Export project. Confirm the dialog and click DOWNLOAD to download the thesaurus project. Removing a thesaurus project \u00a4 To remove a thesaurus project, go to the project level in the navigation tree, click in the upper right of the thesaurus detail view, then select Remove project. Confirm the removal and click REMOVE to delete the thesaurus project. Thesaurus detail view \u00a4 After selecting a thesaurus project in the Thesaurus project catalog, the thesaurus detail view is shown. This view consists of three components. The navigation tree component is displayed to the left. It displays the hierarchical structure of the thesaurus. The upmost element is the project level. Clicking brings you back to the Thesaurus project catalog. Below the project level the concept scheme(s) and concepts are displayed. A Concept scheme serves as a meaningful aggregation of a number of concepts. A concept itself constitutes a unit of thought, a conceptual class or category of objects you want to describe. For example, all concepts on a particular topic or domain could belong to the same concept scheme. If a concept in the navigation tree has narrower (child) concepts, you can expand this branch of the tree by clicking the arrow displayed in front of the concept. Arrows are only shown for concepts for which narrower concepts exist. Clicking the name of a concept scheme or concept updates both the detail view as well as the concept list to the right of the navigation tree. The tab STATISTICS shows statistical information about the content of the thesaurus project. A list of concept schemes of the thesaurus project is shown in the lower part as a searchable table. To edit project metadata in the thesaurus detail view click . Concept detail view \u00a4 Clicking on a Concept scheme or a Concept in the navigation tree displays information on that resource in the concept detail view to the right. The concept detail view displays the concept\u2019s preferred and alternative labels in the OVERVIEW tab, as well as definitions and other metadata. A list of top concepts of a concept scheme or narrower (child) concepts of a concept is shown below these metadata as a searchable table called concept list. To find specific sub concepts of a concept, you can use the search bar in the concept list. Click any result row to display more information on the concept, like its labels, definition and notation. Click the concept name in the row itself to open the concept detail view for this concept. Selecting a concept in the concept list updates the navigation tree to highlight the concept\u2019s position in the tree. In case of concept schemes, the STATISTICS tab can be used to learn more about the number of concepts in this concept scheme, as well as their relations. The TRIPLES tab shows the RDF source code of the concept as rdf/turtle serialization. The triples in this view are editable, but be aware that changes in the triple editor can cause major errors in the Thesaurus module. When you are not an expert in working with triples do not use the triple view as an editor. The HISTORY tab shows information about changes made to the concept as well as the author of these changes. If the selected resource has not been changed yet or versioning is disabled, the tab is not displayed. Adding new concepts and concept schemes \u00a4 Concept schemes can be added on project level in an opened thesaurus project. Click on the project name in the navigation tree, then click the context menu and select Create new concept scheme. You can return to this level by clicking the name of the thesaurus project above the navigation tree. Enter at least one preferred label naming the concept scheme. Select the language of the preferred label to the right of the label field. Click + under the label field to set multiple labels. Create the concept scheme by clicking SAVE. A new concept scheme will appear in the concept list as well as the navigation tree. Top concepts can be added in the same way. Instead of starting on project level, select an existing concept scheme first. Click the context menu and a new option Create new top concept is available. Use it to create a new top concept. Selecting a normal concept brings up the option Create new concept in the same menu. Using this option creates a new concept and automatically add it as a narrower concept to the concept you created it on. It also automatically adds the broader back link on the newly created concept. To edit a concept or concept scheme, click to get a form displaying all available fields, like labels and definition. Info For each concept, you have to enter one preferred label per language. Be aware that SKOS allows only one preferred label per language. If you want to enter more labels use alternative or hidden labels as specified by SKOS. Creating relations between concepts \u00a4 Besides the narrower relation automatically generated when a new concept is created, you can add further relations between concepts. You can add, for example, a second broader concept for an existing concept or a related concept to indicate associative relations. To add relations, select the concept in the navigation tree. In the detail view, click to open the edit mode. To add an associative relation to another concept, enter the concept name in the field Related concept . To add a further broader relation, enter the name of the broader concept in the field Broader concepts . You can only choose from existing concepts. Click SAVE to confirm your changes. In the same way you can also add a top concept to a second concept scheme. Use therefore the field Top concept of in the editing mode of a top concept. When adding relations the inverse relation is automatically added, too. Removing concepts and concept schemes \u00a4 Info Be aware that removing a concept or concept scheme with child elements (top concepts or narrower concepts) means that the complete substructure, i.e. all childs, are also deleted regardless whether they are used in another concept scheme. To remove concepts or concept schemes, select the resource in the navigation tree, click the context menu and select the remove option. Confirm the dialog and click REMOVE.","title":"Thesauri Management"},{"location":"explore-and-author/thesauri-management/#thesauri-management","text":"","title":"Thesauri Management"},{"location":"explore-and-author/thesauri-management/#introduction","text":"The Thesaurus module provides a user interface to create, browse and edit thesaurus resources and general taxonomical data modeled in the Simple Knowledge Organization System (SKOS) .A thesaurus is a reference work that lists concepts with similar meaning, containing for example synonyms, often including taxonomical relations between these concepts. Taxonomies describe classifications of concepts into categories sharing particular features and their relations to broader (parent) and narrower (child) concepts. An example for a taxonomy or classification is how companies can be categorized into industries, industry groups and sectors. An airport belongs to the sector Airport Services, the broader category Transportation Infrastructure, the broader category Transportation and the broader category Industrials. You can think of these relations as a hierarchical tree representing the relations as individual branches like shown in the navigation tree on the left side of the Thesaurus view. In a concept scheme Industries, a top branch in this tree, as for example the sub-industry Industrials or Health Care, is called a top concept. All branches together belong to the concept scheme Industries. Info In order to correctly build thesauri or taxonomies with the Thesaurus module you should be familiar with SKOS structures. SKOS is a convenient way to model taxonomical data. The SKOS Reference provides detailed documentation on the usage of SKOS. The Thesaurus module allows to create, browse and edit such structures, providing a way to structure your hierarchical data in a simple interface and make it accessible for use cases like documentation and master data management. Info Before you start working with the Thesaurus module ensure that the vocabulary Simple Knowledge Organization System is installed in the Vocabulary catalog (see section Vocabulary Catalog ). Click THESAURUS in the Module bar to open the Thesaurus project catalog.","title":"Introduction"},{"location":"explore-and-author/thesauri-management/#thesaurus-project-catalog","text":"The Thesaurus project catalog lists thesaurus projects with relevant metadata in a searchable, sortable table. In order to get more information on a thesaurus project and edit its metadata, click in the table row. The view expands showing the project metadata. Click on the right side of the row to open the edit mode, enter your changes and click SAVE. To open the detail view of a thesaurus project, click the project name in the catalog.","title":"Thesaurus project catalog"},{"location":"explore-and-author/thesauri-management/#creating-a-new-thesaurus-project","text":"In order to create a new thesaurus project, click on the lower right of the thesaurus project catalog or click and then Create new thesaurus project. Enter a name for your thesaurus project and more metadata if required and click SAVE. The thesaurus detail view is shown, displaying an empty thesaurus project.","title":"Creating a new thesaurus project"},{"location":"explore-and-author/thesauri-management/#importing-an-existing-thesaurus","text":"You can import existing thesaurus data in a thesaurus project. To import a thesaurus make sure that the data is in \u2018.ttl\u2019 format and you are on project level in the thesaurus detail view. This means that the project name must be selected in the top left. Click the context menu in the upper right of the thesaurus detail view, then select Import data. Upload the file containing the thesaurus. Click SAVE to import the data.","title":"Importing an existing thesaurus"},{"location":"explore-and-author/thesauri-management/#exporting-a-thesaurus-project","text":"To export a thesaurus project, go to the project level, click in the upper right of the thesaurus detail view, then select Export project. Confirm the dialog and click DOWNLOAD to download the thesaurus project.","title":"Exporting a thesaurus project"},{"location":"explore-and-author/thesauri-management/#removing-a-thesaurus-project","text":"To remove a thesaurus project, go to the project level in the navigation tree, click in the upper right of the thesaurus detail view, then select Remove project. Confirm the removal and click REMOVE to delete the thesaurus project.","title":"Removing a thesaurus project"},{"location":"explore-and-author/thesauri-management/#thesaurus-detail-view","text":"After selecting a thesaurus project in the Thesaurus project catalog, the thesaurus detail view is shown. This view consists of three components. The navigation tree component is displayed to the left. It displays the hierarchical structure of the thesaurus. The upmost element is the project level. Clicking brings you back to the Thesaurus project catalog. Below the project level the concept scheme(s) and concepts are displayed. A Concept scheme serves as a meaningful aggregation of a number of concepts. A concept itself constitutes a unit of thought, a conceptual class or category of objects you want to describe. For example, all concepts on a particular topic or domain could belong to the same concept scheme. If a concept in the navigation tree has narrower (child) concepts, you can expand this branch of the tree by clicking the arrow displayed in front of the concept. Arrows are only shown for concepts for which narrower concepts exist. Clicking the name of a concept scheme or concept updates both the detail view as well as the concept list to the right of the navigation tree. The tab STATISTICS shows statistical information about the content of the thesaurus project. A list of concept schemes of the thesaurus project is shown in the lower part as a searchable table. To edit project metadata in the thesaurus detail view click .","title":"Thesaurus detail view"},{"location":"explore-and-author/thesauri-management/#concept-detail-view","text":"Clicking on a Concept scheme or a Concept in the navigation tree displays information on that resource in the concept detail view to the right. The concept detail view displays the concept\u2019s preferred and alternative labels in the OVERVIEW tab, as well as definitions and other metadata. A list of top concepts of a concept scheme or narrower (child) concepts of a concept is shown below these metadata as a searchable table called concept list. To find specific sub concepts of a concept, you can use the search bar in the concept list. Click any result row to display more information on the concept, like its labels, definition and notation. Click the concept name in the row itself to open the concept detail view for this concept. Selecting a concept in the concept list updates the navigation tree to highlight the concept\u2019s position in the tree. In case of concept schemes, the STATISTICS tab can be used to learn more about the number of concepts in this concept scheme, as well as their relations. The TRIPLES tab shows the RDF source code of the concept as rdf/turtle serialization. The triples in this view are editable, but be aware that changes in the triple editor can cause major errors in the Thesaurus module. When you are not an expert in working with triples do not use the triple view as an editor. The HISTORY tab shows information about changes made to the concept as well as the author of these changes. If the selected resource has not been changed yet or versioning is disabled, the tab is not displayed.","title":"Concept detail view"},{"location":"explore-and-author/thesauri-management/#adding-new-concepts-and-concept-schemes","text":"Concept schemes can be added on project level in an opened thesaurus project. Click on the project name in the navigation tree, then click the context menu and select Create new concept scheme. You can return to this level by clicking the name of the thesaurus project above the navigation tree. Enter at least one preferred label naming the concept scheme. Select the language of the preferred label to the right of the label field. Click + under the label field to set multiple labels. Create the concept scheme by clicking SAVE. A new concept scheme will appear in the concept list as well as the navigation tree. Top concepts can be added in the same way. Instead of starting on project level, select an existing concept scheme first. Click the context menu and a new option Create new top concept is available. Use it to create a new top concept. Selecting a normal concept brings up the option Create new concept in the same menu. Using this option creates a new concept and automatically add it as a narrower concept to the concept you created it on. It also automatically adds the broader back link on the newly created concept. To edit a concept or concept scheme, click to get a form displaying all available fields, like labels and definition. Info For each concept, you have to enter one preferred label per language. Be aware that SKOS allows only one preferred label per language. If you want to enter more labels use alternative or hidden labels as specified by SKOS.","title":"Adding new concepts and concept schemes"},{"location":"explore-and-author/thesauri-management/#creating-relations-between-concepts","text":"Besides the narrower relation automatically generated when a new concept is created, you can add further relations between concepts. You can add, for example, a second broader concept for an existing concept or a related concept to indicate associative relations. To add relations, select the concept in the navigation tree. In the detail view, click to open the edit mode. To add an associative relation to another concept, enter the concept name in the field Related concept . To add a further broader relation, enter the name of the broader concept in the field Broader concepts . You can only choose from existing concepts. Click SAVE to confirm your changes. In the same way you can also add a top concept to a second concept scheme. Use therefore the field Top concept of in the editing mode of a top concept. When adding relations the inverse relation is automatically added, too.","title":"Creating relations between concepts"},{"location":"explore-and-author/thesauri-management/#removing-concepts-and-concept-schemes","text":"Info Be aware that removing a concept or concept scheme with child elements (top concepts or narrower concepts) means that the complete substructure, i.e. all childs, are also deleted regardless whether they are used in another concept scheme. To remove concepts or concept schemes, select the resource in the navigation tree, click the context menu and select the remove option. Confirm the dialog and click REMOVE.","title":"Removing concepts and concept schemes"},{"location":"explore-and-author/versioning-of-graph-changes/","text":"Versioning of Graph Changes \u00a4 Introduction \u00a4 This feature keeps track of changes to your Knowledge Graphs by creating change set data based on the user\u2019s editing activities. Usage \u00a4 If enabled on a graph, all changes using shaped user interfaces will be tracked in the configured Versioning Graph. Setup \u00a4 To enable this feature on a specific graph you need to setup the following steps. 1. Create a Versioning Graph In Exploration, create a new graph and define it as a Versioning Graph. 2. Configure a graph to use this Versioning Graph In Exploration, edit this graph and add the Versioning Graph property to select the newly created Versioning Graph. Technical Background \u00a4 For each editing activity (\u2192 Save a Form), a ChangeSet resource will be created. This resource has some metadata (user, timestamp, label) as well as links to added and deleted Statements (using RDF Reification). The details of the used vocabulary are available at Changeset Vocabulary page.","title":"Versioning of Graph Changes"},{"location":"explore-and-author/versioning-of-graph-changes/#versioning-of-graph-changes","text":"","title":"Versioning of Graph Changes"},{"location":"explore-and-author/versioning-of-graph-changes/#introduction","text":"This feature keeps track of changes to your Knowledge Graphs by creating change set data based on the user\u2019s editing activities.","title":"Introduction"},{"location":"explore-and-author/versioning-of-graph-changes/#usage","text":"If enabled on a graph, all changes using shaped user interfaces will be tracked in the configured Versioning Graph.","title":"Usage"},{"location":"explore-and-author/versioning-of-graph-changes/#setup","text":"To enable this feature on a specific graph you need to setup the following steps. 1. Create a Versioning Graph In Exploration, create a new graph and define it as a Versioning Graph. 2. Configure a graph to use this Versioning Graph In Exploration, edit this graph and add the Versioning Graph property to select the newly created Versioning Graph.","title":"Setup"},{"location":"explore-and-author/versioning-of-graph-changes/#technical-background","text":"For each editing activity (\u2192 Save a Form), a ChangeSet resource will be created. This resource has some metadata (user, timestamp, label) as well as links to added and deleted Statements (using RDF Reification). The details of the used vocabulary are available at Changeset Vocabulary page.","title":"Technical Background"},{"location":"explore-and-author/vocabulary-catalog/","text":"Vocabulary Catalog \u00a4 Introduction \u00a4 Vocabularies are the foundation for semantic data lifting activities.This module shows the list of all managed vocabularies in Corporate Memory that are accessible for the user. The table represents the list of known vocabularies. Installed vocabularies are indicated by the orange switch in the column Installed . Add new vocabulary \u00a4 Click to register a new vocabulary. A new form will be shown, fill it an add the file to import the vocabulary to Corporate Memory. Use the Search bar to find vocabularies based on name or other metadata. Extended information and options \u00a4 Each table row provides a menu with more options clicking on or in the Vocabulary column. A vocabulary which is known and available but not installed, looks like this: Example of extended information of uninstalled Vocabulary Catalog Use Install or the switch in the column Installed to install the Catalog. Use View to access the Vocabulary. A vocabulary which is installed looks like this Example of extended information of installed Vocabulary Catalog Use Uninstall to remove an installed vocabulary or Install to install a vocabulary. Use View to access the Vocabulary. Use Upload to install or overwrite the vocabulary from a file.","title":"Vocabulary Catalog"},{"location":"explore-and-author/vocabulary-catalog/#vocabulary-catalog","text":"","title":"Vocabulary Catalog"},{"location":"explore-and-author/vocabulary-catalog/#introduction","text":"Vocabularies are the foundation for semantic data lifting activities.This module shows the list of all managed vocabularies in Corporate Memory that are accessible for the user. The table represents the list of known vocabularies. Installed vocabularies are indicated by the orange switch in the column Installed .","title":"Introduction"},{"location":"explore-and-author/vocabulary-catalog/#add-new-vocabulary","text":"Click to register a new vocabulary. A new form will be shown, fill it an add the file to import the vocabulary to Corporate Memory. Use the Search bar to find vocabularies based on name or other metadata.","title":"Add new vocabulary"},{"location":"explore-and-author/vocabulary-catalog/#extended-information-and-options","text":"Each table row provides a menu with more options clicking on or in the Vocabulary column. A vocabulary which is known and available but not installed, looks like this: Example of extended information of uninstalled Vocabulary Catalog Use Install or the switch in the column Installed to install the Catalog. Use View to access the Vocabulary. A vocabulary which is installed looks like this Example of extended information of installed Vocabulary Catalog Use Uninstall to remove an installed vocabulary or Install to install a vocabulary. Use View to access the Vocabulary. Use Upload to install or overwrite the vocabulary from a file.","title":"Extended information and options"},{"location":"explore-and-author/workflow-trigger/","text":"Workflow Trigger \u00a4 Introduction \u00a4 Workflow Triggers allow for execution of data integration workflows inside of the exploration interface. Optionally, a reference to the resource in view on workflow execution can be sent, allowing an executed workflow to act specifically on this resource (or a specific portion of the Knowledge Graph related to it). Workflow triggers are associated to Node Shapes by defining special-purpose non-validating Property Shape resources. Defining Workflow Triggers \u00a4 Workflow Triggers can be defined and used in any active Shape Catalog (active means, it is imported from the main Shape Catalog). A workflow trigger resource references a data integration workflow by URI. To define a workflow trigger the following information is needed: Label : The trigger resource needs a label (can be given in different languages), which is used for the button presentation. Description : The trigger resource needs a description, which is used as text that is sitting left of the button for further documentation of the activity to the user. Workflow : the workflow parameter defines the workflow that shall be executed upon clicking the button. The workflow can be selected from a dropdown list. Refresh View : can be either true or false . If this value is set to true , the view that contains the workflow trigger will be reloaded upon workflow completion Send Resource Reference : can be either true or false . If this value is set to true , a payload that consists of the resource IRI that is represented in the view as well as the graph IRI of the graph that is currently selected . Integration \u00a4 Once a trigger resources is defined, it can be attached to a Node Shape by using a special-purpose non-validating Property Shape resources. Such property shapes use a shui:provideWorkflowTrigger statement to define, which workflow trigger are to be represented. SHACL path statements on such Property Shape resources are meaningless and ignored, but may be provided. Payload Structure \u00a4 When Send Resource Reference is set to true, a payload is added to the call of the workflow. The payload consists of a JSON document with two attributes: Workflow Payload { \"graphIRI\" : \"http://example.org/example-graph\" , \"resourceIRI\" : \"http://example.org/example-graph/examle-resource\" } graphIRI is the IRI of the graph that is currently viewed and resourceIRI is the IRI of the resource that is viewed","title":"Workflow Trigger"},{"location":"explore-and-author/workflow-trigger/#workflow-trigger","text":"","title":"Workflow Trigger"},{"location":"explore-and-author/workflow-trigger/#introduction","text":"Workflow Triggers allow for execution of data integration workflows inside of the exploration interface. Optionally, a reference to the resource in view on workflow execution can be sent, allowing an executed workflow to act specifically on this resource (or a specific portion of the Knowledge Graph related to it). Workflow triggers are associated to Node Shapes by defining special-purpose non-validating Property Shape resources.","title":"Introduction"},{"location":"explore-and-author/workflow-trigger/#defining-workflow-triggers","text":"Workflow Triggers can be defined and used in any active Shape Catalog (active means, it is imported from the main Shape Catalog). A workflow trigger resource references a data integration workflow by URI. To define a workflow trigger the following information is needed: Label : The trigger resource needs a label (can be given in different languages), which is used for the button presentation. Description : The trigger resource needs a description, which is used as text that is sitting left of the button for further documentation of the activity to the user. Workflow : the workflow parameter defines the workflow that shall be executed upon clicking the button. The workflow can be selected from a dropdown list. Refresh View : can be either true or false . If this value is set to true , the view that contains the workflow trigger will be reloaded upon workflow completion Send Resource Reference : can be either true or false . If this value is set to true , a payload that consists of the resource IRI that is represented in the view as well as the graph IRI of the graph that is currently selected .","title":"Defining Workflow Triggers"},{"location":"explore-and-author/workflow-trigger/#integration","text":"Once a trigger resources is defined, it can be attached to a Node Shape by using a special-purpose non-validating Property Shape resources. Such property shapes use a shui:provideWorkflowTrigger statement to define, which workflow trigger are to be represented. SHACL path statements on such Property Shape resources are meaningless and ignored, but may be provided.","title":"Integration"},{"location":"explore-and-author/workflow-trigger/#payload-structure","text":"When Send Resource Reference is set to true, a payload is added to the call of the workflow. The payload consists of a JSON document with two attributes: Workflow Payload { \"graphIRI\" : \"http://example.org/example-graph\" , \"resourceIRI\" : \"http://example.org/example-graph/examle-resource\" } graphIRI is the IRI of the graph that is currently viewed and resourceIRI is the IRI of the resource that is viewed","title":"Payload Structure"},{"location":"getting-started/","text":"","title":"Index"},{"location":"glossary/","text":"","title":"Index"},{"location":"release-notes/corporate-memory-19-10/","text":"Corporate Memory 19.10 \u00a4 Corporate Memory 19.10 is the third release in 2019. The highlights of this release are: Switched from an own OAuth 2.0 authorization server implementation to a more capable and supported solution based on Keycloak. Largely enhanced JDBC / SQL support: Overall performance improvements. Allowing hierarchical mappings to write to JDBC/SQL datasets. Support for Oracle SQL databases. Input validation of mandatory fields in shaceline resource details views. Resource Table results can be directly downloaded in Excel and other formats. Warning With this release of Corporate Memory the DataPlatform configuration must be adapted according to the migration notes below. Consequently this release delivers the following component versions: eccenca DataPlatform v19.10 eccenca DataIntegration v19.10 eccenca DataManager v19.10 More detailed release notes for these versions are listed below. eccenca DataIntegration v19.10 \u00a4 This version of eccenca DataIntegration adds the following new features: Write support for hierarchical data via JDBC. Allow arbitrary column names on SqlEndpoint datasets. Support for Oracle SQL. JSON Rest endpoint that allows to evaluate portions of a linking rules. SPARQL 1.1 endpoint each RdfDataset thereby allowing instantaneous SPARQL access via REST and SPARQL SERVICE keyword. In addition to that, these changes are shipped: Add parameter that decides how empty values are handled by the concat transformer. Add \u2018ZIP file regex\u2019 parameter to all bulk resource datasets that allows to filter resources inside the bulk resource container (currently ZIP files only). Simplify Hive Serialization, refactor SQL utility methods. SPARQL Update operator Do not read from input data source when the SPARQL Update template is static, i.e. when it always runs the same static SPARQL Update query exactly once. Add SPARQL Update execution report containing various statistics, e.g. number of queries, query throughput etc. Support Apache Velocity Engine based templates. This adds logic like conditional branching and loops to the templates. For more information visit https://velocity.apache.org SPARQL Select operator generates an execution report with various statistics, e.g. rows processed, runtime etc. SPARQL dataset generates execution report when executing SPARQL Update queries e.g. from the SPARQL Update operator with statistics like remaining queries, time estimation etc. Extended SQL Endpoint documentation. Added local execution to JDBC dataset. The execution is more efficient and pushes limits and group-by columns into the database. While workspace is initializing, subsequent requests will timeout. Config parameter workspace.timeouts.waitForWorkspaceInitialization (in milliseconds, default: 5000ms). Scheduler: Added \u2018Stop On Error\u2019 parameter. If enabled this will stop a scheduler on the first execution error. Default: false . Added addMarkdownDocumentation parameter to /plugins and /plugins/:pluginType REST endpoints in order to request the optional Markdown documentation for plugins. Added SPARQL query timeout parameter in order to limit query execution times to \u2018Knowledge Graph\u2019 and \u2018SPARQL endpoint\u2019 datasets and to the SPARQL Select operator. RDF serialization: Tasks referenced in RDF serialization with di:output and di:task use the correct project task URIs instead of artificial URIs or literals. Complete Zip Stream support (replacing reliability on zip files only). MultiCsvZip are now BulkResourceDatasets. JDBC dataset: Removed database parameter. If required, the database needs to be specified as part of the JDBC URL, e.g., jdbc:mysql://localhost:port/databaseName . Appending connection parameters to the URL is supported now. Safer defaults: SPARQL Endpoint and Knowledge Graph dataset: Do not clear graphs before execution to prevent accidental deletion of graphs. Knowledge Graph dataset: Increase page size from 1000 to 100,000, because small page sizes lead to suboptimal execution performance. SPARQL Update operator: Decrease batch size from 10 to 1. Upgraded to Apache Spark 2.3.3 SQL datasets: If a URI attribute is specified, the URI will be added as a new column of that name. All projects are loaded first, before any cache or any other autorun activity is started. This improves loading when both the workspace provider and some caches load from the same database. Improved JDBC dataset performance for MySQL and MariaDB by using the \u2018Load Data\u2019 command to load a local CSV file into the database. Shipping with MariaDB JDBC driver. Redirect to original request URL instead of the start page after authenticating a user and logging in. Revised generation of table names in JDBC dataset (see documentation). Keep letter case of table names. Schedulers are always started by the super user, if the super user is configured. On start-up if DataPlatform is configured, DI will wait for DP to become healthy for a configurable amount of time. Parameters: eccencaDataPlatform.health.waitingTimeInSeconds : Overall time in seconds DI should wait for DP to be up and healthy. Setting this to 0 will disable this check. Default: 60. eccencaDataPlatform.health.delayBetweenRetriesInSeconds : Amount of time in seconds to wait between retries. Default: 5. Enhanced JdbcDataset and SqlEndpoint parameters and improve their descriptions. Suppress case changes in SqlEndpoint table names. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v19.10 \u00a4 This version of eccenca DataManager adds the following new features: New module task Offers a direct resource actions. Interfaces only available by URL. See documentation for more details. Path /task/resource/create allows to create a new resource by given graph and type. General Config parameter js.config.api.defaultTimeout for default UI queries timeout. Config parameter js.config.resourceTable.timeoutDownload for Resource Table timeout on download requests on Explore and Query modules. Validation of mandatory fields in shacline view. Add new property shui:onUpdateUpdate for sh:NodeShape . Module Explore Config parameter js.config.modules.explore.graphlist.whiteList to filter specific graphs. Config parameter js.config.modules.explore.graphlist.internalGraphs to hide specific graphs. Config parameter js.config.modules.explore.navigation.itemsPerPage show items per page in navigation box. Support for inverse property relations. Module Query Config parameter js.config.modules.query.timeout for manual queries. Config parameter js.config.modules.query.graph to define the graph were data is saved and requested. In addition to that, these changes are shipped: General Default pagination size of 20 elements for all Resource Tables. Allow datatype xsd:anyURI for literals. Upgraded to react 16. Module Explore Merged graph view RDFDoc into \u2018resource details view\u2019. Renamed global search label. Graph creation will add the type void:Dataset instead of owl:Ontology . Use the label of the type of the instances for the name of the CSV file downloaded from the Resource Table. Display the context graph in properties and references tables. Module Dataset Adjusted position and tooltip of parameter uriProperty in \u2018Add data stepper\u2019. Module Query Use the dataset label for the name of the CSV file downloaded from the Resource Table. Module Login Renew tokens when they expire. Module Administration Allow to search in IRIs for list of readable and writeable graphs. The following features have been removed in this release: Module Explore Config parameter js.config.modules.explore.graphlist.listQuery which is now obsolete. Config parameter js.config.modules.explore.details.history which is now obsolete as the feature is no longer supported. \u2018History\u2019 tab. Module Sync also known as SubscriptionManagement . In addition to that, multiple stability issues were solved. eccenca DataPlatform v19.10 \u00a4 This version of eccenca DataPlatform adds the following new features: SPARQL 1.1 Query endpoint An in-iris property to the JSON search parameter to enable search over IRIs. A timeout parameter which allows to configure the maximal amount of milliseconds that a query execution can run. Support for Microsoft Excel ( .xlsx ) file download for SELECT queries. SPARQL 1.1 Update endpoint A timeout parameter which allows to configure the maximal amount of milliseconds that an update execution can run (Stardog only). SPARQL 1.1 Graph Store Protocol multipart/form-data support for HTTP PUT. Added the timeout parameter, which allows to configure the maximal amount of milliseconds that a request execution should run. Documentation for content negotiation by format query parameter. The following features have been removed in this release: Data Sharing: A WebSub based Publish-Subscribe service for RDF named graphs. IoT Permissions Plugin: A plugin which enables the usage of the IoT Permissions Service API 2. OAuth 2.0 authorization server: Issues access tokens to a client after successfully authenticating a user. Authentication: User management via authentication providers as it was only needed by the OAuth 2.0 authorization server. In addition to that, these changes are shipped: Stardog Upgraded support to version 7.0.2. Versioning does no longer work with Stardog 7. Legacy versioning support for Stardog 6 (deprecated). OAuth 2.0: Resource protection is now mandatory (can no longer be disabled, use anonymous access instead). SPARQL 1.1 Query endpoint The value of the string property of the JSON search parameter is now tokenized which means that each token will be searched separately. Only results matching all tokens will be returned. Updated Spring Boot version from 1.5.21 to 1.5.22. In addition to that, multiple performance and stability issues were solved. Migration Notes \u00a4 With the removal of the OAuth 2.0 authorization server capability, many configuration properties have been changed. Removed The properties oauth2.clients.* have been removed. The properties authentication.* have been removed. Moved The property oauth2.jwt.signing.verificationKey has been moved to security.oauth2.resource.jwt.keyValue . The property oauth2.anonymous has been moved to security.oauth2.resource.anonymous . The claims mapping properties under oauth2.resourceServer.claimsMapping.* have been moved to security.oauth2.resource.jwt.claims.* . The properties oauth2.authorizeRequests.* to configure the resources to be protected by the resource server have been moved to security.oauth2.resource.authorizeRequests.* . Added The value of the property security.oauth2.resource.id (defaults to dataplatform ) must be part of the aud (audience) claim in the JWT used to access a protected resource. Don\u2019t forget to update your configuration accordingly. For instance, assuming you have the following old configuration: oauth2 : anonymous : true clients : - id : client secret : secret grantTypes : - authorization_code redirectUris : - http://example.org/oauth/client jwt : enabled : true signing : verificationKey : | -----BEGIN PUBLIC KEY----- ... -----END PUBLIC KEY----- resourceServer : claimsMapping : username : 'preferred_username' clientId : 'azp' groups : key : 'groups' The migrated properties should look like this: security : oauth2 : resource : anonymous : true # optional, defaults to `false` jwt : keyValue : | -----BEGIN PUBLIC KEY----- ... -----END PUBLIC KEY----- claims : username : preferred_username # optional, defaults to `preferred_username` groups : groups # optional, defaults to `groups` clientId : azp # optional, defaults to `azp`","title":"Corporate Memory 19.10"},{"location":"release-notes/corporate-memory-19-10/#corporate-memory-1910","text":"Corporate Memory 19.10 is the third release in 2019. The highlights of this release are: Switched from an own OAuth 2.0 authorization server implementation to a more capable and supported solution based on Keycloak. Largely enhanced JDBC / SQL support: Overall performance improvements. Allowing hierarchical mappings to write to JDBC/SQL datasets. Support for Oracle SQL databases. Input validation of mandatory fields in shaceline resource details views. Resource Table results can be directly downloaded in Excel and other formats. Warning With this release of Corporate Memory the DataPlatform configuration must be adapted according to the migration notes below. Consequently this release delivers the following component versions: eccenca DataPlatform v19.10 eccenca DataIntegration v19.10 eccenca DataManager v19.10 More detailed release notes for these versions are listed below.","title":"Corporate Memory 19.10"},{"location":"release-notes/corporate-memory-19-10/#eccenca-dataintegration-v1910","text":"This version of eccenca DataIntegration adds the following new features: Write support for hierarchical data via JDBC. Allow arbitrary column names on SqlEndpoint datasets. Support for Oracle SQL. JSON Rest endpoint that allows to evaluate portions of a linking rules. SPARQL 1.1 endpoint each RdfDataset thereby allowing instantaneous SPARQL access via REST and SPARQL SERVICE keyword. In addition to that, these changes are shipped: Add parameter that decides how empty values are handled by the concat transformer. Add \u2018ZIP file regex\u2019 parameter to all bulk resource datasets that allows to filter resources inside the bulk resource container (currently ZIP files only). Simplify Hive Serialization, refactor SQL utility methods. SPARQL Update operator Do not read from input data source when the SPARQL Update template is static, i.e. when it always runs the same static SPARQL Update query exactly once. Add SPARQL Update execution report containing various statistics, e.g. number of queries, query throughput etc. Support Apache Velocity Engine based templates. This adds logic like conditional branching and loops to the templates. For more information visit https://velocity.apache.org SPARQL Select operator generates an execution report with various statistics, e.g. rows processed, runtime etc. SPARQL dataset generates execution report when executing SPARQL Update queries e.g. from the SPARQL Update operator with statistics like remaining queries, time estimation etc. Extended SQL Endpoint documentation. Added local execution to JDBC dataset. The execution is more efficient and pushes limits and group-by columns into the database. While workspace is initializing, subsequent requests will timeout. Config parameter workspace.timeouts.waitForWorkspaceInitialization (in milliseconds, default: 5000ms). Scheduler: Added \u2018Stop On Error\u2019 parameter. If enabled this will stop a scheduler on the first execution error. Default: false . Added addMarkdownDocumentation parameter to /plugins and /plugins/:pluginType REST endpoints in order to request the optional Markdown documentation for plugins. Added SPARQL query timeout parameter in order to limit query execution times to \u2018Knowledge Graph\u2019 and \u2018SPARQL endpoint\u2019 datasets and to the SPARQL Select operator. RDF serialization: Tasks referenced in RDF serialization with di:output and di:task use the correct project task URIs instead of artificial URIs or literals. Complete Zip Stream support (replacing reliability on zip files only). MultiCsvZip are now BulkResourceDatasets. JDBC dataset: Removed database parameter. If required, the database needs to be specified as part of the JDBC URL, e.g., jdbc:mysql://localhost:port/databaseName . Appending connection parameters to the URL is supported now. Safer defaults: SPARQL Endpoint and Knowledge Graph dataset: Do not clear graphs before execution to prevent accidental deletion of graphs. Knowledge Graph dataset: Increase page size from 1000 to 100,000, because small page sizes lead to suboptimal execution performance. SPARQL Update operator: Decrease batch size from 10 to 1. Upgraded to Apache Spark 2.3.3 SQL datasets: If a URI attribute is specified, the URI will be added as a new column of that name. All projects are loaded first, before any cache or any other autorun activity is started. This improves loading when both the workspace provider and some caches load from the same database. Improved JDBC dataset performance for MySQL and MariaDB by using the \u2018Load Data\u2019 command to load a local CSV file into the database. Shipping with MariaDB JDBC driver. Redirect to original request URL instead of the start page after authenticating a user and logging in. Revised generation of table names in JDBC dataset (see documentation). Keep letter case of table names. Schedulers are always started by the super user, if the super user is configured. On start-up if DataPlatform is configured, DI will wait for DP to become healthy for a configurable amount of time. Parameters: eccencaDataPlatform.health.waitingTimeInSeconds : Overall time in seconds DI should wait for DP to be up and healthy. Setting this to 0 will disable this check. Default: 60. eccencaDataPlatform.health.delayBetweenRetriesInSeconds : Amount of time in seconds to wait between retries. Default: 5. Enhanced JdbcDataset and SqlEndpoint parameters and improve their descriptions. Suppress case changes in SqlEndpoint table names. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v19.10"},{"location":"release-notes/corporate-memory-19-10/#eccenca-datamanager-v1910","text":"This version of eccenca DataManager adds the following new features: New module task Offers a direct resource actions. Interfaces only available by URL. See documentation for more details. Path /task/resource/create allows to create a new resource by given graph and type. General Config parameter js.config.api.defaultTimeout for default UI queries timeout. Config parameter js.config.resourceTable.timeoutDownload for Resource Table timeout on download requests on Explore and Query modules. Validation of mandatory fields in shacline view. Add new property shui:onUpdateUpdate for sh:NodeShape . Module Explore Config parameter js.config.modules.explore.graphlist.whiteList to filter specific graphs. Config parameter js.config.modules.explore.graphlist.internalGraphs to hide specific graphs. Config parameter js.config.modules.explore.navigation.itemsPerPage show items per page in navigation box. Support for inverse property relations. Module Query Config parameter js.config.modules.query.timeout for manual queries. Config parameter js.config.modules.query.graph to define the graph were data is saved and requested. In addition to that, these changes are shipped: General Default pagination size of 20 elements for all Resource Tables. Allow datatype xsd:anyURI for literals. Upgraded to react 16. Module Explore Merged graph view RDFDoc into \u2018resource details view\u2019. Renamed global search label. Graph creation will add the type void:Dataset instead of owl:Ontology . Use the label of the type of the instances for the name of the CSV file downloaded from the Resource Table. Display the context graph in properties and references tables. Module Dataset Adjusted position and tooltip of parameter uriProperty in \u2018Add data stepper\u2019. Module Query Use the dataset label for the name of the CSV file downloaded from the Resource Table. Module Login Renew tokens when they expire. Module Administration Allow to search in IRIs for list of readable and writeable graphs. The following features have been removed in this release: Module Explore Config parameter js.config.modules.explore.graphlist.listQuery which is now obsolete. Config parameter js.config.modules.explore.details.history which is now obsolete as the feature is no longer supported. \u2018History\u2019 tab. Module Sync also known as SubscriptionManagement . In addition to that, multiple stability issues were solved.","title":"eccenca DataManager v19.10"},{"location":"release-notes/corporate-memory-19-10/#eccenca-dataplatform-v1910","text":"This version of eccenca DataPlatform adds the following new features: SPARQL 1.1 Query endpoint An in-iris property to the JSON search parameter to enable search over IRIs. A timeout parameter which allows to configure the maximal amount of milliseconds that a query execution can run. Support for Microsoft Excel ( .xlsx ) file download for SELECT queries. SPARQL 1.1 Update endpoint A timeout parameter which allows to configure the maximal amount of milliseconds that an update execution can run (Stardog only). SPARQL 1.1 Graph Store Protocol multipart/form-data support for HTTP PUT. Added the timeout parameter, which allows to configure the maximal amount of milliseconds that a request execution should run. Documentation for content negotiation by format query parameter. The following features have been removed in this release: Data Sharing: A WebSub based Publish-Subscribe service for RDF named graphs. IoT Permissions Plugin: A plugin which enables the usage of the IoT Permissions Service API 2. OAuth 2.0 authorization server: Issues access tokens to a client after successfully authenticating a user. Authentication: User management via authentication providers as it was only needed by the OAuth 2.0 authorization server. In addition to that, these changes are shipped: Stardog Upgraded support to version 7.0.2. Versioning does no longer work with Stardog 7. Legacy versioning support for Stardog 6 (deprecated). OAuth 2.0: Resource protection is now mandatory (can no longer be disabled, use anonymous access instead). SPARQL 1.1 Query endpoint The value of the string property of the JSON search parameter is now tokenized which means that each token will be searched separately. Only results matching all tokens will be returned. Updated Spring Boot version from 1.5.21 to 1.5.22. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v19.10"},{"location":"release-notes/corporate-memory-19-10/#migration-notes","text":"With the removal of the OAuth 2.0 authorization server capability, many configuration properties have been changed. Removed The properties oauth2.clients.* have been removed. The properties authentication.* have been removed. Moved The property oauth2.jwt.signing.verificationKey has been moved to security.oauth2.resource.jwt.keyValue . The property oauth2.anonymous has been moved to security.oauth2.resource.anonymous . The claims mapping properties under oauth2.resourceServer.claimsMapping.* have been moved to security.oauth2.resource.jwt.claims.* . The properties oauth2.authorizeRequests.* to configure the resources to be protected by the resource server have been moved to security.oauth2.resource.authorizeRequests.* . Added The value of the property security.oauth2.resource.id (defaults to dataplatform ) must be part of the aud (audience) claim in the JWT used to access a protected resource. Don\u2019t forget to update your configuration accordingly. For instance, assuming you have the following old configuration: oauth2 : anonymous : true clients : - id : client secret : secret grantTypes : - authorization_code redirectUris : - http://example.org/oauth/client jwt : enabled : true signing : verificationKey : | -----BEGIN PUBLIC KEY----- ... -----END PUBLIC KEY----- resourceServer : claimsMapping : username : 'preferred_username' clientId : 'azp' groups : key : 'groups' The migrated properties should look like this: security : oauth2 : resource : anonymous : true # optional, defaults to `false` jwt : keyValue : | -----BEGIN PUBLIC KEY----- ... -----END PUBLIC KEY----- claims : username : preferred_username # optional, defaults to `preferred_username` groups : groups # optional, defaults to `groups` clientId : azp # optional, defaults to `azp`","title":"Migration Notes"},{"location":"release-notes/corporate-memory-20-03/","text":"Corporate Memory 20.03 \u00a4 Corporate Memory 20.03 is the first release in 2020. The highlights of this release are: DataIntegration supports resources to be stored in an AWS S3 buckets. Rich SHACL forms can be used for the creation of new resources. New BUILD module is introduced in DataManager to provide an experts shortcut to DataIntegration. SPARQL queries can now be used to define arbitrary result tables directly in SHACL views . Object properties can be switched between chips and resource table view in SHACL views . cmemc , our Corporate Memory Command Line Interface is now generally available Warning With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v20.03 eccenca DataIntegration v20.03 eccenca DataManager v20.03 eccenca Corporate Memory Control (cmemc) v20.03 More detailed release notes for these versions are listed below. eccenca DataIntegration v20.03 \u00a4 This version of eccenca DataIntegration adds the following new features: Support for additional value types for mapping targets (XML Schema date/time types, duration, etc.). More date types to DateTypeParser . Script operator can also be used in local execution mode. Operator search in mapping rule editor. Safe-mode that prevents access to external data systems, e.g. JDBC, SPARQL dataset: Data access in executed workflows is not affected by the safe-mode. Safe-mode can be toggled on and off at runtime in the UI. To enable safe-mode, set following parameter in the config: config.production.safeMode = true . Config parameter caches.config.enableAutoRun , to enable/disable automatic execution of caches (default: true ). Knowledge Graph File Upload Operator: Lets the user upload N-Triples files from the file repository into a DataPlatform graph. Support for file resource repositories on S3. In addition to that, these changes are shipped: Improved password encryption Using AES-256 instead of AES-128. If no valid key has been configured in production mode, application does not start. Better error messages, if key is invalid. Secret AES-256 key is generated from the configured key using SHA256 hashing, allowing for arbitrarily long keys. Improved SQL writing performance for MariaDB and MySQL. Rework of the dataset view: If a dataset is opened, the SPARQL (for RDF datasets) or table view (other datasets) is directly opened. Added Material Design formatting. Added scrollbars to tables with many columns. Active learning UI uses Material Design cards. the config endpoint /core/config is no longer available when running in production mode. If a mapping reads from a CSV column that does not exist, the mapping still executes successfully, but a warning is displayed in the execution report. With XML dataset in streaming mode default URIs are now created by using the row and column numbers of the XML element instead of a hash value. Reduce memory foot print of linking evaluation and execution. We are now sorting tasks in workspace by label. Now displaying the modification date in resource dialog. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v20.03 \u00a4 This version of eccenca DataManager adds the following new features: General Blank nodes are filtered in shacline views. Open external links in a new browser window. shui:valueQuery for tabular representation load of pre-defined queries as a shui:valueQuery . ResourceTable now allow to resolve labels on download results. Access Control Allow to create user and groups providing just a label. Module Explore Shacl views now allow to switch object property links between chip and ResourceTable view allow to add additional columns, search and filter using a ResourceTable . sh:path is no longer mandatory on Shacl. One of both sh:path or shui:valueQuery is now mandatory. In addition to that, these changes are shipped: General Layout make better use of widescreen estate. Show existing resources linked by an object property in a ResourceTable in edit mode. Module Explore Navigation box uses search query only when a search term is present. Creation of new resources can now make use of rich shacline forms. Add new config parameter modules.explore.navigation.defaultClass that selects a default class EXPLORE should start with when modules.explore.graphlist.defaultGraph is defined. The following features have been removed in this release: Datasets management Config parameter includeOAuthToken is no longer used. DataIntegration authentication will be done in an iFrame instead. Access Control Support for parameter Requires client has been removed from Access Control module. In addition to that, multiple stability issues were solved. eccenca DataPlatform v20.03 \u00a4 This version of eccenca DataPlatform adds the following new features: SPARQL 1.1 Query endpoint Support for non-string literals when using the contains , startsWith and endsWith filter functions. Server side label resolution by using resolveLabels , which allows NONE and LABEL for resolving IRIs to literals. The search parameter utilizes Stardog\u2019s built-in text match instead of SPARQL CONTAINS if a Stardog database is used. The search string is cleaned from special characters and english stop words and conjuncts all search terms. SPARQL 1.1 Update endpoint owl:imports resolution on USING / USING NAMED clauses. /info and /health in addition to defaults /actuator/info and /actuator/health for backward compatibility. Show Redis status in application health if used as cache. The property spring.security.oauth2.resourceserver.jwt.issuerUri or spring.security.oauth2.resourceserver.jwt.jwk-set-uri must now be set in order to allow for JWT (signature) validation (see migration notes below). Access Conditions Allow embedded creation of elements of type eccauth:Account or eccauth:Group . In addition to that, these changes are shipped: Upgraded Stardog support to version 7.1.1. The default value of the property spring.security.oauth2.resourceserver.jwt.claims.clientId has been changed from azp to clientId . The properties under security.oauth2.resource.jwt.claims.* have been moved to spring.security.oauth2.resourceserver.jwt.claims.* . The property security.oauth2.resource.anonymous has been moved to spring.security.oauth2.resourceserver.anonymous . The property http.cors.allowOriginRegex has been moved to http.cors.allowedOrigins . The property http.cors.allowMethods has been moved to http.cors.allowedMethods . The property http.cors.allowHeaders has been moved to http.cors.allowedHeaders . The property http.cors.exposeHeaders has been moved to http.cors.exposedHeaders . The following features have been removed in this release: Versioning support has been removed. Access Conditions Support for eccauth:requiresProtocol and eccauth:requiresClient has been removed. The properties security.oauth2 have been removed. The property http.cors.enabled has been removed. In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v20.03 \u00a4 This version of eccenca Corporate Memory Control (cmemc) adds the following new features: config command group, to list , edit and check configurations graph command group, to list , import , export , delete and open graphs project command group, to list , import , export , create and delete projects query command group, to list and execute local and remote SPARQL queries workflow command group, to list , execute , open or inspect workflows workspace command group, to import and export the workspace ability to work with SSL enabled deployments (add CA certs) Migration Notes \u00a4 DataIntegration \u00a4 With v20.03 the following changes need to be made in your dataintegration.conf file when upgrading from v19.10: Remove the play.crypto.secret property, it has been deprecated with v20.03. Two properties need to be added: play.http.secret.key and plugin.parameters.password.crypt.key both take an arbitrary alpha numerical string of minimum 16 characters length depending on your deployment set them in your production.conf or application.conf DataIntegration configuration file ... play.http.secret.key = \"uiodshfoun78qwg8asd7gfasdasddfgn87gsn8fdsngasdfsngf8ds\" ... plugin.parameters.password.crypt.key = \"uiodshfoun78qwg8\" ... Note In case you are deploying based on the DataIntegration docker images eccenca provides a production.conf configuration file needs to be used, the dataintegration.conf cannot be used to set the play.http.secret.key parameter. Warning The property plugin.parameters.password.crypt.key is used to encrypt / decrypt the passwords stored with you project configuration (e.g. JDBC passwords). When you set or change this property, all passwords in your DataIntegration projects need to be re-entered. DataManager \u00a4 With v20.03 a the new BUILD module is introduced. In order to enable and configure it add the following section to you application.yml : DataManager application.yml BUILD module configuration js.config.modules.build : enable : true url : \"<DI-BASE_URI>/workspace\" Where <DI-BASE-URI> need to point to the DataIntegration URI (e.g. https://host.domain.com/dataintegration ). DataPlatform \u00a4 With v20.03 the following changes need to be made in your application.yml file when upgrading from v19.10: the key http.cors.enabled has been removed the key http.cors.allowOriginRegex has been renamed to http.cors.allowedOrigins and takes now a list of origins: DataPlatform application.yml http.cors configuration http : cors : allowedOrigins : # optional, defaults to allow all: \"*\" - \"http://docker.local\" - \"https://docker.local\" the key security.oauth2.resource.jwt.keyValue has been removed the key spring.security.oauth2.resourceserver.jwt.jwk-set-uri need to be specified. Refer to your keycloaks Corporate Memory (cmem) realm \u201cOpenID Endpoint Configuration\u201d details where the relevant uri is listed as jwks_uri : DataPlatform application.yml spring.security configuration spring : ## OAuth2Properties security : oauth2 : resourceserver : jwt : jwk-set-uri : http://keycloak:8080/auth/realms/cmem/protocol/openid-connect/certs","title":"Corporate Memory 20.03"},{"location":"release-notes/corporate-memory-20-03/#corporate-memory-2003","text":"Corporate Memory 20.03 is the first release in 2020. The highlights of this release are: DataIntegration supports resources to be stored in an AWS S3 buckets. Rich SHACL forms can be used for the creation of new resources. New BUILD module is introduced in DataManager to provide an experts shortcut to DataIntegration. SPARQL queries can now be used to define arbitrary result tables directly in SHACL views . Object properties can be switched between chips and resource table view in SHACL views . cmemc , our Corporate Memory Command Line Interface is now generally available Warning With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v20.03 eccenca DataIntegration v20.03 eccenca DataManager v20.03 eccenca Corporate Memory Control (cmemc) v20.03 More detailed release notes for these versions are listed below.","title":"Corporate Memory 20.03"},{"location":"release-notes/corporate-memory-20-03/#eccenca-dataintegration-v2003","text":"This version of eccenca DataIntegration adds the following new features: Support for additional value types for mapping targets (XML Schema date/time types, duration, etc.). More date types to DateTypeParser . Script operator can also be used in local execution mode. Operator search in mapping rule editor. Safe-mode that prevents access to external data systems, e.g. JDBC, SPARQL dataset: Data access in executed workflows is not affected by the safe-mode. Safe-mode can be toggled on and off at runtime in the UI. To enable safe-mode, set following parameter in the config: config.production.safeMode = true . Config parameter caches.config.enableAutoRun , to enable/disable automatic execution of caches (default: true ). Knowledge Graph File Upload Operator: Lets the user upload N-Triples files from the file repository into a DataPlatform graph. Support for file resource repositories on S3. In addition to that, these changes are shipped: Improved password encryption Using AES-256 instead of AES-128. If no valid key has been configured in production mode, application does not start. Better error messages, if key is invalid. Secret AES-256 key is generated from the configured key using SHA256 hashing, allowing for arbitrarily long keys. Improved SQL writing performance for MariaDB and MySQL. Rework of the dataset view: If a dataset is opened, the SPARQL (for RDF datasets) or table view (other datasets) is directly opened. Added Material Design formatting. Added scrollbars to tables with many columns. Active learning UI uses Material Design cards. the config endpoint /core/config is no longer available when running in production mode. If a mapping reads from a CSV column that does not exist, the mapping still executes successfully, but a warning is displayed in the execution report. With XML dataset in streaming mode default URIs are now created by using the row and column numbers of the XML element instead of a hash value. Reduce memory foot print of linking evaluation and execution. We are now sorting tasks in workspace by label. Now displaying the modification date in resource dialog. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v20.03"},{"location":"release-notes/corporate-memory-20-03/#eccenca-datamanager-v2003","text":"This version of eccenca DataManager adds the following new features: General Blank nodes are filtered in shacline views. Open external links in a new browser window. shui:valueQuery for tabular representation load of pre-defined queries as a shui:valueQuery . ResourceTable now allow to resolve labels on download results. Access Control Allow to create user and groups providing just a label. Module Explore Shacl views now allow to switch object property links between chip and ResourceTable view allow to add additional columns, search and filter using a ResourceTable . sh:path is no longer mandatory on Shacl. One of both sh:path or shui:valueQuery is now mandatory. In addition to that, these changes are shipped: General Layout make better use of widescreen estate. Show existing resources linked by an object property in a ResourceTable in edit mode. Module Explore Navigation box uses search query only when a search term is present. Creation of new resources can now make use of rich shacline forms. Add new config parameter modules.explore.navigation.defaultClass that selects a default class EXPLORE should start with when modules.explore.graphlist.defaultGraph is defined. The following features have been removed in this release: Datasets management Config parameter includeOAuthToken is no longer used. DataIntegration authentication will be done in an iFrame instead. Access Control Support for parameter Requires client has been removed from Access Control module. In addition to that, multiple stability issues were solved.","title":"eccenca DataManager v20.03"},{"location":"release-notes/corporate-memory-20-03/#eccenca-dataplatform-v2003","text":"This version of eccenca DataPlatform adds the following new features: SPARQL 1.1 Query endpoint Support for non-string literals when using the contains , startsWith and endsWith filter functions. Server side label resolution by using resolveLabels , which allows NONE and LABEL for resolving IRIs to literals. The search parameter utilizes Stardog\u2019s built-in text match instead of SPARQL CONTAINS if a Stardog database is used. The search string is cleaned from special characters and english stop words and conjuncts all search terms. SPARQL 1.1 Update endpoint owl:imports resolution on USING / USING NAMED clauses. /info and /health in addition to defaults /actuator/info and /actuator/health for backward compatibility. Show Redis status in application health if used as cache. The property spring.security.oauth2.resourceserver.jwt.issuerUri or spring.security.oauth2.resourceserver.jwt.jwk-set-uri must now be set in order to allow for JWT (signature) validation (see migration notes below). Access Conditions Allow embedded creation of elements of type eccauth:Account or eccauth:Group . In addition to that, these changes are shipped: Upgraded Stardog support to version 7.1.1. The default value of the property spring.security.oauth2.resourceserver.jwt.claims.clientId has been changed from azp to clientId . The properties under security.oauth2.resource.jwt.claims.* have been moved to spring.security.oauth2.resourceserver.jwt.claims.* . The property security.oauth2.resource.anonymous has been moved to spring.security.oauth2.resourceserver.anonymous . The property http.cors.allowOriginRegex has been moved to http.cors.allowedOrigins . The property http.cors.allowMethods has been moved to http.cors.allowedMethods . The property http.cors.allowHeaders has been moved to http.cors.allowedHeaders . The property http.cors.exposeHeaders has been moved to http.cors.exposedHeaders . The following features have been removed in this release: Versioning support has been removed. Access Conditions Support for eccauth:requiresProtocol and eccauth:requiresClient has been removed. The properties security.oauth2 have been removed. The property http.cors.enabled has been removed. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v20.03"},{"location":"release-notes/corporate-memory-20-03/#eccenca-corporate-memory-control-cmemc-v2003","text":"This version of eccenca Corporate Memory Control (cmemc) adds the following new features: config command group, to list , edit and check configurations graph command group, to list , import , export , delete and open graphs project command group, to list , import , export , create and delete projects query command group, to list and execute local and remote SPARQL queries workflow command group, to list , execute , open or inspect workflows workspace command group, to import and export the workspace ability to work with SSL enabled deployments (add CA certs)","title":"eccenca Corporate Memory Control (cmemc) v20.03"},{"location":"release-notes/corporate-memory-20-03/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-20-03/#dataintegration","text":"With v20.03 the following changes need to be made in your dataintegration.conf file when upgrading from v19.10: Remove the play.crypto.secret property, it has been deprecated with v20.03. Two properties need to be added: play.http.secret.key and plugin.parameters.password.crypt.key both take an arbitrary alpha numerical string of minimum 16 characters length depending on your deployment set them in your production.conf or application.conf DataIntegration configuration file ... play.http.secret.key = \"uiodshfoun78qwg8asd7gfasdasddfgn87gsn8fdsngasdfsngf8ds\" ... plugin.parameters.password.crypt.key = \"uiodshfoun78qwg8\" ... Note In case you are deploying based on the DataIntegration docker images eccenca provides a production.conf configuration file needs to be used, the dataintegration.conf cannot be used to set the play.http.secret.key parameter. Warning The property plugin.parameters.password.crypt.key is used to encrypt / decrypt the passwords stored with you project configuration (e.g. JDBC passwords). When you set or change this property, all passwords in your DataIntegration projects need to be re-entered.","title":"DataIntegration"},{"location":"release-notes/corporate-memory-20-03/#datamanager","text":"With v20.03 a the new BUILD module is introduced. In order to enable and configure it add the following section to you application.yml : DataManager application.yml BUILD module configuration js.config.modules.build : enable : true url : \"<DI-BASE_URI>/workspace\" Where <DI-BASE-URI> need to point to the DataIntegration URI (e.g. https://host.domain.com/dataintegration ).","title":"DataManager"},{"location":"release-notes/corporate-memory-20-03/#dataplatform","text":"With v20.03 the following changes need to be made in your application.yml file when upgrading from v19.10: the key http.cors.enabled has been removed the key http.cors.allowOriginRegex has been renamed to http.cors.allowedOrigins and takes now a list of origins: DataPlatform application.yml http.cors configuration http : cors : allowedOrigins : # optional, defaults to allow all: \"*\" - \"http://docker.local\" - \"https://docker.local\" the key security.oauth2.resource.jwt.keyValue has been removed the key spring.security.oauth2.resourceserver.jwt.jwk-set-uri need to be specified. Refer to your keycloaks Corporate Memory (cmem) realm \u201cOpenID Endpoint Configuration\u201d details where the relevant uri is listed as jwks_uri : DataPlatform application.yml spring.security configuration spring : ## OAuth2Properties security : oauth2 : resourceserver : jwt : jwk-set-uri : http://keycloak:8080/auth/realms/cmem/protocol/openid-connect/certs","title":"DataPlatform"},{"location":"release-notes/corporate-memory-20-06/","text":"Corporate Memory 20.06 \u00a4 Corporate Memory 20.06 is the second release in 2020. The highlights of this release are: Jinja template support in DataIntegration workflows Physical unit normalization and distance measure operators Versioning of user edits in SHACL based forms Named query API to get data without SPARQL know-how OpenAPI compliant DataPlatform API specification and UI Preview/Beta release of the upcoming DataIntegration Workspace Preview/Beta support for GraphDB as triple store backend \u2192 Vendor Homepage Warning With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below. In addition to that, cmemc has some changed default outputs. This release delivers the following component versions: eccenca DataPlatform v20.06 eccenca DataIntegration v20.06 eccenca DataManager v20.06 eccenca Corporate Memory Control (cmemc) v20.06 eccenca Corporate Memory PowerBI Connector v20.06 More detailed release notes for these versions are listed below. eccenca DataIntegration v20.06 \u00a4 This version of eccenca DataIntegration adds the following new features: Workflow operator that evaluates a user-defined template on entities. Jinja templating language is supported. Can be used after a transformation or directly after datasets that output a single table, such as CSV or Excel. For each input entity, an output entity is generated that provides a single output attribute, which contains the evaluated template. DataIntegration transformation tasks can be used as Jinja filters. Operators to normalize and compare physical quantities. Transform rule operator to normalize physical quantities to a base unit. Distance measure to compare two physical quantities. The transform evaluate tab allows the selection of the mapping rule to be evaluated. Rule operator for generating UUIDs. Complete rework of the workspace UI In addition to that, these changes are shipped: Removed deprecated SQL query strategies. Updated eccenca logo and favicon. Consistent resource deletion behavior for resource repositories: For resource repositories that do not share resources between projects, resources are removed on project deletion. For resource repositories that do share resources between projects, resources are NOT removed on project deletion. In both cases, the user is informed in the UI about the behavior of the configured resource repository. RDF Workspace Provider: Improved reading of project data if Graph Store protocol is supported by RDF endpoint. RDF Workspace Provider: Improved import of projects if Graph Store protocol is supported by RDF endpoint. More consistent labels for tasks, operators and their parameters. If active learning is started with an existing linkage rule, it\u2019s also used to generate the unlabeled pool. ExcelMapTransformer reloads the referenced resource if it\u2019s modification time changed. For performance reasons the check may be deferred by some seconds. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v20.06 \u00a4 This version of eccenca DataManager adds the following new features: Query Module The results of CONSTRUCT queries can now be downloaded. General Support to send base64 encoded queries in SPARQL and framed requests (configure with js.config.api.sparqlQueryBase64Enconded ). Add new helper for unify datatype info (as regex) Build module Show/hide build module based on user (ACL) action urn:eccenca:di . Shacline Allow sh:name and sh:description to used multiple times - in different languages - in the SHAPE definitions. Support for the xsd formats gYearMonth , gYear , gMonthDay , gDay , gMonth and duration . In addition to that, these changes are shipped: General Upgrade node, npm and yarn. Allow sh:pattern and sh:flags for shapes definition of literals of type string, numeric and dates. Shacline Shacl-groups without content are now hidden. ReadOnly properties are not available for adding on edit mode. Use languages from js.config.titleHelper.languages from config file as default languages. In addition to that, multiple stability issues were solved. eccenca DataPlatform v20.06 \u00a4 This version of eccenca DataPlatform adds the following new features: SPARQL 1.1 Query & Update endpoint Support for base64 encoded query strings. Manual Edit Endpoint Fine grained access control schemes for edits performed on SHACL shapes. Basic versioning of those edits. Shapes that support the configuration of those features directly in DataManager OpenAPI 3 compliant API documentation OpenAPI compliant documentation of all DataPlatform APIs Swagger UI based browser interface for interactive learning and experiments with the APIs Backend Support Support for GraphDB was added. Additional APIs Title Helper ( /api/explore/title ): Finds short labels for Resources. Named Query ( /api/queries ): Allows passing the identifier of the query and a parameterization to generate a CSV report In addition to that, these changes are shipped: OpenAPI 3 compliant API documentation All REST controllers have been annotated with OpenAPI metadata annotations SecurityConfiguration has been modified to allow access to the /v3/api-docs ( .yaml ) and /swagger-ui endpoints. The following features have been removed in this release: OpenAPI 3 compliant API documentation RAML documentation In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v20.06 \u00a4 This version of eccenca Corporate Memory Control (cmemc) adds the following new features: Shipped with a MacOSX binary (installable as copy deployment or via the homebrew package manager). Support for base64 encoded SPARQL queries and updates ( --base64 ) - this needs eccenca DataPlatform v20.06. Extension of the query list command to list labels and parameters of queries in the query catalog (also the --id-only parameter to get an ID only list). Support for parameterized SPARQL queries: the query execute command now has the option -p / --parameter to provide parameter/value pairs for the query execution. In addition to that, these changes are shipped: query execute default result format is now text/csv for tables and text/turtle for graphs (was * before) Migration Note: use --accept in case you need the old defaults query list now has a tabularized output Migration Note: use --id-only option to get the old URI only list In addition to that, multiple performance and stability issues were solved: cmemc now supports all SPARQL query types when executing a query from a file (there were errors on ASK , DESCRIBE and CONSTRUCT before) all tab completion results are now correctly sorted as well as filtered case-insensitively eccenca Corporate Memory PowerBI Connector (v20.06) \u00a4 This is the first release of our PowerBI Connector which enables PowerBI users to retrieve data into PowerBI, based on collected queries in the Corporate Memory Query Catalog. The feature of this release are: add and delete eccenca Corporate Memory data sources get data out of SELECT queries from the Query Catalog We provided a tutorial for this new component: Consuming Graphs in Power BI Migration Notes \u00a4 DataIntegration \u00a4 With v20.06 the API has been improved: The JSON format of transform, linking and workflow tasks has changed and is now consistent with dataset and custom tasks, i.e. all config parameters are now under property parameters . The JSON format of transform tasks has in addition following changes: output instead of outputs property that is a single string value instead of an array of strings. mappingRule instead of root for the property of the mapping rule. The JSON format of linking tasks had in addition following change: output instead of outputs property that is a single string value instead of an array of strings. The JSON format for the /plugins endpoint has changed. The type attribute is now correctly specifying the JSON schema data type, e.g. string or object . The actual, more specific, parameter type has been renamed to parameterType . JSON format of resources have no relative and absolute path anymore. DataManager \u00a4 With v20.06 a new title helper configuration section is introduced. It is used to define you language preferences in the data: DataManager application.yml BUILD module configuration js.config.titleHelper : languages : - en - '' In addition to that, we introduced a new Access Condition Action which represents the right to use the EXPLORE tab. Please have a look at the access condition documentation and add urn:eccenca:ExploreUserInterface to already existing conditions if needed. cmemc \u00a4 With v20.06 the following changed need to be made: query execute default result format has changed, to keep the previous behavior change your cmemc query execute calls to: cmemc query execute --accept '*' query list has a different default output, to return to the previous behavior change your cmemc query list calls to: cmemc query list --id-only","title":"Corporate Memory 20.06"},{"location":"release-notes/corporate-memory-20-06/#corporate-memory-2006","text":"Corporate Memory 20.06 is the second release in 2020. The highlights of this release are: Jinja template support in DataIntegration workflows Physical unit normalization and distance measure operators Versioning of user edits in SHACL based forms Named query API to get data without SPARQL know-how OpenAPI compliant DataPlatform API specification and UI Preview/Beta release of the upcoming DataIntegration Workspace Preview/Beta support for GraphDB as triple store backend \u2192 Vendor Homepage Warning With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below. In addition to that, cmemc has some changed default outputs. This release delivers the following component versions: eccenca DataPlatform v20.06 eccenca DataIntegration v20.06 eccenca DataManager v20.06 eccenca Corporate Memory Control (cmemc) v20.06 eccenca Corporate Memory PowerBI Connector v20.06 More detailed release notes for these versions are listed below.","title":"Corporate Memory 20.06"},{"location":"release-notes/corporate-memory-20-06/#eccenca-dataintegration-v2006","text":"This version of eccenca DataIntegration adds the following new features: Workflow operator that evaluates a user-defined template on entities. Jinja templating language is supported. Can be used after a transformation or directly after datasets that output a single table, such as CSV or Excel. For each input entity, an output entity is generated that provides a single output attribute, which contains the evaluated template. DataIntegration transformation tasks can be used as Jinja filters. Operators to normalize and compare physical quantities. Transform rule operator to normalize physical quantities to a base unit. Distance measure to compare two physical quantities. The transform evaluate tab allows the selection of the mapping rule to be evaluated. Rule operator for generating UUIDs. Complete rework of the workspace UI In addition to that, these changes are shipped: Removed deprecated SQL query strategies. Updated eccenca logo and favicon. Consistent resource deletion behavior for resource repositories: For resource repositories that do not share resources between projects, resources are removed on project deletion. For resource repositories that do share resources between projects, resources are NOT removed on project deletion. In both cases, the user is informed in the UI about the behavior of the configured resource repository. RDF Workspace Provider: Improved reading of project data if Graph Store protocol is supported by RDF endpoint. RDF Workspace Provider: Improved import of projects if Graph Store protocol is supported by RDF endpoint. More consistent labels for tasks, operators and their parameters. If active learning is started with an existing linkage rule, it\u2019s also used to generate the unlabeled pool. ExcelMapTransformer reloads the referenced resource if it\u2019s modification time changed. For performance reasons the check may be deferred by some seconds. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v20.06"},{"location":"release-notes/corporate-memory-20-06/#eccenca-datamanager-v2006","text":"This version of eccenca DataManager adds the following new features: Query Module The results of CONSTRUCT queries can now be downloaded. General Support to send base64 encoded queries in SPARQL and framed requests (configure with js.config.api.sparqlQueryBase64Enconded ). Add new helper for unify datatype info (as regex) Build module Show/hide build module based on user (ACL) action urn:eccenca:di . Shacline Allow sh:name and sh:description to used multiple times - in different languages - in the SHAPE definitions. Support for the xsd formats gYearMonth , gYear , gMonthDay , gDay , gMonth and duration . In addition to that, these changes are shipped: General Upgrade node, npm and yarn. Allow sh:pattern and sh:flags for shapes definition of literals of type string, numeric and dates. Shacline Shacl-groups without content are now hidden. ReadOnly properties are not available for adding on edit mode. Use languages from js.config.titleHelper.languages from config file as default languages. In addition to that, multiple stability issues were solved.","title":"eccenca DataManager v20.06"},{"location":"release-notes/corporate-memory-20-06/#eccenca-dataplatform-v2006","text":"This version of eccenca DataPlatform adds the following new features: SPARQL 1.1 Query & Update endpoint Support for base64 encoded query strings. Manual Edit Endpoint Fine grained access control schemes for edits performed on SHACL shapes. Basic versioning of those edits. Shapes that support the configuration of those features directly in DataManager OpenAPI 3 compliant API documentation OpenAPI compliant documentation of all DataPlatform APIs Swagger UI based browser interface for interactive learning and experiments with the APIs Backend Support Support for GraphDB was added. Additional APIs Title Helper ( /api/explore/title ): Finds short labels for Resources. Named Query ( /api/queries ): Allows passing the identifier of the query and a parameterization to generate a CSV report In addition to that, these changes are shipped: OpenAPI 3 compliant API documentation All REST controllers have been annotated with OpenAPI metadata annotations SecurityConfiguration has been modified to allow access to the /v3/api-docs ( .yaml ) and /swagger-ui endpoints. The following features have been removed in this release: OpenAPI 3 compliant API documentation RAML documentation In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v20.06"},{"location":"release-notes/corporate-memory-20-06/#eccenca-corporate-memory-control-cmemc-v2006","text":"This version of eccenca Corporate Memory Control (cmemc) adds the following new features: Shipped with a MacOSX binary (installable as copy deployment or via the homebrew package manager). Support for base64 encoded SPARQL queries and updates ( --base64 ) - this needs eccenca DataPlatform v20.06. Extension of the query list command to list labels and parameters of queries in the query catalog (also the --id-only parameter to get an ID only list). Support for parameterized SPARQL queries: the query execute command now has the option -p / --parameter to provide parameter/value pairs for the query execution. In addition to that, these changes are shipped: query execute default result format is now text/csv for tables and text/turtle for graphs (was * before) Migration Note: use --accept in case you need the old defaults query list now has a tabularized output Migration Note: use --id-only option to get the old URI only list In addition to that, multiple performance and stability issues were solved: cmemc now supports all SPARQL query types when executing a query from a file (there were errors on ASK , DESCRIBE and CONSTRUCT before) all tab completion results are now correctly sorted as well as filtered case-insensitively","title":"eccenca Corporate Memory Control (cmemc) v20.06"},{"location":"release-notes/corporate-memory-20-06/#eccenca-corporate-memory-powerbi-connector-v2006","text":"This is the first release of our PowerBI Connector which enables PowerBI users to retrieve data into PowerBI, based on collected queries in the Corporate Memory Query Catalog. The feature of this release are: add and delete eccenca Corporate Memory data sources get data out of SELECT queries from the Query Catalog We provided a tutorial for this new component: Consuming Graphs in Power BI","title":"eccenca Corporate Memory PowerBI Connector (v20.06)"},{"location":"release-notes/corporate-memory-20-06/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-20-06/#dataintegration","text":"With v20.06 the API has been improved: The JSON format of transform, linking and workflow tasks has changed and is now consistent with dataset and custom tasks, i.e. all config parameters are now under property parameters . The JSON format of transform tasks has in addition following changes: output instead of outputs property that is a single string value instead of an array of strings. mappingRule instead of root for the property of the mapping rule. The JSON format of linking tasks had in addition following change: output instead of outputs property that is a single string value instead of an array of strings. The JSON format for the /plugins endpoint has changed. The type attribute is now correctly specifying the JSON schema data type, e.g. string or object . The actual, more specific, parameter type has been renamed to parameterType . JSON format of resources have no relative and absolute path anymore.","title":"DataIntegration"},{"location":"release-notes/corporate-memory-20-06/#datamanager","text":"With v20.06 a new title helper configuration section is introduced. It is used to define you language preferences in the data: DataManager application.yml BUILD module configuration js.config.titleHelper : languages : - en - '' In addition to that, we introduced a new Access Condition Action which represents the right to use the EXPLORE tab. Please have a look at the access condition documentation and add urn:eccenca:ExploreUserInterface to already existing conditions if needed.","title":"DataManager"},{"location":"release-notes/corporate-memory-20-06/#cmemc","text":"With v20.06 the following changed need to be made: query execute default result format has changed, to keep the previous behavior change your cmemc query execute calls to: cmemc query execute --accept '*' query list has a different default output, to return to the previous behavior change your cmemc query list calls to: cmemc query list --id-only","title":"cmemc"},{"location":"release-notes/corporate-memory-20-10/","text":"Corporate Memory 20.10 \u00a4 Corporate Memory 20.10 is the third release in 2020. The highlights of this release are: Release of the new DataIntegration workspace . Support for statement annotations , in order to express knowledge about specific statements. Support for tracking change sets for all shape based editing activities. Support for automation of vocabulary and dataset management with cmemc . Warning With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc has a change default behaviour. This release delivers the following component versions: eccenca DataPlatform v20.10.1 eccenca DataIntegration v20.10 eccenca DataManager v20.10.1 eccenca Corporate Memory Control (cmemc) v20.10 eccenca Corporate Memory PowerBI Connector v20.10 More detailed release notes for these versions are listed below. eccenca DataIntegration v20.10.1 \u00a4 This version of eccenca DataIntegration adds the following new features: Improvements to new Workspace UI: New Workspace UI allows to export projects with and without file resources. Basic support for multiple languages in the New Workspace UI. Initially English and German are supported and plugins are not translated yet. Multi-step Project import in new workspace UI. Multi-step, asynchronous project import REST API. Profiling UI component to start dataset profiling and show profiling information in the dataset preview. Navigation menu in new workspace UI. In link tables, clicking on an entity redirects to the corresponding resource in DataManager, if the entity is coming from an RDF dataset. New/improved operators: New transform operator to retrieve lat/long of a location from a specified API in order to normalize location data. New operator to scale similarity values in linking rules by a specified factor. Email operator improvements: multiple recipients in TO, CC and BCC CC and BCC recipients Timeout parameter SSL support Improvements to datasets CSV Dataset supports UTF-8-BOM encoding for writing CSVs that open correctly in Excel. Support for #id and #text paths in JSON sources. API improvements Task activities API that allows to fetch a list of task activities with optional project and status filter. Profiling data is available via the API. Global vocabulary cache that holds all installed vocabularies from the DataPlatform. REST endpoint to trigger cache updates. In addition to that, these changes are shipped: Vocabulary caches are not persisted between reboots and workspace reloads Disable geo location data type detector by default via plugin.blacklist parameter Item search API returns plugin IDs where available Expose some Amazon S3 client configuration. Can be changed in the Dataintegration configuration now Improvements to Spark execution engine Entities are stored in DataFrames instead of RDDs Performance improvements Bugfixes Check for usages of resources in all tasks, before deleting them. This was checked only in datasets before File management improvements Allow multi file uploads Ask to replace existing files Allow to delete uploaded files in upload dialog When deleting files check for usages of resources in all items, before deleting them, e.g. transform tasks. This was checked only in datasets before When deleting files that are in use, link the dependent items Upload modal does not close when clicking outside of the modal If the limit parameter of the itemSearch API is set to 0, it will now return all search results instead of none Frontend initialisation endpoint returns initial language preference and configured DM base URL Finally, the following performance and stability issues were solved: Regression: the output of a transformation is lost after reloading Added warning to the CSV datasets \u2018maxCharsPerColumn\u2019 parameter to make it clear that it affects the heap size Fixed reading of JSON files that contain Unicode byte order marks (BOMs) Workflow not interrupted on invalid XML from Triple-store Fixed generating paths for JSON files that contain keys with special characters, such as spaces. Those will be encoded now Project\u2019s rdfs:label uses project ID instead of label Generate consistent URIs for object mappings on JSON files Caches have not been written if the XML workspace provider was used Do not recreate caches on every run In link tables, the header shows the task labels instead of the task ids Fixed search field in link tables (did not work with characters that need to be URL encoded) Meta data description does not maintain whitespace formatting in XML serialisation New workspace UI has invalid favicon Creating a new project with description does not store the description in the new workspace UI XML Dataset: Values that include HTML entities are not retrieved Support for MS Internet Explorer 11 in new workspace Logout action not working. Should perform a global logout Deleting S3 backed resources broken due to a slash added to filenames Update PostgreSQL driver to v42.2.14 because of security vulnerability eccenca DataManager v20.10.1 \u00a4 This version of eccenca DataManager adds the following new features: General Add translations and i18n language selection (and ship english and german translations) Allow for Annotation of Statements with additional meta data Integrate with the new DataIntegration workspace (Data Integration Tab) Shacline Add support for \u2018sh:languageIn\u2019 (as multiple values) in literal properties Resource Tables Allow Lucene syntax in the search field of any resource table ( Query Syntax ) This search will be applied to the label(s) configured in proxy.labelProperties (cf. DataPlatform ); by default the search will only be applied to the first column, the labels of the selected resource In addition to that, these changes are shipped: Shacline Use the new resource/shaped API to generate / save shacl forms. Rendering empty fields on every change Add class triple to save only if class is a string. Prevent labels to be cloned on adding a new block. Nested Table query now defines default graph {graph} can now be used as a placeholder in RFC6570 URI Template string ResourceTable Download data does not retain column order Add pagination/limit on config file Lock Drag and Drop while adding columns to prevent collision Update default pagination limit to 25 and default pagination interval to 5, 10, 25, 100, 500, 1000 General use new backend API to retrieve labels. use new backend API to retrieve facets (possible columns) DEPRECATE titleHelper configuration parameters BREAKING remove support for Internet Explorer 11. Disable Datasets module, moved to Data Integration Disable Build module, moved to Data Integration ResourceSelect Wait until click on it to load values. Explore Cyclic references on Tabs content crash the app modules.explore.navigation.topQuery changed in order to list configured graph classes ( shui:managedClasses ) Update Navigation pagination limit to 15 Load ResourceTable pagination limit from config file In addition to that, multiple performance and stability issues were solved. eccenca DataPlatform v20.10 \u00a4 This version of eccenca DataPlatform adds the following new features: Custom endpoint Create custom json endpoints by defining a query for retrieving the data and a template for transforming the result. Concise Boundary Description retrieval depth is adjustable. New submodule :src:it for integration tests Statement Annotations/Metadata APIs for providing access and managing existing relations Additional APIs Explore Facets ( /api/explore/facets ): Lists the properties of a class or query. Graph List ( /api/graphs/list ): Returns a list of graphs readable by the current user, optionally including OWL imports. Graph List Detailed ( /api/graphs/list-detailed ): Like the previous one, but adding details of triples, classes and instances counts. Added openapi.server.urls env variable in order to define custom baseUrl to be used in Added resource shaping to the backend, this includes Resource ( /api/resources ) api for getting information about individual resources Shape ( /api/shapes ) api for applying shape information onto the graph Statement Level Metadata ( /api/statementmetadata/ ) management for adding statement annotations. Added Caching to internal handling of prefixes, vocabularies and shapes lists. Caches are invalidated by updates. Added Showcase ( /api/admin/showcase ) endpoint, which inserts a scalable test dataset into the configured endpoint. In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v20.10 \u00a4 This version of cmemc adds the following new features: A dataset command group, enabling users to create , delete and update datasets as well as upload and download dataset file resources. A vocabulary command group, enabling users to manage vocabularies similar to the vocabulary catalog . The query execute command has some new options for limit, offset distinct and timeout settings. In addition to that, these changes are shipped: Added: The workflow status command has a --project option Changed: The graph import command outputs a replace/add status message per graph. Much faster workflow status retrieval by using a new activity API The dataset export command default file template changed to {{date}}-{{connection}}-{{id}}.project The query execute command now uses POST instead of GET requests for SPARQL queries Fixed: The graph import --replace command does not re-replace a same graph with a different file anymore. The completion of --filename-template resulted in files with wrong chars. The python version is disabled in completion mode. eccenca Corporate Memory PowerBI Connector (v20.10) \u00a4 This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI Migration Notes \u00a4 DataIntegration \u00a4 XML serialization for meta data elements is not forward compatible, i.e. projects exported with this version cannot be imported in older DataIntegration versions. The logout URL needs to be set to make sure that DataIntegration also triggers a logout inside the Keycloak instance: oauth.logoutRedirectUrl = ${DEPLOY_BASE_URL}\"/auth/realms/cmem/protocol/openid-connect/logout?redirect_uri=\"${DEPLOY_BASE_URL} DataManager \u00a4 The graphInfo flag in the explore module is now enabled by default. Due to the introduction of the new DataIntegration workspace these changes need to be applied: The modules build as well as datasets are disabled now by default. The module explore is the default first entry point ( startsWith ). This section needs to be added to each workspace configuration: yaml DIWorkspace: enable: true url: /dataintegration/workbench cmemc \u00a4 If your automation scripts rely on the created file name of the project export command, you need to change your scripts and set the old export name explicitly with -t {{id}} .","title":"Corporate Memory 20.10"},{"location":"release-notes/corporate-memory-20-10/#corporate-memory-2010","text":"Corporate Memory 20.10 is the third release in 2020. The highlights of this release are: Release of the new DataIntegration workspace . Support for statement annotations , in order to express knowledge about specific statements. Support for tracking change sets for all shape based editing activities. Support for automation of vocabulary and dataset management with cmemc . Warning With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc has a change default behaviour. This release delivers the following component versions: eccenca DataPlatform v20.10.1 eccenca DataIntegration v20.10 eccenca DataManager v20.10.1 eccenca Corporate Memory Control (cmemc) v20.10 eccenca Corporate Memory PowerBI Connector v20.10 More detailed release notes for these versions are listed below.","title":"Corporate Memory 20.10"},{"location":"release-notes/corporate-memory-20-10/#eccenca-dataintegration-v20101","text":"This version of eccenca DataIntegration adds the following new features: Improvements to new Workspace UI: New Workspace UI allows to export projects with and without file resources. Basic support for multiple languages in the New Workspace UI. Initially English and German are supported and plugins are not translated yet. Multi-step Project import in new workspace UI. Multi-step, asynchronous project import REST API. Profiling UI component to start dataset profiling and show profiling information in the dataset preview. Navigation menu in new workspace UI. In link tables, clicking on an entity redirects to the corresponding resource in DataManager, if the entity is coming from an RDF dataset. New/improved operators: New transform operator to retrieve lat/long of a location from a specified API in order to normalize location data. New operator to scale similarity values in linking rules by a specified factor. Email operator improvements: multiple recipients in TO, CC and BCC CC and BCC recipients Timeout parameter SSL support Improvements to datasets CSV Dataset supports UTF-8-BOM encoding for writing CSVs that open correctly in Excel. Support for #id and #text paths in JSON sources. API improvements Task activities API that allows to fetch a list of task activities with optional project and status filter. Profiling data is available via the API. Global vocabulary cache that holds all installed vocabularies from the DataPlatform. REST endpoint to trigger cache updates. In addition to that, these changes are shipped: Vocabulary caches are not persisted between reboots and workspace reloads Disable geo location data type detector by default via plugin.blacklist parameter Item search API returns plugin IDs where available Expose some Amazon S3 client configuration. Can be changed in the Dataintegration configuration now Improvements to Spark execution engine Entities are stored in DataFrames instead of RDDs Performance improvements Bugfixes Check for usages of resources in all tasks, before deleting them. This was checked only in datasets before File management improvements Allow multi file uploads Ask to replace existing files Allow to delete uploaded files in upload dialog When deleting files check for usages of resources in all items, before deleting them, e.g. transform tasks. This was checked only in datasets before When deleting files that are in use, link the dependent items Upload modal does not close when clicking outside of the modal If the limit parameter of the itemSearch API is set to 0, it will now return all search results instead of none Frontend initialisation endpoint returns initial language preference and configured DM base URL Finally, the following performance and stability issues were solved: Regression: the output of a transformation is lost after reloading Added warning to the CSV datasets \u2018maxCharsPerColumn\u2019 parameter to make it clear that it affects the heap size Fixed reading of JSON files that contain Unicode byte order marks (BOMs) Workflow not interrupted on invalid XML from Triple-store Fixed generating paths for JSON files that contain keys with special characters, such as spaces. Those will be encoded now Project\u2019s rdfs:label uses project ID instead of label Generate consistent URIs for object mappings on JSON files Caches have not been written if the XML workspace provider was used Do not recreate caches on every run In link tables, the header shows the task labels instead of the task ids Fixed search field in link tables (did not work with characters that need to be URL encoded) Meta data description does not maintain whitespace formatting in XML serialisation New workspace UI has invalid favicon Creating a new project with description does not store the description in the new workspace UI XML Dataset: Values that include HTML entities are not retrieved Support for MS Internet Explorer 11 in new workspace Logout action not working. Should perform a global logout Deleting S3 backed resources broken due to a slash added to filenames Update PostgreSQL driver to v42.2.14 because of security vulnerability","title":"eccenca DataIntegration v20.10.1"},{"location":"release-notes/corporate-memory-20-10/#eccenca-datamanager-v20101","text":"This version of eccenca DataManager adds the following new features: General Add translations and i18n language selection (and ship english and german translations) Allow for Annotation of Statements with additional meta data Integrate with the new DataIntegration workspace (Data Integration Tab) Shacline Add support for \u2018sh:languageIn\u2019 (as multiple values) in literal properties Resource Tables Allow Lucene syntax in the search field of any resource table ( Query Syntax ) This search will be applied to the label(s) configured in proxy.labelProperties (cf. DataPlatform ); by default the search will only be applied to the first column, the labels of the selected resource In addition to that, these changes are shipped: Shacline Use the new resource/shaped API to generate / save shacl forms. Rendering empty fields on every change Add class triple to save only if class is a string. Prevent labels to be cloned on adding a new block. Nested Table query now defines default graph {graph} can now be used as a placeholder in RFC6570 URI Template string ResourceTable Download data does not retain column order Add pagination/limit on config file Lock Drag and Drop while adding columns to prevent collision Update default pagination limit to 25 and default pagination interval to 5, 10, 25, 100, 500, 1000 General use new backend API to retrieve labels. use new backend API to retrieve facets (possible columns) DEPRECATE titleHelper configuration parameters BREAKING remove support for Internet Explorer 11. Disable Datasets module, moved to Data Integration Disable Build module, moved to Data Integration ResourceSelect Wait until click on it to load values. Explore Cyclic references on Tabs content crash the app modules.explore.navigation.topQuery changed in order to list configured graph classes ( shui:managedClasses ) Update Navigation pagination limit to 15 Load ResourceTable pagination limit from config file In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v20.10.1"},{"location":"release-notes/corporate-memory-20-10/#eccenca-dataplatform-v2010","text":"This version of eccenca DataPlatform adds the following new features: Custom endpoint Create custom json endpoints by defining a query for retrieving the data and a template for transforming the result. Concise Boundary Description retrieval depth is adjustable. New submodule :src:it for integration tests Statement Annotations/Metadata APIs for providing access and managing existing relations Additional APIs Explore Facets ( /api/explore/facets ): Lists the properties of a class or query. Graph List ( /api/graphs/list ): Returns a list of graphs readable by the current user, optionally including OWL imports. Graph List Detailed ( /api/graphs/list-detailed ): Like the previous one, but adding details of triples, classes and instances counts. Added openapi.server.urls env variable in order to define custom baseUrl to be used in Added resource shaping to the backend, this includes Resource ( /api/resources ) api for getting information about individual resources Shape ( /api/shapes ) api for applying shape information onto the graph Statement Level Metadata ( /api/statementmetadata/ ) management for adding statement annotations. Added Caching to internal handling of prefixes, vocabularies and shapes lists. Caches are invalidated by updates. Added Showcase ( /api/admin/showcase ) endpoint, which inserts a scalable test dataset into the configured endpoint. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v20.10"},{"location":"release-notes/corporate-memory-20-10/#eccenca-corporate-memory-control-cmemc-v2010","text":"This version of cmemc adds the following new features: A dataset command group, enabling users to create , delete and update datasets as well as upload and download dataset file resources. A vocabulary command group, enabling users to manage vocabularies similar to the vocabulary catalog . The query execute command has some new options for limit, offset distinct and timeout settings. In addition to that, these changes are shipped: Added: The workflow status command has a --project option Changed: The graph import command outputs a replace/add status message per graph. Much faster workflow status retrieval by using a new activity API The dataset export command default file template changed to {{date}}-{{connection}}-{{id}}.project The query execute command now uses POST instead of GET requests for SPARQL queries Fixed: The graph import --replace command does not re-replace a same graph with a different file anymore. The completion of --filename-template resulted in files with wrong chars. The python version is disabled in completion mode.","title":"eccenca Corporate Memory Control (cmemc) v20.10"},{"location":"release-notes/corporate-memory-20-10/#eccenca-corporate-memory-powerbi-connector-v2010","text":"This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI","title":"eccenca Corporate Memory PowerBI Connector (v20.10)"},{"location":"release-notes/corporate-memory-20-10/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-20-10/#dataintegration","text":"XML serialization for meta data elements is not forward compatible, i.e. projects exported with this version cannot be imported in older DataIntegration versions. The logout URL needs to be set to make sure that DataIntegration also triggers a logout inside the Keycloak instance: oauth.logoutRedirectUrl = ${DEPLOY_BASE_URL}\"/auth/realms/cmem/protocol/openid-connect/logout?redirect_uri=\"${DEPLOY_BASE_URL}","title":"DataIntegration"},{"location":"release-notes/corporate-memory-20-10/#datamanager","text":"The graphInfo flag in the explore module is now enabled by default. Due to the introduction of the new DataIntegration workspace these changes need to be applied: The modules build as well as datasets are disabled now by default. The module explore is the default first entry point ( startsWith ). This section needs to be added to each workspace configuration: yaml DIWorkspace: enable: true url: /dataintegration/workbench","title":"DataManager"},{"location":"release-notes/corporate-memory-20-10/#cmemc","text":"If your automation scripts rely on the created file name of the project export command, you need to change your scripts and set the old export name explicitly with -t {{id}} .","title":"cmemc"},{"location":"release-notes/corporate-memory-20-12/","text":"Corporate Memory 20.12 \u00a4 Corporate Memory 20.12 is the fourth release in 2020. The highlights of this release are: Build: With the integration of all main views of the Data Integration build workbench, building Knowledge Graphs was never so smooth and streamlined. Automate: With cmemc\u2019s workflow io command, execution of workflows with variable file payload (input) as well as receiving data from a workflows (output) was never so easy before. Please refer to the corresponding tutorial. Warning With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc deprecates a command which will be removed in the next release. This release delivers the following component versions: eccenca DataPlatform v20.12 eccenca DataIntegration v20.12 eccenca DataManager v20.12 eccenca Corporate Memory Control (cmemc) v20.12 eccenca Corporate Memory PowerBI Connector v20.12 More detailed release notes for these versions are listed below. eccenca DataIntegration v20.12 \u00a4 This version of eccenca DataIntegration adds the following new features: Improvements to new Workspace UI Existing views (e.g. editors and reporting interfaces) are now integrated. Knowledge graph datasets show an embedded query and explore view. Quick search dialog: The hotkey / allows to quickly switch between projects and tasks. Optimized DataPlatform entity retrieval strategy for stability and performance. RDF workspace backend does write additional convenience properties for transform tasks and linking tasks di:usedSourceClass : lists all classes for which entities are read by this task. di:usedSourceProperty : lists all properties that are read by this task. di:usedTargetClass : lists all classes for which entities are written by this task. di:usedTargetProperty : lists all properties that are written by this task. Project resource download button. Persisted execution reports: Added a configurable report manager that persists execution reports and allows to retrieve previous reports. Configurable retention time. Reports older than that will be deleted. Error output for transformations: Transformations have a new parameter \u201cerror output\u201d. When executing the workflow, all erroneous entities together with the error description are written to the configured dataset. Add JSON sink Simple variable workflow execution REST endpoint Allows to execute simple variables workflow (at most one variable input and/or output dataset) Input is provided via query parameters or directly in the request body Output is defined by the ACCEPT header and is output in the corresponding MIME type in the response body In addition to that, these changes are shipped: ExcelMap operator: If there are multiple values for a given key, it now returns all values instead of just the last one Project file multi-upload: Upload one file after the other instead of all at once The configuration parameter workbench.showHeader is no longer supported. Instead, a URL parameter is used to decide whether the header should be shown. Data preview for XML, CSV and JSON now automatically loaded and shown on the dataset details page. After deleting an item the user is now redirected to the project page (previously to the workbench page). In addition to that, multiple performance and stability issues were solved. eccenca DataManager v20.12 \u00a4 This version of eccenca DataManager adds the following new features: General: Support for datatype rdf:HTML in object view. Data Integration: Add config parameter DIWorkspace.baseURL for logout. In addition to that, these changes are shipped General: Add error message if browser is not compatible: Safari, IE. Override default config language with workspace language if user did not select a language before. Shacline: Redirect to new resource after use the clone resource feature. Load graph permissions directly from the data request. In addition to that, multiple performance and stability issues were solved. eccenca DataPlatform v20.12 \u00a4 This version of eccenca DataPlatform ships the following changes: Spring libraries: Spring Boot version upgraded to 2.2.10.RELEASE , Spring Cloud version upgraded to Hoxton.RELEASE . Bootstrap & Vocabularies: cmem ontologies and graphs can now be updated using the bootstrap endpoint. Statement Annotation: additional endpoints for browsing and bulk editing Graph List: enriched endpoint with additional information Label Resolution: per default, in case no language matches, the precedence is ignored an any one value of the defined properties is taken as fallback. in case of multiple matches, the alphabetically first entry is chosen. Localization / i18n (20.10.1): language selection in titlehelper, shapes and facets API uses shui:languageIn instead of sh:languageIn In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v20.12 \u00a4 This version of cmemc adds the following new features: The workflow io command was added to allow for executing workflows with variable file payload (input) and receive data from a workflow (output). This feature is described in the advanced tutorial Processing data with variable input workflows . In addition to that, the concepts of io workflows is described in Workflow execution and orchestration . The admin command group was added and includes the following commands: bootstrap - Update/Import bootstrap data. showcase - Create showcase data. status - Output health and version information. The template option of the graph export command allows for using the {{iriname}} placeholder now. In addition to that, these changes are shipped: The config check command outputs a deprecation warning now (use the admin status command instead). cmemc now sends a User-Agent header with every call, currently: cmemc/20.12 (Python 3.7.7) eccenca Corporate Memory PowerBI Connector v20.12 \u00a4 This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI . Migration Notes \u00a4 DataIntegration \u00a4 No Forward Compatibility of Dataintegration Projects Exports Due to a change in the internal XML serialization DI projects from this version can not be imported into instances running older version of DataIntegration. Please try the following workaround if this is something you need to perform. Contact our support team in case this procedure does not work in your case: Info download your project resources (if needed) export the project using cmemc: cmemc project export \u2013type rdfTurtle import the project at the back-level instance using cmemc: cmemc project import .project.ttl upload your file resources to the back-level instance (if needed) Configuration Remove the following configuration parameter: workbench.showHeader In order to enable the new Execution Report Manager you need to configure the respective plugin, see \u201cExecution Report Manager\u201d in DataIntegration for Details. DataManager \u00a4 In your workspaces configuration add DIWorkspace.baseUrl (mostly this will be \"/dataintegration\" ): js.config.workspaces : default : ... DIWorkspace : ... baseUrl : /dataintegration cmemc \u00a4 The config check command has been deprecated, please use the admin status command instead.","title":"Corporate Memory 20.12"},{"location":"release-notes/corporate-memory-20-12/#corporate-memory-2012","text":"Corporate Memory 20.12 is the fourth release in 2020. The highlights of this release are: Build: With the integration of all main views of the Data Integration build workbench, building Knowledge Graphs was never so smooth and streamlined. Automate: With cmemc\u2019s workflow io command, execution of workflows with variable file payload (input) as well as receiving data from a workflows (output) was never so easy before. Please refer to the corresponding tutorial. Warning With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc deprecates a command which will be removed in the next release. This release delivers the following component versions: eccenca DataPlatform v20.12 eccenca DataIntegration v20.12 eccenca DataManager v20.12 eccenca Corporate Memory Control (cmemc) v20.12 eccenca Corporate Memory PowerBI Connector v20.12 More detailed release notes for these versions are listed below.","title":"Corporate Memory 20.12"},{"location":"release-notes/corporate-memory-20-12/#eccenca-dataintegration-v2012","text":"This version of eccenca DataIntegration adds the following new features: Improvements to new Workspace UI Existing views (e.g. editors and reporting interfaces) are now integrated. Knowledge graph datasets show an embedded query and explore view. Quick search dialog: The hotkey / allows to quickly switch between projects and tasks. Optimized DataPlatform entity retrieval strategy for stability and performance. RDF workspace backend does write additional convenience properties for transform tasks and linking tasks di:usedSourceClass : lists all classes for which entities are read by this task. di:usedSourceProperty : lists all properties that are read by this task. di:usedTargetClass : lists all classes for which entities are written by this task. di:usedTargetProperty : lists all properties that are written by this task. Project resource download button. Persisted execution reports: Added a configurable report manager that persists execution reports and allows to retrieve previous reports. Configurable retention time. Reports older than that will be deleted. Error output for transformations: Transformations have a new parameter \u201cerror output\u201d. When executing the workflow, all erroneous entities together with the error description are written to the configured dataset. Add JSON sink Simple variable workflow execution REST endpoint Allows to execute simple variables workflow (at most one variable input and/or output dataset) Input is provided via query parameters or directly in the request body Output is defined by the ACCEPT header and is output in the corresponding MIME type in the response body In addition to that, these changes are shipped: ExcelMap operator: If there are multiple values for a given key, it now returns all values instead of just the last one Project file multi-upload: Upload one file after the other instead of all at once The configuration parameter workbench.showHeader is no longer supported. Instead, a URL parameter is used to decide whether the header should be shown. Data preview for XML, CSV and JSON now automatically loaded and shown on the dataset details page. After deleting an item the user is now redirected to the project page (previously to the workbench page). In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v20.12"},{"location":"release-notes/corporate-memory-20-12/#eccenca-datamanager-v2012","text":"This version of eccenca DataManager adds the following new features: General: Support for datatype rdf:HTML in object view. Data Integration: Add config parameter DIWorkspace.baseURL for logout. In addition to that, these changes are shipped General: Add error message if browser is not compatible: Safari, IE. Override default config language with workspace language if user did not select a language before. Shacline: Redirect to new resource after use the clone resource feature. Load graph permissions directly from the data request. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v20.12"},{"location":"release-notes/corporate-memory-20-12/#eccenca-dataplatform-v2012","text":"This version of eccenca DataPlatform ships the following changes: Spring libraries: Spring Boot version upgraded to 2.2.10.RELEASE , Spring Cloud version upgraded to Hoxton.RELEASE . Bootstrap & Vocabularies: cmem ontologies and graphs can now be updated using the bootstrap endpoint. Statement Annotation: additional endpoints for browsing and bulk editing Graph List: enriched endpoint with additional information Label Resolution: per default, in case no language matches, the precedence is ignored an any one value of the defined properties is taken as fallback. in case of multiple matches, the alphabetically first entry is chosen. Localization / i18n (20.10.1): language selection in titlehelper, shapes and facets API uses shui:languageIn instead of sh:languageIn In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v20.12"},{"location":"release-notes/corporate-memory-20-12/#eccenca-corporate-memory-control-cmemc-v2012","text":"This version of cmemc adds the following new features: The workflow io command was added to allow for executing workflows with variable file payload (input) and receive data from a workflow (output). This feature is described in the advanced tutorial Processing data with variable input workflows . In addition to that, the concepts of io workflows is described in Workflow execution and orchestration . The admin command group was added and includes the following commands: bootstrap - Update/Import bootstrap data. showcase - Create showcase data. status - Output health and version information. The template option of the graph export command allows for using the {{iriname}} placeholder now. In addition to that, these changes are shipped: The config check command outputs a deprecation warning now (use the admin status command instead). cmemc now sends a User-Agent header with every call, currently: cmemc/20.12 (Python 3.7.7)","title":"eccenca Corporate Memory Control (cmemc) v20.12"},{"location":"release-notes/corporate-memory-20-12/#eccenca-corporate-memory-powerbi-connector-v2012","text":"This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI .","title":"eccenca Corporate Memory PowerBI Connector v20.12"},{"location":"release-notes/corporate-memory-20-12/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-20-12/#dataintegration","text":"No Forward Compatibility of Dataintegration Projects Exports Due to a change in the internal XML serialization DI projects from this version can not be imported into instances running older version of DataIntegration. Please try the following workaround if this is something you need to perform. Contact our support team in case this procedure does not work in your case: Info download your project resources (if needed) export the project using cmemc: cmemc project export \u2013type rdfTurtle import the project at the back-level instance using cmemc: cmemc project import .project.ttl upload your file resources to the back-level instance (if needed) Configuration Remove the following configuration parameter: workbench.showHeader In order to enable the new Execution Report Manager you need to configure the respective plugin, see \u201cExecution Report Manager\u201d in DataIntegration for Details.","title":"DataIntegration"},{"location":"release-notes/corporate-memory-20-12/#datamanager","text":"In your workspaces configuration add DIWorkspace.baseUrl (mostly this will be \"/dataintegration\" ): js.config.workspaces : default : ... DIWorkspace : ... baseUrl : /dataintegration","title":"DataManager"},{"location":"release-notes/corporate-memory-20-12/#cmemc","text":"The config check command has been deprecated, please use the admin status command instead.","title":"cmemc"},{"location":"release-notes/corporate-memory-21-02/","text":"Corporate Memory 21.02 \u00a4 Corporate Memory 21.02 is the first release in 2021. The highlights of this release are: Build: Our re-designed mapping suggestion wizard make bootstrapping of your transformations a quick and easy exercise. Explore: interactively and visually browse through your graph with our integrated GRAPH visualization. Automate: The command line has never been more colorful, enjoyable and helpful (guessing your wishes as typos might happen). Experience the UX centric redesign of our command line client cmemc. Warning With this release of Corporate Memory the DataIntegration, DataManager, DataPlatform configuration and cmemc command behavior has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.02.1 eccenca DataIntegration v21.02.1 eccenca DataManager v21.02 eccenca Corporate Memory Control (cmemc) v21.02 More detailed release notes for these versions are listed below. eccenca DataIntegration v21.02.1 \u00a4 This version of eccenca DataIntegration adds the following new features: REST request operator: added \u201cAccept all SSL certificates\u201d parameter in advanced section. Default: false . If the option eccencaDataPlatform.writeGraphType is set to true the a graph in CMEM generated by Dataintegration will contain a triple ( <graph> rdf:type di:Dataset ) indicating that this graph was written by DataIntegration. Improved mapping suggestion: Allows to select from multiple matching candidates. The user can switch between data source and target vocabulary view. A sub-set of the vocabularies can be selected. The user can pick any (non-matched) property from the target vocabularies via search. Improved filtering and sorting. Added multi word text search filter. Allows to choose am existing or custom URI prefix for the auto-generated target properties. Added tooltip for source paths with e.g. example values. Added tooltip for target property selection with meta data and a link to DataManager for that property resource. In addition to that, these changes are shipped: The \u201cTimestamp to date\u201d and \u201cDate to timestamp\u201d transformers support configurable time units and full xsd:dateTime values. Updated build to Spark 2.4, Scala 2.12 and sbt 1.x If a transformer inside a linkage rule throws a validation error, it will no longer fail the entire linking task, but will not generate a value for that transformer. In the linking evaluation view and the reference links view, validation errors are shown in the evaluation tree. Allow to persist caches between restarts in order to reduce application start-up time. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v21.02 \u00a4 This version of eccenca DataManager adds the following new features: Explore Add a tab with the Ontodia tool to explore graph detail view. Make the Ontodia tab configurable: js.modules.explore.details.ontodia.enable Shacline Display rdfs:comment of the selected node shape in the shacl form view. Add group:comment to the header of groups. In addition to that, these changes are shipped: Explore Remove js.modules.explore.graphlist.internalGraphs from DataManager config since DataPlatform is now providing this information. Rename tab: ontodia (Data@en, Daten@de). Rename tab: visualization (Vocab@en, Vocab@de). Move ontodia tab next to visualization tab. Vocabulary Fix format to new endpoint with Titles already loaded. In addition to that, multiple performance and stability issues were solved. eccenca DataPlatform v21.02.1 \u00a4 This version of eccenca DataPlatform ships the following new features: GraphDB datastore implementation Added spring.servlet.multipart.max-file-size: \"20GB\" in properties files and dist file. Added useDirectTransfer to use native Graph Store operation without shared folder Admin Endpoint for listing currently running queries. Graph API New endpoints for owl:imports resolution. Bootstrap Data Shapes for managing prefix declarations Shape catalogs have prefix declarations as managed classes. Imports resolution owl:imports self references ignored. owl:imports are now resolved for graph uris in request parameters. Vocabulary List Title resolve according to standard i18n settings. Shapes Endpoint Comments of shapes included. In addition to that, these changes are shipped: Dist Config skos:prefLabel now preferred over rdfs:label . Query Logging Query logging uses console appender only. Query logging level set to DEBUG . Environment variables no longer needed: QUERY_LOGGING_DIR QUERY_LOGGING_MAX_FILE_SIZE QUERY_LOGGING_TOTAL_SIZE_CAP QUERY_LOGGING_MAX_HISTORY In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v21.02 \u00a4 This version of cmemc adds the following new features: New configuration option OAUTH_GRANT_TYPE=prefetched_token New admin token command, fetch and output an access token New project open command, open projects in the browser graph list command Added table output with graph type and label. --id-only option added: get only graph IRIs. project list command Added table output with project ID and label. --id-only option added: get only project IDs. --raw option added: get raw JSON. workflow list command Added table output with workflow ID and label. --id-only option added: get only graph IRIs. --raw option added: get raw JSON. graph list command --filter imported-by IRI added: filter to all graphs imported recursively by a graph. graph tree command added, output owl:imports tree for each selected graph. graph export and graph delete command --include-imports option added: work with selected graph(s) and all graphs which are imported from the selected graph(s). workflow list command --filter option added: filter by project or io command capability (input, output, both, any). graph export command --create-catalog added: create a Protege XML catalog for import resolution. query status command, list and view still running and executed queries. output coloring json output is highlighted help texts are colored (red terms indicate writing commands, possible dangerous to your data) table headers are colored git-like did-you-mean command suggestion for misspelled commands In addition to that, these changes are shipped: docker image: now based on debian:stable-20201209-slim . graph list command default output changed. use the --id-only or CMEMC_GRAPH_LIST_ID_ONLY=true to get the IRI list. project list command default output changed. use the --id-only or CMEMC_PROJECT_LIST_ID_ONLY=true to get the ID list. workflow list command default output changed. use the --id-only or CMEMC_WORKFLOW_LIST_ID_ONLY=true to get the ID list. graph list command filter option changed. use --filter access readonly | writeable instead of --filter readonly | writeable workflow open command URL changed for new workbench admin status command, now warns you if cmemc is too new for the current backend Migration Notes \u00a4 DataIntegration \u00a4 Timestamp to date operator changed default behavior The \u201cTimestamp to date\u201d now assumes milliseconds instead of seconds by default. In addition, it generates full xsd:dateTime values instead of simple dates. To makes sure that existing usages don\u2019t break, please open {DataIntegration}/api/core/usages/plugins/timeToDate and check all usages. In order to revert to the previous behavior, the following changes have to be made to each usage: Change the unit to \u201cseconds\u201d. Change the format to \u201cyyyy-MM-dd\u201d DataManager \u00a4 In your application.yml the following config property can be removed, if existing. This information is now internally provided by Datalatform: js.modules.explore.graphlist.internalGraphs DataPlatform \u00a4 in case you used Query Logging of DataPlatform you can remove the following environment variables as they are no longer needed: QUERY_LOGGING_DIR , QUERY_LOGGING_MAX_FILE_SIZE , QUERY_LOGGING_TOTAL_SIZE_CAP , QUERY_LOGGING_MAX_HISTORY cmemc \u00a4 workspace import | export | reload commands are deprecated now use admin workspace import | export | reload commands instead Many commands have new default output: graph list command, use the --id-only or CMEMC_GRAPH_LIST_ID_ONLY=true to get the IRI list. project list command, use the --id-only or CMEMC_PROJECT_LIST_ID_ONLY=true to get the ID list. workflow list command, use the --id-only or CMEMC_WORKFLOW_LIST_ID_ONLY=true to get the ID list. graph list command, use --filter access readonly | writeable instead of --filter readonly | writeable The command config check was removed (was deprecated in v20.12)","title":"Corporate Memory 21.02"},{"location":"release-notes/corporate-memory-21-02/#corporate-memory-2102","text":"Corporate Memory 21.02 is the first release in 2021. The highlights of this release are: Build: Our re-designed mapping suggestion wizard make bootstrapping of your transformations a quick and easy exercise. Explore: interactively and visually browse through your graph with our integrated GRAPH visualization. Automate: The command line has never been more colorful, enjoyable and helpful (guessing your wishes as typos might happen). Experience the UX centric redesign of our command line client cmemc. Warning With this release of Corporate Memory the DataIntegration, DataManager, DataPlatform configuration and cmemc command behavior has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.02.1 eccenca DataIntegration v21.02.1 eccenca DataManager v21.02 eccenca Corporate Memory Control (cmemc) v21.02 More detailed release notes for these versions are listed below.","title":"Corporate Memory 21.02"},{"location":"release-notes/corporate-memory-21-02/#eccenca-dataintegration-v21021","text":"This version of eccenca DataIntegration adds the following new features: REST request operator: added \u201cAccept all SSL certificates\u201d parameter in advanced section. Default: false . If the option eccencaDataPlatform.writeGraphType is set to true the a graph in CMEM generated by Dataintegration will contain a triple ( <graph> rdf:type di:Dataset ) indicating that this graph was written by DataIntegration. Improved mapping suggestion: Allows to select from multiple matching candidates. The user can switch between data source and target vocabulary view. A sub-set of the vocabularies can be selected. The user can pick any (non-matched) property from the target vocabularies via search. Improved filtering and sorting. Added multi word text search filter. Allows to choose am existing or custom URI prefix for the auto-generated target properties. Added tooltip for source paths with e.g. example values. Added tooltip for target property selection with meta data and a link to DataManager for that property resource. In addition to that, these changes are shipped: The \u201cTimestamp to date\u201d and \u201cDate to timestamp\u201d transformers support configurable time units and full xsd:dateTime values. Updated build to Spark 2.4, Scala 2.12 and sbt 1.x If a transformer inside a linkage rule throws a validation error, it will no longer fail the entire linking task, but will not generate a value for that transformer. In the linking evaluation view and the reference links view, validation errors are shown in the evaluation tree. Allow to persist caches between restarts in order to reduce application start-up time. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v21.02.1"},{"location":"release-notes/corporate-memory-21-02/#eccenca-datamanager-v2102","text":"This version of eccenca DataManager adds the following new features: Explore Add a tab with the Ontodia tool to explore graph detail view. Make the Ontodia tab configurable: js.modules.explore.details.ontodia.enable Shacline Display rdfs:comment of the selected node shape in the shacl form view. Add group:comment to the header of groups. In addition to that, these changes are shipped: Explore Remove js.modules.explore.graphlist.internalGraphs from DataManager config since DataPlatform is now providing this information. Rename tab: ontodia (Data@en, Daten@de). Rename tab: visualization (Vocab@en, Vocab@de). Move ontodia tab next to visualization tab. Vocabulary Fix format to new endpoint with Titles already loaded. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v21.02"},{"location":"release-notes/corporate-memory-21-02/#eccenca-dataplatform-v21021","text":"This version of eccenca DataPlatform ships the following new features: GraphDB datastore implementation Added spring.servlet.multipart.max-file-size: \"20GB\" in properties files and dist file. Added useDirectTransfer to use native Graph Store operation without shared folder Admin Endpoint for listing currently running queries. Graph API New endpoints for owl:imports resolution. Bootstrap Data Shapes for managing prefix declarations Shape catalogs have prefix declarations as managed classes. Imports resolution owl:imports self references ignored. owl:imports are now resolved for graph uris in request parameters. Vocabulary List Title resolve according to standard i18n settings. Shapes Endpoint Comments of shapes included. In addition to that, these changes are shipped: Dist Config skos:prefLabel now preferred over rdfs:label . Query Logging Query logging uses console appender only. Query logging level set to DEBUG . Environment variables no longer needed: QUERY_LOGGING_DIR QUERY_LOGGING_MAX_FILE_SIZE QUERY_LOGGING_TOTAL_SIZE_CAP QUERY_LOGGING_MAX_HISTORY In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v21.02.1"},{"location":"release-notes/corporate-memory-21-02/#eccenca-corporate-memory-control-cmemc-v2102","text":"This version of cmemc adds the following new features: New configuration option OAUTH_GRANT_TYPE=prefetched_token New admin token command, fetch and output an access token New project open command, open projects in the browser graph list command Added table output with graph type and label. --id-only option added: get only graph IRIs. project list command Added table output with project ID and label. --id-only option added: get only project IDs. --raw option added: get raw JSON. workflow list command Added table output with workflow ID and label. --id-only option added: get only graph IRIs. --raw option added: get raw JSON. graph list command --filter imported-by IRI added: filter to all graphs imported recursively by a graph. graph tree command added, output owl:imports tree for each selected graph. graph export and graph delete command --include-imports option added: work with selected graph(s) and all graphs which are imported from the selected graph(s). workflow list command --filter option added: filter by project or io command capability (input, output, both, any). graph export command --create-catalog added: create a Protege XML catalog for import resolution. query status command, list and view still running and executed queries. output coloring json output is highlighted help texts are colored (red terms indicate writing commands, possible dangerous to your data) table headers are colored git-like did-you-mean command suggestion for misspelled commands In addition to that, these changes are shipped: docker image: now based on debian:stable-20201209-slim . graph list command default output changed. use the --id-only or CMEMC_GRAPH_LIST_ID_ONLY=true to get the IRI list. project list command default output changed. use the --id-only or CMEMC_PROJECT_LIST_ID_ONLY=true to get the ID list. workflow list command default output changed. use the --id-only or CMEMC_WORKFLOW_LIST_ID_ONLY=true to get the ID list. graph list command filter option changed. use --filter access readonly | writeable instead of --filter readonly | writeable workflow open command URL changed for new workbench admin status command, now warns you if cmemc is too new for the current backend","title":"eccenca Corporate Memory Control (cmemc) v21.02"},{"location":"release-notes/corporate-memory-21-02/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-21-02/#dataintegration","text":"Timestamp to date operator changed default behavior The \u201cTimestamp to date\u201d now assumes milliseconds instead of seconds by default. In addition, it generates full xsd:dateTime values instead of simple dates. To makes sure that existing usages don\u2019t break, please open {DataIntegration}/api/core/usages/plugins/timeToDate and check all usages. In order to revert to the previous behavior, the following changes have to be made to each usage: Change the unit to \u201cseconds\u201d. Change the format to \u201cyyyy-MM-dd\u201d","title":"DataIntegration"},{"location":"release-notes/corporate-memory-21-02/#datamanager","text":"In your application.yml the following config property can be removed, if existing. This information is now internally provided by Datalatform: js.modules.explore.graphlist.internalGraphs","title":"DataManager"},{"location":"release-notes/corporate-memory-21-02/#dataplatform","text":"in case you used Query Logging of DataPlatform you can remove the following environment variables as they are no longer needed: QUERY_LOGGING_DIR , QUERY_LOGGING_MAX_FILE_SIZE , QUERY_LOGGING_TOTAL_SIZE_CAP , QUERY_LOGGING_MAX_HISTORY","title":"DataPlatform"},{"location":"release-notes/corporate-memory-21-02/#cmemc","text":"workspace import | export | reload commands are deprecated now use admin workspace import | export | reload commands instead Many commands have new default output: graph list command, use the --id-only or CMEMC_GRAPH_LIST_ID_ONLY=true to get the IRI list. project list command, use the --id-only or CMEMC_PROJECT_LIST_ID_ONLY=true to get the ID list. workflow list command, use the --id-only or CMEMC_WORKFLOW_LIST_ID_ONLY=true to get the ID list. graph list command, use --filter access readonly | writeable instead of --filter readonly | writeable The command config check was removed (was deprecated in v20.12)","title":"cmemc"},{"location":"release-notes/corporate-memory-21-04/","text":"Corporate Memory 21.04 \u00a4 Corporate Memory 21.04 is the second release in 2021. The highlights of this release are: Build: The mapping editor now allows for auto-completion of paths on any level in multi-hop paths, including source type specific paths with special semantics, e.g. #idx for CSV datasets. This feature lowers the barrier for new Corporate Memory users and allows for much master mapping creation. Explore: Manual authoring of resources via SHACL-shape based customized user interfaces is now supported with client-side datatype validation (in addition to store-based validation). This feature provides instant user feedback while typing Literals and therefor allows faster data entry. Automate: The new vocabulary import command of cmemc adds a turtle file as a vocabulary to Corporate Memory (upload and create catalog entry). This allows for automation of CI/CD pipeline which depend on vocabularies managed in a Git Repository. Warning With this release of Corporate Memory the DataIntegration and DataManager configuration as well as cmemc command behavior has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.04 eccenca DataIntegration v21.04 eccenca DataManager v21.04 eccenca Corporate Memory Control (cmemc) v21.04 More detailed release notes for these versions are listed below. eccenca DataIntegration v21.04 \u00a4 This version of eccenca DataIntegration adds the following new features: A new mapping parameter enables the user to specify whether single or multiple values are written by a particular mapping: Supported by both value and object mappings. Replaces the \u201cis Attribute\u201d parameter on value mappings, which has been specific to XML. Generates a validation error if multiple values are written if single values are configured. Datasets may adapt the written schema based on the chosen option (see help text in mapping editor). Improved auto-completion value path mapping field: Allows auto-completion of paths on any level in multi-hop paths. Supports auto-completion of properties inside of property filters. Proposes data source specific paths with special semantics, e.g. #idx for CSV. Validates the syntax of the value path and highlights errors to the user. Tasks can be copied between projects. A new transform task parameter ( abortIfErrorsOccur ) specifies whether the execution will fail if a validation error occurs. Added URI literal type for writing xsd:anyURI values to Knowledge Graphs. Added option for \u201cRegex extract\u201d transformer for extracting all matches. In addition to that, these changes are shipped: Support editing of auto-completed values in mapping editor. Mapping editor auto-complete element: Support editing of auto-completed values. added support for multi-word search highlighting. Show label and value of the selected value, e.g. label and URI for target property. Improvements to execution reports Added an overview section, which displays general information about the execution: Final workflow status Start, finish and cancellation times Users account which started and canceled (if any) the execution All operator executions are persisted. For instance, workflow reports will contain separate task reports for writing and reading a dataset. For each operator execution, an optional operation label can be shown (e.g., read, write, generate queries). The order of the task reports is stable now and reflects the order of execution. Added a scrollbar to the task list, if it is too large to be displayed. Improved JSON writing support: A rewritten implementation supports arbitrarily large JSON files by using a memory-mapped key-value store. An optional template may be specified to customize the written JSON. Values are only wrapped in JSON arrays if the multiple value option is set in the mapping. Transform Evaluation now shows evaluation of the URI rule. Improved workflow saving: On loading, the save button is only enabled after the workflow has been loaded to prevent the user from saving an empty workflow. A spinner is shown while the workflow is saved. The save button is disabled while the workflow is saved. A confirm dialog is shown, if the user tries to leave while a save is in progress. The behavior of linking rules in case of missing values has been improved: The required attribute has been removed, because it has lead to unexpected and sometimes inconsistent behavior. Boolean aggregators have been reworked to interpret missing values as false. The \u201cHandle missing values\u201d aggregator has been added to handle cases in which missing values should default to a user specified score (For instance, if missing value should be interpreted as true). The \u201cDefault value\u201d transformer has been added to generate default values for missing values. Improved inline documentation using markdown rather than text description. Workflow progressbars now show the task labels instead of their internal identifiers. Page header contents are now created directly in the artifact view templates instead being an independent component pulling all information via holistic approach. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v21.04 \u00a4 This version of eccenca DataManager adds the following new features: Shacl Add datatype validation for all supported datatypes Configuration Add a configurable link to account settings in keycloak: js.config.modules.accountSettings.enable: true js.config.modules.accountSettings.url: http://docker.local/auth/realms/cmem/account/?referrer={{REFERRER}}&referrer_uri={{REFERRER_URI}} General Global error handling to display errors preventing most grey screens In addition to that, these changes are shipped: General - Use redux store to manage notifications in DataManager (MessageHandler) and improve error parse / handle - Use redux store to manage main application state. - Change value of js.config.modules.explore.overallSearchQuery and js.config.modules.explore.navigation.searchQuery to use the \"\"\"\" SPARQL string separator. - BREAK please use \"\"\" if you use custom queries for that values Development Switch to GUI elements repository from Github In addition to that, multiple performance and stability issues were solved. eccenca DataPlatform v21.04 \u00a4 These followin changes are shipped: General Virtuoso is now using the custom build-in function to list graphs faster. Bootstrap Data ucum removed as default vocab in the vocabulary catatog qudt added as a default vocab in the vocabulary catalog all vocabularies are provided via download.eccenca.com now In addition to that, multiple performance and stability issues were solved. Health Endpoint race condition in the health condition that leads to rare cases of wrongly reporting DOWN HTTP Connection Pool size increased to increase parallelism and resilience Statement-Level Metadata works now on inverse properties Graph List incorrect type statements are ignored eccenca Corporate Memory Control (cmemc) v21.02 \u00a4 This version of cmemc adds the following new features: new config get command get the value of certain configuration key (such as DP_API_ENDPOINT ) new dataset open command similar to the other open commands, opens a dataset in the browser graph export command --mime-type option added to specify requested mime type the default mime type is still application/n-triples new vocabular import command Import a turtle file as a vocabulary (upload and create catalog entry) project export command --extract option added in order to export projects to directories --help-types option added to get a list of export formats project import command add support for importing projects from extracted directories add --overwrite option to import files/directory to an existing project In addition to that, these changes are shipped: default values for OAUTH_USER and OAUTH_PASSWORD is None except for grant type password docker base image forwarded to debian:stable-20210408-slim graph export command does not load the result in memory anymore but stream the result to the file graph import command does not load the payload in memory anymore but stream it to the endpoint project export command command now fails early if a non-existing project is requested project create command command now fails early if a project is already there project delete command command now fails early if a non-existing project is requested project import command now uses the new project import API removed reference to config keys CMEM_BASE_PROTOCOL and CMEM_BASE_DOMAIN were never used remove workspace command group from root was in 21.02 deprecated now removed from root and available as admin workspace command group fix: project export command command now fails correctly with exit code 1 if a non-existing project is requested fix: project import command command now fails correctly with exit code 1 in case of an import to an existing project Migration Notes \u00a4 DataIntegration \u00a4 The behavior of linking rules in case of missing values has been changed: Now, boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d. Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing. If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases. DataManager \u00a4 To allow special characters to be search in js.config.modules.explore.overallSearchQuery and js.config.modules.explore.navigation.searchQuery , use \"\"\"\" instead of just \" to delimitate strings with search placeholders. e.g: regex(str(?resource),\"{{QUERY}}\",\"i\") should be written as regex(str(?resource),\"\"\"{{QUERY}}\"\"\",\"i\") DataPlatform \u00a4 No migration notes cmemc \u00a4 The exit code values of project import and export commands are fixed (in case of failure) so you may have to change these calls in your scripts. The deprecated workspace command group is now only available as admin workspace command group so you have to change these calls in scripts.","title":"Corporate Memory 21.04"},{"location":"release-notes/corporate-memory-21-04/#corporate-memory-2104","text":"Corporate Memory 21.04 is the second release in 2021. The highlights of this release are: Build: The mapping editor now allows for auto-completion of paths on any level in multi-hop paths, including source type specific paths with special semantics, e.g. #idx for CSV datasets. This feature lowers the barrier for new Corporate Memory users and allows for much master mapping creation. Explore: Manual authoring of resources via SHACL-shape based customized user interfaces is now supported with client-side datatype validation (in addition to store-based validation). This feature provides instant user feedback while typing Literals and therefor allows faster data entry. Automate: The new vocabulary import command of cmemc adds a turtle file as a vocabulary to Corporate Memory (upload and create catalog entry). This allows for automation of CI/CD pipeline which depend on vocabularies managed in a Git Repository. Warning With this release of Corporate Memory the DataIntegration and DataManager configuration as well as cmemc command behavior has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.04 eccenca DataIntegration v21.04 eccenca DataManager v21.04 eccenca Corporate Memory Control (cmemc) v21.04 More detailed release notes for these versions are listed below.","title":"Corporate Memory 21.04"},{"location":"release-notes/corporate-memory-21-04/#eccenca-dataintegration-v2104","text":"This version of eccenca DataIntegration adds the following new features: A new mapping parameter enables the user to specify whether single or multiple values are written by a particular mapping: Supported by both value and object mappings. Replaces the \u201cis Attribute\u201d parameter on value mappings, which has been specific to XML. Generates a validation error if multiple values are written if single values are configured. Datasets may adapt the written schema based on the chosen option (see help text in mapping editor). Improved auto-completion value path mapping field: Allows auto-completion of paths on any level in multi-hop paths. Supports auto-completion of properties inside of property filters. Proposes data source specific paths with special semantics, e.g. #idx for CSV. Validates the syntax of the value path and highlights errors to the user. Tasks can be copied between projects. A new transform task parameter ( abortIfErrorsOccur ) specifies whether the execution will fail if a validation error occurs. Added URI literal type for writing xsd:anyURI values to Knowledge Graphs. Added option for \u201cRegex extract\u201d transformer for extracting all matches. In addition to that, these changes are shipped: Support editing of auto-completed values in mapping editor. Mapping editor auto-complete element: Support editing of auto-completed values. added support for multi-word search highlighting. Show label and value of the selected value, e.g. label and URI for target property. Improvements to execution reports Added an overview section, which displays general information about the execution: Final workflow status Start, finish and cancellation times Users account which started and canceled (if any) the execution All operator executions are persisted. For instance, workflow reports will contain separate task reports for writing and reading a dataset. For each operator execution, an optional operation label can be shown (e.g., read, write, generate queries). The order of the task reports is stable now and reflects the order of execution. Added a scrollbar to the task list, if it is too large to be displayed. Improved JSON writing support: A rewritten implementation supports arbitrarily large JSON files by using a memory-mapped key-value store. An optional template may be specified to customize the written JSON. Values are only wrapped in JSON arrays if the multiple value option is set in the mapping. Transform Evaluation now shows evaluation of the URI rule. Improved workflow saving: On loading, the save button is only enabled after the workflow has been loaded to prevent the user from saving an empty workflow. A spinner is shown while the workflow is saved. The save button is disabled while the workflow is saved. A confirm dialog is shown, if the user tries to leave while a save is in progress. The behavior of linking rules in case of missing values has been improved: The required attribute has been removed, because it has lead to unexpected and sometimes inconsistent behavior. Boolean aggregators have been reworked to interpret missing values as false. The \u201cHandle missing values\u201d aggregator has been added to handle cases in which missing values should default to a user specified score (For instance, if missing value should be interpreted as true). The \u201cDefault value\u201d transformer has been added to generate default values for missing values. Improved inline documentation using markdown rather than text description. Workflow progressbars now show the task labels instead of their internal identifiers. Page header contents are now created directly in the artifact view templates instead being an independent component pulling all information via holistic approach. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v21.04"},{"location":"release-notes/corporate-memory-21-04/#eccenca-datamanager-v2104","text":"This version of eccenca DataManager adds the following new features: Shacl Add datatype validation for all supported datatypes Configuration Add a configurable link to account settings in keycloak: js.config.modules.accountSettings.enable: true js.config.modules.accountSettings.url: http://docker.local/auth/realms/cmem/account/?referrer={{REFERRER}}&referrer_uri={{REFERRER_URI}} General Global error handling to display errors preventing most grey screens In addition to that, these changes are shipped: General - Use redux store to manage notifications in DataManager (MessageHandler) and improve error parse / handle - Use redux store to manage main application state. - Change value of js.config.modules.explore.overallSearchQuery and js.config.modules.explore.navigation.searchQuery to use the \"\"\"\" SPARQL string separator. - BREAK please use \"\"\" if you use custom queries for that values Development Switch to GUI elements repository from Github In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v21.04"},{"location":"release-notes/corporate-memory-21-04/#eccenca-dataplatform-v2104","text":"These followin changes are shipped: General Virtuoso is now using the custom build-in function to list graphs faster. Bootstrap Data ucum removed as default vocab in the vocabulary catatog qudt added as a default vocab in the vocabulary catalog all vocabularies are provided via download.eccenca.com now In addition to that, multiple performance and stability issues were solved. Health Endpoint race condition in the health condition that leads to rare cases of wrongly reporting DOWN HTTP Connection Pool size increased to increase parallelism and resilience Statement-Level Metadata works now on inverse properties Graph List incorrect type statements are ignored","title":"eccenca DataPlatform v21.04"},{"location":"release-notes/corporate-memory-21-04/#eccenca-corporate-memory-control-cmemc-v2102","text":"This version of cmemc adds the following new features: new config get command get the value of certain configuration key (such as DP_API_ENDPOINT ) new dataset open command similar to the other open commands, opens a dataset in the browser graph export command --mime-type option added to specify requested mime type the default mime type is still application/n-triples new vocabular import command Import a turtle file as a vocabulary (upload and create catalog entry) project export command --extract option added in order to export projects to directories --help-types option added to get a list of export formats project import command add support for importing projects from extracted directories add --overwrite option to import files/directory to an existing project In addition to that, these changes are shipped: default values for OAUTH_USER and OAUTH_PASSWORD is None except for grant type password docker base image forwarded to debian:stable-20210408-slim graph export command does not load the result in memory anymore but stream the result to the file graph import command does not load the payload in memory anymore but stream it to the endpoint project export command command now fails early if a non-existing project is requested project create command command now fails early if a project is already there project delete command command now fails early if a non-existing project is requested project import command now uses the new project import API removed reference to config keys CMEM_BASE_PROTOCOL and CMEM_BASE_DOMAIN were never used remove workspace command group from root was in 21.02 deprecated now removed from root and available as admin workspace command group fix: project export command command now fails correctly with exit code 1 if a non-existing project is requested fix: project import command command now fails correctly with exit code 1 in case of an import to an existing project","title":"eccenca Corporate Memory Control (cmemc) v21.02"},{"location":"release-notes/corporate-memory-21-04/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-21-04/#dataintegration","text":"The behavior of linking rules in case of missing values has been changed: Now, boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d. Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing. If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.","title":"DataIntegration"},{"location":"release-notes/corporate-memory-21-04/#datamanager","text":"To allow special characters to be search in js.config.modules.explore.overallSearchQuery and js.config.modules.explore.navigation.searchQuery , use \"\"\"\" instead of just \" to delimitate strings with search placeholders. e.g: regex(str(?resource),\"{{QUERY}}\",\"i\") should be written as regex(str(?resource),\"\"\"{{QUERY}}\"\"\",\"i\")","title":"DataManager"},{"location":"release-notes/corporate-memory-21-04/#dataplatform","text":"No migration notes","title":"DataPlatform"},{"location":"release-notes/corporate-memory-21-04/#cmemc","text":"The exit code values of project import and export commands are fixed (in case of failure) so you may have to change these calls in your scripts. The deprecated workspace command group is now only available as admin workspace command group so you have to change these calls in scripts.","title":"cmemc"},{"location":"release-notes/corporate-memory-21-06/","text":"Corporate Memory 21.06 \u00a4 Corporate Memory 21.06 is the third release in 2021. The highlights of this release are: Build: The Data Integration workflow editor got a complete remake based on a more flexible and better extensible drawing engine. Workflow tasks use the same icons and tags from the workspace now and are better integrated in the build user interface. Explore: The new Data Manager vocabulary viewer visualises classes and its relations (subclasses, domain/range relations) from an installed vocabulary. Automate: cmemc is now able to fetch credentials from external processes in order to integrate with company wide or personal password infrastructure. Warning With this release of Corporate Memory the DataIntegration and cmemc configuration and behaviour has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.06 eccenca DataIntegration v21.06.1 eccenca DataManager v21.06.3 eccenca Corporate Memory Control (cmemc) v21.06 More detailed release notes for these versions are listed below. eccenca DataIntegration v21.06.1 \u00a4 This version of eccenca DataIntegration adds the following new features: New workflow editor: Completely rewritten workflow editor that replaces the old editor. The old workflow editor can be re-enabled by setting the following configuration value: workbench.tabs.legacyWorkflowEditor = true . Added API endpoint to fetch workflow node (input) port configurations. Enable the creation and execution of nested workflows. Workflows can be nested within other workflows. Nested workflow reports can be viewed in the execution report. Already nested workflows cannot be used in other nested workflows to protect from too complex projects. Added script transform operators Python and Scala are supported as scripting languages. Need to be enabled in the configuration. Template transform operator. Synonym-based mapping suggestion Added global vocabulary synonym cache that extracts synonyms for vocabulary properties from existing mapping rules. Use synonyms in mapping suggestion so more properties can be suggested to the user based on existing mapping rules that map similarly named attributes/properties. Config parameters: mapping.suggestion.features.extractSynonymsFromExistingMappingRules.enabled : Enables the synonym based mapping suggestion. Default: true mapping.suggestion.features.extractSynonymsFromExistingMappingRules.timeBetweenRefreshes : The minimum time in milliseconds between synonym cache refreshes. Default: 10 seconds mapping.suggestion.features.extractSynonymsFromExistingMappingRules.waitForCacheToFinish : The max. time to wait for a new cache value during a mapping suggestion request if the current value has gotten stale. Default: 50ms New OpenAPI based documentation of HTTP API: Replaces the previous RAML-based documentation. Can be viewed live in the UI at {DI_URL}/doc/api . New Neo4j dataset, which supports writing into Neo4j graphs and reading them back. Coalesce transform operator that forwards the first input that has any value/s. Add concrete item type, e.g. \u2018CSV\u2019 or \u2018Transform\u2019, to search result and recently viewed items and make it searchable. Add tooltip to search item if item description is too long to show in a single line. In addition to that, these changes are shipped: Mapping suggestion improvements: Support source path column filters: show only already mapped source paths, show only unmapped source paths. Do not show filters in from-vocabulary view that cannot be applied, e.g. show auto-generated only. Shortened source paths are shown as tooltip in full length on hover. Show source path type (data type or object) in source info box. For object source paths show their direct sub-paths. Improve error reporting in mapping suggestion and mapping rule example view. Prefix management improvements: Validate prefix name and value in the UI Ask before updating existing prefix names. Also change button to \u2018Update\u2019 when prefix name matches an existing prefix. Allow object rule mappings with empty target property and non-empty source path in order to change the source resource, but stay on the target resource. The configuration of plugins (in particular the blacklist) has been improved. Plugin configuration has been grouped under a common root Plugins can be enabled/disabled individually See breaking changes for details E-mail operator extensions and improvements: Allow to send multiple e-mails with different configurations (from, to, subject, content, cc, bcc). Add e-mail execution report Add retry mechanism XML dataset (streaming mode): Allow property filters on attributes in object paths The RDF file dataset now autocompletes formats. Centralized error handling. Upgrade to Play 2.8. (21.06.1) Added a retry mechanism if connections on S3 are interrupted (CMEM-3675) Per default, at most 10 retries are attempted. Number of retries can be changed by setting the configuration parameter retryCount (available on workspace.repository.s3 and workspace.repository.projectS3 ). In addition to that, multiple performance and stability issues were solved. eccenca DataManager v21.06.5 \u00a4 This version of eccenca DataManager fixes the following issues: Linkrules several stability improvements for GraphDB backends Explore New Vocabulary Visualization Component to the graph explore view. In addition to that, these changes are shipped: Vocabs Show spinner during installing/uninstalling vocab Install new vocabularies failed if preferredNamespace is not defined. ObjectView Render images without knowing the relation by parsing the value. Images are provided as URI: <data:image_svg+xml......> Linkrules Fix of missing dcterms prefix in a query Fix of the publish/unpublish query Restore Joint.js common styles A warning about saving published rule should be shown once Layout position is preserved for newly added operators Evaluation is presented inline without conflicting with the manual placement Empty text fields are no longer reset to the placeholder value wrong order of selection items in templates ID generation for operators, in case multiple prototypical pipelines are presents Link Rules Type Error Failed DI calls not catched in DM Broken visual connection after evaluation in LinkRuleEditor General prevent errors in login Bearer Token not persistent DM Query fails because endpoint id and token are not set Query Search in labels does not work In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v21.06 \u00a4 This version of cmemc adds the following new features: graph import command new option --skip-existing will not touch graphs which are already there admin token command new option --decode shows content of the decoded auth token in combination with --raw it outputs the decoded token as json configuration new configuration keys to fetch credentials from external processes use the parameter OAUTH_PASSWORD_PROCESS , OAUTH_CLIENT_SECRET_PROCESS and OAUTH_ACCESS_TOKEN_PROCESS to setup an external executable In addition to that, these changes are shipped: docker base image is now debian:stable-20210721-slim support for OAUTH_PASSWORD_ENTRY and OAUTH_CLIENT_SECRET_ENTRY removed fix: freeze click dependency to 7.1.2 Migration Notes \u00a4 DataIntegration \u00a4 All script operators are disabled by default now and need to be re-enabled by configuration. The (not-working) spotlight transform operator is disabled by default now. Optional: When loading existing workflows in the new workflow editor, the operators might overlap and may need to be re-arranged manually. This does not influence the actual execution of the workflows in any way. An auto-layouting feature will be added in the future Plugin configuration has been changed. The \u2018plugin.blacklist\u2019 has been deprecated and will be removed in future versions. See example below for new format: pluginRegistry { # External plugins are loaded from this folder pluginFolder = ${elds.home}\"/etc/dataintegration/plugins/\" # Configuration of individual plugins. plugins { pluginId1.enabled = false ... } } DataManager \u00a4 No migration notes DataPlatform \u00a4 No migration notes cmemc \u00a4 The configuration keys *_ENTRY are not supported anymore. In case you used them, switch to *_PROCESS configuration","title":"Corporate Memory 21.06"},{"location":"release-notes/corporate-memory-21-06/#corporate-memory-2106","text":"Corporate Memory 21.06 is the third release in 2021. The highlights of this release are: Build: The Data Integration workflow editor got a complete remake based on a more flexible and better extensible drawing engine. Workflow tasks use the same icons and tags from the workspace now and are better integrated in the build user interface. Explore: The new Data Manager vocabulary viewer visualises classes and its relations (subclasses, domain/range relations) from an installed vocabulary. Automate: cmemc is now able to fetch credentials from external processes in order to integrate with company wide or personal password infrastructure. Warning With this release of Corporate Memory the DataIntegration and cmemc configuration and behaviour has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.06 eccenca DataIntegration v21.06.1 eccenca DataManager v21.06.3 eccenca Corporate Memory Control (cmemc) v21.06 More detailed release notes for these versions are listed below.","title":"Corporate Memory 21.06"},{"location":"release-notes/corporate-memory-21-06/#eccenca-dataintegration-v21061","text":"This version of eccenca DataIntegration adds the following new features: New workflow editor: Completely rewritten workflow editor that replaces the old editor. The old workflow editor can be re-enabled by setting the following configuration value: workbench.tabs.legacyWorkflowEditor = true . Added API endpoint to fetch workflow node (input) port configurations. Enable the creation and execution of nested workflows. Workflows can be nested within other workflows. Nested workflow reports can be viewed in the execution report. Already nested workflows cannot be used in other nested workflows to protect from too complex projects. Added script transform operators Python and Scala are supported as scripting languages. Need to be enabled in the configuration. Template transform operator. Synonym-based mapping suggestion Added global vocabulary synonym cache that extracts synonyms for vocabulary properties from existing mapping rules. Use synonyms in mapping suggestion so more properties can be suggested to the user based on existing mapping rules that map similarly named attributes/properties. Config parameters: mapping.suggestion.features.extractSynonymsFromExistingMappingRules.enabled : Enables the synonym based mapping suggestion. Default: true mapping.suggestion.features.extractSynonymsFromExistingMappingRules.timeBetweenRefreshes : The minimum time in milliseconds between synonym cache refreshes. Default: 10 seconds mapping.suggestion.features.extractSynonymsFromExistingMappingRules.waitForCacheToFinish : The max. time to wait for a new cache value during a mapping suggestion request if the current value has gotten stale. Default: 50ms New OpenAPI based documentation of HTTP API: Replaces the previous RAML-based documentation. Can be viewed live in the UI at {DI_URL}/doc/api . New Neo4j dataset, which supports writing into Neo4j graphs and reading them back. Coalesce transform operator that forwards the first input that has any value/s. Add concrete item type, e.g. \u2018CSV\u2019 or \u2018Transform\u2019, to search result and recently viewed items and make it searchable. Add tooltip to search item if item description is too long to show in a single line. In addition to that, these changes are shipped: Mapping suggestion improvements: Support source path column filters: show only already mapped source paths, show only unmapped source paths. Do not show filters in from-vocabulary view that cannot be applied, e.g. show auto-generated only. Shortened source paths are shown as tooltip in full length on hover. Show source path type (data type or object) in source info box. For object source paths show their direct sub-paths. Improve error reporting in mapping suggestion and mapping rule example view. Prefix management improvements: Validate prefix name and value in the UI Ask before updating existing prefix names. Also change button to \u2018Update\u2019 when prefix name matches an existing prefix. Allow object rule mappings with empty target property and non-empty source path in order to change the source resource, but stay on the target resource. The configuration of plugins (in particular the blacklist) has been improved. Plugin configuration has been grouped under a common root Plugins can be enabled/disabled individually See breaking changes for details E-mail operator extensions and improvements: Allow to send multiple e-mails with different configurations (from, to, subject, content, cc, bcc). Add e-mail execution report Add retry mechanism XML dataset (streaming mode): Allow property filters on attributes in object paths The RDF file dataset now autocompletes formats. Centralized error handling. Upgrade to Play 2.8. (21.06.1) Added a retry mechanism if connections on S3 are interrupted (CMEM-3675) Per default, at most 10 retries are attempted. Number of retries can be changed by setting the configuration parameter retryCount (available on workspace.repository.s3 and workspace.repository.projectS3 ). In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v21.06.1"},{"location":"release-notes/corporate-memory-21-06/#eccenca-datamanager-v21065","text":"This version of eccenca DataManager fixes the following issues: Linkrules several stability improvements for GraphDB backends Explore New Vocabulary Visualization Component to the graph explore view. In addition to that, these changes are shipped: Vocabs Show spinner during installing/uninstalling vocab Install new vocabularies failed if preferredNamespace is not defined. ObjectView Render images without knowing the relation by parsing the value. Images are provided as URI: <data:image_svg+xml......> Linkrules Fix of missing dcterms prefix in a query Fix of the publish/unpublish query Restore Joint.js common styles A warning about saving published rule should be shown once Layout position is preserved for newly added operators Evaluation is presented inline without conflicting with the manual placement Empty text fields are no longer reset to the placeholder value wrong order of selection items in templates ID generation for operators, in case multiple prototypical pipelines are presents Link Rules Type Error Failed DI calls not catched in DM Broken visual connection after evaluation in LinkRuleEditor General prevent errors in login Bearer Token not persistent DM Query fails because endpoint id and token are not set Query Search in labels does not work In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v21.06.5"},{"location":"release-notes/corporate-memory-21-06/#eccenca-corporate-memory-control-cmemc-v2106","text":"This version of cmemc adds the following new features: graph import command new option --skip-existing will not touch graphs which are already there admin token command new option --decode shows content of the decoded auth token in combination with --raw it outputs the decoded token as json configuration new configuration keys to fetch credentials from external processes use the parameter OAUTH_PASSWORD_PROCESS , OAUTH_CLIENT_SECRET_PROCESS and OAUTH_ACCESS_TOKEN_PROCESS to setup an external executable In addition to that, these changes are shipped: docker base image is now debian:stable-20210721-slim support for OAUTH_PASSWORD_ENTRY and OAUTH_CLIENT_SECRET_ENTRY removed fix: freeze click dependency to 7.1.2","title":"eccenca Corporate Memory Control (cmemc) v21.06"},{"location":"release-notes/corporate-memory-21-06/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-21-06/#dataintegration","text":"All script operators are disabled by default now and need to be re-enabled by configuration. The (not-working) spotlight transform operator is disabled by default now. Optional: When loading existing workflows in the new workflow editor, the operators might overlap and may need to be re-arranged manually. This does not influence the actual execution of the workflows in any way. An auto-layouting feature will be added in the future Plugin configuration has been changed. The \u2018plugin.blacklist\u2019 has been deprecated and will be removed in future versions. See example below for new format: pluginRegistry { # External plugins are loaded from this folder pluginFolder = ${elds.home}\"/etc/dataintegration/plugins/\" # Configuration of individual plugins. plugins { pluginId1.enabled = false ... } }","title":"DataIntegration"},{"location":"release-notes/corporate-memory-21-06/#datamanager","text":"No migration notes","title":"DataManager"},{"location":"release-notes/corporate-memory-21-06/#dataplatform","text":"No migration notes","title":"DataPlatform"},{"location":"release-notes/corporate-memory-21-06/#cmemc","text":"The configuration keys *_ENTRY are not supported anymore. In case you used them, switch to *_PROCESS configuration","title":"cmemc"},{"location":"release-notes/corporate-memory-21-11/","text":"Corporate Memory 21.11 \u00a4 Corporate Memory 21.11 is the fourth release in 2021. The highlights of this release are: Build: The workflow editor user interface allows for undo/redo of you activities now, as well as shows inline (live) progress and statistics of a running workflow. Explore: The Explore interface is now adapted to our new look and feel + the Knowledge Graph list component can be configured to show multiple lists of named graphs e.g. to distinguish between user, vocabulary and system graphs. Automate: cmemc can now interact with workflow scheduler (disable, enable, inspect, list, open) as well as dataset resources (delete, inspect, usage, list). Warning With this release of Corporate Memory the DataIntegration and DataPlatform configuration and behaviour has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.11.1 eccenca DataIntegration v21.11 eccenca DataManager v21.11.5 eccenca Corporate Memory Control (cmemc) v21.11.4 More detailed release notes for these versions are listed below. eccenca DataIntegration v21.11 \u00a4 This version of eccenca DataIntegration adds the following new features: Remote Excel Dataset using Google Drive Spreadsheets as resources. Remote Office 365 Spreadsheets Support. Workflow editor improvements: View navigation via clicking and dragging the mouse on the mini-map. Config input port is hidden by default and can be enabled via menu entry. Inline (live) progress and statistics. Support mapping of RDF literals in object mappings. Literals are handled as entities and can be mapped in object mappings. Special path #text that allows to access the lexical value of the mapped resource. This allows to access the value of a mapped literal in an object mapping. RDF datasets: Special path #lang that allows to access the language tag of a language tagged RDF literal. Extensions to the Excel dataset: Excel columns may be addressed by their letter code ( #A , #B , etc.) as well. A new parameter \u2018hasHeader\u2019 allows handling of pure data sheets with no table header. Support selecting multiple vocabularies with auto-completion support. Transform URI pattern improvements: Validation of URI patterns in the UI and backend, i.e. it checks that URI templates generate valid URIs. Auto-completion support in URI pattern input component. URI pattern validation endpoint URI pattern auto-completion endpoint Change initial URI pattern of complex URI rules from / to {}/<OBJECT_RULE_ID> REST endpoint to fetch an activity execution error report as JSON or Markdown. Activity integration into task detail pages: Primary, running, failed and all related caching activities are shown on the task detail pages. Caching activities are grouped and have additional information and controls like \u2018refresh all caches\u2019, last update etc. For failed activities it is possible to see and download an execution error report. Support for requesting task parameter values in the item search API. New workflow operator to stop the current workflow execution (without failing) if a specified condition has been met. Add global cache for URI patterns that stores all URI patterns extracted from transform tasks. Add API endpoint to fetch all URI patterns used for given target class URIs. Allow to select from existing URI patterns (related to the same target classes) in object mapping rule form. Support setting the URI pattern during creation of an object mapping rule. Application version info in user menu (sidebar on the right side). JDBC dataset does support token-based authentication for MS SQL server now. Undo/redo support in workflow editor. In addition to that, these changes are shipped: Improved task search with highlighting for nodes in the canvas. Added \u201cDefault Value\u201d Transformation to recommended category. OLE2-based Excel documents (e.g., .xls) are now supported in non-streaming mode. Enabled the Excel \u2018LEFT\u2019 transform function. Mapping Rule Editor will show the rule label (if any) and the mapping target. The JSON dataset supports streaming. The change applies to reading JSON, writing was already streamed. If streaming is enabled, files won\u2019t be loaded into memory, allowing to read large JSON files without running into OutOfMemory errors. Allow to open the value mapping rule formula editor from the create/edit value mapping rule form. Improvements to Template operators: Added option to forward input attributes. Allow tests in conditions (e.g., if input1 is sequence ). Updated Jinja library to latest bugfix release. Render markdown (links only) in meta data preview and search item description instead of markdown markup text. Moved Endpoint parameter of Knowledge Graph dataset into advanced section JDBC dataset will retry failed queries due to interrupted connections. Retries will start at the offset of the previously read row. Include query URLs in result from REST operator (multi input). Workflow editor: Adapt node height based on number of inputs. Project import does not show the error message. Added rail navigation bar, replacing the old navigation. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v21.11.5 \u00a4 This version of eccenca DataManager adds the following new features: General Main navigation and application header New header was enabled showing the title of the current view or actually shown resource includes main actions for the page Main navigation was moved to the right sidebar can get expanded permanently offers option to get expanded in a reduced form by hovering it with the cursor we now have various section in the main nav, modules with main navigation items can be configured via subSection parameter (order of sections need to be defined in Navigationbar component, currently we have timetracker , explore , build and other as options, if not set it is automatically organized into explore or other ) Deprecation notice: configuration variables windowTitle and headerName are now deprecated, please use companyName , productName and applicationName from appPresentation Query Module New Query Module v2 Activated per default Improved catalog functionality Richer editor based on yasqe Integrated prefix handling Vocabs Module Allow create new empty ontology without uploading a file. Check if graph exist and show an error while creating a new vocab. Explore Allow hide / show the vocab viz module via configuration details.visualization.enable Center automatically load vocab viz on load Show precise tooltips for controls of vocab viz NavigationListQuery now accepts {{GRAPH}} as placeholder Selectbox load values on focus rather than on click to allow using keyboard. In addition to that, these changes are shipped: Explore bug producing runtime errors in vocab viz Extended existing externalTools mechanism for custom iframes Build simplify configuration object DIWorkspace removing unused url value, letting only enable: BOOLEAN and baseUrl: URL_TO_DATAINTEGRATION Shacl Reload form if language changes Load form data using the current selected language Show group header if there are visible elements with no value in shacline Display selected value in options in select boxes when it is no multi select Don\u2019t send empty datatypes to saveShaped as are interpreted as <file:///data/> . Query Module Improve search on query module Order prefixes alphabetically URL parameters: use query to open a query by IRI or queryString to open a query by query string General Reorder modules. Move Explore to position 1, query to position 4 and manage to the last Explore Group values if only origin graph differs in shacl view. replace markdown-it with react-markdown library and refactor usage update navigation queries navigation.topQuery and navigation.subQuery Vocab Use relative paths properly when requesting DI login prefixes or cache endpoints. Thesaurus Fix links in detail view to point to the selected concept Fix button \u201cSee more in list\u201d to point to explore instance view Prevent infinite loop displaying tabs in detail view In addition to that, multiple performance and stability issues were solved. eccenca DataPlatform v21.11.2 \u00a4 This version of eccenca DataPlatform ships the following new features: Prometheus and Spring metrics endpoints are now exposed per default, i.e. ./actuator/prometheus or actuator/metrics for list and, exemplarily, ./actuator/metrics/cache.size for the metric of interest, see the spring doc for more information. you can deactivate them using the configuration properties in application.yml (or any other spring config) application.yml endpoint : prometheus : enabled : false metrics : enabled : false Users roles need to match values of authorization.abox.adminGroup or authorization.abox.metricsGroup role definition for accessing those endpoints. authorization.abox.metricsGroup defaults to metrics , therefore in keycloak a user needs to metrics added as role, for example via a group and groupmapping. graphdb lucene index support the index is used for example in the explore section to allow fast and userfriendly access Graph List The graph list query is now configurable, using the parameter proxy.graphListQuery with a default value of SELECT distinct ?g {graph ?g {?s ?p ?o}} In addition to that, these changes and fixes are shipped: Middleware Upgrades Upgraded Stardog support to version 7.8.3. Upgraded GraphDB support to version 9.10.2 Change of proxied graph store get endpoint /proxy/{id}/graph Removal of support for timeout and ETags Usage of underlying store graph store endpoints (if available) for performance Upgraded Stardog support to version 7.8.3. Upgraded GraphDB support to version 9.10.2 stop words in search expression are no longer removed eccenca DataPlatform v21.11.1 \u00a4 This version of eccenca DataPlatform ships the following new features: Prefixes are now used in TURTLE serializations Prefixes defined in the Vocaulary catalog are used. Added support for all shacl:path expressions shui:inversePath is still supported, however please use sh:inversePath wherever possible. Property usage analytics endpoints api/vocabusage/* for both explicit Vocab definitions and usage information. Please refer to the OpenAPI definitions for more information. Explicitly defined supported-submit-methods property to enable / disable \u201cTry I Out\u201d button in Swagger UI. Server side UI configuration Support Shapes for WorkspaceConfiguration added Configuration endpoint api/conf/workspace exposes workspace specific information about graph lists. graphs/list endpoint includes graph list association for customization of the navigation list. Shape-intgrated Workflow triggering Shapes extended for linking a Shape to DI workdlows extension of the /api/resource/shaped endpoint to include this information Bootstrap Data: example configuration the default workspace\u2019s navlist Admins can now manually flush all caches per api call. In addition to that, these changes and fixes are shipped: Apache Jena 3.17 is now used. SPARQL requests to virtuoso are now executed over HTTP (see migration notes) Graph exports now sorted by subject resource IT tests: Updated IT test GraphDB Docker image to v9.9.0-1-se parametrization DefaultGraphIT Resources refactoring Bootstrap Data: upgrade some vocabulary references to new versions (fibo, org, qudt, schema, gist, time) Updates Spring Security to mitigate potential security issue: CVE-2021-22119 Improved API documentation for APIs offering multiple HTTP methods (i.e. GET and POST ). Improves OpenAPI client generation GSP Endpoint: read content type from multipart files and use only file extension only as backup Jinja templates: templates no fail on unknown tokens, allowing easier useage of SPARQL OPTIONAL values in templates. Removed: Virtuoso Provisioned Authorization is no longer supported. Use FROM authorization instead. Errors in the configuration graph gracefully fall back to the default config GraphDB FROM Authorization is correctly initialized for the SPARQL proxy Type fetching on stardog could lead to NullPointerException Facets on graphs without imports on Stardog Graph type query was optimized to only fetch types of interest Errors in the configuration graph gracefully fall back to the default config GraphDB FROM Authorization is correctly initialized for the SPARQL proxy Type fetching on stardog could lead to NullPointerException Facets on graphs without imports on Stardog Graph type query was optimized to only fetch types of interest eccenca Corporate Memory Control (cmemc) v21.11.4 \u00a4 This version of cmemc adds the following new features: dataset resource command group with the following resource commands: delete - Delete file resources. inspect -Display all meta data of a file resource. list - List available file resources. usage - Display all usage data of a file resource. scheduler command group with the following workflow scheduler commands: disable - Disable a scheduler. enable - Enable a scheduler. inspect - Display meta data of a scheduler. list - List available schedulers. open - Open scheduler in the browser. config eval command shell environment preparation for configuration In addition to that, these changes are shipped: docker base image is now debian:stable-20211011-slim workflow execute commands now uses ExecuteDefaultWorkflow as activity (instead of ExecuteLocalWorkflow ) admin token --decode table now sorted by Key column dataset inspect table now sorted by Key column query status table now sorted by Key column vocabulary cache list table now sorted by IRI column dataset create --type completes now plugin IDs as well Migration Notes \u00a4 DataIntegration \u00a4 The fix of \u2018XML dataset ignores base path when type URI is set.\u2019, could break existing projects that are relying on the previously broken behaviour. This may affect all use cases with XML datasets that have the \u2018base path\u2019 parameter set to a non-empty value AND where a transformation or linking task overwrites this path by setting a type path/URI. Removing the \u2018base path\u2019 value from the affected XML datasets should solve the issue. Generating URIs for entities failed if XML tags contained dots The JSON dataset uses streaming by default now, which does not support backward paths. Using backward paths will fail and the error message will contain a suggestion to change the streaming parameter to false. DataManager \u00a4 No migration notes DataPlatform \u00a4 Jinja templates will no longer fail on unknown tokens. If this was used for signaling errors or fail-fast evaluation, this has to be implemented in regular conditional checks. Virtuoso config requires adjustments, its HTTP port needs to be configured. Please ensure, that the configured user has the same access rights in virtuoso via ODBC and HTTP application.yml (old) sparqlEndpoints : virtuoso : - id : \"default\" authorization : NONE host : \"store\" port : \"1111\" username : \"dba\" password : \"dba\" becomes application.yml (new) sparqlEndpoints : virtuoso : - id : \"default\" authorization : NONE host : \"store\" port : \"1111\" httpPort : \"80\" username : \"dba\" password : \"dba\" cmemc \u00a4 No migration notes.","title":"Corporate Memory 21.11"},{"location":"release-notes/corporate-memory-21-11/#corporate-memory-2111","text":"Corporate Memory 21.11 is the fourth release in 2021. The highlights of this release are: Build: The workflow editor user interface allows for undo/redo of you activities now, as well as shows inline (live) progress and statistics of a running workflow. Explore: The Explore interface is now adapted to our new look and feel + the Knowledge Graph list component can be configured to show multiple lists of named graphs e.g. to distinguish between user, vocabulary and system graphs. Automate: cmemc can now interact with workflow scheduler (disable, enable, inspect, list, open) as well as dataset resources (delete, inspect, usage, list). Warning With this release of Corporate Memory the DataIntegration and DataPlatform configuration and behaviour has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v21.11.1 eccenca DataIntegration v21.11 eccenca DataManager v21.11.5 eccenca Corporate Memory Control (cmemc) v21.11.4 More detailed release notes for these versions are listed below.","title":"Corporate Memory 21.11"},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataintegration-v2111","text":"This version of eccenca DataIntegration adds the following new features: Remote Excel Dataset using Google Drive Spreadsheets as resources. Remote Office 365 Spreadsheets Support. Workflow editor improvements: View navigation via clicking and dragging the mouse on the mini-map. Config input port is hidden by default and can be enabled via menu entry. Inline (live) progress and statistics. Support mapping of RDF literals in object mappings. Literals are handled as entities and can be mapped in object mappings. Special path #text that allows to access the lexical value of the mapped resource. This allows to access the value of a mapped literal in an object mapping. RDF datasets: Special path #lang that allows to access the language tag of a language tagged RDF literal. Extensions to the Excel dataset: Excel columns may be addressed by their letter code ( #A , #B , etc.) as well. A new parameter \u2018hasHeader\u2019 allows handling of pure data sheets with no table header. Support selecting multiple vocabularies with auto-completion support. Transform URI pattern improvements: Validation of URI patterns in the UI and backend, i.e. it checks that URI templates generate valid URIs. Auto-completion support in URI pattern input component. URI pattern validation endpoint URI pattern auto-completion endpoint Change initial URI pattern of complex URI rules from / to {}/<OBJECT_RULE_ID> REST endpoint to fetch an activity execution error report as JSON or Markdown. Activity integration into task detail pages: Primary, running, failed and all related caching activities are shown on the task detail pages. Caching activities are grouped and have additional information and controls like \u2018refresh all caches\u2019, last update etc. For failed activities it is possible to see and download an execution error report. Support for requesting task parameter values in the item search API. New workflow operator to stop the current workflow execution (without failing) if a specified condition has been met. Add global cache for URI patterns that stores all URI patterns extracted from transform tasks. Add API endpoint to fetch all URI patterns used for given target class URIs. Allow to select from existing URI patterns (related to the same target classes) in object mapping rule form. Support setting the URI pattern during creation of an object mapping rule. Application version info in user menu (sidebar on the right side). JDBC dataset does support token-based authentication for MS SQL server now. Undo/redo support in workflow editor. In addition to that, these changes are shipped: Improved task search with highlighting for nodes in the canvas. Added \u201cDefault Value\u201d Transformation to recommended category. OLE2-based Excel documents (e.g., .xls) are now supported in non-streaming mode. Enabled the Excel \u2018LEFT\u2019 transform function. Mapping Rule Editor will show the rule label (if any) and the mapping target. The JSON dataset supports streaming. The change applies to reading JSON, writing was already streamed. If streaming is enabled, files won\u2019t be loaded into memory, allowing to read large JSON files without running into OutOfMemory errors. Allow to open the value mapping rule formula editor from the create/edit value mapping rule form. Improvements to Template operators: Added option to forward input attributes. Allow tests in conditions (e.g., if input1 is sequence ). Updated Jinja library to latest bugfix release. Render markdown (links only) in meta data preview and search item description instead of markdown markup text. Moved Endpoint parameter of Knowledge Graph dataset into advanced section JDBC dataset will retry failed queries due to interrupted connections. Retries will start at the offset of the previously read row. Include query URLs in result from REST operator (multi input). Workflow editor: Adapt node height based on number of inputs. Project import does not show the error message. Added rail navigation bar, replacing the old navigation. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v21.11"},{"location":"release-notes/corporate-memory-21-11/#eccenca-datamanager-v21115","text":"This version of eccenca DataManager adds the following new features: General Main navigation and application header New header was enabled showing the title of the current view or actually shown resource includes main actions for the page Main navigation was moved to the right sidebar can get expanded permanently offers option to get expanded in a reduced form by hovering it with the cursor we now have various section in the main nav, modules with main navigation items can be configured via subSection parameter (order of sections need to be defined in Navigationbar component, currently we have timetracker , explore , build and other as options, if not set it is automatically organized into explore or other ) Deprecation notice: configuration variables windowTitle and headerName are now deprecated, please use companyName , productName and applicationName from appPresentation Query Module New Query Module v2 Activated per default Improved catalog functionality Richer editor based on yasqe Integrated prefix handling Vocabs Module Allow create new empty ontology without uploading a file. Check if graph exist and show an error while creating a new vocab. Explore Allow hide / show the vocab viz module via configuration details.visualization.enable Center automatically load vocab viz on load Show precise tooltips for controls of vocab viz NavigationListQuery now accepts {{GRAPH}} as placeholder Selectbox load values on focus rather than on click to allow using keyboard. In addition to that, these changes are shipped: Explore bug producing runtime errors in vocab viz Extended existing externalTools mechanism for custom iframes Build simplify configuration object DIWorkspace removing unused url value, letting only enable: BOOLEAN and baseUrl: URL_TO_DATAINTEGRATION Shacl Reload form if language changes Load form data using the current selected language Show group header if there are visible elements with no value in shacline Display selected value in options in select boxes when it is no multi select Don\u2019t send empty datatypes to saveShaped as are interpreted as <file:///data/> . Query Module Improve search on query module Order prefixes alphabetically URL parameters: use query to open a query by IRI or queryString to open a query by query string General Reorder modules. Move Explore to position 1, query to position 4 and manage to the last Explore Group values if only origin graph differs in shacl view. replace markdown-it with react-markdown library and refactor usage update navigation queries navigation.topQuery and navigation.subQuery Vocab Use relative paths properly when requesting DI login prefixes or cache endpoints. Thesaurus Fix links in detail view to point to the selected concept Fix button \u201cSee more in list\u201d to point to explore instance view Prevent infinite loop displaying tabs in detail view In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v21.11.5"},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataplatform-v21112","text":"This version of eccenca DataPlatform ships the following new features: Prometheus and Spring metrics endpoints are now exposed per default, i.e. ./actuator/prometheus or actuator/metrics for list and, exemplarily, ./actuator/metrics/cache.size for the metric of interest, see the spring doc for more information. you can deactivate them using the configuration properties in application.yml (or any other spring config) application.yml endpoint : prometheus : enabled : false metrics : enabled : false Users roles need to match values of authorization.abox.adminGroup or authorization.abox.metricsGroup role definition for accessing those endpoints. authorization.abox.metricsGroup defaults to metrics , therefore in keycloak a user needs to metrics added as role, for example via a group and groupmapping. graphdb lucene index support the index is used for example in the explore section to allow fast and userfriendly access Graph List The graph list query is now configurable, using the parameter proxy.graphListQuery with a default value of SELECT distinct ?g {graph ?g {?s ?p ?o}} In addition to that, these changes and fixes are shipped: Middleware Upgrades Upgraded Stardog support to version 7.8.3. Upgraded GraphDB support to version 9.10.2 Change of proxied graph store get endpoint /proxy/{id}/graph Removal of support for timeout and ETags Usage of underlying store graph store endpoints (if available) for performance Upgraded Stardog support to version 7.8.3. Upgraded GraphDB support to version 9.10.2 stop words in search expression are no longer removed","title":"eccenca DataPlatform v21.11.2"},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataplatform-v21111","text":"This version of eccenca DataPlatform ships the following new features: Prefixes are now used in TURTLE serializations Prefixes defined in the Vocaulary catalog are used. Added support for all shacl:path expressions shui:inversePath is still supported, however please use sh:inversePath wherever possible. Property usage analytics endpoints api/vocabusage/* for both explicit Vocab definitions and usage information. Please refer to the OpenAPI definitions for more information. Explicitly defined supported-submit-methods property to enable / disable \u201cTry I Out\u201d button in Swagger UI. Server side UI configuration Support Shapes for WorkspaceConfiguration added Configuration endpoint api/conf/workspace exposes workspace specific information about graph lists. graphs/list endpoint includes graph list association for customization of the navigation list. Shape-intgrated Workflow triggering Shapes extended for linking a Shape to DI workdlows extension of the /api/resource/shaped endpoint to include this information Bootstrap Data: example configuration the default workspace\u2019s navlist Admins can now manually flush all caches per api call. In addition to that, these changes and fixes are shipped: Apache Jena 3.17 is now used. SPARQL requests to virtuoso are now executed over HTTP (see migration notes) Graph exports now sorted by subject resource IT tests: Updated IT test GraphDB Docker image to v9.9.0-1-se parametrization DefaultGraphIT Resources refactoring Bootstrap Data: upgrade some vocabulary references to new versions (fibo, org, qudt, schema, gist, time) Updates Spring Security to mitigate potential security issue: CVE-2021-22119 Improved API documentation for APIs offering multiple HTTP methods (i.e. GET and POST ). Improves OpenAPI client generation GSP Endpoint: read content type from multipart files and use only file extension only as backup Jinja templates: templates no fail on unknown tokens, allowing easier useage of SPARQL OPTIONAL values in templates. Removed: Virtuoso Provisioned Authorization is no longer supported. Use FROM authorization instead. Errors in the configuration graph gracefully fall back to the default config GraphDB FROM Authorization is correctly initialized for the SPARQL proxy Type fetching on stardog could lead to NullPointerException Facets on graphs without imports on Stardog Graph type query was optimized to only fetch types of interest Errors in the configuration graph gracefully fall back to the default config GraphDB FROM Authorization is correctly initialized for the SPARQL proxy Type fetching on stardog could lead to NullPointerException Facets on graphs without imports on Stardog Graph type query was optimized to only fetch types of interest","title":"eccenca DataPlatform v21.11.1"},{"location":"release-notes/corporate-memory-21-11/#eccenca-corporate-memory-control-cmemc-v21114","text":"This version of cmemc adds the following new features: dataset resource command group with the following resource commands: delete - Delete file resources. inspect -Display all meta data of a file resource. list - List available file resources. usage - Display all usage data of a file resource. scheduler command group with the following workflow scheduler commands: disable - Disable a scheduler. enable - Enable a scheduler. inspect - Display meta data of a scheduler. list - List available schedulers. open - Open scheduler in the browser. config eval command shell environment preparation for configuration In addition to that, these changes are shipped: docker base image is now debian:stable-20211011-slim workflow execute commands now uses ExecuteDefaultWorkflow as activity (instead of ExecuteLocalWorkflow ) admin token --decode table now sorted by Key column dataset inspect table now sorted by Key column query status table now sorted by Key column vocabulary cache list table now sorted by IRI column dataset create --type completes now plugin IDs as well","title":"eccenca Corporate Memory Control (cmemc) v21.11.4"},{"location":"release-notes/corporate-memory-21-11/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-21-11/#dataintegration","text":"The fix of \u2018XML dataset ignores base path when type URI is set.\u2019, could break existing projects that are relying on the previously broken behaviour. This may affect all use cases with XML datasets that have the \u2018base path\u2019 parameter set to a non-empty value AND where a transformation or linking task overwrites this path by setting a type path/URI. Removing the \u2018base path\u2019 value from the affected XML datasets should solve the issue. Generating URIs for entities failed if XML tags contained dots The JSON dataset uses streaming by default now, which does not support backward paths. Using backward paths will fail and the error message will contain a suggestion to change the streaming parameter to false.","title":"DataIntegration"},{"location":"release-notes/corporate-memory-21-11/#datamanager","text":"No migration notes","title":"DataManager"},{"location":"release-notes/corporate-memory-21-11/#dataplatform","text":"Jinja templates will no longer fail on unknown tokens. If this was used for signaling errors or fail-fast evaluation, this has to be implemented in regular conditional checks. Virtuoso config requires adjustments, its HTTP port needs to be configured. Please ensure, that the configured user has the same access rights in virtuoso via ODBC and HTTP application.yml (old) sparqlEndpoints : virtuoso : - id : \"default\" authorization : NONE host : \"store\" port : \"1111\" username : \"dba\" password : \"dba\" becomes application.yml (new) sparqlEndpoints : virtuoso : - id : \"default\" authorization : NONE host : \"store\" port : \"1111\" httpPort : \"80\" username : \"dba\" password : \"dba\"","title":"DataPlatform"},{"location":"release-notes/corporate-memory-21-11/#cmemc","text":"No migration notes.","title":"cmemc"},{"location":"release-notes/corporate-memory-22-1/","text":"Corporate Memory 22.1 \u00a4 Corporate Memory 22.1 is the first release in 2022. The highlights of this release are: Build: The all new linking editor offering a new level of user experience in the linking process supercharged with inline preview and inline validation, improved operator search and much more Python plugin SDK (workflow and transformation plugins) Explore: Shacl: Customizable workflow execute button in Property Shapes allows for declarative embedding of Backend: Support for Amazon Neptune as primary Knowledge Graph Store incl. bulk loading of large files via Amazon S3 Automate: new commands and command groups making the Corporate Memory swiss-command-line-army-knife - cmemc - even more useful Python plugin command group adds capabilities for managing python plugins in your build workspace (admin workspace python) Store command group adds managing commands on quad store level (admin store) Metrics command groups allows for inspecting of server metrics (admin metrics, DataPlatform metrics only at the moment) Warning With this release of Corporate Memory the DataPlatform configuration and behavior has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v22.1 eccenca DataIntegration v22.1 eccenca DataManager v22.1.1 eccenca Corporate Memory Control (cmemc) v22.1.1 More detailed release notes for these versions are listed below. eccenca DataIntegration v22.1 \u00a4 This version of eccenca DataIntegration adds the following new features: Artifact creation dialogs now allows to set the parent project Added a parameter to JDBC datasets to allow clearing the table before workflow execution. Support for custom project and task identifiers at item creation time. Menu option to copy item ID to clipboard. Value type to represent geometry as WKT literals. Python plugin support Initial support for workflow and transform plugins. Plugins are executed in a Python 3 environment. Check documentation for details: Python Plugins User-defined tags on projects and tasks. REST endpoint to fetch rule operator plugins ( /api/core/ruleOperatorPlugins ). convertToComplex query parameter to GET transform rule REST endpoint to always request a complex value transform rule. Add new route to display a task view plugin all by itself without headers, side bar etc. Route: /workbench/projects/:projectId/item/:pluginId/:taskId/view/:viewId REST endpoint to evaluate a linking rule against the reference links only. New linking and transform editors: User created layout and auto-layouting support. Multi-word rule operator search with highlighting. New node actions: select nodes, move selection New edge actions: Connect edge to first free input port via dragging over node, Swap edges Delete selected node(s) or edge via Backspace Select nodes via select box (press Shift + left mouse button & draw rectangle) or multi select (press Alt or Cmd + left clicks): Delete, move clone selection Filter out \u2018Excel\u2019 category of operators when hideGreyListedParameters query parameter is set to true. Supports read-only mode. Query parameter readOnly sets the editor in permanent read-only mode. Allow to edit rule node parameters in a larger modal, so complex parameter values can be more easily edited. All changes done in that modal can be updated in a single transaction (wrt. UNDO/REDO) or cancelled. Inline evaluation Show operator output values and evaluation scores directly inside the linking rule editor nodes Support to show link to external reference links UI via referenceLinksUrl query parameter. Support for real-time evaluation when reference links are available Simple path operator auto-completion In addition to that, these changes are shipped: Support writing large XML files by using a memory-mapped key-value store internally. Request N-Triples instead of Turtle for SPARQL Construct queries in all RDF datasets, because some RDF stores run into memory problems when requesting Turtle. Improved performance of listing project resources on S3. The maximum size of the internal key value store can be configured now. Default value is: caches.persistence.maxSize=10GB Improved writing to PostgresQL by using CSV import. If an operator writes multiple tables into a dataset that cannot hold multiple tables, an error is thrown now. Example: A hierarchical transformation writes into a CSV file. Previously, the last table has been written. Workflows can also be executed asynchronously using the \u201csimple\u201d workflow execution endpoints. If a sub-workflow is running, the workflow editor will display the full sub-workflow report. Keep sort config and page size when switching filters in faceted search views. In generated transform object rules for nested data sources (XML, JSON and RDF) keep the source path even for paths pointing at literal values, e.g. strings. Use DataPlatform\u2019s facets and vocabusage endpoints to fetch available properties for Knowledge Graph datasets in order to improve performance and load. Generated project & task identifiers: Put label-part as prefix and shorten and simplify generated random string. Show labels and links for dependent tasks in delete modal. Added parameter to \u201clower than\u201d and \u201cgreater than\u201d metrics to choose order. In addition to that, multiple performance and stability issues were solved. eccenca DataManager v22.1.1 \u00a4 This version of eccenca DataManager adds the following new features: Shacl: Customizable workflow execute button in Property Shapes. Explore: New Graph List component with configurable lists. In addition to that, these changes are shipped: General Updated typescript to ^4.5.2 and @reduxjs/toolkit to ^1.6.2 . Use session cookie to authenticate in DI requests. Removed Save graph void stats from the Statistics tab. Shacl Allow HTML in Markdown of Shacl descriptions and ObjectView In addition to that, multiple performance and stability issues were solved. eccenca DataPlatform v22.1 \u00a4 This version of eccenca DataPlatform ships the following new features: Admin endpoints for zip backup / restore of all graphs New endpoints for storage, analysis and upload of files (rdf files and zip/tgz) /api/upload/ for storing and analyzing stored files /api/upload/transfer for transferring stored files to rdf triple store stored files on system are removed in housekeeping maintenance job Integrated Neptune as a triple store including bulk loading of large files via Amazon Simple Storage Service (Amazon S3) In addition to that, these changes and fixes are shipped: Changed proxied graph store to get endpoint /proxy/{id}/graph Removed support for timeout and ETags. Used underlying store graph store endpoints (if available) for performance. Middleware Upgrades Upgraded Stardog support to version 7.9.0 Upgraded GraphDB support to version 9.10.2 Library Upgrades Upgrade to Java 11 Upgrades of several libraries including Spring Boot 2.6.6 has been done. Upgrade of Apache Jena 4.4 implies usage of JDK11 http client library instead of apache http client. Query Monitor New fields added to output of /api/admin/currentQueries endpoint: user (Executing user in form of IRI), type (Query type - one of ASK , SELECT , CONSTRUCT , DESCRIBE , UNKNOWN ), traceId (Trace id of call to Dataplatform - this id bundles 1-n child ids for each query call to backend store) Introduced Spring Sleuth tracing for generation of query IDs and tracing IDs of requests In addition to that, multiple performance and stability issues were solved. eccenca Corporate Memory Control (cmemc) v22.1.1 \u00a4 This version of cmemc adds the following new features: admin workspace python command group install - Install a python package to the workspace list - List installed python packages list-plugins - List installed workspace plugins uninstall - Uninstall a python package from the workspace project import command output warnings in case there are failed tasks errors graph export command the --filename-template / -t option has now a completion of common examples query replay command replay query logs from the query status command admin status command Output of DataManager version and status Output of ShapesCatalog version and status query status command Type filter allows for filtering by query type status filter accepts value error to filter for non-successful queries admin store command group export - backup all knowledge graphs to a ZIP archive import - restore graphs from a ZIP archive bootstrap - was admin bootstap showcase - was admin showcase admin metrics command group get - Get sample data of a metric inspect - Inspect a metric list - List metrics for a specific job In addition to that, these changes and fixes are shipped: docker base image is now python:3.9-slim graph tree --id-only option this option now outputs a flat, de-duplicated list of existing graphs the old output was similar to the default tree output and not useful for piping tested and build python version is now 3.9 cmempy tests for python 2.7 are now disabled graph list command SPARQL 1.1 Service Description namespace now recognised as sd: (e.g. in virtuoso used) query list command newly introduced query types are treated correctly now additional types: DELETE , DROP and INSERT docker image has now an empty config.ini in order to avoid warnings when using cmemc with environment variables only The following commands are deprecated: admin bootstap command is now in admin store command group, will be removed with the next release admin showcase command is now in admin store command group, will be removed with the next release In addition to that, multiple performance and stability issues were solved. Migration Notes \u00a4 DataIntegration \u00a4 Writing hierarchical transformations into a CSV dataset or any other datasets that are single tables will lead to an error. The following plugin IDs have been renamed. Old projects can still be loaded with this DI version, but reading projects written with this version using DI releases older than 22.1 can result in project loading errors: Substring comparison: ID substring to substringDistance Constant distance measure: ID constant to constantDistance Negate transformer: ID negate to negateTransformer DI uses new DataPlatform endpoints for schema extraction from knowledge graphs by default, i.e. some functionality will break when run with an older DataPlatform. Set eccencaDataPlatform.sparqlSource.retrievePathsViaDpEndpoints to false in order to use the old approach. DataManager \u00a4 New Graph List component with configurable lists. The lists can be configured in the cmem config graph. Configurations in the <datamanager>/application.yml are ignored any customizations need to be migrated. DataPlatform \u00a4 While updating, property spring.profiles=PROFILE needs to be replaced by spring.config.activate.on-profile . For further information, please see this blog post . Removed custom redirect for Swagger UI under /swagger-ui . Swagger UI only accessible under /swagger-ui.html (Spring Boot Default) cmemc \u00a4 docker image usage the cmemc docker image is now built to run as user cmem (id: 999 ) the container internally used config file has changed old: /root/.config/cmemc/config.ini new: /config/cmemc.ini This means, mounted config volumes need to be changed! deprecated commands admin bootstrap | showcase are deprecated use admin store bootstrap | showcase instead","title":"Corporate Memory 22.1"},{"location":"release-notes/corporate-memory-22-1/#corporate-memory-221","text":"Corporate Memory 22.1 is the first release in 2022. The highlights of this release are: Build: The all new linking editor offering a new level of user experience in the linking process supercharged with inline preview and inline validation, improved operator search and much more Python plugin SDK (workflow and transformation plugins) Explore: Shacl: Customizable workflow execute button in Property Shapes allows for declarative embedding of Backend: Support for Amazon Neptune as primary Knowledge Graph Store incl. bulk loading of large files via Amazon S3 Automate: new commands and command groups making the Corporate Memory swiss-command-line-army-knife - cmemc - even more useful Python plugin command group adds capabilities for managing python plugins in your build workspace (admin workspace python) Store command group adds managing commands on quad store level (admin store) Metrics command groups allows for inspecting of server metrics (admin metrics, DataPlatform metrics only at the moment) Warning With this release of Corporate Memory the DataPlatform configuration and behavior has changed and have to be adapted according to the migration notes below. This release delivers the following component versions: eccenca DataPlatform v22.1 eccenca DataIntegration v22.1 eccenca DataManager v22.1.1 eccenca Corporate Memory Control (cmemc) v22.1.1 More detailed release notes for these versions are listed below.","title":"Corporate Memory 22.1"},{"location":"release-notes/corporate-memory-22-1/#eccenca-dataintegration-v221","text":"This version of eccenca DataIntegration adds the following new features: Artifact creation dialogs now allows to set the parent project Added a parameter to JDBC datasets to allow clearing the table before workflow execution. Support for custom project and task identifiers at item creation time. Menu option to copy item ID to clipboard. Value type to represent geometry as WKT literals. Python plugin support Initial support for workflow and transform plugins. Plugins are executed in a Python 3 environment. Check documentation for details: Python Plugins User-defined tags on projects and tasks. REST endpoint to fetch rule operator plugins ( /api/core/ruleOperatorPlugins ). convertToComplex query parameter to GET transform rule REST endpoint to always request a complex value transform rule. Add new route to display a task view plugin all by itself without headers, side bar etc. Route: /workbench/projects/:projectId/item/:pluginId/:taskId/view/:viewId REST endpoint to evaluate a linking rule against the reference links only. New linking and transform editors: User created layout and auto-layouting support. Multi-word rule operator search with highlighting. New node actions: select nodes, move selection New edge actions: Connect edge to first free input port via dragging over node, Swap edges Delete selected node(s) or edge via Backspace Select nodes via select box (press Shift + left mouse button & draw rectangle) or multi select (press Alt or Cmd + left clicks): Delete, move clone selection Filter out \u2018Excel\u2019 category of operators when hideGreyListedParameters query parameter is set to true. Supports read-only mode. Query parameter readOnly sets the editor in permanent read-only mode. Allow to edit rule node parameters in a larger modal, so complex parameter values can be more easily edited. All changes done in that modal can be updated in a single transaction (wrt. UNDO/REDO) or cancelled. Inline evaluation Show operator output values and evaluation scores directly inside the linking rule editor nodes Support to show link to external reference links UI via referenceLinksUrl query parameter. Support for real-time evaluation when reference links are available Simple path operator auto-completion In addition to that, these changes are shipped: Support writing large XML files by using a memory-mapped key-value store internally. Request N-Triples instead of Turtle for SPARQL Construct queries in all RDF datasets, because some RDF stores run into memory problems when requesting Turtle. Improved performance of listing project resources on S3. The maximum size of the internal key value store can be configured now. Default value is: caches.persistence.maxSize=10GB Improved writing to PostgresQL by using CSV import. If an operator writes multiple tables into a dataset that cannot hold multiple tables, an error is thrown now. Example: A hierarchical transformation writes into a CSV file. Previously, the last table has been written. Workflows can also be executed asynchronously using the \u201csimple\u201d workflow execution endpoints. If a sub-workflow is running, the workflow editor will display the full sub-workflow report. Keep sort config and page size when switching filters in faceted search views. In generated transform object rules for nested data sources (XML, JSON and RDF) keep the source path even for paths pointing at literal values, e.g. strings. Use DataPlatform\u2019s facets and vocabusage endpoints to fetch available properties for Knowledge Graph datasets in order to improve performance and load. Generated project & task identifiers: Put label-part as prefix and shorten and simplify generated random string. Show labels and links for dependent tasks in delete modal. Added parameter to \u201clower than\u201d and \u201cgreater than\u201d metrics to choose order. In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataIntegration v22.1"},{"location":"release-notes/corporate-memory-22-1/#eccenca-datamanager-v2211","text":"This version of eccenca DataManager adds the following new features: Shacl: Customizable workflow execute button in Property Shapes. Explore: New Graph List component with configurable lists. In addition to that, these changes are shipped: General Updated typescript to ^4.5.2 and @reduxjs/toolkit to ^1.6.2 . Use session cookie to authenticate in DI requests. Removed Save graph void stats from the Statistics tab. Shacl Allow HTML in Markdown of Shacl descriptions and ObjectView In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataManager v22.1.1"},{"location":"release-notes/corporate-memory-22-1/#eccenca-dataplatform-v221","text":"This version of eccenca DataPlatform ships the following new features: Admin endpoints for zip backup / restore of all graphs New endpoints for storage, analysis and upload of files (rdf files and zip/tgz) /api/upload/ for storing and analyzing stored files /api/upload/transfer for transferring stored files to rdf triple store stored files on system are removed in housekeeping maintenance job Integrated Neptune as a triple store including bulk loading of large files via Amazon Simple Storage Service (Amazon S3) In addition to that, these changes and fixes are shipped: Changed proxied graph store to get endpoint /proxy/{id}/graph Removed support for timeout and ETags. Used underlying store graph store endpoints (if available) for performance. Middleware Upgrades Upgraded Stardog support to version 7.9.0 Upgraded GraphDB support to version 9.10.2 Library Upgrades Upgrade to Java 11 Upgrades of several libraries including Spring Boot 2.6.6 has been done. Upgrade of Apache Jena 4.4 implies usage of JDK11 http client library instead of apache http client. Query Monitor New fields added to output of /api/admin/currentQueries endpoint: user (Executing user in form of IRI), type (Query type - one of ASK , SELECT , CONSTRUCT , DESCRIBE , UNKNOWN ), traceId (Trace id of call to Dataplatform - this id bundles 1-n child ids for each query call to backend store) Introduced Spring Sleuth tracing for generation of query IDs and tracing IDs of requests In addition to that, multiple performance and stability issues were solved.","title":"eccenca DataPlatform v22.1"},{"location":"release-notes/corporate-memory-22-1/#eccenca-corporate-memory-control-cmemc-v2211","text":"This version of cmemc adds the following new features: admin workspace python command group install - Install a python package to the workspace list - List installed python packages list-plugins - List installed workspace plugins uninstall - Uninstall a python package from the workspace project import command output warnings in case there are failed tasks errors graph export command the --filename-template / -t option has now a completion of common examples query replay command replay query logs from the query status command admin status command Output of DataManager version and status Output of ShapesCatalog version and status query status command Type filter allows for filtering by query type status filter accepts value error to filter for non-successful queries admin store command group export - backup all knowledge graphs to a ZIP archive import - restore graphs from a ZIP archive bootstrap - was admin bootstap showcase - was admin showcase admin metrics command group get - Get sample data of a metric inspect - Inspect a metric list - List metrics for a specific job In addition to that, these changes and fixes are shipped: docker base image is now python:3.9-slim graph tree --id-only option this option now outputs a flat, de-duplicated list of existing graphs the old output was similar to the default tree output and not useful for piping tested and build python version is now 3.9 cmempy tests for python 2.7 are now disabled graph list command SPARQL 1.1 Service Description namespace now recognised as sd: (e.g. in virtuoso used) query list command newly introduced query types are treated correctly now additional types: DELETE , DROP and INSERT docker image has now an empty config.ini in order to avoid warnings when using cmemc with environment variables only The following commands are deprecated: admin bootstap command is now in admin store command group, will be removed with the next release admin showcase command is now in admin store command group, will be removed with the next release In addition to that, multiple performance and stability issues were solved.","title":"eccenca Corporate Memory Control (cmemc) v22.1.1"},{"location":"release-notes/corporate-memory-22-1/#migration-notes","text":"","title":"Migration Notes"},{"location":"release-notes/corporate-memory-22-1/#dataintegration","text":"Writing hierarchical transformations into a CSV dataset or any other datasets that are single tables will lead to an error. The following plugin IDs have been renamed. Old projects can still be loaded with this DI version, but reading projects written with this version using DI releases older than 22.1 can result in project loading errors: Substring comparison: ID substring to substringDistance Constant distance measure: ID constant to constantDistance Negate transformer: ID negate to negateTransformer DI uses new DataPlatform endpoints for schema extraction from knowledge graphs by default, i.e. some functionality will break when run with an older DataPlatform. Set eccencaDataPlatform.sparqlSource.retrievePathsViaDpEndpoints to false in order to use the old approach.","title":"DataIntegration"},{"location":"release-notes/corporate-memory-22-1/#datamanager","text":"New Graph List component with configurable lists. The lists can be configured in the cmem config graph. Configurations in the <datamanager>/application.yml are ignored any customizations need to be migrated.","title":"DataManager"},{"location":"release-notes/corporate-memory-22-1/#dataplatform","text":"While updating, property spring.profiles=PROFILE needs to be replaced by spring.config.activate.on-profile . For further information, please see this blog post . Removed custom redirect for Swagger UI under /swagger-ui . Swagger UI only accessible under /swagger-ui.html (Spring Boot Default)","title":"DataPlatform"},{"location":"release-notes/corporate-memory-22-1/#cmemc","text":"docker image usage the cmemc docker image is now built to run as user cmem (id: 999 ) the container internally used config file has changed old: /root/.config/cmemc/config.ini new: /config/cmemc.ini This means, mounted config volumes need to be changed! deprecated commands admin bootstrap | showcase are deprecated use admin store bootstrap | showcase instead","title":"cmemc"},{"location":"tests/markdown/","text":"Markdown \u00a4 Code Blocks \u00a4 syntax hightligning numbered lines annotation marks file title line hightligning select.rq 1 2 3 4 5 6 PREFIX rdf : <http://www.w3.org/1999/02/22-rdf-syntax-ns#> # (1)! PREFIX rdfs : <http://www.w3.org/2000/01/rdf-schema#> SELECT ?s WHERE { ?s ?p ?o . } This is a namespace prefix definition \ud83d\ude03 Admonitions \u00a4 Foobar ksahdkjhs dkjasd hkajd hkjad Tabs \u00a4 C C++ #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Charts \u00a4 graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; Icons, Emojis \u00a4 Icon Reference Horizontal Rules \u00a4 Emphasis \u00a4 This is bold text This is bold text This is italic text This is italic text Blockquotes \u00a4 Blockquotes can also be nested\u2026 \u2026by using additional greater-than signs right next to each other\u2026 \u2026or with spaces between arrows. Lists \u00a4 Unordered \u00a4 Create a list by starting a line with + , - , or * Sub-lists are made by indenting 2 spaces: Marker character change forces new list start: Ac tristique libero volutpat at Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Very easy! Ordered \u00a4 Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa You can use sequential numbers\u2026 \u2026or keep all the numbers as 1. Start numbering with offset: foo bar Code \u00a4 Inline code Indented code // Some comments line 1 of code line 2 of code line 3 of code Block code \u201cfences\u201d Sample text here... Syntax highlighting var foo = function ( bar ) { return bar ++ ; }; console . log ( foo ( 5 )); Tables \u00a4 Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Right aligned columns Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Links \u00a4 link text link with title Autoconverted link https://github.com/nodeca/pica (enable linkify to see) Images \u00a4 Stand Alone \u00a4 With Title \u00a4 With Caption \u00a4 22.1: DataIntegration - Linking Editor As footnote Style \u00a4 Like links, Images also have a footnote style syntax With a reference later in the document defining the URL location: Definition lists \u00a4 Term 1 Definition 1 with lazy continuation. Term 2 with inline markup Definition 2 { some code, part of Definition 2 } Third paragraph of definition 2. Abbreviations \u00a4 This is HTML abbreviation example. It converts \u201c HTML \u201d, but keep intact partial entries like \u201cxxxHTMLyyy\u201d and so on.","title":"Markdown"},{"location":"tests/markdown/#markdown","text":"","title":"Markdown"},{"location":"tests/markdown/#code-blocks","text":"syntax hightligning numbered lines annotation marks file title line hightligning select.rq 1 2 3 4 5 6 PREFIX rdf : <http://www.w3.org/1999/02/22-rdf-syntax-ns#> # (1)! PREFIX rdfs : <http://www.w3.org/2000/01/rdf-schema#> SELECT ?s WHERE { ?s ?p ?o . } This is a namespace prefix definition \ud83d\ude03","title":"Code Blocks"},{"location":"tests/markdown/#admonitions","text":"Foobar ksahdkjhs dkjasd hkajd hkjad","title":"Admonitions"},{"location":"tests/markdown/#tabs","text":"C C++ #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Tabs"},{"location":"tests/markdown/#charts","text":"graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!];","title":"Charts"},{"location":"tests/markdown/#icons-emojis","text":"Icon Reference","title":"Icons, Emojis"},{"location":"tests/markdown/#horizontal-rules","text":"","title":"Horizontal Rules"},{"location":"tests/markdown/#emphasis","text":"This is bold text This is bold text This is italic text This is italic text","title":"Emphasis"},{"location":"tests/markdown/#blockquotes","text":"Blockquotes can also be nested\u2026 \u2026by using additional greater-than signs right next to each other\u2026 \u2026or with spaces between arrows.","title":"Blockquotes"},{"location":"tests/markdown/#lists","text":"","title":"Lists"},{"location":"tests/markdown/#unordered","text":"Create a list by starting a line with + , - , or * Sub-lists are made by indenting 2 spaces: Marker character change forces new list start: Ac tristique libero volutpat at Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Very easy!","title":"Unordered"},{"location":"tests/markdown/#ordered","text":"Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa You can use sequential numbers\u2026 \u2026or keep all the numbers as 1. Start numbering with offset: foo bar","title":"Ordered"},{"location":"tests/markdown/#code","text":"Inline code Indented code // Some comments line 1 of code line 2 of code line 3 of code Block code \u201cfences\u201d Sample text here... Syntax highlighting var foo = function ( bar ) { return bar ++ ; }; console . log ( foo ( 5 ));","title":"Code"},{"location":"tests/markdown/#tables","text":"Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Right aligned columns Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files.","title":"Tables"},{"location":"tests/markdown/#links","text":"link text link with title Autoconverted link https://github.com/nodeca/pica (enable linkify to see)","title":"Links"},{"location":"tests/markdown/#images","text":"","title":"Images"},{"location":"tests/markdown/#stand-alone","text":"","title":"Stand Alone"},{"location":"tests/markdown/#with-title","text":"","title":"With Title"},{"location":"tests/markdown/#with-caption","text":"22.1: DataIntegration - Linking Editor","title":"With Caption"},{"location":"tests/markdown/#as-footnote-style","text":"Like links, Images also have a footnote style syntax With a reference later in the document defining the URL location:","title":"As footnote Style"},{"location":"tests/markdown/#definition-lists","text":"Term 1 Definition 1 with lazy continuation. Term 2 with inline markup Definition 2 { some code, part of Definition 2 } Third paragraph of definition 2.","title":"Definition lists"},{"location":"tests/markdown/#abbreviations","text":"This is HTML abbreviation example. It converts \u201c HTML \u201d, but keep intact partial entries like \u201cxxxHTMLyyy\u201d and so on.","title":"Abbreviations"},{"location":"tests/tags/","text":"API \u00a4 cmempy - Python API DataIntegration APIs DataPlatform APIs AdvancedTutorial \u00a4 Lift data from JSON and XML source Automate \u00a4 Overview BeginnersTutorial \u00a4 Lift data from tabular data such as CSV, XSLX or database tables CSS \u00a4 Technical Docker \u00a4 Using the docker image HTML5 \u00a4 Technical JavaScript \u00a4 Technical Reference \u00a4 Command Reference SPARQL \u00a4 SPARQL Scripts Workflow \u00a4 Workflow execution and orchestration cmemc \u00a4 Overview Certificate handling and SSL verification Command-Line Completion Command Reference Installation and Configuration Environment based Configuration File based Configuration Getting Credentials from external Processes SPARQL Scripts Troubleshooting and Caveats Using the docker image Workflow execution and orchestration python-plugins \u00a4 Python Plugins","title":"Tags"},{"location":"tests/tags/#api","text":"cmempy - Python API DataIntegration APIs DataPlatform APIs","title":"API"},{"location":"tests/tags/#advancedtutorial","text":"Lift data from JSON and XML source","title":"AdvancedTutorial"},{"location":"tests/tags/#automate","text":"Overview","title":"Automate"},{"location":"tests/tags/#beginnerstutorial","text":"Lift data from tabular data such as CSV, XSLX or database tables","title":"BeginnersTutorial"},{"location":"tests/tags/#css","text":"Technical","title":"CSS"},{"location":"tests/tags/#docker","text":"Using the docker image","title":"Docker"},{"location":"tests/tags/#html5","text":"Technical","title":"HTML5"},{"location":"tests/tags/#javascript","text":"Technical","title":"JavaScript"},{"location":"tests/tags/#reference","text":"Command Reference","title":"Reference"},{"location":"tests/tags/#sparql","text":"SPARQL Scripts","title":"SPARQL"},{"location":"tests/tags/#workflow","text":"Workflow execution and orchestration","title":"Workflow"},{"location":"tests/tags/#cmemc","text":"Overview Certificate handling and SSL verification Command-Line Completion Command Reference Installation and Configuration Environment based Configuration File based Configuration Getting Credentials from external Processes SPARQL Scripts Troubleshooting and Caveats Using the docker image Workflow execution and orchestration","title":"cmemc"},{"location":"tests/tags/#python-plugins","text":"Python Plugins","title":"python-plugins"},{"location":"tests/technical/","tags":["HTML5","JavaScript","CSS"],"text":"Technical \u00a4 For full documentation visit mkdocs.org . Commands \u00a4 mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00a4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Technical"},{"location":"tests/technical/#technical","text":"For full documentation visit mkdocs.org .","title":"Technical"},{"location":"tests/technical/#commands","text":"mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"tests/technical/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"tutorials/","text":"\ud83c\udfaf Tutorials \u00a4 Learn by example and step-by-step guidelines to achieve a concrete goal fast.This list of Tutorial style content can be used to learn by example and by following step-by-step guides. [TAGS]","title":"\ud83c\udfaf Tutorials"},{"location":"tutorials/#tutorials","text":"Learn by example and step-by-step guidelines to achieve a concrete goal fast.This list of Tutorial style content can be used to learn by example and by following step-by-step guides. [TAGS]","title":"\ud83c\udfaf Tutorials"},{"location":"tests/tags/","text":"API \u00a4 cmempy - Python API DataIntegration APIs DataPlatform APIs AdvancedTutorial \u00a4 Lift data from JSON and XML source Automate \u00a4 Overview BeginnersTutorial \u00a4 Lift data from tabular data such as CSV, XSLX or database tables CSS \u00a4 Technical Docker \u00a4 Using the docker image HTML5 \u00a4 Technical JavaScript \u00a4 Technical Reference \u00a4 Command Reference SPARQL \u00a4 SPARQL Scripts Workflow \u00a4 Workflow execution and orchestration cmemc \u00a4 Overview Certificate handling and SSL verification Command-Line Completion Command Reference Installation and Configuration Environment based Configuration File based Configuration Getting Credentials from external Processes SPARQL Scripts Troubleshooting and Caveats Using the docker image Workflow execution and orchestration python-plugins \u00a4 Python Plugins","title":"Tags"},{"location":"tests/tags/#api","text":"cmempy - Python API DataIntegration APIs DataPlatform APIs","title":"API"},{"location":"tests/tags/#advancedtutorial","text":"Lift data from JSON and XML source","title":"AdvancedTutorial"},{"location":"tests/tags/#automate","text":"Overview","title":"Automate"},{"location":"tests/tags/#beginnerstutorial","text":"Lift data from tabular data such as CSV, XSLX or database tables","title":"BeginnersTutorial"},{"location":"tests/tags/#css","text":"Technical","title":"CSS"},{"location":"tests/tags/#docker","text":"Using the docker image","title":"Docker"},{"location":"tests/tags/#html5","text":"Technical","title":"HTML5"},{"location":"tests/tags/#javascript","text":"Technical","title":"JavaScript"},{"location":"tests/tags/#reference","text":"Command Reference","title":"Reference"},{"location":"tests/tags/#sparql","text":"SPARQL Scripts","title":"SPARQL"},{"location":"tests/tags/#workflow","text":"Workflow execution and orchestration","title":"Workflow"},{"location":"tests/tags/#cmemc","text":"Overview Certificate handling and SSL verification Command-Line Completion Command Reference Installation and Configuration Environment based Configuration File based Configuration Getting Credentials from external Processes SPARQL Scripts Troubleshooting and Caveats Using the docker image Workflow execution and orchestration","title":"cmemc"},{"location":"tests/tags/#python-plugins","text":"Python Plugins","title":"python-plugins"}]}