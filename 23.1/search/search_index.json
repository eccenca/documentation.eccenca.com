{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to documentation.eccenca.com \ud83e\udd13","text":"<ul> <li> <p> Release Notes</p> <p>Documentation of changes and enhancements for each version.</p> </li> <li> <p> Tutorials</p> <p>Learn by example and step-by-step guidelines to achieve a concrete goal fast.</p> </li> <li> <p> Getting Started</p> <p>This page describes how to work with Corporate Memory and shortly outlines all functionalities of the user interface.</p> </li> <li> <p> Build</p> <p>Lift your data by integrating multiple datasets into a Knowledge Graph.</p> </li> <li> <p> Explore and Author</p> <p>Explore, author and interact with your Knowledge Graph.</p> </li> <li> <p> Consume</p> <p>This section outlines how to consume data from the Knowledge Graph.</p> </li> <li> <p> Deploy and Configure</p> <p>Deploy in your own environment.</p> </li> <li> <p> Automate</p> <p>Setup processes and automate activities based on and towards your Knowledge Graph.</p> </li> <li> <p> Develop</p> <p>API documentation and programming recipes.</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#advancedtutorial","title":"AdvancedTutorial","text":"<ul> <li>Lift data from JSON and XML source</li> <li>Populate Data to Neo4j</li> </ul>"},{"location":"tags/#api","title":"API","text":"<ul> <li>Provide Data in any Format via a Custom API</li> <li>cmempy - Python API</li> <li>DataIntegration APIs</li> <li>DataPlatform APIs</li> </ul>"},{"location":"tags/#automate","title":"Automate","text":"<ul> <li>cmemc (Overview)</li> <li>cmemc: Command Group - workflow scheduler</li> <li>cmemc: Using Github Actions</li> <li>cmemc: Using Gitlab Pipelines</li> <li>cmemc: SPARQL Scripts</li> <li>cmemc: Workflow Execution and Orchestration</li> <li>Processing Data with Variable Input Workflows</li> <li>Scheduling Workflows</li> <li>Populate Graphs to Apache Kafka</li> </ul>"},{"location":"tags/#beginnerstutorial","title":"BeginnersTutorial","text":"<ul> <li>Active Learning of Linking Rules</li> <li>Lift data from tabular data such as CSV, XSLX or database tables</li> <li>Getting Started</li> </ul>"},{"location":"tags/#bestpractice","title":"BestPractice","text":"<ul> <li>Cool IRIs</li> <li>Define Prefixes / Namespaces</li> </ul>"},{"location":"tags/#cmemc","title":"cmemc","text":"<ul> <li>cmemc (Overview)</li> <li>cmemc: Command Reference</li> <li>cmemc: Command Group - admin</li> <li>cmemc: Command Group - admin metrics</li> <li>cmemc: Command Group - admin store</li> <li>cmemc: Command Group - admin user</li> <li>cmemc: Command Group - admin workspace</li> <li>cmemc: Command Group - admin workspace python</li> <li>cmemc: Command Group - config</li> <li>cmemc: Command Group - dataset</li> <li>cmemc: Command Group - dataset resource</li> <li>cmemc: Command Group - graph</li> <li>cmemc: Command Group - project</li> <li>cmemc: Command Group - query</li> <li>cmemc: Command Group - vocabulary</li> <li>cmemc: Command Group - vocabulary cache</li> <li>cmemc: Command Group - workflow</li> <li>cmemc: Command Group - workflow scheduler</li> <li>cmemc: Configuration</li> <li>cmemc: Certificate handling and SSL verification</li> <li>cmemc: Command-Line Completion</li> <li>cmemc: Environment-based Configuration</li> <li>cmemc: File-based Configuration</li> <li>cmemc: Getting Credentials from External Processes</li> <li>cmemc: Installation</li> <li>cmemc: Invocation</li> <li>cmemc: Using the Docker Image</li> <li>cmemc: Using Github Actions</li> <li>cmemc: Using Gitlab Pipelines</li> <li>cmemc: SPARQL Scripts</li> <li>cmemc: Troubleshooting and Caveats</li> <li>cmemc: Workflow Execution and Orchestration</li> <li>Processing Data with Variable Input Workflows</li> <li>Access Conditions</li> </ul>"},{"location":"tags/#configuration","title":"Configuration","text":"<ul> <li>cmemc: Command Group - config</li> <li>DataIntegration</li> <li>DataManager</li> <li>DataPlatform</li> <li>Docker Orchestration</li> <li>Keycloak</li> <li>Changing Passwords and Keys</li> <li>Configure Corporate Memory with an external Keycloak</li> <li>Production-Ready Settings</li> <li>Quad Store Configuration</li> <li>Reverse Proxy</li> </ul>"},{"location":"tags/#dashboards","title":"Dashboards","text":"<ul> <li>Consuming Graphs in Power BI</li> <li>Consuming Graphs in Redash</li> <li>Embedding Services via the Integrations Module</li> </ul>"},{"location":"tags/#docker","title":"Docker","text":"<ul> <li>cmemc: Using the Docker Image</li> <li>Docker Orchestration</li> </ul>"},{"location":"tags/#experttutorial","title":"ExpertTutorial","text":"<ul> <li>Processing Data with Variable Input Workflows</li> <li>Extracting data from a Web API</li> <li>Loading JDBC datasets incrementally</li> <li>Consuming Graphs with SQL Databases</li> <li>Provide Data in any Format via a Custom API</li> </ul>"},{"location":"tags/#java","title":"Java","text":"<ul> <li>Accessing Graphs with Java Applications</li> </ul>"},{"location":"tags/#keycloak","title":"Keycloak","text":"<ul> <li>cmemc: Command Group - admin user</li> <li>Keycloak</li> <li>Changing Passwords and Keys</li> <li>Configure Corporate Memory with an external Keycloak</li> </ul>"},{"location":"tags/#knowledgegraph","title":"KnowledgeGraph","text":"<ul> <li>cmemc: Command Group - graph</li> <li>Cool IRIs</li> <li>Define Prefixes / Namespaces</li> <li>Lift data from JSON and XML source</li> <li>Lift data from tabular data such as CSV, XSLX or database tables</li> <li>Populate Graphs to Apache Kafka</li> <li>Consuming Graphs in Power BI</li> <li>Consuming Graphs in Redash</li> <li>EasyNav Module</li> <li>Graph Exploration</li> <li>Building a Customized User Interface</li> <li>Statement Annotations</li> <li>Versioning of Graph Changes</li> <li>Query Module</li> <li>Vocabulary Catalog</li> </ul>"},{"location":"tags/#plugin","title":"Plugin","text":"<ul> <li>Python Plugins: Installation and Usage</li> </ul>"},{"location":"tags/#project","title":"Project","text":"<ul> <li>cmemc: Command Group - project</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>cmemc: Command Group - admin workspace python</li> <li>cmempy - Python API</li> <li>Python Plugins: Overview</li> <li>Python Plugins: Development</li> <li>Python Plugins: Installation and Usage</li> <li>Python Plugins: Setup and Configuration</li> </ul>"},{"location":"tags/#pythonplugin","title":"PythonPlugin","text":"<ul> <li>Populate Graphs to Apache Kafka</li> </ul>"},{"location":"tags/#reference","title":"Reference","text":"<ul> <li>cmemc: Command Reference</li> <li>Rule Operators</li> <li>DataIntegration: Activity Reference</li> <li>DataIntegration: Plugin Reference</li> <li>GraphResourcePattern</li> <li>Datatypes</li> <li>Node Shape Reference</li> <li>Property Shapes</li> </ul>"},{"location":"tags/#releasenote","title":"ReleaseNote","text":"<ul> <li>Corporate Memory 19.10</li> <li>Corporate Memory 20.03</li> <li>Corporate Memory 20.06</li> <li>Corporate Memory 20.10</li> <li>Corporate Memory 20.12</li> <li>Corporate Memory 21.02</li> <li>Corporate Memory 21.04</li> <li>Corporate Memory 21.06</li> <li>Corporate Memory 21.11</li> <li>Corporate Memory 22.1</li> <li>Corporate Memory 22.2.3</li> <li>Corporate Memory 23.1</li> </ul>"},{"location":"tags/#security","title":"Security","text":"<ul> <li>cmemc: Command Group - admin user</li> <li>cmemc: Certificate handling and SSL verification</li> <li>cmemc: Getting Credentials from External Processes</li> <li>Access Conditions</li> <li>Keycloak</li> <li>Changing Passwords and Keys</li> <li>Configure Corporate Memory with an external Keycloak</li> <li>Production-Ready Settings</li> </ul>"},{"location":"tags/#sparql","title":"SPARQL","text":"<ul> <li>cmemc: Command Group - admin store</li> <li>cmemc: Command Group - query</li> <li>cmemc: SPARQL Scripts</li> <li>Provide Data in any Format via a Custom API</li> <li>Query Module</li> </ul>"},{"location":"tags/#video","title":"Video","text":"<ul> <li>cmemc: Command-Line Completion</li> <li>Scheduling Workflows</li> </ul>"},{"location":"tags/#vocabulary","title":"Vocabulary","text":"<ul> <li>cmemc: Command Group - vocabulary</li> <li>cmemc: Command Group - vocabulary cache</li> <li>Datatypes</li> <li>Node Shape Reference</li> <li>Property Shapes</li> <li>Thesauri Management</li> <li>Vocabulary Catalog</li> </ul>"},{"location":"tags/#workflow","title":"Workflow","text":"<ul> <li>cmemc: Command Group - workflow</li> <li>cmemc: Workflow Execution and Orchestration</li> <li>Processing Data with Variable Input Workflows</li> <li>Scheduling Workflows</li> <li>Workflow Reconfiguration</li> <li>Workflow Trigger</li> </ul>"},{"location":"automate/","title":"Automate","text":"<p>Setup processes and automate activities based on and towards your Knowledge Graph.</p> <p> Intended audience: Linked Data Experts and Deployment Engineers</p> <ul> <li> <p> cmemc - Command Line Interface</p> <p>cmemc is intended for system administrators and Linked Data experts, who want to automate and control activities on eccenca Corporate Memory remotely.</p> </li> <li> <p> Processing data with variable input workflows</p> <p>This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (i.e., without registering datasets).</p> </li> <li> <p> Scheduling Workflows</p> <p>For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator.</p> </li> <li> <p> Continuous Integration and Delivery</p> <p>Setup processes which continuously integrate data artifacts such as vocabularies and shapes with your Corporate Memory instances.</p> </li> </ul>"},{"location":"automate/cmemc-command-line-interface/","title":"cmemc - Command Line Interface","text":"<ul> <li> <p> Command Line interface for eccenca Corporate Memory</p> <p>Developed with  Python and publicly available as a pypi.org Package and a  Docker Image under the  Apache 2 License.</p> <p>Next Steps:  </p> <p> </p> </li> <li> <p> Intended for Administrators and Linked Data Expert</p> <p>Battle tested in many projects to  Automate Activities and  Remote Control eccenca Corporate Memory instances.</p> Example: List datasets with a specific tag and project.<pre><code>$ cmemc -c prod.knowledge.company.org # (1)!\n  dataset list \\\n  --filter project crm-graph \\\n  --filter tag velocity-daily\n</code></pre> <ol> <li><ul> <li>The option <code>-c</code> is short for <code>--connection</code> and references to a remote Corporate Memory instance.</li> <li>The <code>list</code> command in the <code>dataset</code> command group shows all datasets of an instance.</li> <li>In order to manipulate output dataset list, the <code>--filter</code> option takes two parameter, a filter type (<code>tag</code>, <code>project</code>, \u2026) and a value.</li> </ul> </li> </ol> </li> <li> <p> Fast ad-hoc Execution with Command Completion</p> <p> Create Build Project and Dataset </p> </li> <li> <p> Main Features:</p> <p>Manage (<code>list</code>, <code>inspect</code>, <code>import</code>, <code>export</code>, \u2026) and Manipulate (<code>create</code>, <code>delete</code>, <code>execute</code>, \u2026) Vocabularies, Datasets,  Knowledge Graphs,  Workflows, Projects, Queries, Scheduler, Configurations and much more.</p> Example: Backup the query catalog including imports.<pre><code>$ cmemc graph export \\\nhttps://ns.eccenca.com/data/queries/ \\\n--include-imports --output-dir queries \\\n--filename-template \"{{date}}-{{iriname}}\"\n</code></pre> </li> </ul>","tags":["cmemc","Automate"]},{"location":"automate/cmemc-command-line-interface/command-reference/","title":"Command Reference","text":"<p>Info</p> <p>cmemc is organized as a tree of command groups, each with a set of commands. You can access the command groups in the table as well as in the navigation on the left. You can access the commands directly from the table or by visiting a command group page first.</p> Command Group Command Description admin status Output health and version information. admin token Fetch and output an access token. admin metrics get Get sample data of a metric. admin metrics inspect Inspect a metric. admin metrics list List metrics for a specific job. admin store showcase Create showcase data. admin store bootstrap Update/Import bootstrap data. admin store export Backup all knowledge graphs to a ZIP archive. admin store import Restore graphs from a ZIP archive. admin user list List user accounts. admin user create Create a user account. admin user update Update a user account. admin user delete Delete a user account. admin user password Change the password of a user account. admin user open Open user in the browser. admin workspace export Export the complete workspace (all projects) to a ZIP file. admin workspace import Import the workspace from a file. admin workspace reload Reload the workspace from the backend. admin workspace python install Install a python package to the workspace. admin workspace python uninstall Uninstall a python package from the workspace. admin workspace python list List installed python packages. admin workspace python list-plugins List installed workspace plugins. config list List configured connections. config edit Edit the user-scope configuration file. config get Get the value of a known cmemc configuration key. config eval Export all configuration values of a configuration for evaluation. dataset list List available datasets. dataset delete Delete datasets. dataset download Download the resource file of a dataset. dataset upload Upload a resource file to a dataset. dataset inspect Display metadata of a dataset. dataset create Create a dataset. dataset open Open datasets in the browser. dataset resource list List available file resources. dataset resource delete Delete file resources. dataset resource inspect Display all metadata of a file resource. dataset resource usage Display all usage data of a file resource. graph count Count triples in graph(s). graph tree Show graph tree(s) of the owl:imports hierarchy. graph list List accessible graphs. graph export Export graph(s) as NTriples to stdout (-), file or directory. graph delete Delete graph(s) from the store. graph import Import graph(s) to the store. graph open Open / explore a graph in the browser. project open Open projects in the browser. project list List available projects. project export Export projects to files. project import Import a project from a file or directory. project delete Delete projects. project create Create projects. project reload Reload projects from the workspace provider. query execute Execute queries which are loaded from files or the query catalog. query list List available queries from the catalog. query open Open queries in the editor of the query catalog in your browser. query status Get status information of executed and running queries. query replay Re-execute queries from a replay file. query cancel Cancel a running query. vocabulary open Open / explore a vocabulary graph in the browser. vocabulary list Output a list of vocabularies. vocabulary install Install one or more vocabularies from the catalog. vocabulary uninstall Uninstall one or more vocabularies. vocabulary import Import a turtle file as a vocabulary. vocabulary cache update Reload / updates the data integration cache for a vocabulary. vocabulary cache list Output the content of the global vocabulary cache. workflow execute Execute workflow(s). workflow io Execute a workflow with file input/output. workflow list List available workflow. workflow status Get status information of workflow(s). workflow open Open a workflow in your browser. workflow scheduler open Open scheduler(s) in the browser. workflow scheduler list List available scheduler. workflow scheduler inspect Display all metadata of a scheduler. workflow scheduler disable Disable scheduler(s). workflow scheduler enable Enable scheduler(s).","tags":["Reference","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/","title":"admin Command Group","text":"<p>Import bootstrap data, backup/restore workspace or get status.</p> <p>This command group consists of commands for setting up and configuring eccenca Corporate Memory.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/#admin-status","title":"admin status","text":"<p>Output health and version information.</p> Usage<pre><code>$ cmemc admin status [OPTIONS]\n</code></pre> <p>This command outputs version and health information of the selected deployment. If the version information cannot be retrieved, UNKNOWN is shown.</p> <p>Additionally, this command warns you if the target version of your cmemc client is newer than the version of your backend and if the ShapeCatalog has a different version than your DataPlatform component.</p> <p>To get status information of all configured deployments use this command in combination with parallel.</p> Example<pre><code>$ cmemc config list | parallel --ctag cmemc -c {} admin status\n</code></pre> Options <pre><code>--key TEXT                     Get only specific key(s) from the status /\n                               info output. There are two special keys\n                               available: 'all' will list all available keys\n                               in the table, 'overall.healthy' with result\n                               in  UP in case all health flags are UP as\n                               well (otherwise DOWN).\n\n--exit-1 [never|error|always]  Specify, when this command returns with exit\n                               code 1. Available options are 'never' (exit 0\n                               on errors and warnings), 'error' (exit 1 on\n                               errors, exit 0 on warnings), 'always' (exit 1\n                               on errors and warnings).  [default: never]\n\n--enforce-table                A single value with --key will be returned as\n                               plain text instead of a table with one row\n                               and the header. This default behaviour allows\n                               for more easy integration with scripts. This\n                               flag enforces the use of tabular output, even\n                               for single row tables.\n\n--raw                          Outputs combined raw JSON output of the\n                               health/info endpoints.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/#admin-token","title":"admin token","text":"<p>Fetch and output an access token.</p> Usage<pre><code>$ cmemc admin token [OPTIONS]\n</code></pre> <p>This command can be used to check for correct authentication as well as to use the token with wget / curl or similar standard tools:</p> Example<pre><code>$ curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug\n</code></pre> <p>Please be aware that this command can reveal secrets which you might not want to be present in log files or on the screen.</p> Options <pre><code>--raw       Outputs raw JSON. Note that this option will always try to fetch\n            a new JSON token response. In case you are working with\n            OAUTH_GRANT_TYPE=prefetched_token, this may lead to an error.\n\n--decode    Decode the access token and outputs the raw JSON. Note that the\n            access token is only decoded and esp. not validated.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/","title":"admin metrics Command Group","text":"<p>List and get metrics.</p> <p>This command group consists of commands for reading and listing internal monitoring metrics of eccenca Corporate Memory. A deployment consists of multiple jobs (e.g. DP, DI), which provide multiple metric families for an endpoint.</p> <p>Each metric family can consist of different samples identified by labels with a name and a value (dimensions). A metric has a specific type (counter, gauge, summary and histogram) and additional metadata.</p> <p>Please have a look at https://prometheus.io/docs/concepts/data_model/ for further details.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/#admin-metrics-get","title":"admin metrics get","text":"<p>Get sample data of a metric.</p> Usage<pre><code>$ cmemc admin metrics get [OPTIONS] METRIC_ID\n</code></pre> <p>A metric of a specific job is identified by a metric ID. Possible metric IDs of a job can be retrieved with the <code>metrics list</code> command. A metric can contain multiple samples. These samples are distinguished by labels (name and value).</p> Options <pre><code>--job [DP]               The job from which the metrics data is fetched.\n                         [default: DP]\n\n--filter &lt;TEXT TEXT&gt;...  A set of label name/value pairs in order to filter\n                         the samples of the requested metric family. Each\n                         metric has a different set of labels with different\n                         values. In order to get a list of possible label\n                         names and values, use the command without this\n                         option. The label names are then shown as column\n                         headers and label values as cell values of this\n                         column.\n\n--enforce-table          A single sample value will be returned as plain\n                         text instead of the normal table. This allows for\n                         more easy integration with scripts. This flag\n                         enforces the use of tabular output, even for single\n                         row tables.\n\n--raw                    Outputs raw prometheus sample classes.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/#admin-metrics-inspect","title":"admin metrics inspect","text":"<p>Inspect a metric.</p> Usage<pre><code>$ cmemc admin metrics inspect [OPTIONS] METRIC_ID\n</code></pre> <p>This command outputs the data of a metric. The first table includes basic metadata about the metric. The second table includes sample labels and values.</p> Options <pre><code>--job [DP]  The job from which the metrics data is fetched.  [default: DP]\n--raw       Outputs raw JSON of the table data.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/metrics/#admin-metrics-list","title":"admin metrics list","text":"<p>List metrics for a specific job.</p> Usage<pre><code>$ cmemc admin metrics list [OPTIONS]\n</code></pre> <p>For each metric, the output table shows the metric ID, the type of the metric, a count of how many labels (label names) are describing the samples (L) and a count of how many samples are currently available for a metric (S).</p> Options <pre><code>--job [DP]  The job from which the metrics data is fetched.  [default: DP]\n--id-only   Lists metric identifier only. This is useful for piping the IDs\n            into other commands.\n\n--raw       Outputs (sorted) JSON dict, parsed from the metrics API output.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/","title":"admin store Command Group","text":"<p>Import, export and bootstrap the knowledge graph store.</p> <p>This command group consist of commands to administrate the knowledge graph store as a whole.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-showcase","title":"admin store showcase","text":"<p>Create showcase data.</p> Usage<pre><code>$ cmemc admin store showcase [OPTIONS]\n</code></pre> <p>This command creates a showcase scenario of multiple graphs including integration graphs, shapes, statement annotations, etc.</p> <p>Note</p> <p>There is currently no deletion mechanism for the showcase data and you need to remove the showcase graphs manually (or just remove all graphs).</p> Options <pre><code>--scale INTEGER  The scale factor provides a way to set the target size of\n                 the scenario. A value of 10 results in around 40k triples,\n                 a value of 50 in around 350k triples.  [default: 10]\n\n--create         Delete old showcase data if present and create new showcase\n                 databased on the given scale factor.\n\n--delete         Delete existing showcase data if present.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-bootstrap","title":"admin store bootstrap","text":"<p>Update/Import bootstrap data.</p> Usage<pre><code>$ cmemc admin store bootstrap [OPTIONS]\n</code></pre> <p>This command imports the bootstrap data needed for managing shapes and configuration objects.</p> <p>Note</p> <p>The command will first remove all existing bootstrap data (identified with the isSystemResource flag) and will then import the new data to the corresponding graphs (shape catalog, vocabulary catalog, configuration graph).</p> Options <pre><code>--import    Delete existing bootstrap data if present and import bootstrap\n            data which was delivered\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-export","title":"admin store export","text":"<p>Backup all knowledge graphs to a ZIP archive.</p> Usage<pre><code>$ cmemc admin store export [OPTIONS] BACKUP_FILE\n</code></pre> <p>The backup file is a ZIP archive containing all knowledge graphs (one Turtle file + configuration file per graph).</p> <p>This command will create lots of load on the server. It can take a long time to complete.</p> Options <pre><code>--overwrite  Overwrite existing files. This is a dangerous option, so use it\n             with care.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/store/#admin-store-import","title":"admin store import","text":"<p>Restore graphs from a ZIP archive.</p> Usage<pre><code>$ cmemc admin store import BACKUP_FILE\n</code></pre> <p>The backup file is a ZIP archive containing all knowledge graphs  (one Turtle file + configuration file per graph).</p> <p>The command will load a single backup ZIP archive into the triple store by replacing all graphs with the content of the Turtle files in the archive and deleting all graphs which are not in the archive.</p> <p>This command will create lots of load on the server. It can take a long time to complete. The backup file will be transferred to the server, then unzipped and imported graph by graph. After the initial transfer the network connection is not used anymore and may be closed by proxies. This does not mean that the import failed.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/","title":"admin user Command Group","text":"<p>List, create, delete and modify user accounts.</p> <p>This command group is an opinionated interface to the Keycloak realm of your Corporate Memory instance. In order to be able to manage user data, the configured cmemc connection account needs to be equipped with the <code>manage-users</code> role in the used realm.</p> <p>User accounts are identified by a username which unique in the scope of the used realm.</p> <p>In case your Corporate Memory deployment does not use the default deployment layout, the following additional config variables can be used in your connection configuration: <code>KEYCLOAK_BASE_URI</code> defaults to <code>/auth</code> on <code>CMEM_BASE_URI</code> and locates your Keycloak deployment; <code>KEYCLOAK_REALM_ID</code> defaults to <code>cmem</code> and identifies the used realm.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-list","title":"admin user list","text":"<p>List user accounts.</p> Usage<pre><code>$ cmemc admin user list [OPTIONS]\n</code></pre> <p>Outputs a list of user accounts, which can be used to get an overview as well as a reference for the other commands of the <code>admin user</code> command group.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only username. This is useful for piping the IDs into\n            other commands.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-create","title":"admin user create","text":"<p>Create a user account.</p> Usage<pre><code>$ cmemc admin user create USERNAME\n</code></pre> <p>This command creates a new user account.</p> <p>Note</p> <p>The created user account has no metadata such as personal data or group assignments. In order to add these details to a user account, use the <code>admin user update</code> command.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-update","title":"admin user update","text":"<p>Update a user account.</p> Usage<pre><code>$ cmemc admin user update [OPTIONS] USERNAME\n</code></pre> <p>This command updates metadata and group assignments of a user account.</p> <p>For each data value, a separate option needs to be used. All options can be combined in a single execution.</p> <p>Note</p> <p>In order to assign a group to a user account, the group need to be added or imported to the realm upfront.</p> Options <pre><code>--first-name TEXT      Set a new first name.\n--last-name TEXT       Set a new last name.\n--email TEXT           Set a new email.\n--assign-group TEXT    Assign a group.\n--unassign-group TEXT  Unassign a group.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-delete","title":"admin user delete","text":"<p>Delete a user account.</p> Usage<pre><code>$ cmemc admin user delete USERNAME\n</code></pre> <p>This command deletes a user account from a realm.</p> <p>Note</p> <p>The deletion of a user account does not delete the assigned groups of this account, only the assignments to these groups.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-password","title":"admin user password","text":"<p>Change the password of a user account.</p> Usage<pre><code>$ cmemc admin user password [OPTIONS] USERNAME\n</code></pre> <p>With this command, the password of a user account can be changed. The default execution mode of this command is an interactive prompt which asks for the password (twice). In order automate password changes, you can use the <code>--value</code> option.</p> <p>Warning</p> <p>Providing passwords on the command line can be dangerous (e.g. due to a potential exploitation in the shell history). A suggested more save way for automation is to provide the password in a variable first (e.g. with <code>NEW_PASS=$(pwgen -1 40)</code>) and use it afterwards in the cmemc call: <code>cmemc admin user password max --value ${NEW_PASS}</code>.</p> Options <pre><code>--value TEXT  With this option, the new password can be set in a non-\n              interactive way.\n\n--temporary   If enabled, the user must change the password on next login.\n</code></pre>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/user/#admin-user-open","title":"admin user open","text":"<p>Open user in the browser.</p> Usage<pre><code>$ cmemc admin user open [USERNAMES]...\n</code></pre> <p>With this command, you can open a user in the keycloak console in your browser to change them.</p> <p>The command accepts multiple usernames which results in opening multiple browser tabs.</p>","tags":["Keycloak","Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/","title":"admin workspace Command Group","text":"<p>Import, export and reload the project workspace.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/#admin-workspace-export","title":"admin workspace export","text":"<p>Export the complete workspace (all projects) to a ZIP file.</p> Usage<pre><code>$ cmemc admin workspace export [OPTIONS] [FILE]\n</code></pre> <p>Depending on the requested export type, this ZIP file contains either one Turtle file per project (type <code>rdfTurtle</code>) or a substructure of resource files and XML descriptions (type <code>xmlZip</code>).</p> <p>The file name is optional and will be generated with by the template if absent.</p> Options <pre><code>-o, --overwrite               Overwrite existing files. This is a dangerous\n                              option, so use it with care.\n\n--type TEXT                   Type of the exported workspace file.\n                              [default: xmlZip]\n\n-t, --filename-template TEXT  Template for the export file name. Possible\n                              placeholders are (Jinja2): {{connection}}\n                              (from the --connection option) and {{date}}\n                              (the current date as YYYY-MM-DD). The file\n                              suffix will be appended. Needed directories\n                              will be created.  [default:\n                              {{date}}-{{connection}}.workspace]\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/#admin-workspace-import","title":"admin workspace import","text":"<p>Import the workspace from a file.</p> Usage<pre><code>$ cmemc admin workspace import [OPTIONS] FILE\n</code></pre> Options <pre><code>--type TEXT  Type of the exported workspace file.  [default: xmlZip]\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/#admin-workspace-reload","title":"admin workspace reload","text":"<p>Reload the workspace from the backend.</p> Usage<pre><code>$ cmemc admin workspace reload\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/","title":"admin workspace python Command Group","text":"<p>List, install, or uninstall python packages.</p> <p>Python packages are used to extend the DataIntegration workspace with python plugins. To get a list of installed packages, execute the list command.</p> <p>Warning</p> <p>Installing packages from unknown sources is not recommended. Plugins are not verified for malicious code.</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-install","title":"admin workspace python install","text":"<p>Install a python package to the workspace.</p> Usage<pre><code>$ cmemc admin workspace python install PACKAGE\n</code></pre> <p>This command is essentially a <code>pip install</code> in the remote python environment.</p> <p>You can install a package by uploading a source distribution .tar.gz file, by uploading a build distribution .whl file, or by specifying a package name, i.e., a pip requirement specifier with a package name available on pypi.org (e.g. <code>requests==2.27.1</code>).</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-uninstall","title":"admin workspace python uninstall","text":"<p>Uninstall a python package from the workspace.</p> Usage<pre><code>$ cmemc admin workspace python uninstall PACKAGE_NAME\n</code></pre> <p>This command is essentially a <code>pip uninstall</code> in the remote python environment.</p>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-list","title":"admin workspace python list","text":"<p>List installed python packages.</p> Usage<pre><code>$ cmemc admin workspace python list [OPTIONS]\n</code></pre> <p>This command is essentially a <code>pip list</code> in the remote python environment.</p> <p>It outputs a table of python package identifiers with version information.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only package identifier. This is useful for piping the IDs\n            into other commands.\n</code></pre>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/admin/workspace/python/#admin-workspace-python-list-plugins","title":"admin workspace python list-plugins","text":"<p>List installed workspace plugins.</p> Usage<pre><code>$ cmemc admin workspace python list-plugins [OPTIONS]\n</code></pre> <p>This commands lists all discovered plugins.</p> <p>Note</p> <p>The plugin discovery is restricted to package prefix (<code>cmem-</code>).</p> Options <pre><code>--raw              Outputs raw JSON.\n--id-only          Lists only plugin identifier.\n--package-id-only  Lists only plugin package identifier.\n</code></pre>","tags":["Python","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/","title":"config Command Group","text":"<pre><code>List and edit configs as well as get config values.\n\nConfigurations are identified by the section identifier in the\nconfig file. Each configuration represent a Corporate Memory deployment\nwith its specific access method as well as credentials.\n\nA minimal configuration which uses client credentials has the following\nentries:\n\n\b\n[example.org]\nCMEM_BASE_URI=https://cmem.example.org/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=my-secret-account-pass\n\nNote that OAUTH_GRANT_TYPE can be either client_credentials, password or\nprefetched_token.\n\nIn addition to that, the following config parameters can be used as well:\n\n\b\nSSL_VERIFY=False    - for ignoring certificate issues (not recommended)\nDP_API_ENDPOINT=URL - to point to a non-standard DataPlatform location\nDI_API_ENDPOINT=URL - to point to a non-standard DataIntegration location\nOAUTH_TOKEN_URI=URL - to point to an external IdentityProvider location\nOAUTH_USER=username - only if OAUTH_GRANT_TYPE=password\nOAUTH_PASSWORD=password - only if OAUTH_GRANT_TYPE=password\nOAUTH_ACCESS_TOKEN=token - only if OAUTH_GRANT_TYPE=prefetched_token\n\nIn order to get credential information from an external process, you can\nuse the parameter OAUTH_PASSWORD_PROCESS, OAUTH_CLIENT_SECRET_PROCESS and\nOAUTH_ACCESS_TOKEN_PROCESS to setup an external executable.\n\n\b\nOAUTH_CLIENT_SECRET_PROCESS=/path/to/getpass.sh\nOAUTH_PASSWORD_PROCESS=[\"getpass.sh\", \"parameter1\", \"parameter2\"]\n\nThe credential executable can use the cmemc environment for fetching the\ncredential (e.g. CMEM_BASE_URI and OAUTH_USER).\nIf the credential executable is not given with a full path, cmemc\nwill look into your environment PATH for something which can be executed.\nThe configured process needs to return the credential on the first line\nof stdout. In addition to that, the process needs to exit with exit\ncode 0 (without failure). There are examples available in the online\nmanual.\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-list","title":"config list","text":"<p>List configured connections.</p> Usage<pre><code>$ cmemc config list\n</code></pre> <p>This command lists all configured connections from the currently used config file.</p> <p>The connection identifier can be used with the <code>--connection</code> option in order to use a specific Corporate Memory instance.</p> <p>In order to apply commands on more than one instance, you need to use typical unix gear such as xargs or parallel.</p> Example<pre><code>$ cmemc config list | xargs -I % sh -c 'cmemc -c % admin status'\n</code></pre> Example<pre><code>$ cmemc config list | parallel --jobs 5 cmemc -c {} admin status\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-edit","title":"config edit","text":"<p>Edit the user-scope configuration file.</p> Usage<pre><code>$ cmemc config edit\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-get","title":"config get","text":"<p>Get the value of a known cmemc configuration key.</p> Usage<pre><code>$ cmemc config get [CMEM_BASE_URI|SSL_VERIFY|REQUESTS_CA_BUNDLE|DP_API_END\n             POINT|DI_API_ENDPOINT|KEYCLOAK_BASE_URI|KEYCLOAK_REALM_ID|OAUTH_T\n             OKEN_URI|OAUTH_GRANT_TYPE|OAUTH_USER|OAUTH_PASSWORD|OAUTH_CLIENT_\n             ID|OAUTH_CLIENT_SECRET|OAUTH_ACCESS_TOKEN]\n</code></pre> <p>In order to automate processes such as fetching custom API data from multiple Corporate Memory instances, this command provides a way to get the value of a cmemc configuration key for the selected deployment.</p> Example<pre><code>$ curl -H \"Authorization: Bearer $(cmemc -c my admin token)\" $(cmemc -c my config get DP_API_ENDPOINT)/api/custom/slug\n</code></pre> <p>The commands return with exit code 1 if the config key is not used in the current configuration.</p>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/config/#config-eval","title":"config eval","text":"<p>Export all configuration values of a configuration for evaluation.</p> Usage<pre><code>$ cmemc config eval [OPTIONS]\n</code></pre> <p>The output of this command is suitable to be used by a shell\u2019s <code>eval</code> command. It will output the complete configuration as <code>export key=\"value\"</code> statements, which allow for the preparation of a shell environment.</p> Example<pre><code>$ eval $(cmemc -c my config eval)\n</code></pre> <p>Warning</p> <p>Please be aware that credential details are shown in cleartext with this command.</p> Options <pre><code>--unset     Instead of exporting all configuration keys, this option will\n            unset all keys.\n</code></pre>","tags":["Configuration","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/","title":"dataset Command Group","text":"<p>List, create, delete, inspect, up-/download or open datasets.</p> <p>This command group allows for managing workspace datasets as well as dataset file resources. Datasets can be created and deleted. File resources can be uploaded and downloaded. Details of dataset parameters can be listed with inspect.</p> <p>Datasets are identified by a combined key of the <code>PROJECT_ID</code> and a <code>DATASET_ID</code> (e.g: <code>my-project:my-dataset</code>).</p> <p>Note</p> <p>To get a list of existing datasets, execute the <code>dataset list</code> command or use tab-completion.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-list","title":"dataset list","text":"<p>List available datasets.</p> Usage<pre><code>$ cmemc dataset list [OPTIONS]\n</code></pre> <p>Output and filter a list of available datasets. Each dataset is listed with its ID, type and label.</p> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  Filter datasets based on metadata. First parameter\n                         can be one of the following values: project, regex,\n                         tag, type. The options for the second parameter\n                         depend on the first parameter.\n\n--raw                    Outputs raw JSON objects of the dataset search API\n                         response.\n\n--id-only                Lists only dataset IDs and no labels or other\n                         metadata. This is useful for piping the IDs into\n                         other cmemc commands.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-delete","title":"dataset delete","text":"<p>Delete datasets.</p> Usage<pre><code>$ cmemc dataset delete [OPTIONS] [DATASET_IDS]...\n</code></pre> <p>This command deletes existing datasets in integration projects from Corporate Memory. The corresponding dataset resources will not be deleted.</p> <p>Warning</p> <p>Datasets will be deleted without prompting.</p> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Options <pre><code>-a, --all                Delete all datasets. This is a dangerous option, so\n                         use it with care.\n\n--project TEXT           In combination with the '--all' flag, this option\n                         allows for deletion of all datasets of a certain\n                         project. The behaviour is similar to the 'dataset\n                         list --project' command.\n\n--filter &lt;TEXT TEXT&gt;...  Delete datasets based on metadata. First parameter\n                         --filter CHOICE can be one of ['project', 'regex',\n                         'tag', 'type']. The second parameter is based on\n                         CHOICE.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-download","title":"dataset download","text":"<p>Download the resource file of a dataset.</p> Usage<pre><code>$ cmemc dataset download [OPTIONS] DATASET_ID OUTPUT_PATH\n</code></pre> <p>This command downloads the file resource of a dataset to your local file system or to standard out (<code>-</code>). Note that this is not possible for dataset types such as Knowledge Graph (<code>eccencaDataplatform</code>) or SQL endpoint (<code>sqlEndpoint</code>).</p> <p>Without providing an output path, the output file name will be the same as the remote file resource.</p> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Options <pre><code>--replace   Replace existing files. This is a dangerous option, so use it\n            with care!\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-upload","title":"dataset upload","text":"<p>Upload a resource file to a dataset.</p> Usage<pre><code>$ cmemc dataset upload DATASET_ID INPUT_PATH\n</code></pre> <p>This command uploads a file to a dataset. The content of the uploaded file replaces the remote file resource. The name of the remote file resource will not be changed.</p> <p>Warning</p> <p>If the remote file resource is used in more than one dataset, all of these datasets are affected by this command.</p> <p>Warning</p> <p>The content of the uploaded file is not tested, so uploading a JSON file to an XML dataset will result in errors.</p> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Example<pre><code>$ cmemc dataset upload cmem:my-dataset new-file.csv\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-inspect","title":"dataset inspect","text":"<p>Display metadata of a dataset.</p> Usage<pre><code>$ cmemc dataset inspect [OPTIONS] DATASET_ID\n</code></pre> <p>Note</p> <p>Datasets can be listed by using the <code>dataset list</code> command.</p> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-create","title":"dataset create","text":"<p>Create a dataset.</p> Usage<pre><code>$ cmemc dataset create [OPTIONS] [DATASET_FILE]\n</code></pre> <p>Datasets are created in projects and can have associated file resources. Each dataset has a type (such as <code>csv</code>) and a list of parameters which can alter or specify the dataset behaviour.</p> <p>To get more information about available dataset types and associated parameters, use the <code>--help-types</code> and <code>--help-parameter</code> options.</p> Example<pre><code>$ cmemc dataset create --project my-project --type csv my-file.csv\n</code></pre> Options <pre><code>-t, --type TEXT                 The dataset type of the dataset to create.\n                                Example types are 'csv','json' and\n                                'eccencaDataPlatform' (-&gt; Knowledge Graph).\n\n--project TEXT                  The project, where you want to create the\n                                dataset in. If there is only one project in\n                                the workspace, this option can be omitted.\n\n-p, --parameter &lt;TEXT TEXT&gt;...  A set of key/value pairs. Each dataset type\n                                has different parameters (such as charset,\n                                arraySeparator, ignoreBadLines, ...). In\n                                order to get a list of possible parameter,\n                                use the'--help-parameter' option.\n\n--replace                       Replace remote file resources in case there\n                                already exists a file with the same name.\n\n--id TEXT                       The dataset ID of the dataset to create. The\n                                dataset ID will be automatically created in\n                                case it is not present.\n\n--help-types                    Lists all possible dataset types on given\n                                Corporate Memory instance. Note that this\n                                option already needs access to the instance.\n\n--help-parameter                Lists all possible (optional and mandatory)\n                                parameter for a dataset type. Note that this\n                                option already needs access to the instance.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/#dataset-open","title":"dataset open","text":"<p>Open datasets in the browser.</p> Usage<pre><code>$ cmemc dataset open DATASET_IDS...\n</code></pre> <p>With this command, you can open a dataset in the workspace in your browser.</p> <p>The command accepts multiple dataset IDs which results in opening multiple browser tabs.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/","title":"dataset resource Command Group","text":"<p>List, inspect or delete dataset file resources.</p> <p>File resources are identified by their paths and project IDs.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-list","title":"dataset resource list","text":"<p>List available file resources.</p> Usage<pre><code>$ cmemc dataset resource list [OPTIONS]\n</code></pre> <p>Outputs a table or a list of dataset resources (files).</p> Options <pre><code>--raw                    Outputs raw JSON.\n--id-only                Lists only resource names and no other metadata.\n                         This is useful for piping the IDs into other\n                         commands.\n\n--filter &lt;TEXT TEXT&gt;...  Filter file resources based on metadata. First\n                         parameter CHOICE can be one of ['project',\n                         'regex']. The second parameter is based on CHOICE,\n                         e.g. a project ID or a regular expression string.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-delete","title":"dataset resource delete","text":"<p>Delete file resources.</p> Usage<pre><code>$ cmemc dataset resource delete [OPTIONS] [RESOURCE_IDS]...\n</code></pre> <p>There are three selection mechanisms: with specific IDs, only those specified resources will be deleted; by using <code>--filter</code>, resources based on the filter type and value will be deleted; using <code>--all</code> will delete all resources.</p> Options <pre><code>--force                  Delete resource even if in use by a task.\n-a, --all                Delete all resources. This is a dangerous option,\n                         so use it with care.\n\n--filter &lt;TEXT TEXT&gt;...  Filter file resources based on metadata. First\n                         parameter CHOICE can be one of ['project',\n                         'regex']. The second parameter is based on CHOICE,\n                         e.g. a project ID or a regular expression string.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-inspect","title":"dataset resource inspect","text":"<p>Display all metadata of a file resource.</p> Usage<pre><code>$ cmemc dataset resource inspect [OPTIONS] RESOURCE_ID\n</code></pre> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/dataset/resource/#dataset-resource-usage","title":"dataset resource usage","text":"<p>Display all usage data of a file resource.</p> Usage<pre><code>$ cmemc dataset resource usage [OPTIONS] RESOURCE_ID\n</code></pre> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/","title":"graph Command Group","text":"<p>List, import, export, delete, count, tree or open graphs.</p> <p>Graphs are identified by an IRI.</p> <p>Note</p> <p>The get a list of existing graphs, execute the <code>graph list</code> command or use tab-completion.</p>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-count","title":"graph count","text":"<p>Count triples in graph(s).</p> Usage<pre><code>$ cmemc graph count [OPTIONS] [IRIS]...\n</code></pre> <p>This command lists graphs with their triple count. Counts do not inlude imported graphs.</p> Options <pre><code>-a, --all        Count all graphs\n-s, --summarize  Display only a sum of all counted graphs together\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-tree","title":"graph tree","text":"<p>Show graph tree(s) of the owl:imports hierarchy.</p> Usage<pre><code>$ cmemc graph tree [OPTIONS] [IRIS]...\n</code></pre> <p>You can output one or more trees of the import hierarchy.</p> <p>Imported graphs which do not exist are shown as <code>[missing: IRI]</code>. Imported graphs which will result in an import cycle are shown as <code>[ignored: IRI]</code>. Each graph is shown with label and IRI.</p> Options <pre><code>-a, --all   Show tree of all (readable) graphs.\n--raw       Outputs raw JSON of the graph importTree API response.\n--id-only   Lists only graph identifier (IRIs) and no labels or other\n            metadata. This is useful for piping the IRIs into other\n            commands. The output with this option is a sorted, flat, de-\n            duplicated list of existing graphs.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-list","title":"graph list","text":"<p>List accessible graphs.</p> Usage<pre><code>$ cmemc graph list [OPTIONS]\n</code></pre> Options <pre><code>--raw                      Outputs raw JSON of the graphs list API response.\n--id-only                  Lists only graph identifier (IRIs) and no labels\n                           or other metadata. This is useful for piping the\n                           IRIs into other commands.\n\n--filter &lt;CHOICE TEXT&gt;...  Filter graphs based on effective access\n                           conditions or import closure. First parameter\n                           CHOICE can be 'access' or 'imported-by'. The\n                           second parameter can be 'readonly' or 'writeable'\n                           in case of 'access' or any readable graph in case\n                           of 'imported-by'.\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-export","title":"graph export","text":"<p>Export graph(s) as NTriples to stdout (-), file or directory.</p> Usage<pre><code>$ cmemc graph export [OPTIONS] [IRIS]...\n</code></pre> <p>In case of file export, data from all selected graphs will be concatenated in one file. In case of directory export, .graph and .ttl files will be created for each graph.</p> Options <pre><code>-a, --all                       Export all readable graphs.\n--include-imports               Export selected graph(s) and all graphs\n                                which are imported from these selected\n                                graph(s).\n\n--create-catalog                In addition to the .ttl and .graph files,\n                                cmemc will create an XML catalog file\n                                (catalog-v001.xml) which can be used by\n                                applications such as Prot\u00e9g\u00e9.\n\n--output-dir DIRECTORY          Export to this directory.\n--output-file FILE              Export to this file.  [default: -]\n-t, --filename-template TEXT    Template for the export file name(s). Used\n                                together with --output-dir. Possible\n                                placeholders are (Jinja2): {{hash}} - sha256\n                                hash of the graph IRI, {{iriname}} - graph\n                                IRI converted to filename, {{connection}} -\n                                from the --connection option and {{date}} -\n                                the current date as YYYY-MM-DD. The file\n                                suffix will be appended. Needed directories\n                                will be created.  [default: {{hash}}]\n\n--mime-type [application/n-triples|text/turtle]\n                                Define the requested mime type  [default:\n                                application/n-triples]\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-delete","title":"graph delete","text":"<p>Delete graph(s) from the store.</p> Usage<pre><code>$ cmemc graph delete [OPTIONS] [IRIS]...\n</code></pre> Options <pre><code>-a, --all          Delete all writeable graphs.\n--include-imports  Delete selected graph(s) and all writeable graphs which\n                   are imported from these selected graph(s).\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-import","title":"graph import","text":"<p>Import graph(s) to the store.</p> Usage<pre><code>$ cmemc graph import [OPTIONS] INPUT_PATH [IRI]\n</code></pre> <p>If input is a directory, it scans for file-pairs such as <code>xyz.ttl</code> and <code>xyz.ttl.graph</code> where <code>xyz.ttl</code> is the actual triples file and <code>xyz.ttl.graph</code> contains the graph IRI as one string: <code>https://mygraph.de/xyz/</code>.</p> <p>If input is a file, content will be uploaded to IRI. If <code>--replace</code> is set, the data will be overwritten, if not, it will be added.</p> Options <pre><code>--replace        Replace / overwrite the graph - instead of just adding new\n                 triples the graph.\n\n--skip-existing  Skip importing a file if the target graph already exists in\n                 the store. Note that the graph list is fetched once at the\n                 beginning of the process, so that you can still add\n                 multiple files to one single graph (if it does not exist).\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/graph/#graph-open","title":"graph open","text":"<p>Open / explore a graph in the browser.</p> Usage<pre><code>$ cmemc graph open IRI\n</code></pre>","tags":["KnowledgeGraph","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/","title":"project Command Group","text":"<p>List, import, export, create, delete or open projects.</p> <p>Projects are identified by a <code>PROJECT_ID</code>.</p> <p>Note</p> <p>To get a list of existing projects, execute the <code>project list</code> command or use tab-completion.</p>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-open","title":"project open","text":"<p>Open projects in the browser.</p> Usage<pre><code>$ cmemc project open PROJECT_IDS...\n</code></pre> <p>With this command, you can open a project in the workspace in your browser to change them.</p> <p>The command accepts multiple project IDs which results in opening multiple browser tabs.</p>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-list","title":"project list","text":"<p>List available projects.</p> Usage<pre><code>$ cmemc project list [OPTIONS]\n</code></pre> <p>Outputs a list of project IDs which can be used as reference for the project create, delete, export and import commands.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only project identifier and no labels or other metadata.\n            This is useful for piping the IDs into other commands.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-export","title":"project export","text":"<p>Export projects to files.</p> Usage<pre><code>$ cmemc project export [OPTIONS] [PROJECT_IDS]...\n</code></pre> <p>Projects can be exported with different export formats. The default type is a zip archive which includes metadata as well as dataset resources. If more than one project is exported, a file is created for each project. By default, these files are created in the current directory with a descriptive name (see <code>--template</code> option default).</p> <p>Note</p> <p>Projects can be listed by using the <code>project list</code> command.</p> <p>You can use the template string to create subdirectories.</p> Example<pre><code>$ cmemc config list | parallel -I% cmemc -c % project export --all -t \"dump/{{connection}}/{{date}}-{{id}}.project\"\n</code></pre> Options <pre><code>-a, --all                     Export all projects.\n-o, --overwrite               Overwrite existing files. This is a dangerous\n                              option, so use it with care.\n\n--output-dir DIRECTORY        The base directory, where the project files\n                              will be created. If this directory does not\n                              exist, it will be silently created.  [default:\n                              .]\n\n--type TEXT                   Type of the exported project file(s). Use the\n                              --help-types option or tab completion to see a\n                              list of possible types.  [default: xmlZip]\n\n-t, --filename-template TEXT  Template for the export file name(s). Possible\n                              placeholders are (Jinja2): {{id}} (the project\n                              ID), {{connection}} (from the --connection\n                              option) and {{date}} (the current date as\n                              YYYY-MM-DD). The file suffix will be appended.\n                              Needed directories will be created.  [default:\n                              {{date}}-{{connection}}-{{id}}.project]\n\n--extract                     Export projects to a directory structure\n                              instead of a ZIP archive. Note that the\n                              --filename-template option is ignored here.\n                              Instead, a sub-directory per exported project\n                              is created under the output directory. Also\n                              note that not all export types are\n                              extractable.\n\n--help-types                  Lists all possible export types.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-import","title":"project import","text":"<p>Import a project from a file or directory.</p> Usage<pre><code>$ cmemc project import [OPTIONS] PATH [PROJECT_ID]\n</code></pre> Example<pre><code>$ cmemc project import my_project.zip my_project\n</code></pre> Options <pre><code>-o, --overwrite  Overwrite an existing project. This is a dangerous option,\n                 so use it with care.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-delete","title":"project delete","text":"<p>Delete projects.</p> Usage<pre><code>$ cmemc project delete [OPTIONS] [PROJECT_IDS]...\n</code></pre> <p>This command deletes existing data integration projects from Corporate Memory.</p> <p>Warning</p> <p>Projects will be deleted without prompting!</p> <p>Note</p> <p>Projects can be listed with the <code>project list</code> command.</p> Options <pre><code>-a, --all   Delete all projects. This is a dangerous option, so use it with\n            care.\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-create","title":"project create","text":"<p>Create projects.</p> Usage<pre><code>$ cmemc project create PROJECT_IDS...\n</code></pre> <p>This command creates one or more new projects. Existing projects will not be overwritten.</p> <p>Note</p> <p>Projects can be listed by using the <code>project list</code> command.</p>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/project/#project-reload","title":"project reload","text":"<p>Reload projects from the workspace provider.</p> Usage<pre><code>$ cmemc project reload [OPTIONS] [PROJECT_IDS]...\n</code></pre> <p>This command reloads all tasks of a project from the workspace provider. This is similar to the <code>workspace reload</code> command, but for a single project only.</p> <p>Note</p> <p>You need this in case you changed project data externally or loaded a project which uses plugins which are not installed yet. In this case, install the plugin(s) and reload the project afterwards.</p> <p>Warning</p> <p>Depending on the size your datasets esp. your Knowledge Graphs, reloading a project can take a long time to re-create the path caches.</p> Options <pre><code>-a, --all   Reload all projects\n</code></pre>","tags":["Project","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/","title":"query Command Group","text":"<p>List, execute, get status or open SPARQL queries.</p> <p>Queries are identified either by a file path, a URI from the query catalog or a shortened URI (qname, using a default namespace).</p> <p>One or more queries can be executed one after the other with the execute command. With open command you can jump to the query editor in your browser.</p> <p>Queries can use a mustache like syntax to specify placeholder for parameter values (e.g. <code>{{resourceUri}}</code>). These parameter values need to be given as well, before the query can be executed (use the<code>-p</code> option).</p> <p>Note</p> <p>In order to get a list of queries from the query catalog, execute the <code>query list</code> command or use tab-completion.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-execute","title":"query execute","text":"<p>Execute queries which are loaded from files or the query catalog.</p> Usage<pre><code>$ cmemc query execute [OPTIONS] QUERIES...\n</code></pre> <p>Queries are identified either by a file path, a URI from the query catalog, or a shortened URI (qname, using a default namespace).</p> <p>If multiple queries are executed one after the other, the first failing query stops the whole execution chain.</p> <p>Limitations: All optional parameters (e.g. accept, base64, \u2026) are provided for ALL queries in an execution chain. If you need different parameters for each query in a chain, run cmemc multiple times and use the logical operators &amp;&amp; and || of your shell instead.</p> Options <pre><code>--accept TEXT                   Accept header for the HTTP request(s).\n                                Setting this to 'default' means that cmemc\n                                uses an appropriate accept header for\n                                terminal output (text/csv for tables,\n                                text/turtle for graphs, * otherwise). Please\n                                refer to the Corporate Memory system manual\n                                for a list of accepted mime types.\n                                [default: default]\n\n--no-imports                    Graphs which include other graphs (using\n                                owl:imports) will be queried as merged\n                                overall-graph. This flag disables this\n                                default behaviour. The flag has no effect on\n                                update queries.\n\n--base64                        Enables base64 encoding of the query\n                                parameter for the SPARQL requests (the\n                                response is not touched). This can be useful\n                                in case there is an aggressive firewall\n                                between cmemc and Corporate Memory.\n\n-p, --parameter &lt;TEXT TEXT&gt;...  In case of a parameterized query\n                                (placeholders with the '{{key}}' syntax),\n                                this option fills all placeholder with a\n                                given value before the query is\n                                executed.Pairs of placeholder/value need to\n                                be given as a tuple 'KEY VALUE'. A key can\n                                be used only once.\n\n--limit INTEGER                 Override or set the LIMIT in the executed\n                                SELECT query. Note that this option will\n                                never give you more results than the LIMIT\n                                given in the query itself.\n\n--offset INTEGER                Override or set the OFFSET in the executed\n                                SELECT query.\n\n--distinct                      Override the SELECT query by make the result\n                                set DISTINCT.\n\n--timeout INTEGER               Set max execution time for query evaluation\n                                (in milliseconds).\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-list","title":"query list","text":"<p>List available queries from the catalog.</p> Usage<pre><code>$ cmemc query list [OPTIONS]\n</code></pre> <p>Outputs a list of query URIs which can be used as reference for the query execute command.</p> Options <pre><code>--id-only   Lists only query identifier and no labels or other metadata.\n            This is useful for piping the ids into other cmemc commands.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-open","title":"query open","text":"<p>Open queries in the editor of the query catalog in your browser.</p> Usage<pre><code>$ cmemc query open QUERIES...\n</code></pre> <p>With this command, you can open (remote) queries from the query catalog in the query editor in your browser (e.g. in order to change them). You can also load local query files into the query editor, in order to import them into the query catalog.</p> <p>The command accepts multiple query URIs or files which results in opening multiple browser tabs.</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-status","title":"query status","text":"<p>Get status information of executed and running queries.</p> Usage<pre><code>$ cmemc query status [OPTIONS] [QUERY_UUID]\n</code></pre> <p>With this command, you can access the latest executed SPARQL queries on the DataPlatform. These queries are identified by UUIDs and listed ordered by starting timestamp.</p> <p>You can filter queries based on status and runtime in order to investigate slow queries. In addition to that, you can get the details of a specific query by using the ID as a parameter.</p> Options <pre><code>--id-only                Lists only query identifier and no labels or other\n                         metadata. This is useful for piping the ids into\n                         other cmemc commands.\n\n--raw                    Outputs raw JSON response of the query status API.\n--filter &lt;TEXT TEXT&gt;...  Filter queries based on execution status and time.\n                         First parameter --filter CHOICE can be one of\n                         ['graph', 'regex', 'slower-than', 'status', 'trace-\n                         id', 'type', 'user']. The second parameter is based\n                         on CHOICE, e.g. int in case of slower-than, or a\n                         regular expression string.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-replay","title":"query replay","text":"<p>Re-execute queries from a replay file.</p> Usage<pre><code>$ cmemc query replay [OPTIONS] REPLAY_FILE\n</code></pre> <p>This command reads a <code>REPLAY_FILE</code> and re-executes the logged queries. A <code>REPLAY_FILE</code> is a JSON document which is an array of JSON objects with at least a key <code>queryString</code> holding the query text OR a key <code>iri</code> holding the IRI of the query in the query catalog. It can be created with the <code>query status</code> command.</p> Example<pre><code>$ query status --raw &gt; replay.json\n</code></pre> <p>The output of this command shows basic query execution statistics.</p> <p>The queries are executed one after another in the order given in the input <code>REPLAY_FILE</code>. Query placeholders / parameters are ignored. If a query results in an error, the duration is not counted.</p> <p>The optional output file is the same JSON document which is used as input, but each query object is annotated with an additional <code>replays</code> object, which is an array of JSON objects which hold values for the replay|loop|run IDs, start and end time as well as duration and other data.</p> Options <pre><code>--raw               Output the execution statistic as raw JSON.\n--loops INTEGER     Number of loops to run the replay file.  [default: 1]\n--wait INTEGER      Number of seconds to wait between query executions.\n                    [default: 0]\n\n--output-file FILE  Save the optional output to this file. Input and output\n                    of the command can be the same file. The output is\n                    written at the end of a successful command execution.\n                    The output can be stdout ('-') - in this case, the\n                    execution statistic output is oppressed.\n\n--run-label TEXT    Optional label of this replay run.\n</code></pre>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/query/#query-cancel","title":"query cancel","text":"<p>Cancel a running query.</p> Usage<pre><code>$ cmemc query cancel QUERY_ID\n</code></pre> <p>With this command, you can cancel a running query. Depending on the backend triple store, this will result in a broken result stream (stardog, neptune and virtuoso) or a valid result stream with incomplete results (graphdb)</p>","tags":["SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/","title":"vocabulary Command Group","text":"<p>List, (un-)install, import or open vocabs / manage cache.</p>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-open","title":"vocabulary open","text":"<p>Open / explore a vocabulary graph in the browser.</p> Usage<pre><code>$ cmemc vocabulary open IRI\n</code></pre> <p>Vocabularies are identified by their graph IRI. Installed vocabularies can be listed with the <code>vocabulary list</code> command.</p>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-list","title":"vocabulary list","text":"<p>Output a list of vocabularies.</p> Usage<pre><code>$ cmemc vocabulary list [OPTIONS]\n</code></pre> <p>Vocabularies are graphs (see <code>graph</code> command group) which consists of class and property descriptions.</p> Options <pre><code>--id-only                       Lists only vocabulary identifier (IRIs) and\n                                no labels or other metadata. This is useful\n                                for piping the ids into other cmemc\n                                commands.\n\n--filter [all|installed|installable]\n                                Filter list based on status.  [default:\n                                installed]\n\n--raw                           Outputs raw JSON.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-install","title":"vocabulary install","text":"<p>Install one or more vocabularies from the catalog.</p> Usage<pre><code>$ cmemc vocabulary install [OPTIONS] [IRIS]...\n</code></pre> <p>Vocabularies are identified by their graph IRI. Installable vocabularies can be listed with the vocabulary list command.</p> Options <pre><code>-a, --all   Install all vocabularies from the catalog.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-uninstall","title":"vocabulary uninstall","text":"<p>Uninstall one or more vocabularies.</p> Usage<pre><code>$ cmemc vocabulary uninstall [OPTIONS] [IRIS]...\n</code></pre> <p>Vocabularies are identified by their graph IRI. Already installed vocabularies can be listed with the vocabulary list command.</p> Options <pre><code>-a, --all   Uninstall all installed vocabularies.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/#vocabulary-import","title":"vocabulary import","text":"<p>Import a turtle file as a vocabulary.</p> Usage<pre><code>$ cmemc vocabulary import [OPTIONS] FILE\n</code></pre> <p>With this command, you can import a local ontology file as a named graph and create a corresponding vocabulary catalog entry.</p> <p>The uploaded ontology file is analysed locally in order to discover the named graph and the prefix declaration. This requires an OWL ontology description which correctly uses the <code>vann:preferredNamespacePrefix</code> and <code>vann:preferredNamespaceUri</code> properties.</p> Options <pre><code>--namespace &lt;TEXT TEXT&gt;...  In case the imported vocabulary file does not\n                            include a preferred namespace prefix, you can\n                            manually add a namespace prefix with this\n                            option. Example: --namespace ex\n                            https://example.org/\n\n--replace                   Replace (overwrite) existing vocabulary, if\n                            present.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/cache/","title":"vocabulary cache Command Group","text":"<p>List und update the vocabulary cache.</p>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/cache/#vocabulary-cache-update","title":"vocabulary cache update","text":"<p>Reload / updates the data integration cache for a vocabulary.</p> Usage<pre><code>$ cmemc vocabulary cache update [OPTIONS] [IRIS]...\n</code></pre> Options <pre><code>-a, --all   Update cache for all installed vocabularies.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/vocabulary/cache/#vocabulary-cache-list","title":"vocabulary cache list","text":"<p>Output the content of the global vocabulary cache.</p> Usage<pre><code>$ cmemc vocabulary cache list [OPTIONS]\n</code></pre> Options <pre><code>--id-only   Lists only vocabulary term identifier (IRIs) and no labels or\n            other metadata. This is useful for piping the ids into other\n            cmemc commands.\n\n--raw       Outputs raw JSON.\n</code></pre>","tags":["Vocabulary","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/","title":"workflow Command Group","text":"<p>List, execute, status or open (io) workflows.</p> <p>Workflows are identified by a <code>WORKFLOW_ID</code>. The get a list of existing workflows, execute the list command or use tab-completion. The <code>WORKFLOW_ID</code> is a concatenation of a <code>PROJECT_ID</code> and a <code>TASK_ID</code>, such as <code>my-project:my-workflow</code>.</p>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-execute","title":"workflow execute","text":"<p>Execute workflow(s).</p> Usage<pre><code>$ cmemc workflow execute [OPTIONS] [WORKFLOW_IDS]...\n</code></pre> <p>With this command, you can start one or more workflows at the same time or in a sequence, depending on the result of the predecessor.</p> <p>Executing a workflow can be done in two ways: Without <code>--wait</code> just sends the starting signal and does not look for the workflow and its result (fire and forget). Starting workflows in this way, starts all given workflows at the same time.</p> <p>The optional <code>--wait</code> option starts the workflows in the same way, but also polls the status of a workflow until it is finished. In case of an error of a workflow, the next workflow is not started.</p> Options <pre><code>-a, --all                       Execute all available workflows.\n--wait                          Wait until all executed workflows are\n                                completed.\n\n--polling-interval INTEGER RANGE\n                                How many seconds to wait between status\n                                polls. Status polls are cheap, so a higher\n                                polling interval is most likely not needed.\n                                [default: 1]\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-io","title":"workflow io","text":"<p>Execute a workflow with file input/output.</p> Usage<pre><code>$ cmemc workflow io [OPTIONS] WORKFLOW_ID\n</code></pre> <p>With this command, you can execute a workflow that uses variable datasets as input, output or for configuration. Use the input parameter to feed data into the workflow. Likewise, use output for retrieval of the workflow result. Workflows without a variable dataset will throw an error.</p> Options <pre><code>-i, --input FILE                From which file the input is taken: note\n                                that the maximum file size to upload is\n                                limited to a server configured value. If the\n                                workflow has no defined variable input\n                                dataset, this can be ignored.\n\n-o, --output FILE               To which file the result is written to: use\n                                '-' in order to output the result to stdout.\n                                If the workflow has no defined variable\n                                output dataset, this can be ignored. Please\n                                note that the io command will not warn you\n                                on overwriting existing output files.\n\n--input-mimetype [guess|application/xml|application/json|text/csv]\n                                Which input format should be processed: If\n                                not given, cmemc will try to guess the mime\n                                type based on the file extension or will\n                                fail\n\n--output-mimetype [guess|application/xml|application/json|application/n-triples|application/vnd.openxmlformats-officedocument.spreadsheetml.sheet|text/csv]\n                                Which output format should be requested: If\n                                not given, cmemc will try to guess the mime\n                                type based on the file extension or will\n                                fail. In case of an output to stdout, a\n                                default mime type will be used (currently\n                                xml).\n\n--autoconfig / --no-autoconfig  Setup auto configuration of input datasets,\n                                e.g. in order to process CSV files with\n                                semicolon- instead of comma-separation.\n                                [default: True]\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-list","title":"workflow list","text":"<p>List available workflow.</p> Usage<pre><code>$ cmemc workflow list [OPTIONS]\n</code></pre> Options <pre><code>--filter &lt;TEXT TEXT&gt;...  List workflows based on metadata. First parameter\n                         --filter CHOICE can be one of ['io', 'project',\n                         'regex', 'tag']. The second parameter is based on\n                         CHOICE.\n\n--id-only                Lists only workflow identifier and no labels or\n                         other metadata. This is useful for piping the IDs\n                         into other commands.\n\n--raw                    Outputs raw JSON objects of workflow task search\n                         API response.\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-status","title":"workflow status","text":"<p>Get status information of workflow(s).</p> Usage<pre><code>$ cmemc workflow status [OPTIONS] [WORKFLOW_IDS]...\n</code></pre> Options <pre><code>--project TEXT                  The project, from which you want to list the\n                                workflows. Project IDs can be listed with\n                                the 'project list' command.\n\n--raw                           Output raw JSON info.\n--filter [Idle|Not executed|Finished|Cancelled|Failed|Successful|Canceling|Running|Waiting]\n                                Show only workflows of a specific status.\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/#workflow-open","title":"workflow open","text":"<p>Open a workflow in your browser.</p> Usage<pre><code>$ cmemc workflow open WORKFLOW_ID\n</code></pre>","tags":["Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/","title":"workflow scheduler Command Group","text":"<p>List, inspect, enable/disable or open scheduler.</p> <p>Schedulers execute workflows in specified intervals. They are identified with a <code>SCHEDULER_ID</code>. To get a list of existing schedulers, execute the list command or use tab-completion.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-open","title":"workflow scheduler open","text":"<p>Open scheduler(s) in the browser.</p> Usage<pre><code>$ cmemc workflow scheduler open [OPTIONS] SCHEDULER_IDS...\n</code></pre> <p>With this command, you can open a scheduler in the workspace in your browser to change it.</p> <p>The command accepts multiple scheduler IDs which results in opening multiple browser tabs.</p> Options <pre><code>--workflow  Instead of opening the scheduler page, open the page of the\n            scheduled workflow.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-list","title":"workflow scheduler list","text":"<p>List available scheduler.</p> Usage<pre><code>$ cmemc workflow scheduler list [OPTIONS]\n</code></pre> <p>Outputs a table or a list of scheduler IDs which can be used as reference for the scheduler commands.</p> Options <pre><code>--raw       Outputs raw JSON.\n--id-only   Lists only task identifier and no labels or other metadata. This\n            is useful for piping the IDs into other commands.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-inspect","title":"workflow scheduler inspect","text":"<p>Display all metadata of a scheduler.</p> Usage<pre><code>$ cmemc workflow scheduler inspect [OPTIONS] SCHEDULER_ID\n</code></pre> Options <pre><code>--raw       Outputs raw JSON.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-disable","title":"workflow scheduler disable","text":"<p>Disable scheduler(s).</p> Usage<pre><code>$ cmemc workflow scheduler disable [OPTIONS] [SCHEDULER_IDS]...\n</code></pre> <p>The command accepts multiple scheduler IDs which results in disabling them one after the other.</p> Options <pre><code>-a, --all   Disable all scheduler.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/command-reference/workflow/scheduler/#workflow-scheduler-enable","title":"workflow scheduler enable","text":"<p>Enable scheduler(s).</p> Usage<pre><code>$ cmemc workflow scheduler enable [OPTIONS] [SCHEDULER_IDS]...\n</code></pre> <p>The command accepts multiple scheduler IDs which results in enabling them one after the other.</p> Options <pre><code>-a, --all   Enable all scheduler.\n</code></pre>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/","title":"Configuration","text":"<p>In order to work with cmemc, you have to configure it according to your needs.</p> <ul> <li> <p> File-based Configuration</p> <p>The most common way to configure cmemc is with a central configuration file.</p> </li> <li> <p> Environment-based Configuration</p> <p>In addition to configuration files, cmemc can be widely configured and parameterized with environment variables.</p> </li> <li> <p> Completion Setup</p> <p>Setting up command completion is optional but highly recommended and will greatly speed up your cmemc terminal sessions.</p> </li> <li> <p> Security Considerations</p> <p>cmemc can be configured to fetch your credentials from external processes, such as password stores. In addition to that, cmemc can work with custom certificates.</p> </li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/","title":"Certificate handling and SSL verification","text":"","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/#introduction","title":"Introduction","text":"<p>In a reasonable production deployment, all client-accessible Corporate Memory APIs will be securely available as HTTPS endpoints. This document clarifies how to deal with certificates. cmemc will validate the certificates of your HTTPS endpoints and indicate validation errors. If the certificates of your Corporate Memory deployment are based on a common and publicly available Certificate Authority (such as Let\u2019s Encrypt), cmemc is able to validate your certificates out of the box.</p> <p>However, in some cases you need to do one of the following:</p> <ul> <li>provide your own certificates CA bundle in order to allow cmem to trust your servers</li> <li>disable SSL verification entirely (for debugging and testing purpose only)</li> </ul>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/#provide-your-own-ca-bundle","title":"Provide your own CA bundle","text":"<p>cmemc will validate all used certificates of your HTTPS API endpoints by using a built-in CA bundle that comes from python\u2019s certifi package.</p> <p>Certifi is a carefully curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the identity of TLS hosts.</p> <p>If you need to configure a custom CA bundle to use with a specific connection, you can do so by using the <code>REQUESTS_CA_BUNDLE</code> key in the config or as an environment variable.</p> <p>You can validate which CA bundle is used by switching on debugging (<code>\u2013debug</code>) and watch for a CA bundle debug line (here, line 10).</p> using the debug mode to watch for the CA bundle<pre><code>$ cmemc --debug -c ssltest.eccenca.com graph list\n[2020-03-11 17:50:59.135898] Set config to /home/user/Library/Application Support/cmemc/config.ini\n[2020-03-11 17:50:59.136284] Config loaded: /home/user/Library/Application Support/cmemc/config.ini\n[2020-03-11 17:50:59.137476] Use connection config: ssltest.eccenca.com\n[2020-03-11 17:50:59.137564] CMEM_BASE_URI set by config to https://ssltest.eccenca.com\n[2020-03-11 17:50:59.137611] REQUESTS_CA_BUNDLE set by config to cacert.pem\n[2020-03-11 17:50:59.137718] OAUTH_GRANT_TYPE set by config to client_credentials\n[2020-03-11 17:50:59.137760] OAUTH_CLIENT_ID set by config to cmem-service-account\n[2020-03-11 17:50:59.137804] OAUTH_CLIENT_SECRET set by config\n[2020-03-11 17:50:59.137978] CA bundle loaded from /home/user/cacert.pem\nhttp://di.eccenca.com/project/cmem\nurn:elds-backend-access-conditions-graph\n</code></pre> <p>The CA bundle must be available in PEM format. You can use the openssl command line tool to fetch all certificates from an HTTPS URL and create a PEM CA Bundle out of it.</p> <p>Here is an example line producing the cacert.pem file used in the example above:</p> <pre><code>$ openssl s_client -showcerts -connect ssltest.eccenca.com:443 &lt;/dev/null 2&gt;/dev/null | openssl x509 -outform PEM &gt;cacert.pem\n$ cat cacert.pem\n-----BEGIN CERTIFICATE-----\nMIIFyzCCA7MCFDoiAY9Ry8dfH0rS/rINUb6inlvGMA0GCSqGSIb3DQEBCwUAMIGh\n[...]\nmiGId7jMXd24bpfYZSiniC0+SHiCwEmzN818Ss9aIMChymAnV3RRB/UqKLlOMnA=\n-----END CERTIFICATE-----\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/certificate-handling-and-ssl-verification/#disabling-ssl-verification-at-all","title":"Disabling SSL Verification at all","text":"<p>You can also disable SSL Verification completely by setting the <code>SSL_VERIFY</code> key in the config or environment to <code>false</code>.</p> <p>However, this will lead to warnings: <pre><code>$ cmemc -c ssltest.eccenca.com graph list\nSSL verification is disabled (SSL_VERIFY=False).\nhttp://di.eccenca.com/project/cmem\nurn:elds-backend-access-conditions-graph\n</code></pre></p>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/completion-setup/","title":"Command-Line Completion","text":"","tags":["cmemc","Video"]},{"location":"automate/cmemc-command-line-interface/configuration/completion-setup/#introduction","title":"Introduction","text":"<p>In case you are using bash or zsh as your terminal shell, you should enable Command-line tab completion for cmemc.</p> <p>Tab completion is a powerful feature and will save you a lot of typing work. Furthermore, it will help you to learn the different commands, parameters and options and will auto-complete parameter values taken live from your Corporate Memory instance (such as graph IRIs, project IDs, etc.).</p> <p></p> <p>We suggest using zsh so you can take advantage of its advanced menu-completion feature.</p>","tags":["cmemc","Video"]},{"location":"automate/cmemc-command-line-interface/configuration/completion-setup/#installation","title":"Installation","text":"<p>In order to enable tab completion with zsh run the following command:</p> completion setup for zsh<pre><code>$ eval \"$(_CMEMC_COMPLETE=source_zsh cmemc)\"\n</code></pre> <p>In order to enable tab completion with bash run the following command:</p> completion setup for bash<pre><code>$ eval \"$(_CMEMC_COMPLETE=source cmemc)\"\n</code></pre> <p>You may want to add this line to your <code>.bashrc</code> or <code>.zshrc</code>.</p>","tags":["cmemc","Video"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/","title":"Environment-based Configuration","text":"","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#introduction","title":"Introduction","text":"<p>In addition to using configuration files, cmemc can also be widely configured and parameterized with environment variables.</p> <p>Typical use cases for when you may want to do this include:</p> <ul> <li>set a default connection (see below)</li> <li>enable session-wide debugging output</li> <li>control cmemc with variables from a calling process</li> <li>avoid having client and user credentials lying around in a file</li> </ul> <p>There are two major categories of environment variables you can use.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#environment-variables-for-configuration","title":"Environment variables for configuration","text":"<p>For these variables the rules are simple: You can use any variable from the config file in the same way as an environment variable.</p> <p>The following commands provide the same result as given in the basic example for a config file:</p> <pre><code>$ export CMEM_BASE_URI=http://localhost/\n$ export OAUTH_GRANT_TYPE=client_credentials\n$ export OAUTH_CLIENT_ID=cmem-service-account\n$ export OAUTH_CLIENT_SECRET=...\n</code></pre> <p>Info</p> <p>When you combine file-based and environment-based configuration, the config file always overwrites the environment.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#environment-variables-for-parameters-or-options","title":"Environment variables for parameters or options","text":"<p>The general pattern for parameter and option settings via environment variables is:</p> <ul> <li>all variables start with the prefix <code>CMEMC_</code></li> <li>command group and command follow the prefix in uppercase and separated by <code>_</code></li> <li>the option is in uppercase at the end.</li> <li>The naming scheme is: <code>CMEM[_&lt;COMMAND-GROUP&gt;_&lt;COMMAND&gt;][_&lt;OPTION&gt;]</code></li> </ul> <p>The next sections demonstrate this pattern with examples.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#example-set-a-default-connection","title":"Example: Set a default connection","text":"<p>We first run a cmemc command via command line parameter:</p> <pre><code>$ cmemc --config-file cmemc.ini --connection mycmem graph list --raw\n[\n  {\n    \"iri\": \"urn:elds-backend-access-conditions-graph\",\n... more JSON output ...\n</code></pre> <p>As a next step, we replace all connection parameters with environment variables:</p> <pre><code>$ export CMEMC_CONFIG_FILE=cmemc.ini\n$ export CMEMC_CONNECTION=mycmem\n</code></pre> <p>This alone allows us to save a lot of typing for a series of commands on the same Corporate Memory instance.</p> <pre><code>$ cmemc graph list --raw\n[... same output as above ...]\n</code></pre> <p>However, you can also pre-define command options in the same way:</p> <pre><code>$ export CMEMC_GRAPH_LIST_RAW=true\n</code></pre> <p>Again, the same command but <code>--raw</code> is set per default.</p> <pre><code>$ cmemc graph list\n[... same output as above ...]\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#example-enable-session-wide-debugging-output","title":"Example: enable session wide debugging output","text":"<p>Since there is a top level <code>--debug</code> option, the corresponding variable name is <code>CMEMC_DEBUG</code>:</p> <pre><code>$ export CMEMC_DEBUG=true\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/environment-based-configuration/#configuration-environment-export-from-the-config-file","title":"Configuration environment export from the config file","text":"<p>Beginning with v21.11, cmemc can export a configuration environment from a configuration file to set up an environment for later use with the <code>config eval</code> command.</p> <pre><code>$ cmemc -c my-cmem.example.org config eval\nexport CMEM_BASE_URI=\"https://my-cmem.example.org\"\nexport DI_API_ENDPOINT=\"https://my-cmem.example.org/dataintegration\"\nexport DP_API_ENDPOINT=\"https://my-cmem.example.org/dataplatform\"\nunset OAUTH_ACCESS_TOKEN\nexport OAUTH_CLIENT_ID=\"cmem-service-account\"\nexport OAUTH_CLIENT_SECRET=\"...\"\nexport OAUTH_GRANT_TYPE=\"client_credentials\"\nunset OAUTH_PASSWORD\nexport OAUTH_TOKEN_URI=\"https://my-cmem.example.org/auth/realms/cmem/protocol/openid-connect/token\"\nunset OAUTH_USER\nexport REQUESTS_CA_BUNDLE=\".../certifi/cacert.pem\"\nexport SSL_VERIFY=\"True\"\n</code></pre> <p>This can be used to export a full <code>config.env</code> or to <code>eval</code> it in an environment for other processes:</p> <pre><code>$ cmemc -c my-cmem.example.org config eval &gt; config.env\n$ eval $(cmemc -c my-cmem.example.org config eval)\n</code></pre> <p>Please note that the following command has the same effect but needs the <code>cmemc.ini</code> for evaluating the <code>config</code> values for the config section <code>my-cmem.example.org</code>:</p> <pre><code>$ export CMEMC_CONNECTION=\"my-cmem.example.org\"\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/","title":"File-based Configuration","text":"","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#introduction","title":"Introduction","text":"<p>This page documents how to configure cmemc via configuration files.</p> <p>cmemc looks for a default configuration file in a location depending on your operating system:</p> <ul> <li>For Linux, this is <code>$HOME/.config/cmemc/config.ini</code><sup>1</sup>.</li> <li>For Windows, this is <code>%APPDATA%\\cmemc\\config.ini</code>.</li> <li>For MacOS, this is <code>$HOME/Library/Application Support/cmemc/config.ini</code>.</li> </ul> <p>If you need to change this location and want to use another config file, you have two options:</p> <ul> <li>run cmemc with the <code>--config-file path/to/your/config.ini</code> option</li> <li>set a new config file with the environment variable <code>CMEMC_CONFIG_FILE</code></li> </ul> <p>However, once you start cmemc the first time without any command or option, it will create an empty config file at this location and will output a general introduction.</p> First cmemc run \u2026 <pre><code>$ cmemc\nEmpty config created: /home/user/.config/cmemc/config.ini\nUsage: cmemc [OPTIONS] COMMAND [ARGS]...\n  eccenca Corporate Memory Control (cmemc).\n  cmemc is the eccenca Corporate Memory Command Line Interface (CLI).\n  Available commands are grouped by affecting resource type (such as graph,\n  project and query). Each command and group has a separate --help screen\n  for detailed documentation. In order to see possible commands in a group,\n  simply execute the group command without further parameter (e.g. cmemc\n  project).\n  If your terminal supports colors, these coloring rules are applied: Groups\n  are colored in white; Commands which change data are colored in red; all\n  other commands as well as options are colored in green.\n  Please also have a look at the cmemc online documentation:\n                      https://eccenca.com/go/cmemc\n  cmemc is \u00a9 2023 eccenca GmbH, licensed under the Apache License 2.0.\nOptions:\n  -c, --connection TEXT  Use a specific connection from the config file.\n  --config-file FILE     Use this config file instead of the default one.\n                         [default: /Users/seebi/Library/Application\n                         Support/cmemc/config.ini]\n  -q, --quiet            Suppress any non-error info messages.\n  -d, --debug            Output debug messages and stack traces after errors.\n  --version              Show the version and exit.\n  -h, --help             Show this message and exit.\nCommands:\n  admin       Import bootstrap data, backup/restore workspace or get status.\n  config      List and edit configs as well as get config values.\n  dataset     List, create, delete, inspect, up-/download or open datasets.\n  graph       List, import, export, delete, count, tree or open graphs.\n  project     List, import, export, create, delete or open projects.\n  query       List, execute, get status or open SPARQL queries.\n  vocabulary  List, (un-)install, import or open vocabs / manage cache.\n  workflow    List, execute, status or open (io) workflows.\n</code></pre> <p>You can now edit your config file and add credentials and URL parameters for your Corporate Memory deployment. You either search for the config manually in your home directory or you can use the <code>config edit</code> command, which opens the config file in your default text editor (specified by the <code>EDITOR</code> variable).</p> <pre><code>$ cmemc config edit\nOpen editor for config file /home/user/.config/cmemc/config.ini\n</code></pre> <p>The rules for the config file are similar to a Windows INI file and are explained in detail at docs.python.org.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#examples","title":"Examples","text":"<p>Example</p> <pre><code>[my-local]\nCMEM_BASE_URI=http://localhost/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=...\n</code></pre> <p>Here is a minimal example using the <code>client_credentials</code> grant type.</p> <p>This creates a named section <code>my-local</code> which is a connection to a Corporate Memory deployment on <code>http://localhost/</code>. The authorization will be done with a system account <code>cmem-service-account</code> and the given client secret. Using this combination of config parameters is based on a typical installation where all components are available under the same hostname.</p> <p>Example</p> <pre><code>[my-local]\nCMEM_BASE_URI=http://localhost/\nOAUTH_GRANT_TYPE=password\nOAUTH_CLIENT_ID=cmemc\nOAUTH_USER=user\nOAUTH_PASSWORD=...\n</code></pre> <p>Another example using <code>password</code> grant type.</p> <p>This creates a named section <code>my-local</code>, which is a connection to a Corporate Memory deployment on <code>http://localhost/</code>. The authorization will be done with the given <code>OAUTH_USER</code> and the <code>OAUTH_PASSWORD</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#configuration-variables","title":"Configuration Variables","text":"<p>The above example provides access to an installation where all components including keycloak are deployed with the default URL base. However, if you need to fine-tune all locations or want to use special functionality, the following config file parameters can be used.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#location-related","title":"Location related","text":"<p>The following configuration variables specify where cmemc can find the relevant HTTP endpoints. Most of them are optional.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#cmem_base_uri","title":"CMEM_BASE_URI","text":"<p>This is the base location (HTTP(S) URL) of your eccenca Corporate Memory deployment.</p> <p>You always have to set this configuration variable.</p> <p>This variable defaults to <code>http://docker.localhost/</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#di_api_endpoint","title":"DI_API_ENDPOINT","text":"<p>This is the base location (HTTP(S) URL) of all Data Integration APIs.</p> <p>Usually you do not need to set this configuration variable.</p> <p>This variable defaults to <code>$CMEM_BASE_URI/dataintegration/</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#dp_api_endpoint","title":"DP_API_ENDPOINT","text":"<p>This is the base location (HTTP(S) URL) of all Data Platform APIs.</p> <p>Usually you do not need to set this configuration variable.</p> <p>This variable defaults to <code>$CMEM_BASE_URI/dataplatform/</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#keycloak_base_uri","title":"KEYCLOAK_BASE_URI","text":"<p>This is the base location (HTTP(S) URL) of all Keycloak APIs.</p> <p>Usually you do not need to set this configuration variable.</p> <p>This variable defaults to <code>$CMEM_BASE_URI/auth/</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#keycloak_realm_id","title":"KEYCLOAK_REALM_ID","text":"<p>This is the identifier of your  Keycloak Realm.</p> <p>Usually you do not need to set this configuration variable.</p> <p>This variable defaults to <code>cmem</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_token_uri","title":"OAUTH_TOKEN_URI","text":"<p>This is the OpenID Connect (OIDC) OAuth 2.0 token endpoint location (HTTP(S) URL).</p> <p>Usually you do not need to set this configuration variable.</p> <p>This variable defaults to <code>$KEYCLOAK_BASE_URI/realms/$KEYKLOAK_REALM_ID/protocol/openid-connect/token</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#authentication-related","title":"Authentication related","text":"<p>The following configuration variables specify how cmemc can fetch a token to authenticate on the endpoints.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_grant_type","title":"OAUTH_GRANT_TYPE","text":"<p>This configures the OAuth Grant Type used to specify how cmemc is able to get a valid token for accessing the Corporate Memory APIs.</p> <p>Depending on the value of this variable, other authentication-related variables will become mandatory or obsolete. The following values can be used:</p> <ul> <li><code>client_credentials</code> - this refers to the OAuth 2.0 Client Credentials Grant Type. Mandatory variables for this grant type are <code>OAUTH_CLIENT_ID</code>, <code>OAUTH_CLIENT_SECRET</code> or <code>OAUTH_CLIENT_SECRET_PROCESS</code>.</li> <li><code>password</code> - this refers to the OAuth 2.0 Password Grant Type. Mandatory variables for this grant type are <code>OAUTH_CLIENT_ID</code>, <code>OAUTH_USER</code>, <code>OAUTH_PASSWORD</code> or <code>OAUTH_PASSWORD_PROCESS</code>.</li> <li><code>prefetched_token</code> - this value can be used in case you can provide a token that was fetched outside of cmemc. Mandatory variables for this grant type are <code>OAUTH_ACCESS_TOKEN</code> or <code>OAUTH_ACCESS_TOKEN_PROCESS</code>.</li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_client_id","title":"OAUTH_CLIENT_ID","text":"<p>This configures the used client ID. Ususally, the following cmemc related clients are configured in the standard Corporate Memory realm:</p> <ul> <li><code>cmem-service-account</code> is the client which is configured to be used with the <code>client_credentials</code> grant type.</li> <li><code>cmemc</code> is the client which is configured to be used with the <code>password</code> grant type.</li> </ul> <p>You usually have to set this configuration variable (exception: you use the <code>prefetched_token</code> grant type).</p> <p>This variable defaults to <code>cmem-service-account</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_user","title":"OAUTH_USER","text":"<p>This variable specifies your user account.</p> <p>You only need to set this configuration variable if you use the <code>password</code> grant type.</p> <p>This variable defaults to <code>admin</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_password","title":"OAUTH_PASSWORD","text":"<p>This variable specifies your user password.</p> <p>You only need to set this configuration variable if you use the <code>password</code> grant type.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_client_secret","title":"OAUTH_CLIENT_SECRET","text":"<p>This variable specifies your client secret (password).</p> <p>You only need to set this configuration variable if you use the <code>client_credentials</code> grant type.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_access_token","title":"OAUTH_ACCESS_TOKEN","text":"<p>This variable specifies a prefetched access token.</p> <p>You only need to set this configuration variable if you use the <code>prefetched_token</code> grant type.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_password_process","title":"OAUTH_PASSWORD_PROCESS","text":"<p>In order to avoid saving credentials in config files you can use this optional configuration variable instead of the <code>OAUTH_PASSWORD</code> variable.</p> <p>Please refer to Getting Credentials from external Processes for more information.</p> <p>This variable defaults to <code>none</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_client_secret_process","title":"OAUTH_CLIENT_SECRET_PROCESS","text":"<p>In order to avoid saving credentials in config files you can use this optional configuration variable instead of the <code>OAUTH_CLIENT_SECRET</code> variable.</p> <p>Please refer to Getting Credentials from external Processes for more information.</p> <p>This variable defaults to <code>none</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#oauth_access_token_process","title":"OAUTH_ACCESS_TOKEN_PROCESS","text":"<p>In order to avoid saving credentials in config files you can use this optional configuration variable instead of the <code>OAUTH_ACCESS_TOKEN</code> variable.</p> <p>Please refer to Getting Credentials from external Processes for more information.</p> <p>This variable defaults to <code>none</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#network-related","title":"Network related","text":"","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#ssl_verify","title":"SSL_VERIFY","text":"<p>Setting this to <code>True</code> will disable certification verification (not recommended).</p> <p>Please refer to Certificate handling and SSL verification for more information.</p> <p>This variable defaults to <code>False</code>.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/file-based-configuration/#requests_ca_bundle","title":"REQUESTS_CA_BUNDLE","text":"<p>Setting this to a PEM file allows for using private Certificate Authorities for certificate validation.</p> <p>Please refer to Certificate handling and SSL verification for more information.</p> <p>This variable defaults to <code>$PYTHON_HOME/site-packages/certifi/cacert.pem</code>.</p> <ol> <li> <p>More precisely, it is <code>$XDG_CONFIG_HOME/cmemc/config.ini</code>, see also the XDG Base Directory Specification.\u00a0\u21a9</p> </li> </ol>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/","title":"Getting Credentials from External Processes","text":"","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#introduction","title":"Introduction","text":"<p>This page discusses how to avoid passwords in configuration files by using configured credential processes or environment variables. This is particularly useful when credentials often change and / or are stored in central infrastructures such as personal or company wide password managers. Moreover, you might find it useful when working with cmemc in CI/CD pipelines.</p>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#environment-variables","title":"Environment Variables","text":"<p>As described in the Configuration with Environment Variables document, cmemc can be configured with environment variables. The following code snippet demonstrates the behaviour:</p> <pre><code>$ export CMEM_BASE_URI=\"https://your-cmem.eccenca.dev/\"\n$ export OAUTH_GRANT_TYPE=\"client_credentials\"\n$ export OAUTH_CLIENT_ID=\"cmem-service-account\"\n$ export OAUTH_CLIENT_SECRET=\"...secret...\"\n$ cmemc graph list\n</code></pre> <p>In the context of a CI/CD pipeline, e.g., on github, these credentials can be taken from the repository secrets:</p> <pre><code>jobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- name: run cmemc\nenv:\nCMEM_BASE_URI: https://your-cmem.eccenca.dev/\nOAUTH_GRANT_TYPE: client_credentials\nOAUTH_CLIENT_ID: cmem-service-account\nOAUTH_CLIENT_SECRET: ${{ secrets.OAUTH_CLIENT_SECRET }}\nrun: |\ncmemc graph list\n</code></pre> <p>In shell context, you can fetch the secret from an external process to the variable:</p> <pre><code>$ export OAUTH_CLIENT_SECRET=$(get-my-secret.sh)\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#external-processes","title":"External Processes","text":"<p>Another option, which is interesting when working with multiple Corporate Memory instances, is the configuration of an external process in your cmemc configuration file.</p> <p>In order to get credential information from an external process you need to use the following configuration variables to set up an external executable:</p> <ul> <li><code>OAUTH_PASSWORD_PROCESS</code>, to set up the process to get the user password when using the <code>password</code> grant type.</li> <li><code>OAUTH_CLIENT_SECRET_PROCESS</code>, to set up the process to get the client secret when using <code>client_credentials</code> grant type .</li> <li><code>OAUTH_ACCESS_TOKEN_PROCESS</code>, to set up the process to get the direct access token (<code>prefetched_token</code>).</li> </ul> <p>The credential executable can use the other cmemc environment keys of the configuration block for fetching the credentials (e.g. <code>CMEM_BASE_URI</code> and <code>OAUTH_USER</code>).</p> <p>If the credential executable is not given with a a full path, cmemc will look into your environment <code>PATH</code> for something that can be executed.</p> <p>The configured process needs to return the credentials on the first line of <code>stdout</code>. In addition to that, the process needs to exit with exit code 0 (without failure).</p> <p>The following config section demonstrates this behaviour:</p> <pre><code>[your-cmem]\nCMEM_BASE_URI=https://your-cmem.eccenca.dev/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET_PROCESS=get-my-secret.sh\n</code></pre> <p>If you need to add options to the call, you can write the call as a list:</p> <pre><code>[your-cmem]\nCMEM_BASE_URI=https://your-cmem.eccenca.dev/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET_PROCESS=[\"getpass.sh\", \"parameter1\", \"parameter2\"]\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/configuration/getting-credentials-from-external-processes/#example-macos-keychain","title":"Example: MacOS Keychain","text":"<p>Here is a working example with the MacOS Keychain, which can be queried with the command line tool <code>security</code>.</p> <p>This example fetches a password for the account <code>cmem-service-account</code> for the service <code>https://your-cmem.eccenca.dev/</code>.</p> <pre><code>OAUTH_CLIENT_SECRET_PROCESS=[\"security\", \"find-generic-password\", \"-w\", \"-a\", \"cmem-service-account\", \"-s\", \"https://your-cmem.eccenca.dev/\" ]\n</code></pre> <p>The corresponding keychain entry looks like this:</p> <p></p> <p>In order to avoid repeating this long line in a cmemc configuration with lots of entries, it can be wrapped in a shell script like this:</p> <pre><code>#!/usr/bin/env bash\nif [ \"${OAUTH_GRANT_TYPE}\" = \"client_credentials\" ]; then\nsecurity find-generic-password -w -a \"${OAUTH_CLIENT_ID}\" -s \"${CMEM_BASE_URI}\" || exit 1\nexit 0\nfi\nif [ \"${OAUTH_GRANT_TYPE}\" = \"password\" ]; then\nsecurity find-generic-password -w -a \"${OAUTH_USER}\" -s \"${CMEM_BASE_URI}\" || exit 1\nexit 0\nfi\nexit 1\n</code></pre>","tags":["Security","cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/","title":"Installation","text":"<p>cmemc can be installed using the python package from pypi.org, the release package or by pulling the docker image.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/#via-pypiorg","title":"\u2026 via pypi.org","text":"<p>cmemc is available as an official pypi package so installation can be done with pip or pipx (preferred):</p> <pre><code>$ pipx install cmem-cmemc\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/#via-release-package","title":"\u2026 via release package","text":"<p>The cmemc release package consists of the following files:</p> <ul> <li><code>cmem_cmemc-vXX.YY.tar.gz</code> - the source package of cmemc</li> <li><code>cmem_cmempy-vXX.YY.tar.gz</code> - the source package of cmempy (the used python API to access Corporate Memory)</li> <li><code>cmemc_vXX.YY_Manual.pdf</code> - the cmemc documentation manual (this document)</li> <li><code>cmemc_vXX.YY_Manual.ttl</code> - the cmemc documentation as structured data (RDF graph)</li> <li><code>requirements.txt</code> - additional requirements needed by cmemc</li> </ul> <p>The following script demonstrates how to install cmemc from these files:</p> <pre><code>$ pip install -r requirements.txt\n...\n$ pip install cmem_cmempy-v22.1.tar.gz\n...\n$ pip install cmem_cmemc-v22.1.tar.gz\n...\n$ cmemc --version\ncmemc, version 22.1, running under python 3.9.11\n</code></pre>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/installation/#via-docker-image","title":"\u2026 via docker image","text":"<p>This topic is described on a stand-alone page.</p> <p>Note</p> <p>Once you have installed cmemc, you need to configure a connection with a config file or learn how to use environment variables to control cmemc.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/","title":"Invocation","text":"<p>cmemc is intended to be useful for administrators and Linked Data experts.</p> <p>Besides the plain ad-hoc invocation from a users terminal, the following recipes show additional invocation methods.</p> <ul> <li> <p> Executing cmemc as a Docker Container.</p> </li> <li> <p> Running cmemc jobs as part of Github Actions.</p> </li> <li> <p> Running cmemc jobs as part of Gitlab Pipelines.</p> </li> <li> <p> Preparing SPARQL Scripts to fetch data from your Knowledge Graphs.</p> </li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/","title":"Using the Docker Image","text":"","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/#introduction","title":"Introduction","text":"<p>In addition to the cmemc distribution package, you can use the eccenca cmemc docker image which is based on the official debian slim image. This is especially needed if you want to use cmemc in orchestrations.</p>","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/#image-and-tags","title":"Image and Tags","text":"<p>The following image - tag combinations are available for public use:</p> <ul> <li><code>docker-registry.eccenca.com/eccenca-cmemc:v22.1</code> - a specific release</li> <li><code>docker-registry.eccenca.com/eccenca-cmemc:latest</code> - same as the latest release</li> </ul> Image retrieval and check cmemc version<pre><code>$ docker run -it --rm docker-registry.eccenca.com/eccenca-cmemc:v22.1 --version\nUnable to find image 'docker-registry.eccenca.com/eccenca-cmemc:v22.1' locally\nv22.1: Pulling from eccenca-cmemc\nDigest: sha256:29bdd320e02f1b7758df22528740964225b62530c73c773a55c36c0e9e18b647\nStatus: Downloaded newer image for docker-registry.eccenca.com/eccenca-cmemc:v22.1\ncmemc, version v22.1.1, running under python 3.9.13\n</code></pre>","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/docker-image/#volumes","title":"Volumes","text":"<p>cmemc processes a configuration file and can import and export files which represent graph, project or workspace payloads. These files need to be mounted via docker volumes to be accessible for the dockerized cmemc.</p> <ul> <li><code>/config/cmemc.ini</code> (file) - the loaded configuration file</li> <li><code>/data</code> (directory) - the working directory</li> </ul> Using a volume to mount the config.<pre><code>$ cat cmemc.ini\n[my-deployment]\nCMEM_BASE_URI=https://data.example.org/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=credentialshere\n$ docker run -it --rm -v \"$(pwd)\"/cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.1 config list\nmy-deployment\n</code></pre> Using a volume to additionally mount the data directory.<pre><code>$ cat list-graphs.sparql\nSELECT DISTINCT ?graph (COUNT(?graph) AS ?triples)\nWHERE {\n    GRAPH ?graph\n    {\n        ?s ?p ?o\n    }\n}\nGROUP BY ?graph\nORDER BY DESC(?triples)\n$ docker run -it --rm -v $(pwd):/data -v $(pwd)/cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.1 -c my-deployment query execute ./list-graphs.sparql\ngraph,triples\nhttp://schema.org/,8809\nhttps://vocab.eccenca.com/shacl/,1752\n[...]\n</code></pre>","tags":["Docker","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/","title":"Using Github Actions","text":"","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#introduction","title":"Introduction","text":"<p>Github Actions allow for the automation and execution of workflows based on pushes, merge requests and other trigger events to your git repository. In order to control eccenca Corporate Memory instances from within Github Action based workflows, you need to provide cmemc as well as credentials for your instance to the workflow.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#runner-provisioning","title":"Runner Provisioning","text":"<p>Providing a working cmemc command is simple. You just need to install a python environment suitable to run cmemc (currently <code>3.9</code>). This can be done with the setup-python action. After that, simply use <code>pip</code> to install cmemc:</p> Partial github action yaml showing cmemc provisioning<pre><code>      - uses: actions/setup-python@v4\nwith:\npython-version: '3.9'\n- name: install cmemc\nrun: |\npip install -q cmem-cmemc\ncmemc --version\n</code></pre> <p>Adding the above to your workflow yaml description will provide a cmemc command which can be used in all subsequent steps of the same workflow.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#credentials-and-usage","title":"Credentials and Usage","text":"<p>Since we should never save credentials in your repository, we need to provide them as an encrypted secret managed outside of the repository. Github provides you with an Encrypted Secrets interface where you can add secrets for your repository, which in turn can be used in your workflows.</p> <p>Given the following workflow step, you need to add <code>MY_CMEM_BASE_URI</code>, <code>MY_OAUTH_GRANT_TYPE</code>, <code>MY_OAUTH_CLIENT_ID</code> and <code>MY_OAUTH_CLIENT_SECRET</code> as encrypted secrets to your repository:</p> Partial github action yaml showing credential provisioning<pre><code>      - name: use cmemc\nrun: |\ncmemc graph import graph.ttl $GRAPH\ncmemc graph count $GRAPH\ncmemc graph delete $GRAPH\nenv:\nGRAPH: \"https://github.com/eccenca/cmemc-workflow\"\nCMEM_BASE_URI: ${{ secrets.MY_CMEM_BASE_URI }}\nOAUTH_GRANT_TYPE: ${{ secrets.MY_OAUTH_GRANT_TYPE }}\nOAUTH_CLIENT_ID: ${{ secrets.MY_OAUTH_CLIENT_ID }}\nOAUTH_CLIENT_SECRET: ${{ secrets.MY_OAUTH_CLIENT_SECRET }}\n</code></pre> <p>The above snippet also demonstrates how you can map your encrypted secrets to cmemc\u2019s configuration variables.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/github-action/#example-project","title":"Example Project","text":"<p>The Github project eccenca/cmemc-workflow provides an example workflow description which uses cmemc to import a graph, count the triples and removes the graph afterwards. Here is an example output:</p> <p></p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/","title":"Using Gitlab Pipelines","text":"","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#introduction","title":"Introduction","text":"<p>Gitlab CI/CD allows for the automation and execution of workflows based on pushes, merge requests and other trigger events to your git repository. In order to control eccenca Corporate Memory instances from within Gitlab CI/CD based workflows you need to provide cmemc and the credentials for your instance to the Gitlab CI/CD pipeline.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#runner-provisioning","title":"Runner Provisioning","text":"<p>In order to use cmemc in Gitlab pipelines you can use the cmemc docker image.</p> Partial .gitlab-ci.yml showing cmemc provisioning<pre><code>test:\nimage:\nname: docker-registry.eccenca.com/eccenca-cmemc:latest\nentrypoint: [\"\"]\nscript:\n- cmemc --version\n</code></pre> <p>Adding the above to your pipeline description will provide a cmemc command which can be used in the workflow step.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#credentials-and-usage","title":"Credentials and Usage","text":"<p>Since we should never commit credentials in your repository, we need to provide them as an encrypted secret managed outside of the repository. You can add CI/CD variables to a project\u2019s settings directly in Gitlab.</p> <p>Given the following pipeline step, you need to add <code>MY_CMEM_BASE_URI</code>, <code>MY_OAUTH_GRANT_TYPE</code>, <code>MY_OAUTH_CLIENT_ID</code> and <code>MY_OAUTH_CLIENT_SECRET</code> as encrypted secrets to your repository:</p> Partial .gitlab-ci.yml showing credential provisioning<pre><code>test:\nimage:\nname: docker-registry.eccenca.com/eccenca-cmemc:latest\nentrypoint: [\"\"]\nstage: test\nscript:\n- cmemc --version\n- cmemc graph import graph.ttl $GRAPH\n- cmemc graph count $GRAPH\n- cmemc graph delete $GRAPH\nvariables:\nGRAPH: \"https://github.com/seebi/cmemc-workflow\"\nCMEM_BASE_URI: $MY_CMEM_BASE_URI\nOAUTH_GRANT_TYPE: $MY_OAUTH_GRANT_TYPE\nOAUTH_CLIENT_ID: $MY_OAUTH_CLIENT_ID\nAUTH_CLIENT_SECRET: $MY_OAUTH_CLIENT_SECRET\n</code></pre> <p>The above snippet also demonstrates how you can map your project variables to cmemc\u2019s configuration variables.</p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/gitlab-pipeline/#example-project","title":"Example Project","text":"<p>The Github project eccenca/cmemc-workflow provides an example gitlab pipeline description which uses cmemc to import a graph, count the triples and removes the graph afterwards. Here is an example output:</p> <p></p>","tags":["Automate","cmemc"]},{"location":"automate/cmemc-command-line-interface/invocation/sparql-scripts/","title":"SPARQL Scripts","text":"<p>By prepending a Shebang line to a SPARQL query file and making the file executable, it can be treated as an executable script. To give an example, below is a simple text file with a generic SPARQL query that counts all triples in all graphs and outputs an ordered list.</p> count-graphs.sh<pre><code>SELECT DISTINCT ?graph (COUNT(?graph) AS ?triples)\nWHERE {\nGRAPH ?graph\n{\n?s ?p ?o\n}\n}\nGROUP BY ?graph\nORDER BY DESC(?triples)\n</code></pre> <p>In order to convert this text file into a SPARQL script you need to add the following line to the top of the file:</p> shebang line for SPARQL scripts<pre><code>#!/usr/bin/env -S cmemc query execute --accept text/csv\n</code></pre> <p>This will set cmemc as an interpreter for the rest of the file, and by using the query execute command, the rest of the file will be used as a SPARQL query.</p> <p>Now you need to define your SPARQL file as executable and run it:</p> <pre><code>$ chmod a+x ./count-graphs.sh\n</code></pre> <pre><code>$ ./count-graphs.sh\ngraph,triples\nhttps://vocab.eccenca.com/shacl/,1796\nhttps://vocab.eccenca.com/dsm/,736\nhttps://vocab.eccenca.com/sketch/,395\nhttps://ns.eccenca.com/example/data/dataset/,233\nhttps://ns.eccenca.com/example/data/vocabs/,128\nurn:elds-backend-access-conditions-graph,97\nhttps://ns.eccenca.com/data/queries/,32\nhttp://di.eccenca.com/project/cmem,7\n</code></pre>","tags":["Automate","SPARQL","cmemc"]},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/","title":"Troubleshooting and Caveats","text":"<p>This page lists and documents possible issues and warnings when working with cmemc.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#proxy-is-in-the-way","title":"Proxy is in the way","text":"<p>If you feel that your system\u2019s proxy configuration negatively impacts the communication between cmemc and Corporate Memory, you can disable using any proxy by setting this variable:</p> <pre><code>export no_proxy='*'\n</code></pre> <p>This is due to the python requests library proxy handling.</p> <p>The no_proxy environment variable can be used to specify hosts which shouldn\u2019t be reached via proxy; if set, it should be a comma-separated list of hostname suffixes, optionally with :port appended, for example cern.ch,ncsa.uiuc.edu,some.host:8080.</p>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/troubleshooting-and-caveats/#gateway-time-out","title":"Gateway Time-out","text":"<p>A gateway timeout occurs if your Corporate Memory infrastructure is not setup correctly.</p> <pre><code>$ cmemc -c my-cmem project import my-project.zip my-project\nImport file my-project.zip to project my-project ... 504 Server Error: Gateway Time-out for url: https://my-cmem/dataintegration/workspace/projects\n</code></pre> <p>This can have multiple reasons - please check in the following order:</p> <ul> <li><code>application.yaml</code> of DataIntegration</li> <li>reverse proxy configuration</li> </ul>","tags":["cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/","title":"Workflow Execution and Orchestration","text":"","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#introduction","title":"Introduction","text":"<p>In some cases, you need to automate a complete graph of integration workflows which depend on each other and can sometimes run in parallel or consecutively. Although cmemc is not a workflow orchestration tool, you can easily use it do some basic workflow orchestration. This page describes how you can execute and orchestrate workflows. For simplicity, all the given examples do not select a specific connection (<code>--connection your-cmem</code>). We simply assume that you selected your instance via an environment variable (<code>export CMEMC_CONNECTION=your-cmem</code>).</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#simple-execution","title":"Simple Execution","text":"<p>cmemc allows for execution of workflows with the <code>workflow execute</code> command. To start a workflow, simply use this command:</p> workflow execute command<pre><code>$ cmemc workflow execute cmem:my-workflow\ncmem:my-workflow ... Started\n</code></pre> <p>Workflow identifier can be extended with command-line completion on the command line but you can also get a list of workflows with the <code>workflow list</code> command:</p> workflow list command<pre><code>$ cmemc workflow list\ncmem:my-workflow\ncmem:second-workflow\n</code></pre> <p>The default working mode of the <code>workflow execute</code> command starts a workflow without waiting for a response. In order to wait until the workflow is finished you need to use the <code>--wait</code> option:</p> workflow execute command with wait option<pre><code>$ cmemc workflow execute cmem:my-workflow --wait\ncmem:my-workflow ... Started ... Finished (Finished in 32.931s, just now)\n</code></pre> <p>For a reference of the <code>workflow execute</code> command, please have a look at the Command Reference or the command-specific help (<code>cmemc workflow execute --help</code>).</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#retrieve-status-information","title":"Retrieve Status Information","text":"<p>At any time, you can retrieve status information of a workflow with the <code>workflow status</code> command:</p> workflow status command<pre><code>$ cmemc workflow status cmem:my-workflow\ncmem:my-workflow ... Finished (Finished in 32.931s, 4 minutes ago)\n</code></pre> <p>Additionally, you can retrieve raw JSON data about a workflow, which can be used for post-processing:</p> workflow status command with JSON output<pre><code>$ cmemc -workflow status cmem:my-workflow --raw\n{\n  \"activity\": \"ExecuteLocalWorkflow\",\n  \"runtime\": 32931,\n  \"project\": \"cmem\",\n  \"failed\": false,\n  \"message\": \"Finished in 32.931s\",\n  \"task\": \"my-workflow\",\n  \"isRunning\": false,\n  \"statusName\": \"Finished\",\n  \"progress\": 100,\n  \"cancelled\": false,\n  \"startTime\": 1593679211989,\n  \"exceptionMessage\": null,\n  \"lastUpdateTime\": 1593679244920\n}\n</code></pre> <p>For a reference of the <code>workflow status</code> command, please have a look at the Command Reference or the command-specific help (<code>cmemc workflow status --help</code>).</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#serial-execution","title":"Serial Execution","text":"<p>The <code>workflow execute</code> command is able to start multiple workflows in a chain, waiting for each of the workflows to finish and exiting if there is an error with one of the workflows.</p> <p>To do this, use the <code>--wait</code> option and simply add more than one workflow identifier as parameters to the the command:</p> workflow execute command<pre><code>$ cmemc workflow execute cmem:my-workflow cmem:second-workflow --wait\ncmem:my-workflow ... Started ... Finished (Finished in 30.984s, just now)\ncmem:second-workflow ... Started ... Finished (Finished in 50.579s, just now)\n</code></pre> <p>Warning</p> <p>Starting workflows in this way means that cmemc exits with an error code 1 at the moment a workflow throws an error. None of the following workflows will be executed.</p>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/cmemc-command-line-interface/workflow-execution-and-orchestration/#parallel-execution","title":"Parallel Execution","text":"<p>Sometimes you may want to execute workflows in parallel, because they do not depend on each other and it fastens up the overall runtime.</p> <p>To do this, there is currently a little bit of extra scripting needed. The main idea is to start the parallel workflows without waiting and then poll the status information until they are not running anymore.</p> <p>Here is an example script which does exactly this:</p> cmemc-parallel-workflows.sh<pre><code>#!/usr/bin/env bash\n# @(#) Example: execute two workflows in parallel and wait for the results (exit 1 on failure)\n# Use the unofficial bash strict mode: http://redsymbol.net/articles/unofficial-bash-strict-mode/\nset -euo pipefail; export FS=$'\\n\\t'\n# setup the used instance\nexport CMEMC_CONNECTION=your-cmem\n\n# check SSL config\nif [ \"$(cmemc config get SSL_VERIFY 2&gt;&amp;1)\" == True ]\nthen\nNUM=0\nelse\nNUM=1\nfi\n# start the given set of workflows\nWORKFLOW_IDS=\"cmem:my-workflow cmem:second-workflow\"\ncmemc workflow execute $WORKFLOW_IDS\n# loop until they are not running anymore\nRUNNING=-1\nuntil [ $RUNNING == $NUM ]\ndo\n# wait 5 seconds - polling time\nsleep 5\n# use the the filter option to show only running workflows\nRUNNING=$(cmemc workflow status $WORKFLOW_IDS --filter running 2&gt;&amp;1 | wc -l)\nif [ $RUNNING != $NUM ]; then\necho \"We still have $RUNNING running workflows ...\"\nfi\ndone\n# look for failed workflows\nFAILED=$(cmemc workflow status $WORKFLOW_IDS --filter failed 2&gt;&amp;1 | wc -l)\nif [ $FAILED != $NUM ]; then\necho \"Some workflows failed :-(\"\nexit 1\nelse\necho \"All workflows finished successfully :-)\"\nexit 0\nfi\n</code></pre>","tags":["Automate","Workflow","cmemc"]},{"location":"automate/continuous-integration/","title":"Continuous Integration and Delivery","text":""},{"location":"automate/continuous-integration/#introduction","title":"Introduction","text":"<p>Project teams often manage crucial parts of their work assets inside of git repositories. This includes Corporate Memory related files such as ontologies, shapes or project configurations. Given such a project setup, it is often wanted to start activities with these Corporate Memory files. Continuous integration (CI) is the practice of automating the integration of changes from multiple contributors into a single project. Originated from software projects, CI can (and should) be applied to Knowledge Graph projects as well.</p>"},{"location":"automate/continuous-integration/#example-project","title":"Example Project","text":"<p>The following depiction shows an example project setup which applies CI/CD principles to a Knowledge Graph project.</p> <p></p> <p>The project has two Corporate Memory stages, DEV and PROD. The DEV Stage produces Knowledge Graphs based on artefacts from one or more Git repositories. Changes on the artefacts are tested by a Build Server. The Build Server pushes tested artefacts (Green Builds) to the DEV stage as well as produces other artefacts such as Visualisations, Documentation and Test Result artefacts. The automated activities in the Build Plan are executed with cmemc on the Corporate Memory instances. Once a release is ready, artefacts are transferred to the PROD Stage.</p>"},{"location":"automate/continuous-integration/#integration-recipes","title":"Integration Recipes","text":"<p>Complex build plans are compiled of simple steps, each executing a specific activity. cmemc is a building block to create complex build plans for your Knowledge Graph project. The following pages provide recipes for different CI/CD solutions:</p> <ul> <li> <p> Github Actions</p> </li> <li> <p> Gitlab Pipelines</p> </li> </ul>"},{"location":"automate/processing-data-with-variable-input-workflows/","title":"Processing Data with Variable Input Workflows","text":"","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#introduction","title":"Introduction","text":"<p>This tutorial shows how you can create and use data integration workflows to process data coming from outside Corporate Memory (i.e., without registering datasets). The concept to achieve this is named a Variable Dataset. A variable dataset is created and used inside of a workflow as an input for other tasks (e.g. a transformation) at the place where a \u201cregular\u201d dataset (such as register CSV file) would be placed.</p> <p>The workflow is then called via an HTTP REST call (or via\u00a0cmemc), thus uploading the payload data \u201cat the place\u201d of this variable input dataset and executing all following parts of the workflow.</p> <p>This allows for solving all kinds of \u2606 Automation tasks when you need to process lots of small data snippets or similar.</p> <p>Abstract</p> <p>The complete tutorial is available as a\u00a0project file. You can import this project</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li> <p>by using the command line interface</p> <pre><code>$ cmemc -c my-cmem project import tutorial-varinput.project.zip varinput\n</code></pre> </li> </ul>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#1-install-the-required-vocabularies","title":"1 Install the required vocabularies","text":"<p>First, install all required ontologies/vocabularies which are needed for mappings later in the tab VOCABULARIES.</p> <p>In this tutorial, we need the Schema.org and the RDFS vocabulary. Press the (toggle switch) button on the right to install them.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#2-create-a-new-project","title":"2 Create a new project","text":"<p>Second, create in the tab DATA INTEGRATION a new project. Provide it with a Title and Description.</p> <p>The project will include everything you need to build a workflow for extracting Feed XML data, transforming it into RDF, and loading it into a Knowledge Graph.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#3-create-an-example-feed-dataset-and-target-graph-dataset","title":"3 Create an (example) feed dataset and target graph dataset","text":"<p>Upload a sample XML dataset (feed data) into your project: Create \u2192 XML \u2192 Upload new file.</p> <p>For this tutorial, you may take this file: feed.xml(1)</p> <ol> <li>Original feed source was: <code>https://www.ecdc.europa.eu/en/taxonomy/term/2942/feed</code></li> </ol> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#4-create-the-feed-transformation","title":"4 Create the feed transformation","text":"<p>Based on the added sample feed XML Dataset, create a mapping to generate RDF triples. The screenshot provides an example mapping to generate WebPages, which includes a label, a URL, a text, and the date they were published in the feed. The mappings are based on classes and properties defined by the Schema.org and RDFS vocabulary.</p> <p>In case you need help with mapping data from XML to RDF, feel free to visit your respective tutorial: Lift data from JSON and XML sources.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#5-create-the-variable-input-and-workflow","title":"5 Create the variable input and workflow","text":"<p>Create a new workflow in your project. Move the input XML feed dataset and the Feed Data Graph into the workflow editor and connect them with your created Transform feed.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#6-use-cmemc-to-feed-data-into-the-workflow","title":"6 Use <code>cmemc</code> to feed data into the workflow","text":"<p>Finally, you can process all the feeds you want by executing the created workflow with a dynamic XML payload.</p> <p>For this, you need to use the <code>workflow io</code> command:</p> <pre><code># process one specific feed xml document\n$ cmemc workflow io varinput:process-feed -i feed.xml\n</code></pre> <p>You can easily automate this for a list of feeds like this:</p> <pre><code>$ cat feeds.txt\nhttps://feeds.npr.org/500005/podcast.xml\nhttp://rss.cnn.com/rss/cnn_topstories.rss\nhttps://lifehacker.com/rss\nhttp://feeds.bbci.co.uk/news/rss.xml\n\u2026\n# fetch the list of urls one by one and feed the content to the corporate memory workflow\n$ cat feeds.txt | xargs -I % sh -c '{ echo %; curl -s % -o feed.xml; cmemc workflow io varinput:process-feed -i feed.xml; rm feed.xml; }'\nhttps://feeds.npr.org/500005/podcast.xml\nhttp://rss.cnn.com/rss/cnn_topstories.rss\nhttps://lifehacker.com/rss\nhttp://feeds.bbci.co.uk/news/rss.xml\n\u2026\n</code></pre>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/processing-data-with-variable-input-workflows/#7-explore-the-fetched-knowledge-graph","title":"7 Explore the fetched Knowledge Graph","text":"<p>In EXPLORATION, you can study the ingested feed data in your Knowledge Graph.</p> <p></p>","tags":["ExpertTutorial","Automate","Workflow","cmemc"]},{"location":"automate/scheduling-workflows/","title":"Scheduling Workflows","text":"","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#introduction","title":"Introduction","text":"<p>For a time-based execution of a workflow, Corporate Memory provides the Scheduler operator. Please note that, in case you want to schedule workflows externally, cmemc can be used for that.</p>","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#create-a-scheduler","title":"Create a scheduler","text":"<ol> <li>Navigate to Build \u2192 Projects section in the workspace and Click Create.</li> <li>Select the Item type Scheduler.</li> <li>Click Add - then the Create new item of type Scheduler dialog box appears.</li> <li>Set the properties of the Scheduler:<ol> <li>Select the target project.</li> <li>Define the label of your scheduler</li> <li>Specify the workflow (task) to be executed.</li> <li>Define the interval for the scheduler to be executed again.     Example: <code>PT15MD</code> (Every 15 minutes)</li> <li>Define the start time for the scheduler to be executed for the first time.</li> <li>Click Enable to enable the scheduler.</li> <li>Click Stop on error to stop the scheduler on after a failed run.</li> </ol> </li> </ol> <p>Once you are ready with the configurations, click Create button. Now, the scheduler will be executed with the given settings.</p> <p></p>","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#modify-enable-or-disable-a-scheduler","title":"Modify, enable or disable a scheduler","text":"<ol> <li>Navigate to Build \u2192 Projects section in the workspace.</li> <li>Search the scheduler you want to modify.</li> <li>Select it or click on Open Details Page in the context menu.</li> <li>Click on the Configure button in the Configuration section.</li> <li>Change the values according to your needs.</li> </ol>","tags":["Workflow","Automate","Video"]},{"location":"automate/scheduling-workflows/#time-interval-specification","title":"Time Interval Specification","text":"<p>The scheduler interval is represented an ISO-8601 time duration string .</p> <p>The following values are possible:</p> <ul> <li><code>P</code> is the duration designator (referred to as \u201cperiod\u201d), and is always placed at the beginning of the duration.</li> <li><code>Y</code> for defining the number of years.</li> <li><code>M</code> for defining the number of months.</li> <li><code>W</code> for defining the number of weeks.</li> <li><code>D</code> for defining the number of days.</li> <li><code>T</code> is the time designator that precedes the time components.</li> <li><code>H</code> for defining the number of hours.</li> <li><code>M</code> for defining the number of minutes.</li> <li><code>S</code> for defining the number of seconds.</li> </ul> <p>A duration with all values being used: <code>P2Y6M4DT12H30M10S</code> (defines a a period of 2 years, 6 months, 4 days, 12 hours, 30 minutes and 10 seconds).</p> <p>More common examples:</p> <ul> <li><code>PT30M</code> - every half hour</li> <li><code>PT1H</code> - every hour</li> <li><code>P1D</code> - every day</li> </ul>","tags":["Workflow","Automate","Video"]},{"location":"build/","title":"Build","text":""},{"location":"build/#build","title":"Build","text":"<p>The Build stage is used to turn your legacy data points from existing datasets into an Enterprise Knowledge Graph structure. The subsections introduce the features of Corporate Memory that support this stage and provide guidance through your first lifting activities.</p> <p>  Intended audience: Linked Data Experts</p> <ul> <li> <p> Introduction and Best Practices</p> <ul> <li>Introduction to the User Interface \u2014 a short introduction to the Build workspace incl. projects and tasks management.</li> <li>Rule Operators \u2014 Overview on operators that can be used to build linkage and transformation rules.</li> <li>Cool IRIs\u00a0\u2014 URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company.</li> <li>Define Prefixes / Namespaces\u00a0\u2014 Define Prefixes / Namespaces \u2014 Namespace declarations allow for abbreviation of IRIs by using a prefixed name instead of an IRI, in particular when writing SPARQL queries or Turtle.</li> </ul> </li> <li> <p> Tutorials</p> <ul> <li>Lift Data from Tabular Data \u2014 Build a Knowledge Graph from from Tabular Data such as CSV, XSLX or Database Tables.</li> <li>Lift data from JSON and XML sources\u00a0\u2014 Build a Knowledge Graph based on input data from hierarchical sources such as JSON and XML files.</li> <li>Extracting data from a Web API\u00a0\u2014 Build a Knowledge Graph based on input data from a Web API.</li> <li>Reconfigure Workflow Tasks\u00a0\u2014 During its execution, new parameters can be loaded from any source, which overwrites originally set parameters.</li> <li>Incremental Database Loading \u2014 Load data incrementally from a JDBC Dataset (relational database Table) into a Knowledge Graph.</li> </ul> </li> </ul>"},{"location":"build/active-learning/","title":"Active Learning of Linking Rules","text":"","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#introduction","title":"Introduction","text":"<p>Active learning infuses expert knowledge and creates new relationships between properties of two datasets. We can learn new rules and refine existing rules.</p>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#usage","title":"Usage","text":"<p>Active learning is a special case of machine learning in which a learning algorithm interactively queries a user to label new data points with the desired outputs. [wikipedia]</p> <p>In Corporate Memory we apply this approach to the process of learning a linking rule by interactively label records from the configured source and target dataset. Labeling in this case means to indicate if the pair of resources (from source and target) should be connected with the configured property. The labeling process creates reference links against which the linking rule can be created and further refined as more input is given by the user.</p> <p>Our active (link) learning is a three step process:</p> <ol> <li>It starts by define properties to compare between entities of the selected datasets.</li> <li>We continue with the interactive labeling process where user feedback is asked, the golden record build and a rule (automatically) calculated and refined (as more reference links are entered)</li> <li>Saving the learned rule and/or the reference links only</li> </ol>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#start-the-learning-dialog","title":"Start the learning dialog","text":"<p>Link learning is a feature available on a link rule in a Build project. See the Lift data from tabular data such as CSV, XSLX or database tables tutorial to learn how to setup a project. Use the Create  button in your project and select Linking to create a new linking rule. In the configuration dialog of your linking rule and setup source and target datasets as well as the linking property that should be yielded. Start the learning dialog by clicking the \u201cLearning\u201d tab in the linking view.</p> <p>The examples process below uses the movies example project which can be added to your workspace with the  Add \u201cmovies\u201d example project button to be found in the  user menu in the top right corner.</p>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#creating-an-automatic-link-rule","title":"Creating an automatic link rule","text":"<ul> <li> <p>Choose properties to compare.     Select from the suggestions or search them by specifying property paths for both entities.</p> <p></p> </li> </ul> <p>Note</p> <p>Based on the dataset suggestions for comparison are produced.</p>","tags":["BeginnersTutorial"]},{"location":"build/active-learning/#add-property-paths-for-both-entities","title":"Add property paths for both entities","text":"<ul> <li> <p>Click on the Source path and select a path.</p> <p></p> </li> <li> <p>Click on the Target path and select a corresponding path.</p> <p></p> </li> <li> <p>Click on the  icon to add the path pair to be examined in the learning algorithm.</p> <p></p> <p>Success</p> <p>Step Result: Both entities\u2019 paths were added.</p> <p></p> </li> <li> <p>Click on  icon to remove the paths.</p> <p></p> </li> <li> <p>Click on Start learning.</p> <p></p> <p>Note</p> <p>Clicking on the  icon uses the property value as the entity label, unselecting the  icon removes the property value from the entity label. Multiple properties can be starred to use them as a combined label.</p> <p></p> <p></p> <p>Success</p> <p>Step Result: The comparison in both paths will be reflected as shown below.</p> <p></p> <p>Cross-check the similarity between the source and target path data with regards to the configured link property (<code>owl:sameAs</code> in this example).</p> <p> Confirm: If the source and target title names are the same, click on Confirm and it is shown in dark blue colour.</p> <p></p> <p>Uncertain: If the title names differ slightly, we can consider the link uncertain. As it might be a spelling mistake, we cannot ensure it is the same nor can we say it is different. If the title names are different it is displayed in light blue colour.</p> <p></p> <p> Decline: If the title names of source and target path are different,click on decline and it displayed in light blue colour.</p> <p></p> </li> <li> <p>On the right side of the page click on the 3 dots, then click on show entity\u2019s URI.</p> <p></p> </li> </ul> <p>Success</p> <p>Step Result: It shows the link entity URIs along with rows numbers in both the dataset files.</p> <p></p> <ul> <li>Click on Save based on our input confirm, uncertain and decline the link rule will get generated automatically and the score changes for these entities in the score bar.</li> </ul> <p></p> <ul> <li>Switch on the save best learned rule, then click on save.</li> </ul> <p></p> <p>Success</p> <p>The new automatically created linking rule based on the input training data consisting of confirmed, uncertain and declined links is shown below. It tokenize all the input values from the connected source path and compares the data with target paths.</p> <p></p>","tags":["BeginnersTutorial"]},{"location":"build/cool-iris/","title":"Cool IRIs","text":"","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/cool-iris/#introduction","title":"Introduction","text":"<p>URIs and IRIs are character strings identifying the nodes and edges in the graph. Defining them is an important step in creating an exploitable Knowledge Graph for your Company.</p> <p>RFC 3986\u00a0defines a generic syntax for URIs:</p> <ul> <li><code>&lt;scheme&gt;:&lt;scheme-specific-part&gt;</code></li> <li>The scheme-specific part is often structured: <code>&lt;authority&gt;/&lt;path&gt;?&lt;query&gt;</code></li> </ul> <p>URIs are limited to ASCII characters. IRIs (Internationalized Resource Identifiers) allow Unicode (RFC 3987).</p> <p>The following list of example IRIs demonstrate the broad scope of this concept:</p> <ul> <li><code>ftp://ftp.is.co.za/rfc/rfc1808.txt</code></li> <li><code>http://www.ietf.org/rfc/rfc2396.txt</code></li> <li><code>ldap://[2001:db8::7]/c=GB?objectClass?one</code></li> <li><code>mailto:John.Doe@example.com</code></li> <li><code>news:comp.infosystems.www.servers.unix</code></li> <li><code>tel:+1-816-555-1212</code></li> <li><code>telnet://192.0.2.16:80/</code></li> <li><code>urn:oasis:names:specification:docbook:dtd:xml:4.1.2</code></li> </ul>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/cool-iris/#best-practices-in-corporate-memory","title":"Best practices in Corporate Memory","text":"<p>A good IRI is unique, stable, simple and manageable.</p> <p>Define a useful IRI-Scheme that can be used for resources.</p> <ul> <li>Define a Base URI which is the common authority for all resources in your graph.<ul> <li>Example:\u00a0<code>https://data.company.org/</code></li> </ul> </li> <li>Define subspaces where necessary, e.g. for each subproject or domain. Provide a\u00a0prefix\u00a0for each subspace. Examples:<ul> <li><code>https://data.company.org/hardware/</code>\u00a0for hardware artifacts</li> <li><code>https://data.company.org/software/</code>\u00a0for software artifacts</li> <li><code>PREFIX cohw: &lt;https://data.company.org/hardware/&gt;</code></li> <li><code>PREFIX cosw: &lt;https://data.company.org/software/&gt;</code></li> </ul> </li> <li>Based on these, build consistent schemes that define how your IRIs have to be build. Examples:<ul> <li><code>https://data.company.org/hardware/&lt;ProductClass&gt;/&lt;Serialnumber&gt;</code> to identify an individual product</li> <li><code>https://data.company.org/hardware/&lt;ProductClass&gt;/&lt;Modelnumber&gt;</code> to identify a product model</li> </ul> </li> </ul> <p>Warning</p> <p>Do not put a trailing slash at the end of resource IRIs as these cannot be used with prefix definitions in Turtle or SPARQL, which makes them more difficult to use.</p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/cool-iris/#more-information","title":"More information","text":"<ul> <li>Spanish Government, URIs for Open Data resources</li> <li>European Union, URIs for Legal Resources</li> <li>UK, \u201cDesigning URI sets for the UK public sector\u201d</li> <li>Other Resources<ul> <li>https://www.w3.org/TR/cooluris/</li> <li>https://www.w3.org/Provider/Style/URI.html</li> <li>https://www.w3.org/wiki/GoodURIs</li> <li>https://www.w3.org/TR/dwbp/</li> <li>https://www.w3.org/TR/ld-bp/</li> </ul> </li> </ul>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/","title":"Define Prefixes / Namespaces","text":"","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#introduction","title":"Introduction","text":"<p>A namespace declaration consists of a prefix name and a namespace IRI. Namespace declarations allow for the abbreviation of IRIs by using a prefixed resource name instead of a full IRI.</p> <p>For example, after defining a namespace with the values</p> <ul> <li>prefix name = <code>cohw</code>, and the</li> <li>namespace IRI = <code>https://data.company.org/hardware/</code></li> </ul> <p>you can use the term\u00a0<code>cohw:test</code>\u00a0as an abbreviation for the full IRI\u00a0<code>https://data.company.org/hardware/test</code>.</p> <p>This is particularly useful when you have to write source code in Turtle and SPARQL.</p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#managing-namespace-declarations","title":"Managing Namespace Declarations","text":"","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#using-the-vocabulary-catalog","title":"Using the Vocabulary Catalog","text":"<p>After installing a vocabulary from the\u00a0Vocabulary Catalog, the vocabulary namespace declaration is automatically added to all integration projects.</p> <p>In order to get the prefix name and the namespace IRI from the vocabulary graph, the following terms from the VANN vocabulary need to be used on the Ontology resource.</p> <ul> <li>vann:preferredNamespacePrefix - to specify the prefix name</li> <li>vann:preferredNamespaceUri - to specify the namespace IRI</li> </ul> <p>In the Explore area, an Ontology with a correct namespace declaration looks like this:</p> <p></p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#using-cmemc","title":"Using cmemc","text":"<p>The <code>vocabulary</code> command group of cmemc has an <code>import</code> command that you can use to install arbitrary vocabulary documents and register them as vocabularies in Corporate Memory.</p> <p>Beginning with v22.2, this command has an additional option <code>--namespace</code> which you can use to set a vocabulary namespace even if the vocabulary does not include the data needed for autodiscovery:</p> <pre><code>$ cmemc vocabulary import my-ont.ttl --namespace myo https//example.org/my/`\n</code></pre>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#using-the-project-configuration","title":"Using the Project Configuration","text":"<p>In addition to the used vocabulary namespace declarations, you may want to add well-known namespaces for organizing the Knowledge Graphs.</p> <p>Such organization use cases include:</p> <ul> <li>Namespaces per class / resource type:<ul> <li>prefix name = <code>persons</code>, namespace IRI = <code>https://example.org/data/persons/</code></li> </ul> </li> <li>Namespaces per data owner or origin:<ul> <li>prefix name = <code>sales</code>, namespace IRI = <code>https://example.org/data/sales/</code></li> </ul> </li> </ul> <p>Prefixes in Data Integration are defined on a project basis. When creating a new project, a list of well-know prefixes is already declared.</p> <p>After selecting a project from the search results the prefix management is available in the project configuration in the lower right area:</p> <p></p> <p>By using the Edit Prefix Settings button in this Configuration area, you will see the Manage Prefixes dialog:</p> <p></p> <p>In this dialog, you are able to</p> <ul> <li>Delete a namespace declaration\u00a0\u2192 Delete Prefix</li> <li>Add a new namespace\u00a0declaration \u2192 Add</li> </ul>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/define-prefixes-namespaces/#validating-namespace-declarations","title":"Validating Namespace Declarations","text":"<p>After adding namespace declarations to a project you are able to use the abbreviated IRIs in the user interface, for instance, in the mapping editor, the Turtle editor or the Query editor:</p> <p></p> <p></p> <p></p>","tags":["KnowledgeGraph","BestPractice"]},{"location":"build/extracting-data-from-a-web-api/","title":"Extracting data from a Web API","text":"","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#introduction","title":"Introduction","text":"<p>This tutorial demonstrates how you can build a Knowledge Graph based on input data from a Web API. The tutorial is based on the GitHub API (v3), which we will use to fetch repository data of a certain organization and create a Knowledge Graph from the response.</p> <p>Abstract</p> <p>The complete tutorial is available as a\u00a0project file. You can import this project:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li>by using the command line interface</li> </ul> <pre><code>$ cmemc -c my-cmem project import tutorial-webapi.project.zip web-api\n</code></pre> <p>In order to get familiar with the API, simply fetch an example response with this command:</p> <pre><code>$ curl https://api.github.com/orgs/vocol/repos\n</code></pre> <p>The HTTP Get request retrieves all repositories of a GitHub organization named vocol.</p> <p>The JSON response includes the data for all repositories (mobivoc, vocol, \u2026). You can also download the response file here:\u00a0repos.json.</p> <pre><code>[\n{\n...\n\"id\": 22646219,\n\"name\": \"mobivoc\",\n...\n},\n{\n...\n\"id\": 22646629,\n\"name\": \"vocol\",\n...\n},\n{\n...\n\"id\": 30964669,\n\"name\": \"scor\",\n...\n},\n...\n]\n</code></pre>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#1-register-a-web-api","title":"1 Register a Web API","text":"<ol> <li> <p>Click the\u00a0Create\u00a0button (top right) in the data integration workspace and select the type\u00a0REST request.</p> <p></p> </li> <li> <p>Define a Label, Description and the URL of the Web API. Example input:\u00a0<code>https://api.github.com/orgs/vocol/repos</code>.</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#2-create-a-json-parser","title":"2 Create a JSON parser","text":"<p>As we are only interested in the HTTP Message Body which holds the JSON repository data, we first have to parse the body from the entire HTTP response.</p> <ol> <li> <p>Click the\u00a0Create\u00a0button (top right) in the data integration workspace and select the type\u00a0Parse JSON.</p> <p></p> </li> <li> <p>Define a Label, a Description, and the Input path. All other fields can keep the default settings. The default input path is always:\u00a0<code>&lt;http://silkframework.org/vocab/taskSpec/RestTaskResult/responseBody&gt;</code></p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#3-create-a-json-dataset","title":"3 Create a JSON Dataset","text":"<p>To create a JSON-to-RDF-mapping within Corporate Memory, we have to first register an example response from the API (repos.json). Based on the schema of the response, we can then define step-by-step the mappings, which are used to build the Knowledge Graph.</p> <ol> <li> <p>Click the Create button (top right) in the data integration workspace and select the type\u00a0JSON.</p> <p></p> </li> <li> <p>Upload the JSON file\u00a0repos.json\u00a0(API response) as a Dataset into Corporate Memory.</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#4-create-a-knowledge-graph","title":"4 Create a Knowledge Graph","text":"<p>The Knowledge Graph will be used to integrate all data coming from one or more APIs. The Knowledge Graph receives RDF triples from the defined Transformations for each API.</p> <ol> <li> <p>Click the Create button (top right) in the data integration workspace and select the type Knowledge Graph.</p> <p></p> </li> <li> <p>Provide the Knowledge Graph with a\u00a0Label\u00a0and\u00a0Description, as well as the following (example)\u00a0Graph\u00a0URI:\u00a0<code>http://ld.company.org/repository-data/</code></p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#5-create-a-transformation","title":"5 Create a Transformation","text":"<p>In order to transform the input data from the API, in our example structured in JSON, we have to define a mapping to create RDF triples which are then written to the Knowledge Graph.</p> <ol> <li> <p>Click the Create button (top right) in the data integration workspace and select the type\u00a0Transform.</p> <p></p> </li> <li> <p>Provide the Transformation with a\u00a0Label\u00a0and\u00a0Description, configure the\u00a0Input Dataset\u00a0(Repos.json) and the\u00a0Output Dataset\u00a0(Repository Knowledge Graph).</p> <p></p> </li> <li> <p>Click the\u00a0Mapping Editor\u00a0button in the previously defined Transformation.</p> <p></p> </li> <li> <p>In the following screenshots, we provide an example mapping for the data received by the GitHub API. For more complex mappings, we recommend the Tutorial\u00a0Lift data from JSON and XML sources.</p> <p></p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/extracting-data-from-a-web-api/#6-create-a-workflow","title":"6 Create a Workflow","text":"<p>To build a workflow that combines all the elements we previously built, we now define a workflow for (1) requesting the data from the GitHub API, (2) parsing the HTTP response we receive, (3) transforming the JSON data into RDF triples and finally (4) writing the RDF triples into the Knowledge Graph.</p> <ol> <li> <p>Click the\u00a0Create\u00a0button (top right) in the data integration workspace and select the type\u00a0Workflow.</p> <p></p> </li> <li> <p>Provide the Transformation with a\u00a0Label\u00a0and a\u00a0Description.</p> <p></p> </li> <li> <p>Click the\u00a0Workflow Editor\u00a0button in the menu of the created workflow.</p> <p></p> </li> <li> <p>Drag and drop the different items into the Workflow Editor and combine them with one another (see example screenshot).\u00a0Save\u00a0the workflow, and press the\u00a0run symbol\u00a0to execute the workflow.</p> <p></p> </li> <li> <p>Validate the result by clicking on the\u00a0Workflow Report\u00a0tab and see the result of your execution. In this example, 15x repositories were found from the GitHub API request.</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/introduction-to-the-user-interface/","title":"Introduction to the user interface","text":"<p>This page provides a short introduction to the BUILD / Data Integration workspace incl. projects and different tasks.</p>"},{"location":"build/introduction-to-the-user-interface/#workbench","title":"Workbench","text":"<p>The workbench is the main entry point to the BUILD interface and provides a list view of your work, structured in projects.</p> <p></p> <p>On the left-hand side, a facet list provides an overview of different item types:</p> <ul> <li>Projects are the main structure and consist of datasets, workflows and different tasks (transformations, linking and other tasks)</li> <li>Datasets are registered data sources from files or endpoints, or internally managed.</li> <li>Transform tasks take an input dataset and execute a (hierarchical) set of mapping and transformation rules, and generate data for an output dataset.</li> <li>Linking tasks compare entities from two datasets according to (hierarchical) sets of comparators and generate links between these datasets in an output (link) dataset.</li> <li>Workflows can combine all items in a project in order to create a structured work plan which can be executed on demand, controlled remotely (e.g. via\u00a0cmemc) or executed\u00a0from a\u00a0scheduler.</li> </ul> <p>The interface can be used to browse from a global level to a project level as well as to search for items globally and project wide.</p> <p>On the top right side in the header, there is a big blue Create (+) button which allows for creation of all possible data integration items:</p> <p></p>"},{"location":"build/introduction-to-the-user-interface/#projects","title":"Projects","text":"<p>Selecting a project from the search results opens the project details view (see above).</p> <p>The project is the main concept to structure your work. In the workspace, you can review your projects, create new or delete old ones, and import or export them.</p> <p>Each project is self-contained and holds all relevant parts, such as the following:</p> <ul> <li>the raw data resources (local or remote data files) that form the starting point for your work</li> <li>the datasets that provide uniform access to different kinds of data resources</li> <li>transformation tasks to transform datasets (e.g., from one format or schema to another)</li> <li>linking tasks to integrate different datasets</li> <li>workflows to orchestrate the different tasks</li> <li>definitions of URI prefixes used in the project</li> </ul> <p>Resources, datasets, tasks and other parts of a project can be added and configured from within the workspace. By clicking the <code>Show Details</code>\u00a0button on any item in a project, you can \u2018look inside\u2019 and define the actual transformation or linking rules, or assemble the actual workflow.</p>"},{"location":"build/introduction-to-the-user-interface/#datasets","title":"Datasets","text":"<p>Selecting a dataset in a project or from the search results opens the dataset details view.</p> <p></p> <p>Here you have the following options:</p> <ul> <li>Change the meta data of the dataset (Summary - label and description, top left main area),</li> <li>Change the Configuration parameters of the dataset (lower right side area),</li> <li>Browse to other Related Items, such as transforms, linking or workflow tasks which use this dataset (top right side area),</li> <li>Get a Data Preview as well as generate and view profiling information (lower left main area).</li> </ul> <p>A dataset represents an abstraction over raw data. In order to work with data, you have to make it available in the form of a dataset. A dataset provides a source or destination for the different kinds of tasks. I.e., it may be used to read entities for transformation or linking, and it can be used to write transformed entities and generated links.</p> <p>There is a range of different dataset types for different kinds of source data. Important dataset types include:</p> <ul> <li>Knowledge Graph - Read RDF from or write RDF to a Knowledge Graph embedded in Corporate Memory.</li> <li>CSV - Read from or write to an CSV file.</li> <li>XML - Read from or write to an XML file.</li> <li>JSON - Read from or write to a JSON file.</li> <li>JDBC endpoint - Connect to an existing JDBC endpoint.</li> <li>Variable dataset - Dataset that acts as a placeholder in workflows and is replaced at request time.</li> <li>Excel - Read from or write to an Excel workbook in Open XML format (XLSX).</li> <li>Multi CSV ZIP - Reads from or writes to multiple CSV files from/to a single ZIP file.</li> </ul> <p>Other options include:</p> <ul> <li>Internal dataset - Dataset for storing entities between workflow steps.</li> <li>RDF - Dataset which retrieves and writes all entities from/to an RDF file. The dataset is loaded in-memory and thus the size is restricted by the available memory. Large datasets should be loaded into an external RDF store and retrieved using the SPARQL dataset instead.</li> <li>SPARQL endpoint - Connect to an existing SPARQL endpoint.</li> <li>In-memory dataset - A dataset that holds all data in-memory.</li> <li>Avro - Read from or write to an Apache Avro file.</li> <li>ORC - Read from or write to an Apache ORC file.</li> <li>Parquet - Read from or write to an Apache Parquet file.</li> <li>Hive database - Read from or write to an embedded Apache Hive endpoint.</li> <li>SQL endpoint - Provides a JDBC endpoint that exposes workflow or transformation results as tables, which can be queried using SQL.</li> <li>Neo4j - Connect to an existing Neo4j property graph database system.</li> </ul>"},{"location":"build/introduction-to-the-user-interface/#transform-tasks","title":"Transform tasks","text":"<p>The purpose of transform tasks\u00a0is to transform datasets (e.g., from one format or schema to another). More specifically, Transform Tasks create Knowledge Graphs out of raw data sources.</p> <p>Selecting a transform task in a project or from the search results\u00a0opens the transform task details view.</p> <p></p> <p>Here you have the following options:</p> <ul> <li>Change the metadata of the transformation task (Summary - label and description, top left main area)</li> <li>Change the Configuration parameters of the transformation task (lower right side area),</li> <li>Browse to other Related Items, such as used datasets or workflows using this task (top right side area),</li> <li>Open the Mapping Editor in order to change the transformation rules,</li> <li>Evaluate the Transformation to check for issues with the rules,</li> <li>Execute the Transformation rules in order to create new data,</li> <li>Clone the task, creating a new transform task with the same rules.</li> </ul>"},{"location":"build/introduction-to-the-user-interface/#linking-tasks","title":"Linking tasks","text":"<p>The purpose of linking tasks is to integrate two datasets (source and target) by generating a linkset that contains links from the source to the target dataset.</p> <p>Selecting a linking task in a project or from the search results opens the transform task details view.</p> <p> </p> <p>Here you have the following options:</p> <ul> <li>Change the metadata of the linking task (Summary - label and description, top left main area),</li> <li>Change the Configuration parameters of the linking task (lower right side area),</li> <li>Browse to other Related Items, such as used datasets or workflows using this task (top right side area),</li> <li>Open the Linking Editor in order to change the comparison rules,</li> <li>Evaluate the Linking to check for issues with the rules,</li> <li>Execute the Linking task in order to create a linkset,</li> <li>Clone the task, creating a new linking task with the same comparisons.</li> </ul> <p>As in the case of transform tasks, the output linkset of a linking task can either be written directly to an output dataset, or provide the input to another task if integrated into a workflow.</p>"},{"location":"build/introduction-to-the-user-interface/#workflows","title":"Workflows","text":"<p>For projects that go beyond one or two simple transform or linking tasks, you can create complex workflows. In a workflow, you can orchestrate datasets, transform tasks and linking tasks via connections between their inputs and outputs, and in this way perform complex tasks.</p> <p>Selecting a workflow in a project or from the search results opens the workflow details view.</p> <p></p> <p>Here you have the following options:</p> <ul> <li>Change the metadata of the linking task (Summary - label and description, top left main area)</li> <li>Browse to\u00a0Related Items, such as used datasets and tasks (top right side area),</li> <li>Open the Worflow Editor in order to change the workflow (here you can as well start the workflow),</li> <li>Clone the workflow creating a new workflow with the same task.</li> </ul>"},{"location":"build/lift-data-from-json-and-xml-sources/","title":"Lift data from JSON and XML source","text":"","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#introduction","title":"Introduction","text":"<p>This tutorial shows how you can build a Knowledge Graph based on input data from hierarchical sources like a JavaScript Object Notation file (.json) or an Extensible Markup Language file (.xml).</p> <p>Abstract</p> <p>The complete tutorial is available as a project file (XML) and a\u00a0project file (JSON). You can import these projects:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li> <p>by using the command line interface</p> <pre><code>cmemc -c my-cmem project import tutorial-xml.project.zip xml-transformation\n</code></pre> <pre><code>cmemc -c my-cmem project import tutorial-json.project.zip json-transformation\n</code></pre> </li> </ul> <p>The documentation consists of the following steps, which are described in detail below.</p> <p>The following material is used in this tutorial:</p> <ul> <li> <p>Sample vocabulary describing the data in the JSON and XML files: products_vocabulary.nt</p> <p></p> </li> <li> <p>Sample JSON file: services.json</p> <pre><code>[\n{\n\"Price\": \"748,40 EUR\",\n\"ProductManager\": \"Lambert.Faust@company.org\",\n\"Products\": \"O491-3823912, I965-1821441, Z655-3173353, ...\",\n\"ServiceID\": \"Y704-9764759\",\n\"ServiceName\": \"Product Analysis\"\n},\n{\n\"Price\": \"1082,00 EUR\",\n\"ProductManager\": \"Corinna.Ludwig@company.org\",\n\"Products\": \"Z249-1364492, L557-1467804, C721-7900144, ...\",\n\"ServiceID\": \"I241-8776317\",\n\"ServiceName\": \"Component Confabulation\"\n},\n...\n]\n</code></pre> </li> <li> <p>Sample XML file: orgmap.xml <pre><code>&lt;orgmap&gt;\n&lt;dept id=\"73191\" name=\"Engineering\"&gt;\n&lt;manager&gt;\n&lt;email&gt;Thomas.Mueller@company.org&lt;/email&gt;\n&lt;name&gt;Thomas Mueller&lt;/name&gt;\n&lt;address&gt;Karl-Liebknecht-Stra\u00dfe 885, 82003 Tettnang&lt;/address&gt;\n&lt;phone&gt;+49-8200-38218301&lt;/phone&gt;\n&lt;/manager&gt;\n&lt;employees&gt;\n&lt;employee&gt;\n&lt;email&gt;Corinna.Ludwig@company.org&lt;/email&gt;\n&lt;name&gt;Corinna Ludwig&lt;/name&gt;\n&lt;address&gt;Ringstra\u00dfe 276&lt;/address&gt;\n&lt;phone&gt;+49-1743-24836762&lt;/phone&gt;\n&lt;productExpert&gt;Memristor, Gauge, Encoder&lt;/productExpert&gt;\n&lt;/employee&gt;\n&lt;employee&gt;\n&lt;email&gt;Karen.Brant@company.org&lt;/email&gt;\n&lt;name&gt;Karen Brant&lt;/name&gt;\n&lt;address&gt;Friedrichstra\u00dfe 664, 30805 Willich&lt;/address&gt;\n&lt;phone&gt;(00530) 5040048&lt;/phone&gt;\n&lt;productExpert&gt;Inductor&lt;/productExpert&gt;\n&lt;/employee&gt;\n...\n        &lt;/employees&gt;\n&lt;products&gt;\n&lt;product id=\"Z249-1364492\" /&gt;\n&lt;product id=\"O184-6903943\" /&gt;\n&lt;product id=\"V404-9975399\" /&gt;\n&lt;product id=\"F344-7012314\" /&gt;\n&lt;product id=\"N463-8050264\" /&gt;\n&lt;product id=\"M605-5951566\" /&gt;\n&lt;product id=\"N733-1946687\" /&gt;\n&lt;/products&gt;\n&lt;services&gt;\n&lt;service id=\"I241-8776317\" /&gt;\n&lt;service id=\"D215-3449390\" /&gt;\n&lt;/services&gt;\n&lt;/dept&gt;\n&lt;dept id=\"22183\" name=\"Product Management\"&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0...\n    &lt;/dept&gt;\n...\n&lt;/orgmap&gt;\n</code></pre></p> </li> </ul>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#1-register-the-vocabulary","title":"1 Register the vocabulary","text":"<p>The vocabulary contains the classes and properties needed to map the source data into entities in the Knowledge Graph.</p> <ol> <li> <p>In Corporate Memory, click Vocabularies in the\u00a0navigation under\u00a0EXPLORE on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Register new vocabulary on the top\u00a0right of the Vocabulary catalog page\u00a0in Corporate Memory.</p> <p></p> </li> <li> <p>Define a Name, a Graph URI and a Description of the vocabulary. In this example we will use:</p> <ul> <li>Name: Product Vocabulary</li> <li>Graph URI: http://ld.company.org/prod-vocab/</li> <li>Description: Example vocabulary modeled to describe relations between products and services.</li> </ul> <p></p> </li> <li> <p>Click\u00a0REGISTER.</p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#2-upload-the-data-file","title":"2 Upload the data file","text":"<p>To add the data files, click Projects under BUILD in the navigation on the left side of the page. Follow the steps below for adding JSON and XML datasets.</p> JSONXML <ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.</p> </li> <li> <p>In\u00a0Create new item\u00a0window, select\u00a0JSON\u00a0and click\u00a0Add.</p> <p></p> </li> <li> <p>Define a Label for the dataset and upload the services.json\u00a0file.\u00a0You can leave all the other fields at default values.</p> <p> </p> </li> <li> <p>Click Create.</p> </li> </ol> <ol> <li> <p>Press the Create button and select XML</p> <p></p> </li> <li> <p>Define a Label for the dataset and upload the orgmap.xml example file.\u00a0You can leave all the other fields at default values.</p> <p></p> </li> <li> <p>Click\u00a0Create.</p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#3-create-a-knowledge-graph","title":"3 Create a Knowledge Graph","text":"<ol> <li> <p>Click\u00a0Create\u00a0at the top of the page.\u202f\u00a0</p> </li> <li> <p>In\u00a0Create new item\u00a0window, select\u00a0Knowledge Graph\u00a0and click\u00a0Add. The Create new item of type Knowledge Graph\u00a0window appears.</p> <p></p> </li> <li> <p>Fill in the required details such as Label and Description.</p> JSONXML <p>Define a Label for the Knowlege Graph and provide Graph uri.\u00a0You can leave all the other fields at default values.\u00a0In this example we use:</p> <ul> <li>Name: Service Knowledge Graph</li> <li>Graph: http://ld.company.org/prod-instances/</li> </ul> <p></p> <p>Define a Label for the Knowledge Graph and provide\u00a0Graph uri. You can leave all the other fields at default values.\u00a0In this example we will use:</p> <ul> <li>Name: Organization Knowledge Graph</li> <li>Graph: http://ld.company.org/organization-data/</li> </ul> <p></p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#4-create-a-transformation","title":"4 Create a Transformation","text":"<p>The transformation defines how an input dataset (e.g.: JSON or XML) will be transformed into an output dataset (e.g.: Knowledge Graph).</p> <ol> <li> <p>Click\u00a0Create\u00a0in your project.\u202f\u00a0</p> </li> <li> <p>On the\u00a0Create New Item\u00a0window, select\u00a0Transform\u00a0and click\u00a0Add\u00a0to create a new transformation.</p> <p></p> </li> <li> <p>In the\u00a0Create new item of type Transform\u00a0window, enter the required fields.</p> JSONXML <p>For this example, enter the following:</p> <ul> <li>Name:\u00a0Create Service Triples</li> <li>(optional) Description:\u00a0Lifts the Service file into the Knowledge Graph</li> <li>Select the Source Dataset:\u00a0Services JSON</li> <li>Select the Output Dataset:\u00a0Service_Knowledge_Graph</li> </ul> <p></p> <p>Click Create.</p> <p>For this example, enter the following:</p> <ul> <li>Name:\u00a0Create Organization Triples</li> <li>(optional) Description:\u00a0Lifts the Orgmap XML file into the Knowledge GraphOrgmap XML</li> <li>Select the Source Dataset:\u00a0Orgmap XML</li> <li>Type: dept (define the Source Type, which defines the XML element that should be iterated when creating resources)</li> <li>Select the Output Dataset:\u00a0Organization_Knowledge_Graph</li> </ul> <p></p> <p>Click Create.</p> </li> <li> <p>Expand the \u00a0menu by clicking the arrow on the right side of the page\u00a0to expand the menu.</p> </li> <li> <p>Click Edit\u00a0to create a base mapping.</p> <p></p> </li> <li> <p>Define the Target entity type from the vocabulary, the URI pattern and a Label for the mapping.</p> JSONXML <p>Target Entity Type defines the class that will be instantiated when the mapping rule is applied.</p> <p>The URI pattern that defines the URI that shall be generated for each individual</p> <ul> <li>http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case,</li> <li>service-instances/ complements the instances prefix by adding a common prefix for all service instances</li> <li>and finally {ServiceID} is a placeholder that will resolve to the json-key ServiceID (e.g. \u201cServiceID\u201d: \u201cY704-9764759\u201d)</li> </ul> <p>In this example we will use:</p> <ul> <li>Target Entity Type:\u00a0Service</li> <li>URI Pattern:\u00a0http://ld.company.org/prod-inst/service-instances/{ServiceID}</li> <li>An optional Label: Service</li> </ul> <p>Click SAVE.</p> <p>Example RDF triple in our Knowledge Graph based on the mapping definition:</p> <pre><code>&lt;http://ld.company.org/services-instances/Y704-9764759&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://ld.company.org/prod-vocab/Service&gt;\n</code></pre> <p>Target Entity Type defines the class that will be instantiated when the mapping rule is applied: Department</p> <p>The URI pattern that defines the URI that shall be generated for each individual: http://ld.company.org/department/{@id}</p> <ul> <li>http://ld.company.org/department/ is a common prefix for the department instances in this use case,</li> <li>and finally {@id} is a placeholder that will resolve the XML attribute of the XML tag dept, which was configured as the Source Type of this transformation (see previous steps)</li> </ul> <p>In this example we will use:</p> <ul> <li>Target Entity Type:\u00a0Department</li> <li>URI Pattern: http://ld.company.org/department/{@id}</li> <li>An optional Label: Department</li> </ul> <p>Click Save.</p> <p>Example RDF triple in our Knowledge Graph based on the mapping definition:</p> <pre><code>&lt;http://ld.company.org/department/73191 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://ld.company.org/prod-vocab/Department&gt;\n</code></pre> </li> <li> <p>Evaluate your mapping by pressing on the  button in the Examples of target data property to see at most three generated base URIs.</p> <p></p> <p>We have now created the Service entities in the Knowledge Graph. Next we will now add the name of our entity.</p> </li> <li> <p>Click the circular blue\u00a0+ icon on the lower right and select Add value mapping.</p> <p></p> JSONXML <p>Define the Target property, the Data type, the Value path (column name) and a Label for your value mapping. In this example, enter the following:</p> <ul> <li>Target Property: has product manager</li> <li>Data type: StringValueType</li> <li>Value path: ProductManager/name<ul> <li>which corresponds to the following element in the json-file: [ {\u201cProductManager\u201d: {\u00a0 \u201cname\u201d: \u201cCorinna Ludwig\u201d} \u2026 } \u2026]</li> </ul> </li> <li>An optional Label: has Product Manager</li> </ul> <p></p> <p>Click Save.</p> <p>Define the Target property, the Data type, the Value path (column name) and a Label for your value mapping. In this example we will use:</p> <ul> <li>Target Property: name</li> <li>Data type: StringValueType</li> <li>Value path:\u00a0dept/@name<ul> <li>which corresponds to the <code>department name</code> attribute in the XML file</li> </ul> </li> <li>An optional Label: department name</li> </ul> <p></p> <p>Click Save.</p> </li> </ol> <p>By clicking on the  button in the Examples of target data property, you can get a preview for 3x value mapping to be created.</p> JSONXML <p></p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#5-evaluate-a-transformation","title":"5 Evaluate a Transformation","text":"<p>Click Transform evaluation to evaluate the transformed entities.</p> <p></p>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-json-and-xml-sources/#6-build-the-knowledge-graph","title":"6 Build the Knowledge Graph","text":"<ol> <li>Click Transform execution</li> <li>Press the  button and validate the results. In this example, 9x Service entities were created in our Knowledge Graph based on the mapping.</li> <li>You can click Knowledge Graphs\u00a0under\u00a0EXPLORE\u00a0to (re-)view of the created Knowledge Graphs</li> <li> <p>Enter the following URIs in the Enter search term for JSON and XML respectively.</p> <ul> <li>JSON / Service: http://ld.company.org/prod-instances/</li> <li>XML / Department: http://ld.company.org/organization-data/</li> </ul> JSONXML <p></p> <p></p> </li> </ol>","tags":["AdvancedTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/","title":"Lift data from tabular data such as CSV, XSLX or database tables","text":"","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#introduction","title":"Introduction","text":"<p>This beginner-level tutorial shows how you can build a Knowledge Graph based on input data from a comma-separated value file (.csv), an excel file (.xlsx) or a database table (jdbc).</p> <p>Abstract</p> <p>The complete tutorial is available as a\u00a0project file. You can import this project</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li>by using the command line interface</li> </ul> <pre><code>$ cmemc -c my-cmem project import tutorial-csv.project.zip tutorial-csv\n</code></pre> <p>This step is optional and makes some of the following steps of the tutorial superfluous.</p> <p>The documentation consists of the following steps, which are described in detail below:</p> <ol> <li>Registration of the target vocabulary</li> <li>Uploading of the data (file)/Connect to JDBC endpoint</li> <li>(Re-)View your data table</li> <li>Creation of a (target) graph</li> <li>Creation of the transformation rules</li> <li>Evaluation of the results of the transformation rules</li> <li>Execution of the transformation to populate the target graph</li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#sample-material","title":"Sample Material","text":"<p>The following material is used in this tutorial, you should download the files and have them at hand throughout the tutorial:</p> <ul> <li> <p>Sample vocabulary which describes the data in the CSV files: products_vocabulary.nt</p> <p></p> </li> <li> <p>Sample CSV file:\u00a0services.csv</p> <p>Info</p> ServiceID ServiceName Products ProductManager Price Y704-9764759 Product Analysis O491-3823912, I965-1821441, Z655-3173353, \u2026 Lambert.Faust@company.org 748,40 EUR I241-8776317 Component Confabulation Z249-1364492, L557-1467804, C721-7900144, \u2026 Corinna.Ludwig@company.org 1082,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 </li> <li> <p>Sample Excel file: products.xlsx</p> <p>Info</p> ProductID ProductName Height Width Depth Weigth ProductManager Price I241-8776317 Strain Compensator 12 68 15 8 Baldwin.Dirksen@company.org 0,50 EUR D215-3449390 Gauge Crystal 77 58 19 15 Wanja.Hoffmann@company.org 2,00 EUR \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 </li> </ul>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#1-register-the-vocabulary","title":"1 Register the vocabulary","text":"<p>The vocabulary contains the classes and properties needed to map the data into the new structure in the Knowledge Graph.</p> Corporate Memorycmemc <ol> <li> <p>In Corporate Memory, click\u00a0Vocabularies\u00a0under\u00a0EXPLORE\u00a0in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Register new vocabulary\u00a0on the top right.</p> <p></p> </li> <li> <p>Define a Name, a Graph URI and a Description of the vocabulary. In this example we will use:</p> <ul> <li>Name: Product Vocabulary</li> <li>Graph URI: http://ld.company.org/prod-vocab/</li> <li>Description: Example vocabulary modeled to describe relations between products and services.</li> <li>Vocabulary File: Browse in your filesystem for the products_vocabulary.nt file and select it to be uploaded.</li> </ul> <p></p> </li> </ol> <pre><code>$ cmemc vocabulary import products_vocabulary.nt\n</code></pre>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#2-upload-the-data-file-connect-to-the-jdbc-endpoint","title":"2 Upload the data file / Connect to the JDBC endpoint","text":"CSV + XLSXJDBCcmemc <ol> <li> <p>In Corporate Memory, click\u00a0Projects\u00a0under\u00a0BUILD\u00a0in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Click\u00a0Create\u00a0at the top of the page.\u202f</p> </li> <li> <p>In\u00a0Create new item\u00a0window, select\u00a0Project\u00a0and click\u00a0Add. The\u00a0Create new item of type Project\u00a0window appears.\u202f\u00a0</p> </li> <li> <p>Fill in the required details such as Title and Description.\u00a0Alternatively, import the existing project by clicking Import Project File and selecting the file from your system.\u202f\u00a0</p> </li> <li> <p>Click\u00a0Create. Your project is created.</p> </li> <li> <p>Within your project, click Create or Create item.</p> </li> <li> <p>In the\u00a0Create new item\u00a0dialog, select\u00a0CSV.</p> <p></p> </li> <li> <p>Fill out a label and upload the\u00a0services.csv sample file.</p> <p></p> </li> <li> <p>Click\u00a0Create.**\u00a0Leave all other parameters at their default values.</p> </li> <li> <p>Create a second dataset. Choose Excel and upload the products.xlsx\u00a0file.</p> </li> </ol> <p>Instead of uploading the services.csv sample file into Corporate Memory, you can also load it into a SQL database and access it from Corporate Memory using the JDBC protocol.</p> <ol> <li> <p>In the project, Click\u00a0Create\u00a0and select the JDBC endpoint type.</p> <p></p> </li> <li> <p>Define a Label for the dataset, specify the JDBC Driver connection URL, the table name and the user and password to connect to the database. In this example we will use:</p> <ul> <li>Name: Services_ServiceDB</li> <li>JDBC Driver Connection URL: jdbc:mysql://mysql:3306/ServicesDB</li> <li>table: Services</li> <li>username: root</li> <li>password: ****</li> </ul> <p></p> <p>The general form of the JDBC connection string is:</p> <pre><code>jdbc:&lt;vendor&gt;://&lt;hostname&gt;:&lt;portNumber&gt;/&lt;databaseName&gt;\n</code></pre> <p>Default JDBC connection strings for popular Relational Database Management Systems:</p> Vendor Default JDBC Connection String Default Port Microsoft SQL Server jdbc:sqlserver::1433/ 1433 PostgreSQL jdbc:postgresql::5432/ 5432 MySQL jdbc:mysql::3306/ 3306 MariaDB jdbc:mariadb::3306/ 3306 IBM DB2* jdbc:db2::50000/ 50000 Oracle* jdbc:oracle:thin::1521/ 1521 <p>Info</p> <p>* IBM DB2 and Oracle JDBC drivers are not by default part of Corporate Memory, but can be added.</p> <p>Info</p> <p>Instead of selecting a table you can also specify a custom SQL query in the source query field.</p> </li> </ol> <pre><code>$ cmemc project create tutorial-csv\n\n$ cmemc dataset create --project tutorial-csv services.csv\n\n$ cmemc dataset create --project tutorial-csv products.xlsx\n</code></pre>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#3-re-view-your-data-table","title":"3 (Re-)View your Data Table","text":"<p>To validate that the input data is correct, you can preview the data table in Corporate Memory.</p> <ol> <li> <p>On the dataset page, press the Load preview button</p> <p></p> </li> <li> <p>Once the preview is loaded, you can view a couple of rows to check that your data is accessible.</p> <p></p> </li> <li> <p>Optionally, you can click start profiling and explore statistics about the dataset.</p> <p></p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#4-create-a-knowledge-graph","title":"4 Create a Knowledge Graph","text":"<ol> <li> <p>Click Create at the top of the page.</p> </li> <li> <p>In Create new item window, select Knowledge Graph and click Add.</p> <p></p> </li> <li> <p>The Create new item of type Knowledge Graph\u00a0window appears.</p> </li> <li> <p>Define a Label for the Knowledge Graph and provide a graph uri. Leave all the other parameters at the default values. In this example we will use:</p> <ul> <li>Label: Service Knowledge Graph</li> <li>Graph: http://ld.company.org/prod-instances/</li> </ul> </li> <li> <p>Click Create.</p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#5-create-a-transformation","title":"5 Create a Transformation","text":"<p>The transformation defines how an input dataset (e.g. CSV) will be transformed into an output dataset (e.g. Knowledge Graph).</p> <ol> <li> <p>Click\u00a0Create\u00a0in your project.\u202f\u00a0</p> </li> <li> <p>On the\u00a0Create New Item\u00a0window, select\u00a0Transform\u00a0and click\u00a0Add\u00a0to create a new transformation.</p> </li> <li> <p>Fill out the the details leaving the target vocabularies field at its default value all installed vocabularies, which will enable us to create a transformation to the previously installed products vocabulary. In this example we will use:</p> <ul> <li>Name: Lift Service Database</li> <li>In the section INPUT TASK in the field Dataset, select the previously created dataset: Services (Input Dataset).</li> <li>Select the previously created dataset as the Output Dataset: Service Knowledge Graph</li> </ul> </li> <li> <p>In the main area you will find the Mapping editor.</p> <p></p> </li> <li> <p>Click\u00a0Mapping in the main area to expand its menu.</p> <p></p> </li> <li> <p>Click\u00a0Edit\u00a0to create a base mapping.</p> <p></p> </li> <li> <p>Define the Target entity type from the vocabulary, the URI pattern and a label for the mapping. In this example we will use:</p> <ul> <li>Target entity type: Service</li> <li> <p>URI pattern:</p> <ul> <li>Click\u00a0Create custom pattern</li> <li>Insert\u00a0http://ld.company.org/prod-inst/{ServiceID}</li> <li>where http://ld.company.org/prod-inst/ is a common prefix for the instances in this use case,</li> <li>and {ServiceID} is a placeholder that will resolve to the column of that name</li> </ul> </li> <li> <p>An optional Label: Service</p> </li> </ul> <p></p> </li> <li> <p>Click\u00a0Save Example RDF triple in our Knowledge Graph based on the mapping definition:</p> <pre><code>&lt;http://ld.company.org/prod-inst/Y704-9764759&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://ld.company.org/prod-vocab/Service&gt;\n</code></pre> </li> <li> <p>Evaluate your mapping by clicking the Expand \u00a0button in the Examples of target data property to see at most three generated base URIs.</p> <p></p> <p>We have now created the Service entities in the Knowledge Graph. As a next step, we will add the name of the Service entity.</p> </li> <li> <p>Press the circular Blue + button on the lower right and select Add value mapping.</p> <p></p> </li> <li> <p>Define the Target property, the Data type, the Value path (column name) and a Label for your value mapping. In this example we will use:</p> <ul> <li>Target Property: name</li> <li>Data type: StringValueType</li> <li>Value path: ServiceName (which corresponds to the column of that name)</li> <li>An optional Label: service name</li> </ul> <p></p> </li> <li> <p>Click Save.</p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#6-evaluate-a-transformation","title":"6 Evaluate a Transformation","text":"<p>Go the Transform evaluation tab of your transformation to view a list of generated entities. By clicking one of the generated entities, more details are provided.</p> <p></p>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/lift-data-from-tabular-data-such-as-csv-xslx-or-database-tables/#7-build-the-knowledge-graph","title":"7 Build the Knowledge Graph","text":"<ol> <li> <p>Go into the mapping and visit the Transform execution tab.</p> <p></p> </li> <li> <p>Press the  button and validate the results. In this example, 9x Service triples were created in our Knowledge Graph based on the mapping.</p> <p></p> </li> <li> <p>Finally you can use the DataManager Knowledge Graphs module to (re-)view of the created Knowledge Graph: http://ld.company.org/prod-instances/</p> <p></p> </li> </ol>","tags":["BeginnersTutorial","KnowledgeGraph"]},{"location":"build/loading-jdbc-datasets-incrementally/","title":"Loading JDBC datasets incrementally","text":"","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#introduction","title":"Introduction","text":"<p>This tutorial walks you through the process of loading data incrementally from a JDBC Dataset (relational database table) into a Knowledge Graph.</p> <p>Abstract</p> <p>The complete tutorial is available as a project file. You can import this project:</p> <ul> <li>by using the web interface (Create\u00a0\u2192 Project\u00a0\u2192\u00a0Import project file) or</li> <li>by using the command line interface</li> </ul> <pre><code>cmemc -c my-cmem project import tutorial-webapi.project.zip web-api\n</code></pre> <p>Example SQL query for selecting a predefined range of rows</p> <p>We use the <code>LIMIT</code> and\u00a0 <code>OFFSET</code> clauses in SQL to retrieve only a portion of rows. This prevents loading all data at once from a table,\u00a0SQL Query with OFFSET AND LIMIT:</p> <pre><code>SELECT * FROM services OFFSET 0 LIMIT 5\n</code></pre> <p>This query retrieves the first 5 rows of a table named \u201cservices\u201d. If the OFFSET is changed to 5, it will retrieve the next 5 rows.</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#1-create-the-jdbc-dataset","title":"1 Create the JDBC dataset","text":"<p>To extract data from a relational database, you need to first register a JDBC endpoint in Corporate Memory. This tutorial assumes that you have access to the relational database from the Corporate Memory instance.</p> <ol> <li> <p>In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page.</p> <p></p> </li> <li> <p>Click Create at the top of the page.</p> </li> <li>In the Create new item window, select Project and click Add.\u00a0The Create new item of type Project window appears.</li> <li> <p>In the Create new item window,\u00a0select Dataset under Item Type,\u00a0search for JDBC endpoint, and click\u00a0Add.</p> <p></p> </li> <li> <p>Provide the required configuration details for the JDBC endpoint:</p> <ul> <li>Label: Provide a table name.</li> <li>Description: Optionally describe your table.</li> <li>JDBC Driver Connection URL: Provide the JDBC connection. In this tutorial we use a MySQL database. The database server is named mysql and the database is named serviceDB.</li> <li>Table: Provide the name of the table in the database.</li> <li>Source query: Provide a default source query. In this tutorial, the source query will be modified later as the OFFSET changes.</li> <li>Limit: Provide a LIMIT for the SQL query. In this tutorial, we choose 5 for demonstrating the functionality. You may select any value which works for your use case.</li> <li>Query strategy: Select: Execute the given source query. No paging or virtual Query. In this tutorial, this needs to be changed so that when this JDBC endpoint is being used, Corporate Memory will always check for the Source Query that was provided earlier.</li> <li>User: Provide the user name which is allowed to access the database.</li> <li>Password: Provide the user password that is allowed to access the database.</li> </ul> </li> </ol> <p></p> <p></p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#2-create-a-metadata-graph","title":"2 Create a Metadata Graph","text":"<p>To incrementally extract data in Corporate Memory, we need to store the information about the OFFSET that will change with each extraction. To accomplish this, we need to define a new Graph named Services Metadata Graph that will hold this information. To identify the changing OFFSET with the JDBC endpoint we previously created, we will use the Graph IRI that Corporate Memory created for us.</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#to-find-the-jdbc-endpoint-iri","title":"To find the JDBC endpoint IRI","text":"<ol> <li>Visit the Exploration Tab of Corporate Memory</li> <li>Select in Graph (top left) your project, which starts with \u201cCMEM DI Project \u2026 \u201d (if you cannot see it, you might not have the necessary access rights. In this case, please contact your administrator)</li> <li>Select in Navigation (bottom left): functions_Plugins_Jdbc</li> <li>Select the previously created JDBC endpoint (in our example: \u201cServices Table (JDBC)\u201d</li> <li>Press the Turtle tab inside your JDBC endpoint view (right)</li> </ol> <p>In our example, the JDBC Endpoint IRI looks like this: http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC</p> <p>See screenshot below for example:</p> <p></p> <p>Now that we have the JDBC endpoint IRI, we will build the Metadata Graph to store the OFFSET information.</p> <p>The following three RDF triples hold the (minimal) necessary information we need for this tutorial:</p> <ol> <li>The first triple imports the CMEM DI Project graph into our Metadata Graph to enable access to the LIMIT property defined earlier and to additional information we may need in the future.</li> <li>The second triple defines a label for the Graph.</li> <li>The third triple defines the &lt;\u2026lastOffset&gt; property we need for this tutorial. As a default, we set it to 0 to start with the first row in the table.</li> </ol> <p>services_metadata_graph</p> <pre><code>&lt;http://di.eccenca.com/project/services/metadata&gt;\n&lt;http://www.w3.org/2002/07/owl#imports&gt;\n&lt;http://di.eccenca.com/project/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload&gt; . # import the original project\n&lt;http://di.eccenca.com/project/services/metadata&gt;\n&lt;http://www.w3.org/2000/01/rdf-schema#label&gt;\n        \"Services Metadata\"@en . # provide the graph with a label\n&lt;http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC&gt;\n&lt;https://vocab.eccenca.com/di/functions/param_Jdbc_lastOffset&gt;\n    \"0\" . # set the initial offset to zero to start with the first row in the table\n</code></pre> <p>For your project, please:</p> <ol> <li>adjust the CMEM DI Project IRI and</li> <li>the JDBC endpoint IRI.</li> </ol> <p>Import the Graph in the Exploration tab \u2192 Graph (menu) \u2192 Add new Graph \u2192 Provide Graph IRI + Select file</p> <p>In our example, we used the following Graph IRI for the Metadata Graph: http://di.eccenca.com/project/services/metadata</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#3-create-a-transformation-to-dynamically-compose-a-sql-query","title":"3 Create a Transformation to dynamically compose a SQL Query","text":"<p>To extract rows based on the predefined (changing) OFFSET and LIMIT from a table, we need to create a Transformation to compose the SQL with each execution.</p> <ol> <li> <p>Click\u00a0Create (top right) in the data integration workspace and select the type Transformation.</p> <ol> <li>Provide a Label.</li> <li>Provide\u00a0Description (Optional).</li> <li>Select the Services Metadata Graph we previously created.</li> </ol> <p></p> </li> <li> <p>Create only a value mapping with the property sourceQuery. The sourceQuery will be used as an input for the JDBC endpoint. A root mapping does not need to be defined. In this screenshot everything is already configured while yours will be empty when you create it for the first time.</p> <p></p> </li> <li> <p>Press the circular pen button to jump into the advanced mapping editor. As source paths we select the data from our Metadata Graph: table, lastOffset and limit. Everything else is defined as a constant as it does not change in the query. For our source paths we defined a \u201cdi\u201d prefix. In case this defintion is missing, your source path may look longer (full IRI).</p> <p></p> </li> </ol>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#4-create-a-transformation-to-update-the-sql-offset","title":"4 Create a Transformation to update the SQL Offset","text":"<p>Each time we execute the transformation, we want to forward the OFFSET in our SQL Query to extract the next rows. As an example, we have a start OFFSET of 0 and LIMIT of 5. After one execution we want to have an OFFSET of 5, after another execution an OFFSET of 10 and so on. In this tutorial, we assume that the table contains an ID column which incrementally increases by 1 in each row.</p> <p>To store the updated OFFSET, we update the triple with a SPARQL Update query:</p> <ol> <li>Press the Create button (top right) in the data integration workspace and select the type Transformation<ol> <li>Provide a Label</li> <li>Provide a Description (optional)</li> <li>Paste the query into the SPARQL update query form.<ol> <li>The following IRIs need to be adapted for your use cases:<ol> <li>Service Metadata Graph</li> <li>JDBC endpoint <code>jdbc_table_data_config</code></li> <li>Knowledge Graph <code>http://ld.company.org/services/</code></li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>This query will look for the last max service ID found in the Knowledge Graph and update the OFFSET information in the Metadata Graph.</p>","tags":["ExpertTutorial"]},{"location":"build/loading-jdbc-datasets-incrementally/#last-offset","title":"Last Offset","text":"<pre><code>PREFIX service_metadata_graph: &lt;http://di.eccenca.com/project/services/metadata&gt;\nPREFIX jdbc_table_dataset_config: &lt;http://dataintegration.eccenca.com/00e0ed25-e76b-42f2-a37d-22b773431210_IncrementalJDBCdatasetload/8d0e4895-1d45-442f-8fd8-b1459ec3dbde_ServicesTableJDBC&gt;\nPREFIX func: &lt;https://vocab.eccenca.com/di/functions/param_Jdbc_&gt;\nPREFIX prod: &lt;http://ld.company.org/prod-vocab/&gt;\nWITH service_metadata_graph:\nDELETE { jdbc_table_dataset_config: func:lastOffset ?lastOffset .}\nINSERT { jdbc_table_dataset_config: func:lastOffset ?newOffset . }\nWHERE {\n{\nSELECT ?lastOffset\nWHERE {\nGRAPH service_metadata_graph: {\njdbc_table_dataset_config: func:lastOffset ?lastOffset\n}\n}\n}\n{\nSELECT (max(?id) as ?newOffset)\nWHERE{\nGRAPH &lt;http://ld.company.org/services/&gt; {\n?services a prod:Service .\n?services prod:id ?id .\n}\n}\n}\n}\n</code></pre> <p>Finally, we can build a Workflow which demonstrates how each step works.</p> <p>We compose the SQL query based on the OFFSET and LIMIT information in our Metadata Graph. This SQL query will be used to configure the sourceQuery of the JDBC endpoint. Next, we do a \u201cregular\u201d transformation of data from a JDBC endpoint to RDF. As this step was omitted here, please feel free to read how this Transformation can be built here: Lift data from tabular data such as CSV, XSLX or database tables. As a final step, we use our SPARQL update query to select the max service ID in our Knowledge Graph and update the RDF Triples in our Metadata Graph accordingly.</p> <p></p>","tags":["ExpertTutorial"]},{"location":"build/rule-operators/","title":"Rule Operators","text":"","tags":["Reference"]},{"location":"build/rule-operators/#introduction","title":"Introduction","text":"<p>This page outlines the basic operators that can be used to build linkage and transformation rules.</p> <p>Transformation rules are trees that consist of two types of operators:</p> <ul> <li>Path Operator: Retrieves all values of a specific property path of each entity, such as its label property. The purpose of the path operator is to enable access to values from the dataset.</li> <li>Transformation Operator: Transforms the values of path or transformation operators according to a specific data transformation function.</li> </ul> <p>Linkage rules may use two additional operator types in addition:</p> <ul> <li>Comparison Operator: Evaluates the similarity between two entities based on the values that are returned by two path or transformation operators by applying a distance measure and a distance threshold. Examples of distance measures include Levenshtein, Jaccard, or geographic distance.</li> <li>Aggregation Operator: Due to the fact that, in most cases, the similarity of two entities cannot be determined by evaluating a single comparison, an aggregation operator combines the similarity scores from multiple comparison or aggregation operators into a single score according to a specific aggregation function. Examples of common aggregation functions include the weighted average or yielding the minimum score of all operators.</li> </ul>","tags":["Reference"]},{"location":"build/rule-operators/#path-operator","title":"Path Operator","text":"<p>A path operator retrieves all values which are connected to the entities by a specific path. Every path statement consists of a series of path elements. If a path cannot be resolved due to a missing property or a too restrictive filter, an empty result set is returned.</p> <p>The following operators can be used to traverse the dataset:</p> Operator Example Description <code>/&lt;property&gt;</code> <code>/dbpedia:director/rdfs:label</code> Moves forward from a subject resource through an operator property to its value(s). <code>\\&lt;property&gt;</code> <code>\\dbpedia:artist</code> Moves backward from a subject resource through an operator property to its value(s). <code>[&lt;property&gt; &lt;comp_operator&gt; value]</code> <code>/dbpedia:work[rdf:type = dbpedia:Album]</code> Filters the currently selected values using a filter expression. <code>comp_operator</code> may be one of <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code> <code>[@lang = 'lang']</code> <code>/rdfs:label[@lang = 'en']</code> Filter literal values by their language.","tags":["Reference"]},{"location":"build/rule-operators/#transformation-operator","title":"Transformation Operator","text":"<p>As different datasets usually use different data formats, a transformation can be used to normalize the values prior to comparison.</p> <p>Examples of transformation functions include case normalization, tokenization or concatenation of values from multiple operators. Multiple transformation operators can be nested in order to apply a chain of transformations.</p>","tags":["Reference"]},{"location":"build/rule-operators/#comparison-operator","title":"Comparison Operator","text":"<p>A comparison operator evaluates two inputs and computes the similarity based on a user-defined distance measure and a user-defined threshold.</p> <p>The distance measure always outputs 0 for a perfect match, and a higher value for an imperfect match. Only distance values between 0 and the threshold will result in a positive similarity score. Therefore it is important to know how the distance measures work and what the range of their output values is in order to set a threshold value sensibly.</p> <p>The following parameters can be set for each comparison:</p> Parameter Description required If required is true, the parent aggregation only yields a confidence value if the given inputs have values for both instances. weight Weight of this operator in the parent aggregation. The weight is used by some aggregations such as the weighted average aggregation. threshold The maximum distance. For normalized distance measures, the threshold should be between 0.0 and 1.0. <p></p> <p>The threshold is used to convert the computed distance to a confidence between -1.0 and 1.0. Links will be generated for confidences above 0 while higher confidence values imply a higher similarity between the compared entities.</p> <p>If distance measures generate multiple distance scores the lowest is used to generate the confidence.</p>","tags":["Reference"]},{"location":"build/rule-operators/#aggregation-operator","title":"Aggregation Operator","text":"<p>An aggregation combines multiple confidence values into a single value. In order to determine if two entities are duplicates it is usually not sufficient to compare a single property. For instance, when comparing geographic entities, an aggregation may aggregate the similarities between the names of the entities and the similarities based on the distance between the entities.</p> <p>If an aggregation is fed with missing values (e.g., if inputs paths returned no values), the behavior is as follows:</p> <ul> <li>Boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d.</li> <li>Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing.</li> <li>If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.</li> </ul>","tags":["Reference"]},{"location":"build/workflow-reconfiguration/","title":"Workflow Reconfiguration","text":"","tags":["Workflow"]},{"location":"build/workflow-reconfiguration/#introduction","title":"Introduction","text":"<p>The operators of a workflow can be reconfigured completely in the context of a workflow.\u00a0During its execution, new parameters are loaded from any possible source and translated by a transformation task to allow an injection into the dataset configuration that overwrites originally set parameters. To reconfigure a workflow operator, the transformation task has to be connected to the red dot at the top of this operator as shown in the following image:</p> <p></p> <p>Although this feature has been developed to support the ingestion of database deltas, the possible applications are various since any parameter can be overwritten to make workflow operators even more dynamic and reusable in various contexts. The incremental ingestion of database content that was implemented as a first use-case can be found the application section of this page. However, we intend to add other use-cases that have been implemented. The following parameters seem to be good starting points for possible applications:</p> <ul> <li>Transformation Task:<ul> <li>Source Type</li> <li>Source Restriction</li> </ul> </li> <li>JDBC endpoint (remote)<ul> <li>Source Query</li> <li>Write Strategy</li> <li>Restriction</li> </ul> </li> <li>Knowledge Graph (embedded)<ul> <li>Clear Graph before workflow execution</li> </ul> </li> <li>Scheduler<ul> <li>Interval</li> <li>Enabled</li> </ul> </li> <li>\u2026</li> </ul>","tags":["Workflow"]},{"location":"build/workflow-reconfiguration/#implementation","title":"Implementation","text":"<p>To reconfigure a workflow operator, you need to create a transformation task, the data source of which is the intended source of the dynamic parameters of the workflow operator. Once you have created this task, you need to create a data value mapping for each parameter you want to overwrite.</p> <p>Info</p> <p>Only one transformation task can be used to reconfigure the workflow operator and one source can be used for a transformation task\u2019s source. Thus, it is necessary to pre-process all parameters that need to be rewritten into one single dataset, e.g. a CSV file or a in-memory dataset. Then, you can use this dataset to inject all parameters with one transformation task.</p> <p>Once you are sure, that your mapping rule entails the correct value, you can set the workflow operator parameter as the target property of the mapping rule. After this is done, you can reconfigure any workflow operator that uses this parameter as part of its configuration.</p> <p>Info</p> <p>The transformation task needs a suffix of the workflow parameter\u2019s URI in the workflow operator\u2019s serialization as its target property. This differs from the documentation that just refers to the parameter\u2019s name. If you want to overwrite the source query of a JDBC endpoint, you need to define <code>sourceQuery</code> as the target property, which is the suffix of <code>&lt;https://vocab.eccenca.com/di/functions/param_Jdbc_sourceQuery&gt;</code>.</p>","tags":["Workflow"]},{"location":"build/workflow-reconfiguration/#applications","title":"Applications","text":"<p>Tutorials that showcase this function in an application context:</p> <ul> <li>Loading JDBC datasets incrementally</li> </ul>","tags":["Workflow"]},{"location":"consume/","title":"Consume","text":""},{"location":"consume/#consume","title":"Consume","text":"<p>This section outlines how to consume data from Corporate Memory Knowledge Graphs. While there are several options to retrieve information from the Knowledge Graph, the most direct way is to issue SPARQL queries. SPARQL queries can be managed and executed in the Query Module UI. External applications may access the query catalog and execute queries through the REST API directly or more conveniently by using the cmemc - Command Line Interface. Since not all applications allow the direct use of SPARQL, this section includes tutorials to access Knowledge Graphs using BI tools (such as Power BI) as well as relational databases.</p> <p> Intended audience: Linked Data Experts</p> <ul> <li> <p> Power BI</p> <p>Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector.</p> </li> <li> <p> Redash</p> <p>Create Dashboards based on your Knowledge Graphs with the open-source application Redash.</p> </li> <li> <p> SQL Databases</p> <p>If direct access to the knowledge graph is not sufficient, fragments of the Knowledge Graph may also be pushed into external SQL databases.</p> </li> <li> <p> Custom APIs</p> <p>Learn how to provide data via a customized Corporate Memory API in a text format of your choice and how to consume it in your applications.</p> </li> <li> <p> Neo4j</p> <p>Learn how to populate graphs to Neo4j.</p> </li> <li> <p> Apache Kafka</p> <p>Use a Apache Kafka Producer in order to export parts of your Knowledge Graph as a message stream.</p> </li> </ul>"},{"location":"consume/consume-graphs-in-apache-kafka/","title":"Populate Graphs to Apache Kafka","text":"","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consume-graphs-in-apache-kafka/#introduction","title":"Introduction","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. Kafka is widely used in enterprises for data pipelines, streaming analytics, data integration and other applications.</p> <p>By using the cmem-plugin-kafka Python Plugin, you can produce and send messages to Apache Kafka from inside of our Corporate Memory Workflows.</p>","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consume-graphs-in-apache-kafka/#installation","title":"Installation","text":"<p>In order to use the Kafka Producer workflow task, you need to extend your Corporate Memory instance with the <code>cmem-plugin-kafka</code> package. This can be done by using cmemc:</p> Installing cmem-plugin-kafka on the instance 'my-cmem'<pre><code>$ cmemc -c my-cmem admin workspace python install cmem-plugin-kafka\nInstall package cmem-plugin-kafka ... done\n</code></pre> <p>You can validate your installation by listing all installed plugins (from all packages):</p> <pre><code>$ cmemc -c my-cmem admin workspace python list-plugins\nID                                 Package ID         Type            Label\n---------------------------------  -----------------  --------------  ---------------------------------\ncmem_plugin_kafka-ReceiveMessages  cmem-plugin-kafka  WorkflowPlugin  Kafka Consumer (Receive Messages)\ncmem_plugin_kafka-SendMessages     cmem-plugin-kafka  WorkflowPlugin  Kafka Producer (Send Messages)\n</code></pre>","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consume-graphs-in-apache-kafka/#usage","title":"Usage","text":"<p>Once you installed the package, you can use the Kafka Producer by simply creating a new task, e.g. search for <code>kafka</code> in the Create new item screen:</p> <p></p> <p>Follow the in-app documentation on how to configure the task (e.g. for providing credentials or preparing data to be sent in messages).</p>","tags":["Automate","KnowledgeGraph","PythonPlugin"]},{"location":"consume/consuming-graphs-in-power-bi/","title":"Consuming Graphs in Power BI","text":"","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#introduction","title":"Introduction","text":"<p>Learn how to consume data from your Corporate Memory Knowledge Graph with our Microsoft Power-BI-Connector.</p> <p>This manual and tutorial describes how you can consume data from your knowledge graph in Microsoft Power BI through our Corporate Memory Power-BI-Connector.</p> <p>Power BI is a business analytics service by Microsoft. It aims to provide interactive visualizations and business intelligence capabilities with an interface simple enough for end users to create their own reports and dashboards. Power BI can be obtained from the official Microsoft page  and/or in the Windows Software Store.</p> <p>The latest (unsigned) version of our Power-BI-Connector is available from its source repository a version signed by eccenca is available with each Corporate Memory release.</p> <ul> <li>eccenca github.com repository (unsigned .mez file) </li> <li>eccenca Corporate Memory Releases (signed .pqx file)</li> <li>Thumbprint of the signature: FB6C562BD0B08107AAA420EDDE94507420C7FE1A</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#installation","title":"Installation","text":"<ul> <li>Download the <code>.pqx</code> or <code>.mez</code> file from the locations linked above.</li> <li>Move the file into the folder <code>Documents\\Power BI Desktop\\Custom Connectors</code> .</li> <li>Create the folder if it does not exist.</li> <li>In case you are running Windows on Parallels Desktop: Do not use the Local <code>Disk\\Users\\UserName\\Documents</code> folder but your shared folder with macOS.</li> <li>Register the Thumbprint (for .pqx) or setup PowerBI Desktop to allow any 3rd party connector (for .pqx or .mez) (we recommend to register the Thumbprint)</li> </ul> Setup Register Thumbprint (.pqx)Allow 3rd Party Connectors (.mez and .pqx) <ul> <li>In order to allow the eccenca Corporate Memory Power-BI-Connector in your Power BI Desktop installation you need to register the Thumbprint of the file signature in the windows registry.</li> <li>Cf. official Microsoft documentation<ul> <li>The registry path is <code>HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Power BI Desktop</code> . Make sure the path exists, or create it.</li> <li>Add a new value under the path specified above. The type should be \u201cMulti-String Value\u201d ( <code>REG_MULTI_SZ</code> ), and it should be called <code>TrustedCertificateThumbprints</code></li> <li>Add the thumbprints of the certificates you want to trust. You can add multiple certificates by using \u201c\\0\u201d as a delimiter, or in the registry editor, right click \u2192 modify and put each thumbprint on a new line. </li> <li>(Re-)Start Power BI Desktop</li> </ul> </li> </ul> <p>If you wish to automate this setup you can use the reg windows command line tool to make this entry like:</p> <pre><code>REM list existing entries in Power BI Desktop &gt; TrustedCertificateThumbprints\nreg query \"\"HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Power BI Desktop\" /v TrustedCertificateThumbprints\nREM add eccenca Corporate Memory Power-BI-Connector Thumbprint\nreg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Power BI Desktop\" /v TrustedCertificateThumbprints /t REG_MULTI_SZ /d FB6C562BD0B08107AAA420EDDE94507420C7FE1A\n</code></pre> <ul> <li>In case you are using the .mez (works for .pqx file too) file or simply want to trust any third party connector extension</li> <li>(Re-)Start Power BI Desktop, go to <code>File \u2192 Options and settings \u2192 Options \u2192 Security</code></li> <li>Under Data Extensions, select(Not Recommended). Allow any extension to load without validation or warning.</li> <li>Select OK, and then restart Power BI Desktop. </li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#add-a-data-source","title":"Add a Data Source","text":"<p>Use the Power-BI-Connector to login with your Corporate Memory Instance:</p> <ul> <li>Open Power BI Desktop</li> <li>Click <code>Edit Queries \u2192 New Source</code> (or directly Get <code>Data</code>) </li> <li>In the dialog search for eccenca <code>Corporate Memory</code>, which is listed in the <code>Database</code> category </li> <li>Select the connector and click <code>Connect</code></li> <li>Read and accept the 3rd party connector notification </li> <li>In the following dialog you need to specify the connection and information and access credentials, ask your Corporate Memory administrator for assistance if you miss any of the requested details. You have the option to use username + password or a client secret for login. In case of a custom setup is used advanced configuration can be provided:</li> </ul> Access-configuration Username + PasswordClientAdvanced Configuration <p>In order to use username +  password based login you need to fill the details shown below:</p> <ul> <li>First Step<ul> <li>Corporate Memory Base URI</li> <li>Grant type = password</li> <li>Client ID   </li> </ul> </li> <li>Second Step<ul> <li>Password / Client Secret   </li> </ul> </li> </ul> <p>In order to use Client Secret based login you need to fill the details shown below:</p> <ul> <li>First Step<ul> <li>Corporate Memory Base URI</li> <li>Grant type = client_credentials</li> <li>Client ID   </li> </ul> </li> <li>Second Step<ul> <li>Password / Client Secret</li> </ul> </li> </ul> <p>In case you installation uses a custom service endpoint layout the individual URIs for DataPlatform and Keycloak can be configured individually. The configuration keys are the same as for cmemc.</p> <ul> <li>The following configuration parameter can be provided:<ul> <li><code>DP_API_ENDPOINT</code> - specifies the DataPlatform URI</li> <li><code>OAUTH_TOKEN_URI</code> - specifies the keycloak token URI</li> <li><code>SSL_VERIFY</code> - can be used to set certificate verification to <code>False</code> </li> </ul> </li> <li>In case a <code>Corporate Memory Base URI</code> is configured too, the values from the <code>Config ini</code> section take precedence</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-in-power-bi/#get-data","title":"Get Data","text":"<p>With the eccenca Corporate Memory Power-BI-Connector you can load data from SELECT queries stored in the query catalog of Corporate Memory. You can use queries without or with placeholders. The steps are different depending if your query uses placeholder:</p> Queries Without placeholdersWith placeholders <ul> <li><code>SELECT</code> queries that use no placeholders are shown with a table icon (e.g. )</li> <li>When selected a preview will be loaded.</li> <li>Check the one(s) you want to load and click <code>OK</code>.</li> <li>The tables will be added to your list of <code>queries</code> and to the <code>fields</code> inventory in Power BI</li> <li>Start using your data in transformations, dashboards and analytics </li> </ul> <ul> <li>SELECT queries that take placeholder arguments are shown with a function icon. (e.g. ) </li> <li>You need to be in <code>Edit Queries</code> mode in Power BI so you can enter the required query parameter.</li> <li>Check the one(s) to be added.</li> <li>Power BI will add the selected query as a <code>query</code> entry.</li> <li>Click \u201cTransform Data\u201d in order to fill in the parameter. </li> <li>This adds a new entry to the list of Power BI queries, which contains the actual data you requested. The new entry will be named \u201cInvoked Function\u201d.</li> <li>It is recommended to rename this automatic generated name to a more speaking one. Right click on \u201cInvoked Function\u201d or select \u201cInvoked Function\u201d and press F2.</li> <li>Rename the table (e.g. to \u201c_search via regex match\u201d). Click \u201cClose &amp; Apply\u201c to save changes. </li> </ul> Hint <p>You can call the function multi times with different parameter values to get different result tables into Power BI.</p> <ul> <li>Start using your data in transformations, dashboards and analytics</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/","title":"Consuming Graphs in Redash","text":"","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/#introduction","title":"Introduction","text":"<p>Redash is an open-source tool designed to help data scientists and analysts visualize and build interactive dashboards of their data. Beside creating Dashboards, users can configure alerts in order to get mails on specific data events. In 03/2021 Redash has added an eccenca Corporate Memory query runner to its core repository, which enables Redash users to query Corporate Memory instances and build visualisations and dashboards based on Knowledge Graphs.</p>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/#adding-an-eccenca-corporate-memory-data-source","title":"Adding an eccenca Corporate Memory Data Source","text":"<p>The key to query your Knowledge Graphs with Redash is to add a new eccenca Corporate Memory data source. To do so, open the Settings &gt; Data Sources Tab, and search for the right type:</p> <p></p> <p>Click on it and you will come into another screen where you have to enter location and access data:</p> <p></p> <p>This configuration screen basically clones the basic configuration of cmemc:</p> <ul> <li>Name is a human friendly identifier for the source,</li> <li>Base URL refers to <code>CMEM_BASE_URI</code>,</li> <li>Client ID refers to <code>OAUTH_CLIENT_ID</code>,</li> <li>Client Secret refers to <code>OAUTH_CLIENT_SECRET</code>.</li> </ul>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-redash/#query-your-knowledge-graph-and-create-dashboards","title":"Query your Knowledge Graph and Create Dashboards","text":"<p>Once you added a eccenca Corporate Memory data source to Redash, you can create queries, configure visualisation widgets based on the query results, and combine these widgets as dashboards.</p> <p>To get familiar with Redash, please have a look at the Redash user guide, especially the Getting Started page.</p> <p>Info</p> <p>In order to query eccenca Corporate Memory data sources in Redash, you have to formulate your query with SPARQL: </p>","tags":["Dashboards","KnowledgeGraph"]},{"location":"consume/consuming-graphs-with-sql-databases/","title":"Consuming Graphs with SQL Databases","text":"","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#introduction","title":"Introduction","text":"<p>If direct access to the Knowledge Graph is not sufficient, fragments of the Knowledge Graph may also be pushed into external SQL databases. While in general all supported databases can be written into, optimized writing support is available and packaged for MySQL and MariaDB.</p> <p>See the documentation of the JDBC dataset for more details.</p>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#writing-a-single-table-into-a-sql-database","title":"Writing a single table into a SQL database","text":"<p>Three buildings blocks are required in eccenca DataIntegration to write into a remote SQL database:</p> <ul> <li>A dataset that allows access to the Knowledge Graph.</li> <li>A transformation that builds tables from specified resources in the Knowledge Graph.</li> <li>A dataset that configures access to the SQL database using JDBC.</li> </ul> <p>A simple workflow to write the contents of a Knowledge Graph into an SQL database looks like this:</p> <p></p> <p>In the following, we have a more detailed look at each of the three operators.</p>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#create-knowledge-graph-dataset","title":"Create Knowledge Graph dataset","text":"<p>Create a dataset of the type Knowledge Graph (embedded) and set the graph parameter to the URI of the graph that contains the resources to be exported:</p> <p></p>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#create-transformation","title":"Create Transformation","text":"<p>Create a transformation that covers the type of the RDF resources to be exported into a table:</p> <p></p> <p>For each column of the target table add a value mapping to the transformation:</p> <p> </p> Basic Mapping <p>The shown transformation will create a table with two columns:</p> <ul> <li>A column name that contains values of the property foaf:name.</li> <li>A column runtime that contains values of the property dbpediaow:runtime.</li> </ul>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#create-sql-dataset","title":"Create SQL dataset","text":"<p>Create a dataset of type JDBC endpoint (remote):</p> <p></p> <p>The most relevant parameters are:</p> <ul> <li>The JDBC Driver Connection URL should contain the database-specific JDBC URL.</li> <li>The table parameter defines the name of the table to be written.</li> <li>The write strategy specifies the behavior for the case that the configured table already exists in the target SQL database.</li> </ul>","tags":["ExpertTutorial"]},{"location":"consume/consuming-graphs-with-sql-databases/#writing-multiple-tables-into-a-sql-database","title":"Writing multiple tables into a SQL database","text":"<p>If multiple tables should be written from several types of resources, there are two options:</p> <ol> <li>If the types are connected by properties, a single transformation with multiple object mappings can be used. The root mapping will write the table specified by the SQL dataset. Each object mapping writes an additional table. The name of the table is generated based on the target type which is defined in the object mapping.</li> <li>If the types are not directly connected by properties, multiple transformations can be created. Note that for each transformation a separate target SQL dataset needs to be created since the table name is specified in it.</li> </ol>","tags":["ExpertTutorial"]},{"location":"consume/populate-data-to-neo4j/","title":"Populate Data to Neo4j","text":"","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#introduction","title":"Introduction","text":"<p>This tutorial walks you through the process of\u00a0using\u00a0the\u00a0Neo4j dataset. You will learn how to transform data from a source dataset into a graph structure\u00a0into Neo4j.</p> <p>Abstract</p> <p>This tutorial uses a specific dataset called Northwind for your understanding. However, the principles can be applied and reused with any dataset. All the source data for this project is stored in a\u00a0Multi CSV Zip file:\u00a0northwind.zip.</p> <p>The vocabulary used in the tutorial can be found here: schema.ttl.</p> <p>Download these files to have them available before you start the tutorial.</p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#1-load-the-source-dataset","title":"1 Load the source dataset","text":"<p>eccenca\u00a0Corporate Memory provides the framework to create Resource Description Framework (RDF) tuples from non-Graph data sources such as tabular (CSV, Excel, SQL) or hierarchical (XML, JSON) sources. Additionally, Corporate Memory supports Neo4j as a source of or target for data. The Northwind dataset is available on github. This tutorial uses the Multi CSV Zip format to efficiently handle the following CSV files:</p> <ul> <li>employees.csv</li> <li>orders.csv</li> <li>products.csv</li> <li>categories.csv</li> <li>Suppliers.csv</li> </ul> <p>To upload multiple files together as an input:</p> <ol> <li>In Corporate Memory, click Projects under BUILD in the navigation on the left side of the page.     </li> <li>Click Create at the top of the page.\u202f</li> <li>In Create new item window, select Project and click Add.\u00a0The Create new item of type Project window appears.</li> <li>Fill in the required details such as Title and Description. \u00a0Alternatively, import the existing project by clicking Import Project File and selecting the file from your system.\u202f</li> <li>Click Create. Your project (Northwind) is created.     </li> <li>In your project, click Create Item.</li> <li>In the Create new item window, select Multi CSV ZIP and click Add.     </li> <li>Specify a Label of the dataset in the Create new item of type Multi CSV ZIP window.</li> <li>Select the Upload new file option as you have the files. The Multi CSV ZIP file containing the above-listed files is available here. If it is an existing project, you can select the files from the project. For the remaining parameters, the default settings are used.</li> <li>Click Create. You can see the message northwind.zip was successfully uploaded in Green.</li> <li>You can see the Multi CSV ZIP file is uploaded with the datasets, and the item has been created.     </li> <li>Click the Play button and review the dataset in the Data preview section. You can see the contents of the loaded zip file consisting of the CSV files introduced above.</li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#2-create-a-neo4j-dataset","title":"2 Create a Neo4j dataset","text":"<p>A Neo4j dataset holding a Labeled Property Graph (LPG) representation is one of the outputs of the process. Perform the following steps to create a Neo4j dataset:</p> <ol> <li>In your existing project, click Create to create a new item.\u00a0</li> <li>In the item category Dataset select Neo4j.     </li> <li>Click Add.</li> <li>Enter the following details:<ul> <li>Label: Name of the item.\u202f\u00a0</li> <li>Uri: URL of the Neo4j instance.\u202f\u00a0</li> <li>User: Username of the instance.</li> <li>Password: Password of the instance.</li> </ul> </li> <li>Click Create.</li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#3-register-the-vocabulary","title":"3 Register the Vocabulary","text":"<p>The vocabulary contains the classes and properties in order to map the source data into a new (domain specific) structure. In this case we will create an RDF and LPG Knowledge Graph. A vocabulary for this tutorial is available as schema.ttl. Follow\u00a0Lift data from tabular data such as CSV, XSLX or database tables to learn how to register your vocabulary in Corporate Memory.</p> <p>The provided vocabulary is inspired by this structure from the original Neo4j tutorial on the Northwind data:</p> <p></p> <p>The vocabulary for the Northwind project can be visualized like this:</p> <p></p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#4-create-transformations","title":"4 Create Transformations","text":"<p>A transformation defines how the input datasets are transformed into output datasets. In this example, each of the CSV files\u00a0is\u00a0represented\u00a0as nodes with the same labels in the application. The following relationships must be established between the nodes:</p> <p>To register the nodes and relationships, perform the following:</p> <ol> <li>Create nodes\u202f\u00a0</li> <li>Establish the relationships through\u00a0transformations</li> </ol> EmployeesOrdersProductsCategorySuppliers","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#employee","title":"Employee","text":"<ol> <li>Click\u00a0Create\u00a0in your project.\u202f\u00a0</li> <li>On the\u00a0Create New Item\u00a0window, select\u00a0Transform\u00a0and click\u00a0Add\u00a0to create a new transformation.</li> <li>In the\u00a0Create new item of type Transform\u00a0window, select your project from the dropdown and click\u00a0Create.</li> <li>For this tutorial, enter the following:<ul> <li>Label: Northwind Employees</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select employees.csv</li> </ul> </li> <li>Click\u00a0Create. A Transformation Northwind Employees is created.</li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-value-mapping","title":"Add Value Mapping","text":"<ol> <li>On the Mapping Editor, expand the\u00a0Root Mapping\u00a0section and click\u00a0Edit.</li> <li> <p>If the vocabulary is pre-defined, specify the\u202fTarget entity type\u202ffrom the vocabulary. Else, specify the Target entity type and click\u00a0Create\u00a0option. In this example, specify the following:</p> <ul> <li>Target entity type:\u202f<code>nw:Employee</code></li> <li> <p>URI pattern:\u00a0<code>urn:empl-{EmployeeID}</code></p> <p>{EmployeeID}\u202fis a placeholder that points to the column name\u00a0of the\u00a0specific dataset.</p> </li> </ul> </li> <li> <p>Click\u202f&gt;\u00a0button\u202fto evaluate the mapping. The examples of the three\u00a0generated base URIs are as follows:\u00a0</p> </li> <li>Click\u00a0Save. The\u00a0Employee node is created.\u00a0\u00a0Now, you have to add the value mapping to indicate the basis on which the current CSV file must be transformed.\u202f\u00a0</li> <li>Click the\u00a0circular blue button on the lower right and\u00a0select\u202fAdd value mapping.</li> <li>Define the\u202fTarget property,\u00a0Data type,\u00a0Value path\u202f(column name), and a\u00a0Label\u202ffor value mapping. In this example, specify the following:<ul> <li>Target Property:\u00a0<code>rdfs:label</code></li> <li>Datatype: \u202fStringValueType</li> <li>Use a complex transformation to concatenate Last name and First name\u00a0fields to ensure that the rows are mapped individually</li> </ul> </li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#orders","title":"Orders","text":"<p>Create another transformation using the node Orders.\u202f\u00a0</p> <ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Order Transformation.</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, specify the following:<ul> <li>Label: Northwind Orders</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select Orders.csv</li> </ul> </li> </ol> <p>Click\u00a0Create. A transformation for Northwind Orders is created.</p> <p>To add a value mapping, repeat the steps followed while adding the mapping to\u00a0the\u00a0Employees node. In this example, for the orders node specify the following:</p> <ul> <li>Target entity type:\u202f<code>nw:Order</code> </li> <li> <p>URI pattern:\u00a0<code>urn:order-{OrderID}</code></p> <p>{OrderID} is a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-object-mapping","title":"Add Object Mapping","text":"<p>In this example, the Employee and Orders nodes have a relationship SOLD. To create the relationship, create an\u00a0Object mapping.</p> <ol> <li>Click the\u00a0circular blue button on the lower right and\u00a0select\u202fAdd object mapping<ul> <li>Target property:\u202f<code>nw:sold</code></li> <li>Target entity type:\u00a0<code>nw:Employee</code></li> <li>URI pattern:\u00a0<code>urn:empl-{EmployeeID}</code> </li> </ul> </li> <li>Select\u00a0Connect from Orders\u00a0as this relationship is\u00a0from Orders to the Employee.</li> </ol> <p>Info</p> <p>The URI (pattern) must be the same URI specified in the Employee transform, <code>urn:empl-{EmployeeID}</code> in this example.</p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#products","title":"Products","text":"<p>Create another transformation using the node Orders.</p> <ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Order Transformation.\u202f\u00a0</li> <li>In the\u00a0Create new item of type Transform\u00a0window, specify the following for this tutorial:<ul> <li>Label: Northwind Products</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select products.csv</li> </ul> </li> </ol> <p>Click\u00a0Create. A transformation for Northwind Products is created.</p> <ol> <li>To add value mapping define, repeat the steps followed while adding the mapping to\u00a0the\u00a0Employees node. In this example, for\u00a0the\u00a0Products node, specify the following:<ul> <li>Target entity type:\u202f<code>nw:Product</code></li> <li> <p>URI pattern:\u00a0<code>urn:prod-{ProductID}</code></p> <p>{ProductID}\u202fis a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul> </li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#category","title":"Category","text":"<ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Category Transformation.\u202f\u00a0</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, specify the following:<ul> <li>Label: Northwind Category</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select categories.csv</li> </ul> </li> </ol> <p>Click\u00a0Create. A Transformation for Northwind Category is created.</p> <p>To add value mapping, repeat the steps followed while adding the mapping to the Employees node. In this example, specify the following for the category node:</p> <ul> <li>Target entity type:\u202f <code>nw:Category</code></li> <li> <p>URI pattern:\u00a0<code>urn:cat-{CategoryID}</code></p> <p>{CategoryID}\u202fis a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-object-mapping_1","title":"Add Object Mapping","text":"<p>In this example, the Product node is connected to the Category node\u00a0through the relationship,\u00a0partOf. To create this, click add object mapping.</p> <p>For this example, specify the following:</p> <ul> <li>Target property: <code>nw:partOf</code></li> <li>Target entity type: <code>nw:Category</code></li> <li>URI pattern: <code>urn:cat-{CategoryID}</code></li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#suppliers","title":"Suppliers","text":"<ol> <li>Repeat steps from 1 to 3 in\u00a0Create Employees Transforms\u00a0section to create the Suppliers Transformation.\u202f</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, specify the following:<ul> <li>Label: Northwind Suppliers</li> <li>Dataset: Select the previously uploaded Northwind dataset from the dropdown.</li> <li>Type: Select suppliers.csv</li> </ul> </li> <li>Click\u00a0Create. A Transformation for Northwind Suppliers is created.</li> </ol> <p>To add value mapping, repeat the steps followed while adding the mapping to\u00a0the\u00a0Employees node. In this example, for\u00a0suppliers\u00a0node, specify the following:</p> <ul> <li>Target entity type: <code>nw:Supplier</code></li> <li> <p>URI pattern: <code>urn:suppl-{SupplierID}</code></p> <p>{SupplierID} is a placeholder that points to the column name specified in the specific dataset.</p> </li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#add-object-mapping_2","title":"Add Object Mapping","text":"<p>In this example, the Product node is connected to the Supplier node through the relationship, SUPPLIES. As the supplier supplies to the Product, navigate to Product transform and then click Add Object\u00a0Mapping. To create this, click\u00a0Add object mapping.\u00a0For this example, specify the following:</p> <ul> <li>Target property: <code>nw:supplies</code></li> <li>Select Connect to Product</li> <li>Target entity type:\u00a0<code>nw:Suppliers</code></li> <li>URI pattern: <code>urn:suppl-{SupplierID}</code></li> </ul>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#5-create-a-workflow","title":"5 Create a workflow","text":"<p>To integrate all the transformations, perform the following steps:</p> <ol> <li>Navigate to the project.</li> <li>Click\u00a0Create</li> <li>In the\u00a0Create new item\u00a0window, select\u00a0Workflow\u00a0and click\u00a0Add.\u00a0</li> <li>In the\u00a0Create new item of type Transform\u00a0window, for this tutorial, enter the following:<ul> <li>Label: Northwind Workflow</li> <li>Click\u00a0Create</li> </ul> </li> </ol> <p>In this example, the dataset must be connected to the transformations as an input, and the output should be the Neo4j output.</p> <p>Drag and drop the options available in the Workflow editor pane and link them accordingly.</p> <p></p> <p>Click the Play icon to validate the results. The nodes and edges created are stored in the Neo4j dataset.</p>","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#6-results-in-neo4j","title":"6 Results in Neo4j","text":"","tags":["AdvancedTutorial"]},{"location":"consume/populate-data-to-neo4j/#7-results-in-knowledge-graph","title":"7 Results in Knowledge Graph","text":"<p>Optionally, you can use the same transformation and workflow to render the resulting graph data into a Knowledge Graph.</p> <ol> <li>Add a Knowledge Graph dataset and use this as\u00a0an\u00a0additional target in your workflow:     </li> <li>The results can then be reviewed in the Knowledge Graph module, e.g., explored visually:     </li> </ol>","tags":["AdvancedTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/","title":"Provide Data in any Format via a Custom API","text":"","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#introduction","title":"Introduction","text":"<p>Learn how to provide data via a customized Corporate Memory API in a text format of your choice and how to consume it in your applications. This tutorial describes how you can provide data in a text format of your choice via your own custom Corporate Memory API, and how you request those APIs.</p> <p>In this tutorial, we describe how you can set up an endpoint which provides iCalendar data. If you want to rebuild the example, you can download this iCalendar RDF data and import it into your Corporate Memory instance: ical_data.ttl</p> <pre><code>BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//hacksw/handcal//NONSGML v1.0//EN\nBEGIN:VEVENT\nUID:20020630T230353Z-3895-69-1-0@jammer\nDTSTAMP:20020630T230353Z\nDTSTART:20020630T090000Z\nDTEND:20020630T103000Z\nSUMMARY:Church\nEND:VEVENT\nEND:VCALENDAR\n</code></pre>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#define-a-sparql-query","title":"Define a SPARQL query","text":"<p>This query selects the event data in our graph which will be provided via the customized API. To rebuild the iCalendar format, we need at least the unique identifier (<code>uid</code>), the datetime start (<code>dtstart</code>), the datetime end (<code>dtend</code>), and the summary of the event. The query filters (<code>REPLACE</code>) the special characters <code>:</code> and <code>-</code> at the end as they are not needed in the iCal DateTime format.</p> <pre><code>PREFIX ical: &lt;http://www.w3.org/2002/12/cal/icaltzd#&gt;\nSELECT DISTINCT ?vevent ?uid ?dtstamp ?dtstart ?dtend ?summary\nWHERE {\n?vevent a ical:Vevent .\n?vevent ical:uid ?uid .\n?vevent ical:dtstamp ?dtstamp_raw .\n?vevent ical:dtstart ?dtstart_raw .\n?vevent ical:dtend ?dtend_raw .\n?vevent ical:summary ?summary  .\nBIND(REPLACE(STR(?dtstamp_raw),\"[: -]\",\"\") AS ?dtstamp) .\nBIND(REPLACE(STR(?dtstart_raw),\"[: -]\",\"\") AS ?dtstart) .\nBIND(REPLACE(STR(?dtend_raw),\"[: -]\",\"\") AS ?dtend) .\n}\n</code></pre>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#define-a-template-for-the-ical-format","title":"Define a Template for the iCal format","text":"<p>As a next step, we will define a template that generates iCalendar data from our previously defined SPARQL query.</p> <p>Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Result Template and click <code>Create a new Select Result Template</code> to create a new template.</p> <p></p> <p></p> <p>Define a Name, a Description and the Body format. You may also define a header and/or a footer, however, this is not necessary for this example.</p> <p>The template engine we are using Jinja. In Jinja, dynamic data within a template needs to be referenced via double curly brackets <code>{{...}}</code>. So the line <code>{{result.uid}}</code> inserts at execution time the <code>?uid</code> value from our previously defined SPARQL query into this template. Everything outside curly brackets it static. As static data in our example, we define the full iCalendar format (<code>..BEGIN:EVENT..</code>). As we receive multiple results (iCalendar Events) from the SPARQL query, we have to iterate through each of them. To define this iteration in the template, the following line needs to be added:</p> <pre><code>{% for result in results %}\n</code></pre> <p>and for the conclusion of the iteration, this line needs to be added at the end:</p> <pre><code>{% endfor %}\n</code></pre> <p></p> Jira Template for our iCalendar format<pre><code>BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//hacksw/handcal//NONSGML v1.0//EN\n{% for result in results %}\nBEGIN:VEVENT\nUID:{{result.uid}}\nDTSTAMP:{{result.dtstamp}}\nDTSTART:{{result.dtstart}}\nDTEND:{{result.dtend}}\nSUMMARY:{{result.summary}}\nEND:VEVENT\n{% endfor %}\nEND:VCALENDAR\n</code></pre>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#create-an-api-based-on-your-template","title":"Create an API based on your template","text":"<p>As a next step, we will set up the API which serves the data in the format we defined in the previous template.</p> <p>Select in Graphs the CMEM Query Catalog graph, select in Navigation the Select Query Endpoint and click \u201cCreate a new Select Query Endpoint\u201d to create a new endpoint.</p> <p></p> <p>Define a Name, a human-readable keyword (i.e. the URL Slug) for the API path, specify if it is a Streaming endpoint (false in our example), enter a Description, and select the defined SPARQL Query from our first step and the Template we created in the second step. Once you press save, your endpoint it set up!</p> <p></p>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#consume-data-via-the-endpoint","title":"Consume data via the endpoint","text":"<p>Now that the endpoint is defined, it is possible to make a request to receive the iCal data. The endpoint URL consist of the path <code>/dataplatform/api/custom</code> and the previously defined URL Slug (<code>/ical</code>). cmemc can be used to get the API base URL as well as a valid token:</p> curl request<pre><code>$ curl \"https://$(cmemc -c my config get DP_API_ENDPOINT)/api/custom/ical\" \\\n-H \"Authorization: Bearer $(cmemc -c my admin token)\"\n</code></pre> API Response<pre><code>BEGIN:VCALENDAR\nVERSION:2.0\nPRODID:-//hacksw/handcal//NONSGML v1.0//EN\n\nBEGIN:VEVENT\nUID:20020630T230353Z-3895-69-1-0@jammer\nDTSTAMP:20020630T230353Z\nDTSTART:20020630T090000Z\nDTEND:20020630T103000Z\nSUMMARY:Church\nEND:VEVENT\n\nBEGIN:VEVENT\nUID:20020630T230445Z-3895-69-1-7@jammer\nDTSTAMP:20020630T230445Z\nDTSTART:20020703\nDTEND:20020706\nSUMMARY:Scooby Conference\nEND:VEVENT\n\nBEGIN:VEVENT\nUID:20020630T230600Z-3895-69-1-16@jammer\nDTSTAMP:20020630T230600Z\nDTSTART:20020718T090000\nDTEND:20020718T093000\nSUMMARY:Federal Reserve Board Meeting\nEND:VEVENT\n\nEND:VCALENDAR\n</code></pre> <p>This result is represented in valid ICalendar (<code>ics</code>) format and can be imported into your calendar client (event_data.ics).</p>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#configuration-remarks","title":"Configuration remarks","text":"","tags":["SPARQL","API","ExpertTutorial"]},{"location":"consume/provide-data-in-any-format-via-a-custom-api/#streaming","title":"Streaming","text":"<p>If Is Streaming is set to false for the endpoint (as in the given example), the respective Jinja Template needs to resolve a results variable, which is a list of all query results that needs to be iterated over using Jinja constructs:</p> <pre><code>{% for result in results %}\n</code></pre> <p>A non-streaming result set (the SPARQL query) is limited to 1000 elements. If more results are expected Is Streaming should be set to true.</p> <p>If Is Streaming is set to <code>true</code> the Jinja Template has to resolve a <code>result</code> variable (without the \u2018<code>s</code>\u2019), which is a single query result. The template engine iterates over the results, i.e. the Body template is repeated for each query result.</p>","tags":["SPARQL","API","ExpertTutorial"]},{"location":"deploy-and-configure/","title":"Deploy and Configure","text":"<p>Deploy and configure eccenca Corporate Memory in your own environment.</p> <p>Intended audience: Deployment Engineers and System Administrators.</p> <ul> <li> <p> System Architecture</p> <p>This page describes the overall system architecture of eccenca Corporate Memory and its components.</p> </li> <li> <p> Requirements</p> <p>This page lists software and hardware requirements for eccenca Corporate Memory deployments.</p> </li> <li> <p> Installation</p> <p>These pages describe proven deployment scenarios for eccenca Corporate Memory.</p> </li> <li> <p> Configuration</p> <p>These pages describe specific topics on how to configure eccenca Corporate Memory.</p> </li> </ul>"},{"location":"deploy-and-configure/configuration/","title":"Configuration","text":"<p>This page describes specific topics on how to configure eccenca Corporate Memory.</p> <ul> <li> Access Conditions\u00a0\u2014 Access conditions specify access rights for users and groups to graphs and actions.</li> <li> DataIntegration\u00a0\u2014 This section is intended to be a reference for all available eccenca DataIntegration configuration options.</li> <li> DataManager\u00a0\u2014 This page describes how to configure eccenca DataManager.</li> </ul>"},{"location":"deploy-and-configure/configuration/access-conditions/","title":"Access Conditions","text":"","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#introduction","title":"Introduction","text":"<p>The\u00a0Access control\u00a0module shows the list of all access conditions manageable by your account. Access conditions specify access rights for users and groups to graphs and actions. To open the\u00a0Access control, open the menu  in the Module bar and click\u00a0Access control.</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#access-conditions_1","title":"Access conditions","text":"<p>The main window shows the list of all access conditions manageable by your account. Access conditions specify access rights for users and groups to graphs and actions.</p> <p>Click a specific condition to get an expanded view with more details.</p> <p></p> Expanded view of an access condition","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#adding-a-new-condition","title":"Adding a new condition","text":"<p>To add a new access condition:</p> <ul> <li>Click the context menu  in the upper right</li> <li>Select \u201cCreate access condition\u201d</li> <li>In the dialog box, enter the new access condition rule as needed. Each access condition can have a\u00a0Name\u00a0and a\u00a0Description\u00a0as well as conditions and grants for actions and graphs.</li> </ul> Note <p>The application uses a set of specific URIs with a precise meaning as listed below:</p> Resource Explanation <code>urn:elds-backend-all-graphs</code> Represents all RDF named graphs. You can use it in the\u00a0Allow reading graph\u00a0or\u00a0Allow writing graph\u00a0field. <code>urn:elds-backend-all-actions</code> Represents all actions. You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:elds-backend-public-group</code> Represents the group which every user is member of (incl. anonymous users). You can use it in the\u00a0Requires group\u00a0field. <code>urn:elds-backend-anonymous-user</code> Represents the anonymous user account. You can use it in the\u00a0Requires account\u00a0field. <code>urn:elds-backend-actions-revision-api</code> Represents the Revision API (see the Developer Manual). You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:elds-backend-actions-auth-access-control</code> Represents the Authorization management API (see the Developer Manual). You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:eccenca:di</code> Represents the action needed to use the eccenca DataIntegration component of eccenca Corporate Memory.\u00a0You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:eccenca:ThesaurusUserInterface</code> Represents the action needed to use the Thesaurus Catalog as well as Thesaurus Project editing interface (needs access to specific thesaurus graphs as well).\u00a0You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:eccenca:AccessInternalGraphs</code> Represents the action needed to list Corporate Memory Internal graphs in the exploration tab.\u00a0You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:eccenca:QueryUserInterface</code> Represents the action needed to use the Query Catalog (needs access to catalog graph as well if changes should be allowed).\u00a0You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:eccenca:VocabularyUserInterface</code> Represents the action needed to use the Vocabulary Catalog (needs access to specific vocabulary graphs as well).\u00a0You can use it in the\u00a0Allowed actions\u00a0field. <code>urn:eccenca:ExploreUserInterface</code> Represents the action needed to use the Explore Tab (needs access to at least one graph as well).\u00a0You can use it in the\u00a0Allowed actions\u00a0field. <p>Click\u00a0CREATE\u00a0to create the new condition or abort your action with\u00a0CANCEL.</p> <p></p> Create a new condition","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#edit-an-existing-condition","title":"Edit an existing condition","text":"<p>In the expanded view of an access condition, click\u00a0DELETE\u00a0to remove the access condition or\u00a0EDIT\u00a0to apply changes.</p> <p>Click on\u00a0SAVE\u00a0to apply your changes or discard them with\u00a0CANCEL.</p>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/access-conditions/#adding-access-conditions-with-cmemc","title":"Adding Access Conditions with cmemc","text":"<p>In addition to the\u00a0Access Control\u00a0module you can add the access conditions directly to the triple store. In this case, the access conditions need to be defined in a Turtle file, for example:</p> <pre><code>@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix eccurn: &lt;urn:eccenca:&gt; .\n@prefix ecc: &lt;http://eccenca.com/&gt; .\n@prefix dcterms: &lt;http://purl.org/dc/terms/&gt; .\n@prefix eccauth: &lt;https://vocab.eccenca.com/auth/&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\necc:f7f3b9c5-4e94-4e7f-a14b-15e39ae046ed\ndcterms:created \"2020-06-12T11:10:19Z\"^^xsd:dateTime ;\ndcterms:creator ecc:admin ;\na eccauth:AccessCondition ;\nrdfs:comment \"single access condition which describes all rights for users in the local-admins group\" ;\nrdfs:label \"Active: access rights of all users of the local-admins group\" ;\neccauth:allowedAction eccurn:AccessInternalGraphs, eccurn:QueryUserInterface, eccurn:ThesaurusUserInterface, eccurn:VocabularyUserInterface, eccurn:di, &lt;urn:elds-backend-actions-auth-access-control&gt; ;\neccauth:requiresGroup ecc:local-admins ;\neccauth:writeGraph &lt;urn:elds-backend-all-graphs&gt; .\necc:5318ffd4-4ca7-46bb-8e0c-8a910376c6b9\ndcterms:created \"2020-06-12T11:10:32Z\"^^xsd:dateTime ;\ndcterms:creator ecc:admin ;\na eccauth:AccessCondition ;\nrdfs:comment \"single access conditions which describes the rights of users from the local-users group\" ;\nrdfs:label \"Active: access rights of all users of the local-users group\" ;\neccauth:allowedAction eccurn:ExploreUserInterface, eccurn:QueryUserInterface, eccurn:ThesaurusUserInterface, eccurn:VocabularyUserInterface, eccurn:di ;\neccauth:requiresGroup ecc:local-users ;\neccauth:writeGraph &lt;urn:elds-backend-all-graphs&gt; .\n</code></pre> <p>In this example, we have listed default access conditions for the\u00a0docker-compose based orchestration.</p> <p>The file defines two access conditions. The <code>eccauth:allowedActions</code> correspond to the URIs listed in the table above. Both access conditions allow the corresponding users (admins or non-admins) to write to all graphs.</p> <p>You can define several access conditions for the same group.</p> <p>When you are finished with creating your access conditions Turtle file, you can add it directly to the <code>urn:elds-backend-access-conditions-graph</code> graph (defined in\u00a0DataPlatform configuration).</p> <p>With\u00a0cmemc you can do this with the following command line:</p> <pre><code>$ cmemc -c my-cmem-instance graph import --replace access-conditions.ttl urn:elds-backend-access-conditions-graph\nImport graph 1/1: urn:elds-backend-access-conditions-graph from access-conditions.ttl ... done\n</code></pre>","tags":["Security","cmemc"]},{"location":"deploy-and-configure/configuration/dataintegration/","title":"DataIntegration","text":"<p>This section is intended to be a reference for all available eccenca DataIntegration configuration options.The configuration format is based on\u00a0HOCON.</p> <p>The following sections introduce the most important configuration parameters. The entire list of available configuration parameters can be found in the\u00a0dataintegration.conf file found in the release package.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#oauth","title":"OAuth","text":"<p>Authorization in eccenca DataIntegration is based on OAuth. Typically the eccenca DataPlatform is used as an OAuth endpoint, but external endpoints can be used as well. The Authorization Code Grant workflow is used for retrieving the OAuth token. The default configuration is the following:</p> <pre><code># The URL of the eccenca DataPlatform.\neccencaDataPlatform.url = \"http://localhost:9090\"\n\n# Use OAuth for authentification against DataPlatform\neccencaDataPlatform.oauth = false\n\n# Enable if user information should be fetched via DP and OAuth. Only uncomment if OAuth is enabled and DP is configured.\nuser.manager.web.plugin = oauthUserManager\n\n# The DataPlatform endpoint that is used for authentification\neccencaDataPlatform.endpointId = \"default\"\n\n# Define the protocol used for accessing the workbench (http or https), defaults to http\nworkbench.protocol = \"http\"\n\n# Optional parameter for specifying the host.\nworkbench.host = \"localhost:9090\"\n\n# The URL to redirect to after logout.\n# If not set, the user will be redirected to the internal logout page.\noauth.logoutRedirectUrl = \"http://localhost:9090/loggedOut\"\n\n# The OAuth client that will be used to load the workspace initially and run the schedulers.\nworkbench.superuser.client = \"elds\"\nworkbench.superuser.clientSecret = \"elds\"\n\n# Optional parameter for specifying an alternative OAuth authorization endpoint.\n# If not specified, the default OAuth authorization endpoint of the specified eccenca Platform URL is used.\n# Note that if the eccenca Platform URL is internal and not accessible for the user, a public authorization URL must be set here.\noauth.authorizationUrl = \"http://localhost:9090/oauth/authorize\"\n\n# Optional parameter for specifying an alternative OAuth authorization endpoint.\noauth.tokenUrl = \"http://localhost:9090/oauth/token\"\n\n# Optional parameter for specifying an alternative OAuth client ID.\noauth.clientId = \"eldsClient\"\n\n# Optional parameter for specifying an alternative OAuth client secret.\noauth.clientSecret = \"secret\"\n\n# Additional request parameters to append to all OAuth authentication requests.\n# oauth.requestParameters = \"&amp;resource=value\"\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#dataplatform-configuration","title":"DataPlatform configuration","text":"<p>eccenca DataIntegration can only be run by OAuth users that are granted the\u00a0<code>urn:eccenca:di</code>\u00a0action by the DataPlatform. An example configuration that grants all users from a predefined group\u00a0<code>dataIntegrators</code>\u00a0access to DataIntegration is shown in the following:</p> <pre><code>@prefix eccauth: &lt;https://vocab.eccenca.com/auth/&gt; .\n@prefix : &lt;http://eccenca.com/&gt; .\n&lt;urn:readAndWriteAllGraphs&gt; a eccauth:AccessCondition ;\neccauth:requiresGroup :dataIntegrators ;\neccauth:allowedAction &lt;urn:eccenca:di&gt; ;\neccauth:readGraph &lt;urn:elds-backend-all-graphs&gt; ;\neccauth:writeGraph &lt;urn:elds-backend-all-graphs&gt; .\n</code></pre> <p>In the shown configuration, the users also get access to all graphs in the RDF store. This is not a requirement for working with DataIntegration. Access may also be restricted to graphs that the data integration users are allowed to work with.</p> Note <p>The\u00a0<code>urn:elds-backend-all-actions</code>\u00a0action set also includes the\u00a0<code>urn:eccenca:di</code>\u00a0action, i.e., users that are granted\u00a0<code>urn:elds-backend-all-actions</code>\u00a0also get access to eccencca DataIntegration.</p> <p>In order to activate OAuth using the eccenca DataPlatform, the following minimal configuration is required:</p> <pre><code>eccencaDataPlatform.url = \"http://localhost:9090\"\neccencaDataPlatform.oauth = true\noauth.clientId = \"eldsClient\"\noauth.clientSecret = \"secret\"\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#super-user","title":"Super User","text":"<p>By default, the workspace is loaded the first time a user opens eccenca DataIntegration in the browser using their credentials. Any scheduler that is part of a project must be started manually.</p> <p>By configuring a super user, the workspace will be loaded at startup. After loading, all schedulers will be started using the credentials of the super user.</p> <p>In addition to the configuration of the eccencaDataPlatform according to the previous section, a super user is configured by specifying the following two parameters:</p> <pre><code>workbench.superuser.client = \"superUserClient\"\nworkbench.superuser.clientSecret = \"superUserClientSecret\"\n</code></pre> Note <p>The client credentials grant type is used to retrieve a token for the super user. Note that the schedulers are only started automatically when running in production mode.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#workspace-providers","title":"Workspace Providers","text":"<p>The backend that holds the workspace can be configured using the\u00a0<code>workspace.provider.plugin</code>\u00a0parameter in\u00a0<code>dataintegration.conf</code></p> <pre><code>workspace.provider.plugin = &lt;workspace-provider-plugin-name&gt;\n</code></pre> <p>The following sections describe the available workspace provider plugins and how they are configured.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#rdf-store-workspace-backend","title":"RDF-store Workspace - backend","text":"<p>When running in Corporate Memory, by default the workspace is held in the RDF store configured in the eccenca DataPlatform.</p> <p>The workspace is held using the eccenca DataPlatform, i.e., it requires the\u00a0<code>eccencaDataPlatform.url</code>\u00a0parameter to be configured.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ cacheDir String Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start. <code>&lt;empty&gt;</code> <p>By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>eccencaDataPlatform.url = &lt;DATAPLATFORM_URL&gt;\n...\nworkspace.provider.plugin = backend\n...\nworkspace.provider.backend = {\n  # Load prefixes defined by all known vocabularies.\n  loadAllVocabularyPrefixes = false\n  # Load prefixes defined by vocabularies that are actually loaded in the RDF store.\n  loadInstalledVocabularyPrefixes = true\n  # The graph that contains the vocabulary meta data.\n  vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\"\n  # Optional directory to persist caches between restarts. If empty, caches will be held in-memory and will be reloaded on each start.\n  cacheDir = \"\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#file-based-workspace-file","title":"File-based Workspace - file","text":"<p>The workspace can also be held on the filesystem.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the workspace is persisted. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.provider.plugin = file\n...\nworkspace.provider.file = {\n  # The directory to which the workspace is persisted.\n  dir = ${user.home}\"/myWorkspace\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#hybrid-workspace-fileanddataplatform","title":"Hybrid workspace - fileAndDataPlatform","text":"<p>The so called hybrid workspace holds the workspace in the filesystem and in eccenca DataPlatform simultaneously. Each time a task is updated, it is written to both the filesystem and to the project RDF graph. In addition, on each (re)load of the Workspace, the contents of the file based workspace are pushed to the RDF store, to make sure that both workspace backends stay synchronized. Contents of the file system may be changed manually (reload the workspace afterwards). Contents of the RDF store are supposed to be read only and will be overwritten on reload.</p> <p>The workspace is held using the eccenca DataPlatform, i.e., it requires the\u00a0<code>eccencaDataPlatform.url</code>\u00a0parameter to be configured.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the workspace is persisted. no default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ failOnDataPlatformError boolean If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning. false <p>By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>eccencaDataPlatform.url = &lt;DATAPLATFORM_URL&gt;\n...\nworkspace.provider.plugin = fileAndDataPlatform\n...\nworkspace.provider.fileAndDataPlatform = {\n  # The directory to which the workspace is persisted.\n  dir = ${user.home}\"/myWorkspace\"\n  # Load prefixes defined by all known vocabularies.\n  loadAllVocabularyPrefixes = false\n  # Load prefixes defined by vocabularies that are actually loaded in the RDF store.\n  loadInstalledVocabularyPrefixes = true\n  # The graph that contains the vocabulary meta data.\n  vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\"\n  # If true, whenever an update is triggered that has been pushed to the xml backend, but failed to be pushed to the DataPlatform, the entire request fails. If false, an update error in the DataPlatform will only log a warning.\n  failOnDataPlatformError = false\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-workspace-inmemory","title":"In-memory Workspace - inMemory","text":"<p>A workspace provider that holds all projects in memory. All contents will be gone on restart.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.provider.plugin = inMemory\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-rdf-workspace-inmemoryrdfworkspace","title":"In-memory RDF Workspace - inMemoryRdfWorkspace","text":"<p>A workspace that is held in a in-memory RDF store and loses all its content on restart (mainly used for testing). Needed if operators are used that require an RDF store backend.</p> <p>This workspace can be configured using the following parameter:</p> Parameter Type Description Default loadAllVocabularyPrefixes boolean Load prefixes defined by all known vocabularies. false loadInstalledVocabularyPrefixes boolean Load prefixes defined by vocabularies that are actually loaded in the RDF store. true vocabularyGraph String The graph that contains the vocabulary meta data. https://ns.eccenca.com/example/data/vocabs/ <p>By default, prefixes are loaded from all installed vocabularies. Only one of loadAllVocabularyPrefixes and loadInstalledVocabularyPrefixes can be set to true.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.provider.plugin = inMemoryRdfWorkspace\n...\nworkspace.provider.inMemoryRdfWorkspace = {\n  # Load prefixes defined by all known vocabularies.\n  loadAllVocabularyPrefixes = false\n  # Load prefixes defined by vocabularies that are actually loaded in the RDF store.\n  loadInstalledVocabularyPrefixes = true\n  # The graph that contains the vocabulary meta data.\n  vocabularyGraph = \"https://ns.eccenca.com/example/data/vocabs/\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#resource-repositories","title":"Resource Repositories","text":"<p>Project resources are held by a resource repository which is configured\u00a0using the\u00a0<code>workspace.repository.plugin</code>\u00a0 parameter in\u00a0<code>dataintegration.conf</code></p> <pre><code>workspace.repository.plugin = &lt;resource-repository-plugin-name&gt;\n</code></pre> <p>The following sections describe the available resource repository plugins and how they are configured.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#project-specific-directories-projectfile","title":"Project Specific Directories - projectFile","text":"<p>By default, resources are held in project specific directories.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the resources are persisted. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = projectFile\n...\nworkspace.repository.projectFile = {\n  dir = ${elds.home}\"/var/dataintegration/workspace/\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#shared-directory-file","title":"Shared Directory - file","text":"<p>Alternatively, all resources across all DataIntegration projects can be held in a single directory on the file system.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default dir String The directory to which the resources are persisted. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = file\n...\nworkspace.repository.file = {\n  dir = ${elds.home}\"/var/dataintegration/resources/\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#hdfs-resources-hdfs","title":"HDFS resources - hdfs","text":"<p>Holds all resources on the HDFS file system.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default path String The directory to which the resources are persisted. no default user String The hadoop user. hadoopuser <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = hdfs\n...\nworkspace.repository.hdfs = {\n  # The directory to which the resources are persisted.\n  dir = \"/data/hdfs-datalake/\"\n  # The hadoop user.\n  user = \"hadoopuser\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#s3-bucket-project-specific-projects3","title":"S3 Bucket, Project Specific -\u00a0projectS3","text":"<p>In addition to storing files in the local filesystem an AWS S3 bucket can be used as the resource repository backend.</p> <p>To use resources stored on S3 the AWS\u00a0<code>keyID</code>\u00a0,\u00a0<code>secretKey</code>\u00a0need to be configured in the\u00a0<code>dataintegration.conf</code>\u00a0\u00a0file. This and the region are used to connect to S3. Further, one bucket name has to be given. This is analog to the root folder of filesystem based resource repositories.</p> <p>This plugin can be configured using the following parameter:</p> Parameter Type Description Default bucket String The S3 bucket name. no default accessKeyId String The S3 access key ID. no default secretKey String The S3 secret key. no default region String The AWS region the S3 bucket is located in. no default path String OPTIONAL. Path (absolute to the bucket (root)) that defines the folder that will be used to hold the workspace. no default <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = projectS3 # project individual resources\n...\nworkspace.repository.projectS3 = {\n  # The S3 bucket name.\n  bucket = \"your-bucket-name\"\n  # The S3 access key ID.\n  accessKeyId = \"BUCKET-ACCESS-KEY\"\n  # The S3 secret key.\n  secretKey = \"BUCKET-SECRET-KEY\"\n  # The AWS region the S3 bucket is located in.\n  region = \"eu-central-1\"\n  # OPTIONAL path in the bucket used to hold the DataIntegration workspace\n  # /path/to/my-workspace/\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#s3-bucket-shared-directory-s3","title":"S3 Bucket, Shared Directory - s3","text":"<p>Holds all resources shared across all your DataIntegration projects in a single S3 bucket.</p> <p>The available configuration options are the same as for\u00a0<code>projectS3</code>\u00a0.</p> <p>The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = s3 # resources shared across projects\n...\nworkspace.repository.s3 = {\n  ... # same configuration as for projectS3\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-inmemory","title":"In-Memory -\u00a0inMemory","text":"<p>Resources can also be held in-memory.\u00a0The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = inMemory\n</code></pre> <p>Note</p> <p>In-memory repositories will be emptied on restart.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#no-resources-empty","title":"No resources -\u00a0empty","text":"<p>In case you do not want to allow to store file resources you can define an\u00a0empty\u00a0repository. The\u00a0<code>empty</code>\u00a0resource repository that does not allow storing any resources.</p> <p>The resource repository can also be specified as empty.\u00a0The corresponding configuration in your\u00a0<code>dataintegration.conf</code>\u00a0looks like the following:</p> <pre><code>workspace.repository.plugin = empty\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-report-manager","title":"Execution Report Manager","text":"<p>The execution report manager is used to persist execution reports. It allows to retrieve previous reports. you can use it with file and in-memory models. In addition you can specify a retention time. Reports older than this time will be deleted, if a new report is added. The retention time is expressed as a Java\u00a0<code>Duration</code>\u00a0string, see\u00a0https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-\u00a0for details.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#disabled-none","title":"Disabled - None","text":"<p>Discards execution reports and does not persist them.</p> <pre><code>workspace.reportManager.plugin = none # Discards execution reports and does not persist them.\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#in-memory-inmemory_1","title":"In-Memory - inMemory","text":"<p>Holds the reports in memory.</p> <pre><code>workspace.reportManager.plugin = inMemory # Holds the reports in memory.\n\nworkspace.reportManager.inMemory = {\n  retentionTime = \"P30D\" # duration how long to keep - default is 30 Days\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#file-based-file","title":"File based - file","text":"<p>Holds the reports in a specified directory on the filesystem.</p> <pre><code>workspace.reportManager.plugin = file # Holds the reports in a specified directory on the filesystem.\n\nworkspace.reportManager.file = {\n  dir = \"/data/reports\" # directory where the reports will be stored\n  retentionTime = \"P30D\" # duration how long to keep - default is 30 Days\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#internal-datasets","title":"Internal Datasets","text":"<p>Internal datasets hold intermediate results during the execution of a DataIntegration Workflow. The type of internal dataset that is used can be configured. By default an in memory dataset is used for storing data between tasks:</p> <pre><code>dataset.internal.plugin = inMemory\n</code></pre> <p>Alternatively, the internal data can also be held in the eccenca DataPlatform:</p> <pre><code>dataset.internal.plugin = eccencaDataPlatform\ndataset.internal.eccencaDataPlatform = {\n  graph = \"https://ns.eccenca.com/dataintegration/internal\"\n}\n</code></pre> <p>If the eccenca DataPlatform is not available, an external store may also be specified:</p> <pre><code>dataset.internal.plugin = sparqlEndpoint\ndataset.internal.sparqlEndpoint = {\n  endpointURI = \"http://localhost:8890/sparql\"\n  graph = \"https://ns.eccenca.com/dataintegration/internal\"\n}\n</code></pre> <p>Warning</p> <p>If an RDF store based internal dataset is used, all internal data is stored in the same graph. For this reason, multiple different internal datasets cannot be used safely in that case.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#timeouts","title":"Timeouts","text":"<p>DataIntegration has a number of timeouts to maintain operation while connection issues or problems in other applications occur. The following sections list the most important global timeouts.</p> Note <p>In addition to these global timeouts, many datasets, such as the Knowledge Graph dataset, do provide additional timeout parameters. Refer to the documentation of datasets for individual timeout mechanisms of different datasets.</p> <p>Warning</p> <p>All timeouts set need to be lower than the gateway timeout in the network infrastructure.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#request-timeout","title":"Request timeout","text":"<p>The request timeout specifies how long a HTTP(S) request may take until it times out and is closed:</p> <pre><code>play.server.akka.requestTimeout = 10m\n</code></pre> <p>This timeout mechanism can be disabled, by setting the timeout to\u00a0<code>\"infinite\"</code>.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#idle-request-timeout","title":"Idle request timeout","text":"<p>The idle timeout specifies the maximum inactivity time of a HTTP(S) connection:</p> <pre><code>play.server.http.idleTimeout = 10m\n</code></pre> <p>The connection will be closed after it has been open for the configured idle timeout, without any request or response being written. This timeout mechanism can be disabled, by setting the timeout to\u00a0<code>\"infinite\"</code>.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#dataplatform-timeouts","title":"DataPlatform timeouts","text":"<p>As DataIntegration depends on DataPlatform for managing the Knowledge Graph, it will query the health of DataPlatform as part of its own health check. The timeout to wait for DataPlatform\u2019s response to a health request can be configured:</p> <pre><code>healthCheck.dataplatform.timeout = 10000 # milliseconds\n</code></pre> <p>In addition, there is a timeout when requesting authorization information from the eccenca DataPlatform, which is fixed to 61 seconds.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#rdf-store-timeouts","title":"RDF store timeouts","text":"<p>When reading and writing RDF there are a number of timeouts applied, depending on whether the Graph Store protocol or the SPARQL endpoint are used.</p> <p>For the Graph Store protocol, the following timeouts can be configured:</p> <pre><code>graphstore.default = {\n  # Timeout in which a connection must be established\n  connection.timeout.ms = 15000 # 15s\n  # Timeout in which a response must be read\n  read.timeout.ms = 150000 # 150s\n  # Max request size of a single GraphStore request, larger data is split into multiple requests\n  max.request.size = 300000000 # 300MB\n  # Timeout in which a file upload of size max.request.size must be uploaded\n  fileUpload.timeout.ms = 1800000 # half hour\n}\n</code></pre> <p>For the SPARQL endpoint, the following parameters are applicable:</p> <pre><code>silk.remoteSparqlEndpoint.defaults = {\n  connection.timeout.ms = 15000 # 15s\n  read.timeout.ms = 180000 # 180s\n}\n</code></pre> <p>Note</p> <p>The Knowledge Graph dataset provides additional timeout parameters for more fine-grained control.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#linking-execution-timeout","title":"Linking execution timeout","text":"<p>When executing linking rules there are a number of timeouts to prevent possibly erroneous linkage rules from generating too many links or staling the execution:</p> <pre><code>linking.execution = {\n  # The maximum amount of links that are generated in the linking execution/evaluation\n  linkLimit = {\n    # The default value a link spec is initialized with, this can be changed for each link spec.\n    default = 1000000 # 1 million\n    # The absolute maximum of links that can be generated. This is necessary since the links are kept in-memory.\n    max = 10000000 # 10 million\n  }\n  # The maximum time the matching task is allowed to run, this does not limit the loading time.\n  matching.timeout.seconds = 3600 # 1 hour\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#maximum-upload-size","title":"Maximum Upload Size","text":"<p>By default, the size of uploaded resources is limited to 10 MB. The upload limit can be increased:</p> <pre><code>play.http.parser.maxDiskBuffer = 100MB\n</code></pre> <p>While uploading, resources are cached on the disk, i.e., the limit may exceed the size of the available memory.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#provenance","title":"Provenance","text":"<p>By default, no provenance data is written to the RDF store. To enable writing provenance data to a named graph, a provenance plugin needs to be configured.</p> <p>Provenance output can be configured using the following parameter:</p> Parameter Type Description Default provenance.graph String Set the graph where generated provenance will be written to in the RDF workspace provider. https://ns.eccenca.com/example/data/dataset/ provenance.persistWorkflowProvenancePlugin.plugin String Provenance plugin to set where provenance data should be written to. Possible options: - rdfWorkflowProvenance - writes provenance data to RDF backend - nopWorkflowProvenance - do NOT write provenance data (disable it) nopWorkflowProvenance <p>To enable provenance output, the following lines can be added to\u00a0<code>dataintegration.conf</code>:</p> <pre><code>provenance.graph = https://ns.eccenca.com/example/data/dataset/\nprovenance.persistWorkflowProvenancePlugin.plugin = rdfWorkflowProvenance\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#logging","title":"Logging","text":"<p>Logging for eccenca DataIntegration is based on the\u00a0Logback\u00a0logging framework. There are two ways to change the logging behavior from the default, the first is to provide a logback.xml file, the second is to set various logging properties in the dataintegration.conf file.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#logback-configuration-file","title":"Logback Configuration File","text":"<p>The\u00a0<code>logback.xml</code>\u00a0file can be added to the\u00a0<code>${ELDS_HOME}/etc/dataintegration/conf/</code>\u00a0folder, from where it is read on application start-up and replaces the default logging config.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-example","title":"Configuration Example","text":"<p>The following example\u00a0<code>logback.xml</code>\u00a0file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n&lt;appender name=\"TIME_BASED_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n&lt;file&gt;/opt/elds/var/log/dataintegration.log&lt;/file&gt;\n&lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt;\n&lt;!-- daily rollover, history for 1 week --&gt;\n&lt;fileNamePattern&gt;/opt/elds/var/log/dataintegration.%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;\n&lt;maxHistory&gt;7&lt;/maxHistory&gt;\n&lt;/rollingPolicy&gt;\n&lt;encoder&gt;\n&lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;\n&lt;/encoder&gt;\n&lt;/appender&gt;\n&lt;logger name=\"com.eccenca\" level=\"INFO\"&gt;\n&lt;appender-ref ref=\"TIME_BASED_FILE\" /&gt;\n&lt;/logger&gt;\n&lt;/configuration&gt;\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#logging-properties","title":"Logging Properties","text":"<p>For debugging purposes and smaller adaptions it is possible to change log levels for any logger in the DataIntegration config file. There are following possibilities:</p> <pre><code># The following log level properties will overwrite the config from the logback.xml file\n\n# Set the root logger level, valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL.\n# This affects any logger that is not explicitly defined in the logback.xml config file\nlogging.root.level = DEBUG\n\n# Set the DI root log level. This affects all logger in DI packages that are not explicitly specified in the logback.xml\nlogging.di.level = TRACE\n\n# Set the Silk root log level. This affects all logger in Silk packages that are not explicitly specified in the logback.xml\nlogging.silk.level = WARN\n\n# Generic log level config: inside logging.level enter package path as key and log level as value.\n# Valid values are: OFF, ERROR, WARN, INFO, DEBUG, TRACE, ALL\n# This can be used to override any log level, also these defined in the logback.xml file.\nlogging.level {\n  # Set log level of oauth package to TRACE, this overrides the config in the default logback config\n  oauth=TRACE\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#plugin-configuration","title":"Plugin Configuration","text":"<p>The plugin architecture of eccenca DataIntegration allows to configure certain characteristics of the application, e.g. the persistence backend for the workspace.\\ A full list over all plugins are given in the eccenca DataIntegration user manual in the sections\u00a0Plugin Reference\u00a0as well as\u00a0Activity Reference.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#blacklisting-plugins","title":"Blacklisting Plugins","text":"<p>In some cases the usage of specific plugins might pose a risk. In order to avoid to load a specific plugin, it can be blacklisted in the configuration file by setting the\u00a0<code>pluginRegistry.plugins.{pluginID}.enabled</code>\u00a0 config parameter to false. The parameter takes a comma-separated list of plugin IDs. The corresponding plugins will not be loaded into the plugin registry and can thus not be selected or executed anymore. The plugin ID for each plugin can be found in the\u00a0Plugin Reference\u00a0and\u00a0Activity Reference\u00a0section of the eccenca DataIntegration user manual.</p> <p>Example config:</p> <pre><code>pluginRegistry {\n  plugins {\n    pluginToBeBlacklisted.enabled = false\n  }\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#spark-configuration","title":"Spark configuration","text":"<p>The following chapters describe configuration options relevant for the execution of eccenca DataIntegration workflows on Spark. All options regarding the execution of DataIntegration on Spark are set in the\u00a0<code>dataintegration.conf</code>\u00a0file. The option to define SparkExecutor as the execution engine is\u00a0<code>execution.manager.plugin</code>\u00a0and needs to be changed from the default value:</p> <pre><code>execution.manager.plugin = LocalExecutionManager\n</code></pre> <p>To the value that specifies the use of the SparkExecutor:</p> <pre><code>execution.manager.plugin = SparkExecutionManager\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-spark-client-mode","title":"Execution in Spark client Mode","text":"<p>Spark provides a simple standalone cluster manager. You can launch a\u00a0standalone\u00a0cluster either manually, by starting a master and workers by hand, or by using the provided launch scripts. Spark can still run alongside Hive, Hadoop and other services in this mode. But in general this mode is preferred if only Spark applications are running in a cluster. When multiple cluster applications are running in parallel (e.g. different databases, interpreters or any software running on top of Yarn) or more advanced monitoring is needed the execution with Yarn is often recommended.</p> <p>For running DataIntegration in client mode the following configuration can be used</p> <pre><code>spark.interpreter.options = {\n  # Specifies local or client-mode, required: local, cluster or client\n  deploymentMode = \"client\"\n  # URL of the Spark cluster master node\n  sparkMaster = \"spark://spark.master:7077\"\n  # The IP of the driver/client program machine in client-mode, required for client mode\n  sparkLocalIP = \"IP or hostname where DataIntegration runs\"\n  # Jars containing the dependencies, required only for client and cluster modes\n  # In client mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included\n  sparkJars = \"eccenca-DataIntegration-assembly.jar\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-cluster-mode","title":"Execution in cluster mode","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#dataintegration-application-configuration","title":"DataIntegration application configuration","text":"<p>Cluster mode is supported with Apache Yarn only at the moment. To run DataIntegration in cluster mode the following configuration can be used:</p> <pre><code>spark.interpreter.options = {\n  # Specifies local or client-mode, required: local, cluster or client\n  deploymentMode = \"cluster\"\n  # URL of the Spark cluster master node\n  sparkMaster = \"yarn-master-hostname\"\n  # The IP of the driver/client program machine in client-mode, required for client mode\n  sparkLocalIP = \"IP or hostname where DataIntegration runs\"\n  # Jars containing the dependencies, required only for client and cluster modes\n  # In cluster mode the artifact 'eccenca-DataIntegration-assembly.jar' must be included\n  sparkJars = \"eccenca-DataIntegration-assembly.jar\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#dataintegration-cluster-deployment-configuration","title":"DataIntegration cluster deployment configuration","text":"<p>In cluster mode one should keep in mind that, normally, DataIntegration will generate a jar and a project export. These artifacts can be copied or send to a cluster and will be executed there via the\u00a0<code>spark-submit</code>\u00a0command. That means the data processing is running in its own remote process separate from the DataIntegration application.</p> <p>An assembly jar and a workflow can be exported by an activity that belongs to each defined workflow. The activity can be configured in the\u00a0<code>dataintegration.conf</code>\u00a0. There are 3 phases of a deployment: staging, transform, loading and 3 types of artifact compositions as well as some other options deciding the target of the export. First the specified resource are copied to the configured resource folder of a project or a temp folder (staging) and then an action decides how the files are deployed.</p> <p>Artifacts (spark.deployment.options.artifact):</p> <ul> <li>\u2018jar\u2019: The assembly jar is deployed</li> <li>\u2018project\u2019: The exported project zip file is deployed (e.g. if the assembly was already globally deployed)</li> <li>\u2018project-jar\u2019: The jar and the project zip are deployed</li> </ul> <p>The artifacts are copied to the configured resource folder off the project the activity belongs to.</p> <p>Types (<code>spark.deployment.options.[phase].type, e.g. spark.deployment.options.staging.type=\"env-script\"</code>\u00a0):</p> <ul> <li>\u2018script\u2019 A shell script is called to copy the files to the cluster (can be user supplied, contain auth, prepare a DataFactory activity etc.)</li> <li>\u2018copy\u2019 The resource are copied to a specified folder</li> <li>\u2018hdfs\u2019 The resource is imported to HDFS</li> <li>\u2018env-script\u2019 A shell script is loaded from a environment variable</li> <li>\u2018var-script\u2019 A shell script is loaded from a configuration variable</li> </ul> <p>Other options:</p> <ul> <li><code>spark.deployment.options.[phase].[typeName]</code>\u00a0Depending on the selected deployment type this contains one or more (separated by a comma) targeted local file system or HDFS paths or location of the scripts to run\\     e.g.\u00a0<code>spark.deployment.options.staging.type =\"script\"</code>\u00a0and\u00a0<code>spark.deployment.options.staging.script=\"/scripts/script.sh\"</code></li> <li><code>spark.deployment.options.overwriteExecution</code>\u00a0Boolean value that decides if the ExecuteSparkWorkflow action is overwritten by the deployment action and will run this instead.</li> </ul> <p>Example:</p> <pre><code>spark.deployment.options = {\n  # Specifies artifacts: Stage the project export and the executable assembly jar\n  artifact = \"project-jar\"\n  # Type of the deployment: Copy project and jar to /data folder, then run a script to start processing the data\n  staging.type = \"copy\"\n  staging.copy = \"/data\"\n  transform.type = \"script\"\n  transform.script = \"conf/runWorkflow.sh\"\n  # Bind the 2 actions to the \"run workflow\" button\n  overwriteExecution = true\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#activity-parameters-to-skip-deployment-phases","title":"Activity parameters to skip deployment phases","text":"<p>In some scenarios (especially deployments where a jar has to be copied to a remote location) it is required that a deployment phase can be skipped. E.g. the jar upload only has to be done once, the upload is defined in the \u201cstaging\u201d phase and the spark-submit call in the \u201ctransform\u201d phase. The parameter \u201cexecuteTransform\u201d (reachable via the activity tab) can\\ be set to false on the second run to avoid re-uploading artifacts.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-of-the-assembly-jar","title":"Configuration of the assembly jar","text":"<p>In cluster mode, usually, we run a Spark Job by submitting an assembly jar to the cluster. This can be seen a command line version of DataIntegration and can also be used manually with \u2018spark-submit\u2019. In this case the configuration in the environment the jar runs in should look like this (options are set by the spark-submit configuration and parameters):</p> <pre><code>spark.interpreter.options = {\n  # Specifies deployment mode, requires: local, cluster, client or submit\n  deploymentMode = \"submit\"\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#execution-in-local-mode","title":"Execution in local mode","text":"<p>Local mode is mainly for testing, but can be used for deployment on a single server. HDFS and Hive are not required. The following configuration parameters have to be set:</p> <pre><code>spark.interpreter.options = {\n  # Specifies deployment mode, requires: local, cluster, client or submit\n  deploymentMode = \"local\"\n  # URL of the Spark cluster master node, the [*] denotes the number of executors\n  sparkMaster = \"local[4]\"\n}\n</code></pre> <p>In this mode the parameters and Spark settings appended to the \u2018spark-submit\u2019 command will always be used and overwrite configuration settings in other sources.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#configuration-and-usage-of-the-sqlendpoint-dataset","title":"Configuration and usage of the SqlEndpoint dataset","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#server-side-configuration","title":"Server Side Configuration","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#general-settings","title":"General Settings","text":"<p>The SqlEndpoint dataset is a table or view behaving analog to a table in a relational database like MySQL. When data is written to an SqlEndpoint dataset, a JDBC server is started and can be queried with any JDBC client. In DataIntegration the SqlEndpoint dataset behaves like any other dataset. It can be used as a target for workflows, be profiled, used as a source for an operation or workflow etc. There are a two of configuration options that are relevant for the JDBC endpoints:</p> <pre><code>spark.sql.options = {\n  # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. SqlEndpoint\n  # datasets can still be started but can only be accessed internally if set to false.\n  startThriftServer = true\n  # Enable Hive integration\n  # Sets Spark to use an infrastructure for meta data that is compatible with the hive metastore\n  enableHiveSupport = true\n  ...\n}\n</code></pre> <p>The port on which the JDBC connections will be available is\u00a0<code>10005</code>\u00a0by default and can be changed in the\u00a0<code>hive-site.xml</code>\u00a0and\u00a0<code>spark-defaults.conf</code>\u00a0configuration files.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#security-settings","title":"Security Settings","text":"<p>A secure connection can be configured with the authentification settings in the\u00a0<code>hive-site.xml</code>\u00a0,\u00a0<code>spark-defaults.conf</code>\u00a0and\u00a0<code>dataintegration.conf</code>\u00a0files.</p> <p>If Hive support is disabled (\u00a0<code>enableHiveSupport = false</code>\u00a0) or if the property\u00a0<code>hive.server2.authentication</code>\u00a0has the value\u00a0<code>None</code>\u00a0security can be disabled.</p> <p>There exist a number of option for secure JDBC connections via Thrift and Hive:</p> <ul> <li>Kerberos</li> <li>LDAP</li> <li>Custom authentication classes</li> <li>User impersonation</li> <li>Server and Client Certificates</li> </ul> <p>Eccenca provides a custom Authentification provider which allows to set 1 user/password combination for JDBC connections via:</p> <pre><code>spark.sql.options = {\n  endpointUser = \"user\"\n  endpointPassword = \"password\"\n}\n</code></pre> <p>The authentication provider class name is\u00a0<code>com.eccenca.di.sql.endpoint.security.SqlEndpointAuth</code>\u00a0. To use it the following configuration is needed:</p> <pre><code>  &lt;configuration&gt;\n&lt;property&gt;\n&lt;name&gt;hive.server2.authentication&lt;/name&gt;\n&lt;value&gt;CUSTOM&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;hive.server2.custom.authentication.class&lt;/name&gt;\n&lt;value&gt;com.eccenca.di.sql.endpoint.security.SqlEndpointAuth&lt;/value&gt;\n&lt;description&gt;\nCustom authentication class. Used when property\n        'hive.server2.authentication' is set to 'CUSTOM'. Provided class\n        must be a proper implementation of the interface\n        org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2\n        will call its Authenticate(user, password) method to authenticate requests.\n        The implementation may optionally implement Hadoop's\n        org.apache.hadoop.conf.Configurable class to grab Hive's Configuration object.\n      &lt;/description&gt;\n&lt;/property&gt;\n...\n  &lt;/configuration&gt;\n</code></pre> <p>Check the Hive documentation for details:\u00a0Hive admin manual\u00a0or the documentation of a Hadoop Distribution (MapR, Hortenworks or AWS and Azure in he cloud etc.). Hadoop distributions usually provides instructions for configuring secure endpoints.</p> <p>Integration with various authentication providers can be configured and is mostly set up in\u00a0<code>hive-site.xml</code>\u00a0.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#sqlendpoint-dataset-parameters","title":"SqlEndpoint Dataset Parameters","text":"<p>The dataset only requires that the\u00a0<code>tableNamePrefix</code>\u00a0parameters is given. This will be used as the prefix for the names of the generated tables. When a set of Entities is\u00a0written\u00a0to the endpoint\u00a0a view is generated for each entity type\u00a0(defined by an\u00a0<code>rdf_type</code>\u00a0attribute). That means that the mapping or data source that are used as input for the SqlEndpoint need to have a type or require a user defined type mapping.</p> <p>The operator has a\u00a0compatibility mode. Using it will avoid complex types such as Arrays. When arrays exit in the input they are converted to a String using the given\u00a0<code>arraySeperator</code>.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#sqlendpoint-activity","title":"SqlEndpoint Activity","text":"<p>The activity will\u00a0start\u00a0automatically, when the SqlEndpoint is used as a data sink and Dataintegration is configured to make the SqlEndpoint accessible remotely.</p> <p>When the activity is started and\u00a0running\u00a0it returns the server status and JDBC Url as its value.</p> <p>Stopping\u00a0the activity will drop all views generated by the activity. It can be\u00a0restarted\u00a0by rerunning the workflow containing it as a sink.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#remote-client-configuration-via-jdbc-and-odbc","title":"Remote Client Configuration (via JDBC and ODBC)","text":"<p>Within Dataintegration the SqlEndpoint can be used as a source or sink like any other dataset. If the\u00a0startThriftServer\u00a0option is set to\u00a0<code>true</code>\u00a0access via JDBC or ODBC is possible.</p> <p>ODBC\u00a0and\u00a0JDBC\u00a0drivers can be used to connect to relational databases. These drivers are used by clients like, Excel, PowerBI or other BI tools and transform standard SQL-queries to Hive-QL queries and handle the respective query results. Hive-QL support a subset of the SQL-92 standard. Depending on the complexity of the driver it \u2013 in case of a simple driver \u2013 supports the same subset or more modern standards. JDBC drivers are similar to ODBC ones, but serve as connectors for Java applications. When selecting a version of a driver the client operating system and its type (32bit/64 bit) are the most important factors. The version of the client drivers sometimes is the same as the servers. When no version of a driver is given the newest driver of the vendor should work, as it\u00a0should\u00a0be backwards compatible.</p> <p>Any JDBC or ODBC client can connect to a JDBC endpoint provided by an SqlEndpoint dataset. SqlEndpoint uses the same query processing as Hive, therefore the requirements for the client are:</p> <ul> <li>A JDBC driver compatible with\u00a0Hive 1.2.1\u00a0(platform independent driver org.apache.hive.jdbc.HiveDriver is needed) or</li> <li>Hive 1.2.1 is\u00a0ODPi\u00a0runtime compliant</li> <li>A JDBC driver compatible with\u00a0Spark 2.3.3</li> <li>A Hive ODBC driver (ODBC driver for the client architecture and operating system needed)</li> </ul> <p>A detailed instruction to connect to a Hive or SqlEndpoint endpoint with various tools (e.g. SQuirreL, beeline, SQL Developer, \u2026) can be found at\u00a0Apache HiveServer2 Clients.\\ The multi platform database client\u00a0DBeaver\u00a0can connect to the SQLEndpoint out of the box.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#partitioning-and-merging-of-data-sets","title":"Partitioning and merging of data sets","text":"<p>The execution on Spark is possible independent of the used file system as long as it can be referenced/is accessible for all cluster nodes. HDFS is recommended and the default settings are recommended for the best performance on a small cluster. Especially when working in local mode on the local file systems some problems can occur with the parallelism settings of Spark and the resulting partitioned output resources.</p> <p>Problems can be avoided by changing the following are the default settings:</p> <pre><code>spark.interpreter.options = {\n  # If true, data will be repartitioned before execution,\n  # otherwise the existing partitioning or no partitioning will be used\n  partitionOnImport = false\n  # Number of partitions for repartitioning on import, default = 16\n  partitionNumber = 16\n  # Specifies if data is combined before output is written to disk\n  combineOutput = false\n}\n</code></pre> <p>When running only locally the configuration should be like the following example (especially\u00a0<code>combineOutput</code>\u00a0has to be true):</p> <pre><code>spark.interpreter.options = {\n  # If true, data will be repartitioned before execution,\n  # otherwise the existing partitioning or no partitioning will be used\n  partitionOnImport = false\n  # Number of partitions for repartitioning on import, default = 16\n  partitionNumber = 4\n  # Specifies if data is combined before output is written to disk\n  combineOutput = true\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/#other-options-specific-to-spark","title":"Other options specific to Spark","text":"<pre><code>#################################################\n# Spark                                        #\n#################################################\n\nspark.interpreter.options = {\n  # the default name used when creating a SparkContext (will override spark.app.name in spark-defaults.conf)\n  sparkAppName = \"eccenca DataIntegration Spark exe\n\n  # Enables more detailed logging, counting of transformed records, etc.\n  debugMode = false\n\n  # Enable or disable a spark executor event log for debugging purposes\n  eventLog = true\n\n  # Folder for logs that people don't want in the log even though the information is necessary for debugging\n  logFolder = ${elds.home}\"/var/dataintegration/logs/\"\n\n  # Enable or disable an execution time log for benchmarking purposes\n  timeLog = true\n\n  # Enable or disable Sparks built-in log for debugging purposes (will override spark.eventLog.enabled in spark-defaults.conf)\n  sparkLog = true\n\n  # If true, data will be repartitioned before execution, otherwise the existing partitioning or no partitioning will be used\n  partitionOnImport = false\n\n  # Number of partitions for repartitioning on Import\n  partitionNumber = 16\n\n  # Specifies number of Spark SQL shuffle partitions (will override spark.sql.shuffle.partitions in spark-defaults.conf)\n  shufflePartitions = 32\n\n  # Minimum partition number for Spark execution\n  defaultMinPartitions = 4\n\n  # Default parallelism partition number for Spark execution (will override spark.sql.shuffle.partitions in spark-defaults.conf)\n  defaultParallelism = 4\n\n  # Specifies if data is combined before output is written to disk. If true the final output will be in a single file on a single partition\n  combineOutput = true\n\n  # Specifies if DataIntegration is allowed to start a thrift server for external JDBC access. Views/virtual datasets can still be started but can only be accessed internally if set to false.\n  startThriftServer = false\n\n  # Internal data model used in Spark Data Frames: 'sequence' or 'simple'. Sequence behaves like the entities used by the local executor of DataIntegration and is\n  # sometimes be needed to work with non relational data (i.e. triple stores, dataplatform). The default value is 'simple' and casts most data objects to Strings\n  # which is fast and works in most situations may lead to less clean data.\n  columnType = sequence\n\n  # General additional Java options that will be passed to the executors (worker nodes) in the cluster, default is \"\".\n  sparkExecutorJavaOptions = \"\"\n\n  # General additional Java options that will be passed to the driver application (DataIntegration), default is \"\".\n  sparkDriverJavaOptions = \"\"\n\n  # Enable or disable the Spark UI. This UI provides an overview of the Spark cluster and running jobs. It will start on port 4040\n  # and increase the port number by 1 if the port is already in use. The final port will be shown in the logs. False by default.\n  enableSparkUI = true\n\n  # This property decides if Hive integration is enabled orr not.\n  # To use hive an external DB (such as MYSQL), the meta store, is needed. Please specify the necessary properties in the hive-site.xml. Note that Hive's default meta store (derby) should not be used in production and may lead to issues.\n  enableHiveSupport = false\n}\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/","title":"Activity Reference","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#project-activities","title":"Project Activities","text":"<p>The following activities are available for each project.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset-matcher","title":"Dataset matcher","text":"<p>Generates matches between schema paths and datasets based on the schema discovery and profiling information          of the datasets.</p> Parameter Type Description Example datasetUri String If set, run dataset matching only for this particular dataset. <p>The identifier for this plugin is <code>DatasetMatcher</code>.</p> <p>It can be found in the package <code>com.eccenca.di.datamatching</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#task-activities","title":"Task Activities","text":"<p>The following activities are available for different types of tasks.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#custom","title":"Custom","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-rest-task","title":"Execute REST Task","text":"<p>Executes the REST task.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteRestTask</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.rest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset","title":"Dataset","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#dataset-profiler","title":"Dataset profiler","text":"<p>Generates profiling data of a dataset, e.g. data types, statistics etc.</p> Parameter Type Description Example datasetUri String Optional URI of the dataset resource that should be profiled. If not specified an URI will be generated. uriPrefix String Optional URI prefix that is prepended to every generated URI, e.g. property URIs for every schema path.  If not specified an URI prefix will be generated. entitySampleLimit String How many entities should be sampled for the profiling. If left blank, all entities will be considered. timeLimit String The time in milliseconds that each of the schema extraction step and profiling step should spend on. Leave blank for unlimited time. classProfilingLimit int The maximum number of classes that are profiled from the extracted schema. schemaEntityLimit int The maximum number of overall schema entities (types, properties/attributes) that will be extracted. executionType String The execution type to be used: SPARK, LEGACY. The legacy execution uses large in-memory maps and takes longer! <p>The identifier for this plugin is <code>DatasetProfiler</code>.</p> <p>It can be found in the package <code>com.eccenca.di.profiling</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#sql-endpoint-status","title":"SQL endpoint status","text":"<p>Shows the SQL endpoint status.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>SqlEndpointStatus</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.endpoint.activity</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#types-cache","title":"Types cache","text":"<p>Holds the most frequent types in a dataset.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>TypesCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#linkspecification","title":"LinkSpecification","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#active-learning","title":"Active learning","text":"<p>Executes an active learning iteration.</p> Parameter Type Description Example fixedRandomSeed boolean No description <p>The identifier for this plugin is <code>ActiveLearning</code>.</p> <p>It can be found in the package <code>org.silkframework.learning.active</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#evaluate-linking","title":"Evaluate linking","text":"<p>Evaluates the linking task by generating links.</p> Parameter Type Description Example includeReferenceLinks boolean Do not generate a link for which there is a negative reference link while always generating positive reference links. useFileCache boolean Use a file cache. This avoids memory overflows for big files. partitionSize int The number of entities in a single partition in the cache. generateLinksWithEntities boolean Generate detailed information about the matched entities. If set to false, the generated links won\u2019t be shown in the Workbench. writeOutputs boolean Write the generated links to the configured output of this task. linkLimit int If defined, the execution will stop after the configured number of links is reached.\\This is just a hint and the execution may produce slightly fewer or more links. timeout int Timeout in seconds after that the matching task of an evaluation should be aborted. Set to 0 or negative to disable the timeout. <p>The identifier for this plugin is <code>EvaluateLinking</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-linking","title":"Execute linking","text":"<p>Executes the linking task using the configured execution.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteLinking</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#linking-paths-cache","title":"Linking paths cache","text":"<p>Holds the most frequent paths for the selected entities.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>LinkingPathsCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#reference-entities-cache","title":"Reference entities cache","text":"<p>For each reference link, the reference entities cache holds all values of the linked entities.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ReferenceEntitiesCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.linking</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#supervised-learning","title":"Supervised learning","text":"<p>Executes the supervised learning.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>SupervisedLearning</code>.</p> <p>It can be found in the package <code>org.silkframework.learning.active</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#scheduler","title":"Scheduler","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#activate","title":"Activate","text":"<p>Executes the scheduler</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteScheduler</code>.</p> <p>It can be found in the package <code>com.eccenca.di.scheduler</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#scripttask","title":"ScriptTask","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-script","title":"Execute Script","text":"<p>Executes the script.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteScript</code>.</p> <p>It can be found in the package <code>com.eccenca.di.scripting.scala</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#transformspecification","title":"TransformSpecification","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-transform","title":"Execute transform","text":"<p>Executes the transformation.</p> Parameter Type Description Example limit IntOptionParameter Limits the maximum number of entities that are transformed. <p>The identifier for this plugin is <code>ExecuteTransform</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.transform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#transform-paths-cache","title":"Transform paths cache","text":"<p>Holds the most frequent paths for the selected entities.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>TransformPathsCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.transform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#target-vocabulary-cache","title":"Target vocabulary cache","text":"<p>Holds the target vocabularies</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>VocabularyCache</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.transform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#workflow","title":"Workflow","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-locally","title":"Execute locally","text":"<p>Executes the workflow locally.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteLocalWorkflow</code>.</p> <p>It can be found in the package <code>org.silkframework.workspace.activity.workflow</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#workflowexecution","title":"WorkflowExecution","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#generate-spark-assembly","title":"Generate Spark assembly","text":"<p>Generate project and Spark assembly artifacts and deploy them using the specified configuration settings: type, artifact and options like destination in case of a simple copy</p> Parameter Type Description Example executeStaging boolean Execute loading phase executeTransform boolean Execute transform phase executeLoading boolean Execute staging phase <p>The identifier for this plugin is <code>DeploySparkWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#default-execution","title":"Default execution","text":"<p>Executes a workflow with the executor defined in the configuration</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteDefaultWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-operator","title":"Execute operator","text":"<p>Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster.</p> Parameter Type Description Example operator TaskReference The workflow to execute. <p>The identifier for this plugin is <code>ExecuteSparkOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-on-spark","title":"Execute on Spark","text":"<p>Executes a workflow on with an executor that uses Apache Spark. Depending on the Spark configuration it can still run on a single local machine or on a cluster.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ExecuteSparkWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#execute-with-payload","title":"Execute with payload","text":"<p>Executes a workflow with custom payload.</p> Parameter Type Description Example configuration MultilineStringParameter No description configurationType String No description <p>The identifier for this plugin is <code>ExecuteWorkflowWithPayload</code>.</p> <p>It can be found in the package <code>org.silkframework.workbench.workflow</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/activity-reference/#generate-view","title":"Generate view","text":"<p>Generate and share a view on a workflow executed by the Spark executor. Executes a workflow on Spark and generates a SparkSQL temporary table instead of serializing the result. The table can be accessed via JDBC</p> Parameter Type Description Example caching boolean Optional parameter that enables caching (default=false). userDefinedName String Optional View name that is used when a view on a non virtual is generated (default = [TASK-ID]_generated_view). <p>The identifier for this plugin is <code>GenerateSparkView</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.virtual</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/","title":"Plugin Reference","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#plugin-tasks","title":"Plugin Tasks","text":"<p>The following plugin tasks are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cancel-workflow","title":"Cancel Workflow","text":"<p>Cancels a workflow if a specified condition is fulfilled.</p> Parameter Type Description Default typeUri Uri The entity type to check the condition on. condition Enum The cancellation condition empty invertCondition boolean If true, the specified condition will be inverted, i.e., the workflow execution will be cancelled if the condition is not fulfilled. false <p>The identifier for this plugin is <code>CancelWorkflow</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.cancel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sql-query","title":"SQL query","text":"<p>Executes a custom SQL query on the first input dataset and returns the result as its output.</p> Parameter Type Description Default command MultilineStringParameter SQL command. The name of the table in the statement must be \u2018dataset\u2019, regardless the input. <p>The identifier for this plugin is <code>CustomSQLExecution</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.operator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-json","title":"Parse JSON","text":"<p>Takes exactly one input and reads either the defined inputPath or the first value of the first entity as a JSON document. Then executes incoming requests as if this were a JSON dataset, e.g. form a transformation task.</p> Parameter Type Description Default inputPath String The Silk path expression of the input entity that contains the JSON document. If not set, the value of the first defined property will be taken. empty string basePath String The path to the elements to be read, starting from the root element, e.g., \u2018/Persons/Person\u2019. If left empty, all direct children of the root element will be read. empty string uriSuffixPattern String A URI pattern that is relative to the base URI of the input entity, e.g., /{ID}, where {path} may contain relative paths to elements. This relative part is appended to the input entity URI to construct the full URI pattern. empty string <p>The identifier for this plugin is <code>JsonParserOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.json</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#join-tables","title":"Join tables","text":"<p>Joins a set of inputs into a single table. Expects a list of entity tables and links.  All entity tables are joined into the first entity table using the provided links.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>Merge</code>.</p> <p>It can be found in the package <code>com.eccenca.di.merge</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#merge-tables","title":"Merge tables","text":"<p>Stores sets of instance and mapping inputs as relational tables with the mapping as an n:m relation. Expects a list of entity tables and links.  All entity tables have a relation to the first entity table using the provided links.</p> Parameter Type Description Default multiTableOutput boolean test true pivotTableName String Name of the pivot table. empty string mappingNames String Name of the mapping tables. Comma separated list. empty string instanceSetNames String Name of the tables joined to the pivot. Comma separated list. empty string <p>The identifier for this plugin is <code>MultiTableMerge</code>.</p> <p>It can be found in the package <code>com.eccenca.di.merge</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pivot","title":"Pivot","text":"<p>The pivot operator takes data in separate rows, aggregates it and converts it into columns. This operator can be used in a workflow right after a mapping task.</p> Parameter Type Description Default pivotProperty String The pivot column. no default firstGroupProperty String The name of the first group column in the range. no default lastGroupProperty String The name of the last group column in the range. If left empty, only the first column is used. no default valueProperty String The property that contains the values that will be aggregated. no default aggregationFunction Enum The aggregation function used to aggregate values. sum uriPrefix String Prefix to prepend to all generated pivot columns. empty string <p>The identifier for this plugin is <code>Pivot</code>.</p> <p>It can be found in the package <code>com.eccenca.di.pivot</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rest-request","title":"REST request","text":"<p>Executes a REST request based on fixed configuration and/or input parameters and returns the result as entity.</p> Parameter Type Description Default url String The URL to execute this request against. This can be overwritten at execution time via input. empty string method String The HTTP method. One of GET, PUT or POST GET accept String The accept header String. empty string requestTimeout int Request timeout in ms. The overall maximum time the request should take. 10000 connectionTimeout int Connection timeout in ms. The time until which a connection with the remote end must be established. 5000 readTimeout int Read timeout in ms. The max. time a request stays idle, i.e. no data is send or received. 10000 contentType String The content-type header String. This can be set in case of PUT or POST. If another content type comes back, the task will fail. empty string content String The content that is send with a POST or PUT request. For handling this payload dynamically this parameter must be overwritten via the task input. empty string httpHeaders MultilineStringParameter Configure additional HTTP headers. One header per line. Each header entry follows the curl syntax. readParametersFromInput boolean If this is set to true, specific parameters can be overwritten at execution time. Else inputs are ignored. Parameters that can currently be overwritten: url, content false multipartFileParameter String If set to a non-empty String then instead of a normal POST a multipart/form-data file upload request is executed. This value is used as the form parameter name. empty string authorizationHeader String The authorization header. This is usually either \u2018Authorization\u2019 or \u2018Proxy-Authorization\u2019If left empty, no authorization header is sent. empty string authorizationHeaderValue PasswordParameter The authorization header value. Usually this has the form \u2018type secret\u2019, e.g. for OAuth \u2018bearer .\u2019This config parameter will be encrypted in the backend. acceptAnySslCertificate boolean If enabled this will accept any SSL certificate, i.e. make SSL connections unsecure. Only enable if you know what you are doing! false <p>The identifier for this plugin is <code>RestOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.rest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#scheduler","title":"Scheduler","text":"<p>Executes a workflow at specified intervals.</p> Parameter Type Description Default task TaskReference The name of the workflow to be executed no default interval Duration The interval at which the scheduler should run the referenced task. Must be in ISO-8601 duration format PnDTnHnMn.nS PT15M startTime String The time when the scheduled task is run for the first time, e.g., 2017-12-03T10:15:30. If no start time is set, midnight on the day the scheduler is started is assumed. empty string enabled boolean Enables or disables the scheduler. true stopOnError boolean If true, this will stop the scheduler, so the failed task is not scheduled again for execution. false <p>The identifier for this plugin is <code>Scheduler</code>.</p> <p>It can be found in the package <code>com.eccenca.di.scheduler</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#search-addresses","title":"Search addresses","text":"<p>Looks up locations from textual descriptions using the configured geocoding API. Outputs results as RDF.</p> Parameter Type Description Default searchAttributes StringTraversableParameter List of attributes that contain search terms. Multiple attributes (comma-separated) will be concatenated into a single search. no default limit IntOptionParameter Optionally limits the number of results for each search. jsonLdContext ResourceOption Optional JSON-LD context to be used for converting the returned JSON to RDF. If not provided, a default context will be used. additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>SearchAddresses</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#send-email","title":"Send eMail","text":"<p>Sends an eMail using an SMTP server. If connected to a dataset that is based on a file in a workflow, it will send that file whenever the workflow is executed It can be used to send the result of a workflow via Mail.</p> Parameter Type Description Default host String The SMTP host, e.g, mail.myProvider.com no default port int The SMTP port 587 user String Username empty string password PasswordParameter Password from String The sender eMail address empty string receiver String The email addresses of the receivers. Email addresses are comma separated. Names must be quoted when containing commas.Example: john.smith@example.com, \u201cDoe, John\u201d john.doe@example.com, needs no quoting needs.no.quoting@example.com empty string cc String The CC-receiver eMail address. Email addresses are comma separated. Names must be quoted when containing commas.Example: john.smith@example.com, \u201cDoe, John\u201d john.doe@example.com, needs no quoting needs.no.quoting@example.com empty string bcc String The BCC-receiver eMail address. Email addresses are comma separated. Names must be quoted when containing commas.Example: john.smith@example.com, \u201cDoe, John\u201d john.doe@example.com, needs no quoting needs.no.quoting@example.com empty string subject String The eMail subject Dataset message MultilineStringParameter The eMail text message withAttachment boolean If enabled a file from the input is attached to the email. A single input to this operator is expected that provides a file, e.g. a file based dataset (XML, JSON etc.). true sslConnection boolean When enabled a SSL/TLS connection will be forced from the start without negotiation with the server. Not to be confused with STARTTLS which upgrades an insecure connection to a SSL/TLS connection, which is done by default. false timeout int Timeout in milliseconds to establish a connection or wait for a server response. Setting it to 0 or negative number will disable the timeout. 10000 readParametersFromInput boolean When enabled this allows to send multiple e-mails. All e-mail configurations are input via the first operator input with each entry representing a different e-mail. The optional second input can be a file based dataset for the attachment. E-mail parameters that can be overwritten are: from, receiver, cc, bcc, subject and message. false nrRetries int The number of retries per email when send errors are encountered. 2 delayBetweenDeliveriesMS int The delay in milliseconds between sending two consecutive e-mails. This applies to the retry mechanism, but also to sending multiple e-mails. 2 <p>The identifier for this plugin is <code>SendEMail</code>.</p> <p>It can be found in the package <code>com.eccenca.di.mail</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#execute-spark-function","title":"Execute Spark function","text":"<p>Applies a specified Scala function to a specified field. E.g. when the inputField is \u2018name\u2019, the inputFunction is \u2018any =&gt; \u201cArrrrgh!\u201d and the alias is \u2018xxx\u2019,)\u2019 a query corresponding to \u2018Function existingField1, existingFiled2, \u2026 \u201cArrrrgh!\u201d as \u201cxxx\u201d\u2019  will be generated. If alias is empty the inputField will be overwritten, otherwise a new field will be added and the rest of the schema stays the same.</p> Parameter Type Description Default function MultilineStringParameter Scala function expression. inputField String Input field. empty string alias String Alias. no default <p>The identifier for this plugin is <code>SparkFunction</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.operator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#evaluate-template","title":"Evaluate template","text":"<p>Evaluates a template on a sequence of entities. Can be used after a transformation or directly after datasets that output a single table, such as CSV or Excel. For each input entity, a output entity is generated that provides a single output attribute, which contains the evaluated template.</p> Parameter Type Description Default template MultilineStringParameter The template no default language Enum The template language. Currently, Jinja is supported. Jinja outputAttribute String The attribute in the output that will hold the evaluated template. output forwardInputAttributes boolean If true, the input attributes will be forwarded to the output. false <p>The identifier for this plugin is <code>Template</code>.</p> <p>It can be found in the package <code>com.eccenca.di.templating.operators</code>.</p> <p>The template operator supports the Jinja templating language. Documentation about Jinja can be found in the official Template Designer Documentation.</p> <p>Currently, the template operator does have the following limitations: - As Jinja does not support special characters, such as colons, in variable names, RDF properties cannot be accessed. For this reason, the transformation that precedes the template operator needs to make sure that it generates attributes that are valid Jinja variable names. - Accessing nested paths is not supported. If the preceding transformation contains hierarchical mappings, only the attributes from the root mapping can be accessed.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#unpivot","title":"Unpivot","text":"<p>Given a list of table columns, transforms those columns into attribute-value pairs. This operator can be used in a workflow right after a mapping task.</p> Parameter Type Description Default firstPivotProperty String The name of the first pivot column in the range. no default lastPivotProperty String the name of the last pivot column in the range. If left empty, all columns starting with the first pivot column are used. no default attributeProperty String The URI of the output column used to hold the attribute. attribute valueProperty String The URI of the output column used to hold the value. value pivotColumns String Comma separated list of pivot column names. This property will override all inferred columns of the first two arguments. empty string <p>The identifier for this plugin is <code>Unpivot</code>.</p> <p>It can be found in the package <code>com.eccenca.di.unpivot</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-xml","title":"Parse XML","text":"<p>Takes exactly one input and reads either the defined inputPath or the first value of the first entity as XML document. Then executes the given output entity schema similar to the XML dataset to construct the result entities.</p> Parameter Type Description Default inputPath String The Silk path expression of the input entity that contains the XML document. If not set, the value of the first defined property will be taken. empty string basePath String The path to the elements to be read, starting from the root element, e.g., \u2018/Persons/Person\u2019. If left empty, all direct children of the root element will be read. empty string uriSuffixPattern String A URI pattern that is relative to the base URI of the input entity, e.g., /{ID}, where {path} may contain relative paths to elements. This relative part is appended to the input entity URI to construct the full URI pattern. empty string <p>The identifier for this plugin is <code>XmlParserOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#upload-file-to-knowledge-graph","title":"Upload File to Knowledge Graph","text":"<p>Uploads an N-Triples file from the file repository to a \u2018Knowledge Graph\u2019 dataset. The output of this operatorcan be the input of datasets that support graph store file upload, e.g. \u2018Knowledge Graph\u2019. The file will be uploaded to the graph specified in that dataset.</p> Parameter Type Description Default fileNT Resource N-Triples file from the resource repository that should be uploaded to the Knowledge Graph. no default maxChunkSizeInMB int The N-Triples file will be split into multiple chunks if the file size exceeds the max chunk size. no default <p>The identifier for this plugin is <code>eccencaDataPlatformGraphStoreFileUploadOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.dataplatform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-construct-query","title":"SPARQL Construct query","text":"<p>A task that executes a SPARQL Construct query on a SPARQL enabled data source and outputs the SPARQL result. If the result should be written to the same RDF store it is read from, the SPARQL Update operator is preferable.</p> Parameter Type Description Default query MultilineStringParameter A SPARQL 1.1 construct query no default tempFile boolean When copying directly to the same SPARQL Endpoint or when copying large amounts of triples, set to True by default true <p>The identifier for this plugin is <code>sparqlCopyOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.tasks</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-select-query","title":"SPARQL Select query","text":"<p>A task that executes a SPARQL Select query on a SPARQL enabled data source and outputs the SPARQL result. If the SPARQL source is defined on a specific graph, a FROM clause will be added to the query at execution time, except when there already exists a GRAPH or FROM clause in the query. FROM NAMED clauses are not injected.</p> Parameter Type Description Default selectQuery MultilineStringParameter A SPARQL 1.1 select query no default limit String If set to a positive integer, the number of results is limited empty string optionalInputDataset SparqlEndpointDatasetParameter An optional SPARQL dataset that can be used for example data, so e.g. the transformation editor shows mapping examples. sparqlTimeout int SPARQL query timeout (select/update) in milliseconds. A value of zero means that there is no timeout set explicitly. If a value greater zero is specified this overwrites possible default timeouts. 0 <p>The identifier for this plugin is <code>sparqlSelectOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.tasks</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-update-query","title":"SPARQL Update query","text":"<p>A task that outputs SPARQL Update queries for every entity from the input based on a SPARQL Update template. The output of this operator should be connected to the SPARQL datasets to which the results should be written. In contrast to the SPARQL select operator, no FROM clause gets injected into the query.</p> Parameter Type Description Default sparqlUpdateTemplate MultilineStringParameter \\This operator takes a SPARQL Update Query Template that depending on the templating mode (Simple/Velocity Engine) supports\\a set of templating features, e.g. filling in input values via placeholders in the template.\\Example for the \u2018Simple\u2019 mode:\\  DELETE DATA { ${} rdf:label ${\u201cPROP_FROM_ENTITY_SCHEMA2\u201d} }\\  INSERT DATA { ${} rdf:label ${\u201cPROP_FROM_ENTITY_SCHEMA3\u201d} }\\  \\  This will insert the URI serialization of the property value PROP_FROM_ENTITY_SCHEMA1 for the ${} expression.\\  And it will insert a plain literal serialization for the property values PROP_FROM_ENTITY_SCHEMA2/3 for the template literal expressions.\\  It is be possible to write something like ${\u201cPROP\u201d}^^http://someDatatype or ${\u201cPROP\u201d}@en.\\Example for the \u2018Velocity Engine\u2019 mode:\\  DELETE DATA { $row.uri(\u201cPROP_FROM_ENTITY_SCHEMA1\u201d) rdf:label $row.plainLiteral(\u201cPROP_FROM_ENTITY_SCHEMA2\u201d) }\\  #if ( $row.exists(\u201cPROP_FROM_ENTITY_SCHEMA1\u201d) )\\    INSERT DATA { $row.uri(\u201cPROP_FROM_ENTITY_SCHEMA1\u201d) rdf:label $row.plainLiteral(\u201cPROP_FROM_ENTITY_SCHEMA3\u201d) }\\  #end\\  Input values are accessible via various methods of the \u2018row\u2019 variable:\\  - uri(inputPath: String): Renders an input value as URI. Throws exception if the value is no valid URI.\\  - plainLiteral(inputPath: String): Renders an input value as plain literal, i.e. escapes problematic characters etc.\\  - rawUnsafe(inputPath: String): Renders an input value as is, i.e. no escaping is done. This should only be used \u2013 better never \u2013 if the input values can be trusted.\\  - exists(inputPath: String): Returns true if a value for the input path exists, else false.\\  The methods uri, plainLiteral and rawUnsafe throw an exception if no input value is available for the given input path.\\  In addition to input values, properties of the input and output tasks can be accessed via the inputProperties and outputProperties objects\\  in the same way as the row object, e.g.\\    $inputProperties.uri(\u201cgraph\u201d)\\  For more information about the Velocity Engine visit http://velocity.apache.org.\\ no default batchSize int How many entities should be handled in a single update request. 1 templatingMode Enum The templating mode. \u2018Simple\u2019 only allows simple URI and literal insertions, whereas \u2018Velocity Engine\u2019 supports complex templating. See \u2018Sparql Update Template\u2019 parameter description for examples and http://velocity.apache.org for details on the Velocity templates. simple <p>The identifier for this plugin is <code>sparqlUpdateOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.tasks</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#request-rdf-triples","title":"Request RDF triples","text":"<p>A task that requests all triples from an RDF dataset.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>tripleRequestOperator</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.tripleRequest</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize-units-of-measurement","title":"Normalize units of measurement","text":"<p>Custom task that will substitute numeric values and pertaining unit symbols with a SI-system-unit normalized representation in three columns:   * The normalized numeric value.   * The unit symbol of the SI-system-unit pertaining to the value.   * The origin unit symbol from which it was normalized (so we are able to reverse this action).</p> Parameter Type Description Default valueProperties String The names (comma-separated) of columns containing numeric values interpreted as quantities of the dimension indicated by the pertaining unit. no default unitProperties String The names (comma-separated) of dedicated columns containing the unit symbol for the pertaining value in the value column (the positions in this list have to align with the pertaining value columns). Either this param or \u2018static unit\u2019 has to be set. empty string staticUnits String Unit symbols (comma-separated) defining the unit for all values in the pertaining value column. If set, the \u2018unitProperty\u2019 param will be ignored and all values of the value column have to be numbers without unit symbols (the positions in this list have to align with the pertaining value columns). empty string targetUnits String Unit symbols (comma-separated) defining the target unit to which the value column will be converted (Note: Make sure the input unit can be converted to the target unit). By default the pertaining SI-base unit will be used as normalization unit (the positions in this list have to align with the pertaining value columns) empty string suppressErrors boolean If true, will ignore any parsing or value conversion error and return an empty result (might happen because of unknown unit symbols or non-numbers as values). Beware, the value will be lost completely! false configFilePath WritableResource An absolute file path for a unit CSV configuration file (for syntax see \u2018configuration\u2019 param). If set, the \u2018configuration\u2019 param will be ignored. EmptyResource configuration MultilineStringParameter While all SI units and decimal prefixes are supported by default, custom or obsolete units have to be added via this configuration.\\       NOTE: when constructing formulae depending on other units defined in the configuration, make sure to order them dependently.\\       ALSO: Rational numbers are not supported by the UCUM syntax, express them as a fraction (see \u2018grain\u2019 example below).\\ # Example configuration, don\u2019t forget to remove the \u2018#\u2019 in front of each row.#      CSV COLUMNS:#       * unit name - the human readable name of the unit#       * override  - (true <p>The identifier for this plugin is <code>ucumNormalizationTask</code>.</p> <p>It can be found in the package <code>com.eccenca.di.measure</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#xslt","title":"XSLT","text":"<p>A task that converts an XML resource via an XSLT script and writes the transformed output into a file resource.</p> Parameter Type Description Default file Resource The XSLT file to be used for transforming XML. no default <p>The identifier for this plugin is <code>xsltOperator</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#dataset-plugins","title":"Dataset Plugins","text":"<p>The following dataset plugins are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#deprecated","title":"Deprecated","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparksql-view","title":"SparkSQL view","text":"<p>Please use SQL endpoint (embedded) instead.</p> Parameter Type Description Default viewName String The name of the view. This specifies the table that can be queried by another virtual dataset or via JDBC (the \u2018default\u2019 schema is used for all virtual datasets). no default query String Optional SQL query on the selected table. Has no effect when used as an output dataset. empty string cache boolean Optional boolean option that selects if the table should be cached by Spark or not (default = true). true uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string charset String The source internal encoding, e.g., UTF8, ISO-8859-1 UTF-8 arraySeparator String The character that is used to separate the parts of array values. Write \u201cback slash t\u201d to specify the tab character. useCompatibleTypes boolean If true, basic types will be used for types that otherwise would result in client errors. This mainly that arrays will be stored as Strings separated by the separator defined above. If the view is only for use within a SparkContext, this can be set to false. true <p>The identifier for this plugin is <code>sparkView</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.virtual</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#uncategorized","title":"Uncategorized","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#text","title":"Text","text":"<p>Reads and writes plain text files.</p> Parameter Type Description Default file WritableResource The plain text file. no default charset String The file encoding, e.g., UTF-8, UTF-8-BOM, ISO-8859-1 UTF-8 typeName String A type name that represents this file. type property String The single property that holds the text. text <p>The identifier for this plugin is <code>text</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.text</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#embedded","title":"embedded","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#hive-database","title":"Hive database","text":"<p>Read from or write to an embedded Apache Hive endpoint.</p> Parameter Type Description Default schema String Name of the hive schema or namespace. empty string table String Name of the hive table. no default query String Optional query for projection and selection (e.g. \u201d SELECT * FROM table WHERE x = true\u201d. empty string uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string charset String The source internal encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>Hive</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#knowledge-graph","title":"Knowledge Graph","text":"<p>Read RDF from or write RDF to a Knowledge Graph embedded in Corporate Memory.</p> Parameter Type Description Default endpoint String The named endpoint within the eccenca DataPlatform. default graph String The URI of the named graph. no default pageSize int The number of solutions to be retrieved per SPARQL query. 100000 pauseTime int The number of milliseconds to wait between subsequent query 0 retryCount int The number of retries if a query fails 3 retryPause int The number of milliseconds to wait until a failed query is retried. 1000 strategy Enum The strategy use for retrieving entities: simple: Retrieve all entities using a single query; subQuery: Use a single query, but wrap it for improving the performance on Virtuoso; parallel: Use a separate Query for each entity property. parallel clearGraphBeforeExecution boolean If set to true this will clear the specified graph before executing a workflow that writes to it. false entityList MultilineStringParameter A list of entities to be retrieved. If not given, all entities will be retrieved. Multiple entities are separated by whitespace. sparqlTimeout int SPARQL query timeout (select/update) in milliseconds. A value of zero means that there is no timeout. If a value greater zero is specified this overwrites possible default timeouts. This timeout is also propagated to DataPlatform and may overwrite default timeouts there. 0 optimizedRetrieve boolean Optimized retrieval method to remove load from the underlying triple store. Query parallelism is limited and cheaper queries are executed against the backend. By putting the main work on DataIntegration side, the RDF backend is kept responsive. true <p>The identifier for this plugin is <code>eccencaDataPlatform</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.dataplatform</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#in-memory-dataset","title":"In-memory dataset","text":"<p>A Dataset that holds all data in-memory.</p> Parameter Type Description Default clearGraphBeforeExecution boolean If set to true this will clear this dataset before it is used in a workflow execution. true <p>The identifier for this plugin is <code>inMemory</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#internal-dataset","title":"Internal dataset","text":"<p>Dataset for storing entities between workflow steps.</p> Parameter Type Description Default graphUri String The RDF graph that is used for storing internal data null <p>The identifier for this plugin is <code>internal</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sql-endpoint","title":"SQL endpoint","text":"<p>Provides a JDBC endpoint that exposes workflow or transformation results as tables, which can be queried using SQL.</p> Parameter Type Description Default tableNamePrefix String Prefix of the table that will be shared. In the case of complex mappings more than one table will be created. If one name is given it will be used as a prefix for table names. If left empty the table names will be generated from the user name and time stamps and start with \u2018root\u2019, \u2018object-mapping\u2019 empty string cache boolean Optional boolean option that selects if the table should be cached by Spark or not (default = true). true arraySeparator String The character that is used to separate  the parts of array values. Write \\t to specify the tab character. useCompatibleTypes boolean If true, basic types will be used for unusual data types that otherwise may result in client errors. Try switching this on, if a client has weird error messages. (Default = true) true map Map Mapping of column names. Similar to aliases E.g. \u2018c1:c2\u2019 would rename column c1 into c2. <p>The identifier for this plugin is <code>sqlEndpoint</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.endpoint</code>.</p> <p>SQL endpoint dataset parameters</p> <p>The dataset only requires that the tableNamePrefix parameter is given. This will be used as the prefix for the names of the generated tables. When a set of entities is written to the endpoint a view is generated for each entity type (defined by an \u2018rdf_type\u2019 attribute). That means that the mapping or data source that are used as input for the SQL endpoint need to have a type or require a user defined type mapping.</p> <p>The operator has a compatibility mode. This mode will avoid complex types such as Arrays. When arrays exist in the input they are converted to a String using the given arraySeparator. This avoids errors and warnings in some Jdbc clients that are unable to handle typed arrays and may make working with software like Excel easier.</p> <p>The parameter aliasMap of the endpoint allows the specification of column aliases. The map is a comma separated list of key-value pairs. Each key and value is denoted by <code>key:value</code>. An example for renaming 2 columns (source1, source2 to target1, target2) in the result would be: <code>source1:target1,source2:target2</code></p> <p>Note: Table and column (mapping target) names will be automatically converted to be valid in as many databases as possible. Table names will be shortened to 128 characters. Only a-z, A-Z, 0-9 and _ are allowed. Others will be replaced with an underscore. Column names undergo the same transformation but will be converted to lower case as well. The log will inform about changes. The table names will be generated based on the target type of each mapping. The user needs to make sure that each object mapping specifies a unique type. If two object mappings define the same type, only the last one will be written.</p> <p>SQL endpoint activity</p> <p>See [ActivityDocumentation] for a general description of the Data Integration activities. The activity will start automatically, when the SQL endpoint is used as a data sink and Data Integration is configured to make the SQL endpoint accessible remotely.</p> <p>When the activity is started and running it returns the server status and JDBC URL as its value.</p> <p>Stopping the activity will drop all views generated by the activity. It can be restarted by rerunning the workflow containing it as a sink.</p> <p>Remote client configuration (via JDBC and ODBC)</p> <p>Within Data Integration the SQL endpoint can be used as a source or sink like any other dataset. If the startThriftServer option is set to \u2018true\u2019 access via JDBC or ODBC is possible.</p> <p>ODBC and JDBC drivers can be used to connect to relational databases.</p> <p>When selecting a version of a driver the client operating system and its type (32bit/64 bit) are the most important factors. The version of the client drivers sometimes is the same as the server\u2019s. If no version of a driver is given, the newest driver of the vendor should work, as it should be backwards compatible.</p> <p>Any JDBC or ODBC client can connect to an SQL endpoint dataset. SparkSQL uses the same query processing as Hive, therefore the requirements for the client are:</p> <ul> <li>A JDBC driver compatible with Hive 1.2.1<sup>1</sup> (platform independent driver org.apache.hive.jdbc.HiveDriver is needed) or</li> <li>A JDBC driver compatible with Spark 2.3.3</li> <li>A Hive ODBC driver (ODBC driver for the client architecture and operating system needed)</li> </ul> <p>A detailed instruction to connect to a Hive or SparkSQL endpoint with various tools (e.g. SQuirreL, beeline, SQL Developer, \u2026) can be found at Apache HiveServer2 Clients. The database client DBeaver can connect to the SQL endpoint out of the box.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#variable-dataset","title":"Variable dataset","text":"<p>Dataset that acts as a placeholder in workflows and is replaced at request time.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>variableDataset</code>.</p> <p>It can be found in the package <code>org.silkframework.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#file","title":"file","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#alignment","title":"Alignment","text":"<p>Writes the alignment format specified at http://alignapi.gforge.inria.fr/format.html.</p> Parameter Type Description Default file WritableResource The alignment file. no default <p>The identifier for this plugin is <code>alignment</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#avro","title":"Avro","text":"<p>Read from or write to an Apache Avro file.</p> Parameter Type Description Default file WritableResource Path (e.g. relative like \u2018path/filename.avro\u2019 or absolute \u2018hdfs:///path/filename.avro\u2019). no default uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>avro</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#csv","title":"CSV","text":"<p>Read from or write to an CSV file.</p> Parameter Type Description Default file WritableResource The CSV file. This may also be a zip archive of multiple CSV files that share the same schema. no default properties String Comma-separated list of properties. If not provided, the list of properties is read from the first line. Properties that are no valid (relative or absolute) URIs will be encoded. empty string separator String The character that is used to separate values. If not provided, defaults to \u2018,\u2019, i.e., comma-separated values. \u201c\\t\u201d for specifying tab-separated values, is also supported. , arraySeparator String The character that is used to separate the parts of array values. Write \u201c\\t\u201d to specify the tab character. empty string quote String Character used to quote values. \u201c uri String Deprecated A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string charset String The file encoding, e.g., UTF-8, UTF-8-BOM, ISO-8859-1 UTF-8 regexFilter String A regex filter used to match rows from the CSV file. If not set all the rows are used. empty string linesToSkip int The number of lines to skip in the beginning, e.g. copyright, meta information etc. 0 maxCharsPerColumn int The maximum characters per column. Warning: System will request heap memory of that size (2 bytes per character) when reading the CSV. If there are more characters found, the parser will fail. 128000 ignoreBadLines boolean If set to true then the parser will ignore lines that have syntax errors or do not have to correct number of fields according to the current config. false quoteEscapeCharacter String Escape character to be used inside quotes, used to escape the quote character. It must also be used to escape itself, e.g. by doubling it, e.g. \u201c\u201d. If left empty, it defaults to quote. \u201c zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. .*\\.csv$ <p>The identifier for this plugin is <code>csv</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.csv</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel","title":"Excel","text":"<p>Read from or write to an Excel workbook in Open XML format (XLSX).</p> Parameter Type Description Default file WritableResource File name inside the resources directory. no default streaming boolean Streaming enables reading and writing large Excels files. Warning: Be careful to disable streaming for large datasets (&gt; 10MB), because of high memory consumption. true linesToSkip int The number of lines to skip in the beginning when reading files. 0 hasHeader boolean If true, the first line will be read as the table header, which defines the column names. If false, the first line will be read as data. In that case, the columns need to be adressed using #A, #B, etc. true outputObjectValues boolean Output results from object rules (URIs). true <p>The identifier for this plugin is <code>excel</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rdf","title":"RDF","text":"<p>Dataset which retrieves and writes all entities from/to an RDF file. The dataset is loaded in-memory and thus the size is restricted by the available memory. Large datasets should be loaded into an external RDF store and retrieved using the SPARQL dataset instead.</p> Parameter Type Description Default file WritableResource The RDF file. This may also be a zip archive of multiple RDF files. no default format String Optional RDF format. If left empty, it will be auto-detected based on the file extension. N-Triples is the only format that can be written, while other formats can only be read. empty string graph String The graph name to be read. If not provided, the default graph will be used. Must be provided if the format is N-Quads. empty string entityList MultilineStringParameter A list of entities to be retrieved. If not given, all entities will be retrieved. Multiple entities are separated by whitespace. zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. .* <p>The identifier for this plugin is <code>file</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#json","title":"JSON","text":"<p>Read from or write to a JSON file.</p> Parameter Type Description Default file WritableResource Json file. no default template MultilineStringParameter Template for writing JSON. The term {{output}} will be replaced by the written JSON. {{output}} basePath String The path to the elements to be read, starting from the root element, e.g., \u2018/Persons/Person\u2019. If left empty, all direct children of the root element will be read. empty string uriPattern String A URI pattern, e.g., http://namespace.org/{ID}, where {path} may contain relative paths to elements empty string maxDepth int Maximum depth of written JSON. This acts as a safe guard if a recursive structure is written. 15 streaming boolean Streaming allows for reading large JSON files. If streaming is enabled, backward paths are not supported. true <p>The identifier for this plugin is <code>json</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.json</code>.</p> <p>Typically, this dataset is used to transform an JSON file to another format, e.g., to RDF.</p> <p>It supports a number of special paths: - <code>#id</code> Is a special syntax for generating an id for a selected element. It can be used in URI patterns for entities which do not provide an identifier. Examples: <code>http://example.org/{#id}</code> or <code>http://example.org/{/pathToEntity/#id}</code>. - <code>#text</code> retrieves the text of the selected node. - The backslash can be used to navigate to the parent JSON node, e.g., <code>\\parent/key</code>. The name of the backslash key (here <code>parent</code>) is ignored.</p> <p>When storing entities in Json format all entities will be stored in an array at the top-level of the Json document. The option makeFirstEntityJsonObject (false by default) can change this. If activated a top level object will be used. To preserve valid Json, only the first entity will be stored in this case.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#multi-csv-zip","title":"Multi CSV ZIP","text":"<p>Reads from or writes to multiple CSV files from/to a single ZIP file.</p> Parameter Type Description Default file WritableResource Zip file name inside the resources directory/repository. no default separator String The character that is used to separate values. If not provided, defaults to \u2018,\u2019, i.e., comma-separated values. \u201c\\t\u201d for specifying tab-separated values, is also supported. , arraySeparator String The character that is used to separate the parts of array values. Write \u201c\\t\u201d to specify the tab character. empty string quote String Character used to quote values. \u201c charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 linesToSkip int The number of lines to skip in the beginning, e.g. copyright, meta information etc. 0 maxCharsPerColumn int The maximum characters per column. If there are more characters found, the parser will fail. 128000 ignoreBadLines boolean If set to true then the parser will ignore lines that have syntax errors or do not have to correct number of fields according to the current config. false quoteEscapeCharacter String Escape character to be used inside quotes, used to escape the quote character. It must also be used to escape itself, e.g. by doubling it, e.g. \u201c\u201d. If left empty, it defaults to quote. \u201c append boolean If \u2018True\u2019 then files in the ZIP archive are only added or updated, all other files in the ZIP stay untouched. If \u2018False\u2019 then a new ZIP file will be created on every dataset write. true zipFileRegex String Filter file paths inside the ZIP file via this regex. By default sub folders or files not ending with .csv are ignored. ^[^/]*\\.csv$ <p>The identifier for this plugin is <code>multiCsv</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.csv</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#orc","title":"ORC","text":"<p>Read from or write to an Apache ORC file.</p> Parameter Type Description Default file WritableResource Path (e.g. relative like \u2018path/filename.orc\u2019 or absolute \u2018hdfs:///path/filename.orc\u2019). no default uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string partition String Optional specification of the attribute for output partitioning empty string compression String Optional compression algorithm (e.g. snappy, zlib) snappy charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>orc</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parquet","title":"Parquet","text":"<p>Read from or write to an Apache Parquet file.</p> Parameter Type Description Default file WritableResource Path (e.g. relative like \u2018path/filename.orc\u2019 or absolute \u2018hdfs:///path/filename.parquet\u2019). no default uriPattern String A pattern used to construct the entity URI. If not provided the prefix + the line number is used. An example of such a pattern is \u2018urn:zyx:{id}\u2019 where id is a name of a property. empty string properties String Comma-separated list of URL-encoded properties. If not provided, the list of properties is read from the first line. empty string partition String Optional specification of the attribute for output partitioning empty string compression String Optional compression algorithm (e.g. snappy, zlib) empty string charset String The file encoding, e.g., UTF8, ISO-8859-1 UTF-8 <p>The identifier for this plugin is <code>parquet</code>.</p> <p>It can be found in the package <code>com.eccenca.di.spark.dataset</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#xml","title":"XML","text":"<p>Read from or write to an XML file.</p> Parameter Type Description Default file WritableResource The XML file. This may also be a zip archive of multiple XML files that share the same schema. no default basePath String The base path when writing XML. For instance: /RootElement/Entity. Should no longer be used for reading XML! Instead, set the base path by specifying it as input type on the subsequent transformation or linking tasks. empty string uriPattern String A URI pattern, e.g., http://namespace.org/{ID}, where {path} may contain relative paths to elements empty string outputTemplate MultilineStringParameter The output template used for writing XML. Must be valid XML. The generated entity is identified through a processing instruction of the form &lt;?MyEntity?&gt;. &lt;?Entity?&gt; streaming boolean Streaming allows for reading large XML files. true maxDepth int Maximum depth of written XML. This acts as a safe guard if a recursive structure is written. 15 zipFileRegex String If the input resource is a ZIP file, files inside the file are filtered via this regex. .*\\.xml$ <p>The identifier for this plugin is <code>xml</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.xml</code>.</p> <p>Typically, this dataset is used to transform an XML file to another format, e.g., to RDF. When this dataset is used as an input for another task (e.g., a transformation task), the input type of the consuming task selects the path where the entities to be read are located.</p> <p>Example:</p> <pre><code>&lt;Persons&gt;\n  &lt;Person&gt;\n    &lt;Name&gt;John Doe&lt;/Name&gt;\n    &lt;Year&gt;1970&lt;/Year&gt;\n  &lt;/Person&gt;\n  &lt;Person&gt;\n    &lt;Name&gt;Max Power&lt;/Name&gt;\n    &lt;Year&gt;1980&lt;/Year&gt;\n  &lt;/Person&gt;\n&lt;/Persons&gt;\n</code></pre> <p>A transformation for reading all persons of the above XML would set the input type to <code>/Person</code>. The transformation iterates all entities matching the given input path. In the above example the first entity to be read is:</p> <pre><code>&lt;Person&gt;\n  &lt;Name&gt;John Doe&lt;/Name&gt;\n  &lt;Year&gt;1970&lt;/Year&gt;\n&lt;/Person&gt;\n</code></pre> <p>All paths used in the consuming task are relative to this, e.g., the person name can be addressed with the path <code>/Name</code>.</p> <p>Path examples:</p> <ul> <li>The empty path selects the root element.</li> <li><code>/Person</code> selects all persons.</li> <li><code>/Person[Year = \"1970\"]</code> selects all persons which are born in 1970.</li> <li><code>/#id</code> Is a special syntax for generating an id for a selected element. It can be used in URI patterns for entities which do not provide an identifier. Examples: <code>http://example.org/{#id}</code> or <code>http://example.org/{/pathToEntity/#id}</code>.</li> <li>The wildcard * enumerates all direct children, e.g., <code>/Persons/*/Name</code>.</li> <li>The wildcard ** enumerates all direct and indirect children.</li> <li>The backslash can be used to navigate to the parent XML node, e.g., <code>\\Persons/SomeHeader</code>.</li> <li><code>#text</code> retrieves the text of the selected node.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remote","title":"remote","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jdbc-endpoint","title":"JDBC endpoint","text":"<p>Connect to an existing JDBC endpoint.</p> Parameter Type Description Default url String JDBC URL, must contain the database as parameter, i.g. with ;database=DBNAME or /database depending on the vendor. no default table String Table name. Can be empty if the read-strategy is not set to read the full table. If non-empty it has to contain at least an existing table. empty string sourceQuery MultilineStringParameter Source query (e.g. \u2018SELECT TOP 10 * FROM table WHERE x = true\u2019. Warning: Uses Driver (mySql, HiveQL, MSSql, Postgres) specific syntax. Can be left empty when full tables are loaded. Note: Even if columns with spaces/special characters are named in the query, they need to be referred to URL-encoded in subsequent transformations. groupBy String Comma separated list of attributes appearing in the outer SELECT clause that should be grouped by. The attributes are matched case-insensitive. All other attributes will be grouped via an aggregation function that depends on the supported DBMS, e.g. (JSON) array aggregation. empty string orderBy String Optional column to sort the result set. empty string limit IntOptionParameter Optional limit of returned records. This limit should be pushed to the source. No value implies that no limit will be applied. 10 queryStrategy Enum Query strategy. The strategy decides how the source system is queried. Possible values are: \u2018access-complete-table\u2019 and \u2018query\u2019. access-complete-table writeStrategy Enum Write strategy. If this dataset is a sink, it can be selected if data is overwritten or appended. Possible values are: \u2018update-table\u2019 and \u2018overwrite-table\u2019 default clearTableBeforeExecution boolean If set to true this will clear the specified table before executing a workflow that writes to it. false user String Username. Must be empty in some cases e.g. if secret key and client id are used empty string password PasswordParameter Password. Can be empty in some cases e.g. if secret key and client id are used tokenEndpoint String URL for retrieving tokens, when using MS SQL Active Directory token based authentication. Can be found in the Azure AD Admin Center under OAuth2 endpoint or cab be constructed with the general endpoint URL combined with the tenant id and the suffix /outh/v2/authortized. empty string spnName String Service Principal Name identifying the resource. Usually a static URL like https://database.windows.net. empty string clientId String Client id or application id. Client id used for MS SQL token based authentication. String seperated by - char. empty string clientSecret PasswordParameter Client secret. Client secret used for MS SQL token based authentication. Can be generated in Azure AD admin center. restriction String An SQL WHERE clause to filter the records to be retrieved. empty string retries int Optional number of retries per query 0 pause int Optional pause between queries in ms. 2000 charset String The source internal encoding, e.g., UTF-8, ISO-8859-1 UTF-8 forceSparkExecution boolean If set to true, Spark will be used for querying the database, even if the local execution manager is configured. false <p>The identifier for this plugin is <code>Jdbc</code>.</p> <p>It can be found in the package <code>com.eccenca.di.sql.jdbc</code>.</p> <p>General usage</p> <p>The JDBC dataset supports connections to Hive, Microsoft SQL Server, MySQL, Oracle Database, DB2 and PostgreSQL databases. A login and password and JDBC URL need to be provided. This dataset supports queries or simply schema and table names to define what should be retrieved from a source DB. If the dataset is used as a sink, queries are ignored and only schema and table parameters are used. If the dataset is used as a sink for a hierarchical mapping it behaves similar to the SqlEndpoint: One table is generated per entity type.</p> <p>The names of the written tables are generated as follows:</p> <ul> <li>The table name of the root mapping is defined by the table parameter of the dataset.   If the table name is empty, a name is generated from the first type of the mapping.   Special characters are removed and the name shortened to maximum of 128 characters.</li> <li>For each object mapping, the table name is generated from its type.</li> </ul> <p>JDBC Connnection Strings/URLs</p> <p>Most of the dataset prameters are directly forwrded to the respective driver. Please make sure to use the correct syntax for each DBS as rather unintuitive errors might occur otherwise.</p> <p>Here are templates for supported database systems: <pre><code>oracle (external driver needed):\njdbc:oracle:thin:@{host}[:{port}]/{database}\n\npostgres (integrated):\njdbc:postgresql://{host}[:{port}]/[{database}]\n\nMySQL/MAriaDB (integrated):\njdbc:{mysql|mariadb}://{host}[:{port}]/[{database}]\n\nSnowSQL (external driver needed):\njdbc:snowflake://}AWSAccount}.{AWS region}.snowflakecomputing.com?db={database}&amp;schema={schema}\n\nMSSqlServer (integrated):\njdbc:sqlserver://{host}[:{port}];databaseName={database}\n\nH2 (integrated):\njdbc:h2:{file} or jdbc:h2:tcp://{host}:[{port}][/{database}]\n\nDB2 (external driver needed):\njdbc:db2//{host}[:{port}]/{database}\n</code></pre></p> <p>Read and write strategies</p> <p>There are multiple read and write strategies which can be selected depending on the purpose of the dataset in a workflow.</p> <p>Read strategies decide how the database is queried:</p> <ul> <li>full-table: Queries or wraps a complete table. Only the DB schema and table name need to be set</li> <li>query: The given source query is passed along to the database. The table name is not necessary in this case but a valid query in the SQL-dialect of the source database system must be provided.</li> </ul> <p>Write strategies decide how a new table is written:</p> <ul> <li>default: An error will occur if the table exists. If not a new one will be created.</li> <li>overwrite: The old table will be removed and a new one will be created.</li> <li>append: Data will be appended to the existing table. The schema of the data written has to be the same as the existing table schema.</li> </ul> <p>Optimized Writing</p> <p>Usually specific database systems have custom commands for loading large amounts of data, e.g. from a CSV file into a database table. For some DBMS and specific JDBC dataset configurations we support these optimized methods of loading data.</p> <p>Supported DBMS:</p> <ul> <li>MySQL and MariaDB (full support for versions 8.0.19+ and 10.4+, resp.):</li> <li>if older DBMS versions are used some dataset options like \u2018groupBy\u2019 might not be supported but equivalent queries will</li> <li>the same is true when older driver jars then the one provided by eccenca are used</li> <li>both use the MariaDB JDBC driver</li> <li>uses <code>LOAD DATA LOCAL INFILE</code> internally</li> <li>only applies when appending data to an existing table and having <code>Force Spark Execution</code> disabled</li> <li>Both the server parameter <code>local_infile</code> and the client parameter <code>allowLoadLocalInfile</code> must be enabled, e.g. by     adding <code>allowLoadLocalInfile=true</code> to the JDBC URL. For MySQL starting with version 8 the <code>local_infile</code> parameter is     by default disabled!</li> </ul> <p>Registering JDBC drivers</p> <p>More 3rd party databases are supported via adding their JDBC drivers to the classpath of Data Integration. Drivers are usually provided by the database manufactures. If 32 bit and 64 bit versions are provided the latter is usually needed and should aways equal the bit-level of the JVM. To make sure that the drivers are loaded correctly their class name (in case are jar contains multiple drivers) and location in the file system can be set with the spark.sql.options.jdbc option in the <code>dataintegration.conf</code> configuration file.</p> <p>An example for adding both the DB2 and MySQL drivers to Data Integration configuration file <code>spark.sql.options.*</code> section:</p> <pre><code>spark.sql.options {\n\n  ...\n\n  # List of database identifiers to specify user provided JDBC drivers. The second part of the protocol of a JDBC URI (e.g. db2 from\n  # jdbc:db2://host:port)  is used to specify the driver. For each protocol on the list a jar classname and optional download\n  # location can be provided.\n  jdbc.drivers = \"db2,mysql\"\n\n  # Some database systems use licenses that are to loose or restrictive for us to ship the drivers. Therefore a path\n  # to a jar file containing the driver and the name of driver can be specified here.\n  jdbc.db2.jar = \"/home/user/Jars/db2jcc-db2jcc4.jar\"\n  jdbc.mysql.jar = \"/home/user/drivers/mysql.jar\"\n\n  # Name of the actual driver class for each db\n  jdbc.db2.name = \"com.ibm.db2.jcc.DB2Driver\"\n  jdbc.mysql.name = \"com.mysql.jdbc.Driver\"\n}\n</code></pre> <p>Recommended DBMS versions:</p> <p>Microsoft SQL Server 2017: Older versions might work, but do not support the <code>groupBy</code> parameter. PostgreSQL 9.5: The <code>groupBy</code> parameter needs at least version 8.4. MySQL v8.0.19: Older versions do not support the <code>groupBy</code> parameter. DB2 v11.5.x: The <code>groupBy</code> feature needs at least version 9.7 to function. Oracle 12.2.x: The <code>groupBy</code> feature does not work for versions prior to 11g Release 2.</p> <p>These limitations are the same for JDBC drivers that are older than the fully supported databases. Queries can achieve a similar outcome if <code>groupBy</code> is not supported.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel-google-drive","title":"Excel (Google Drive)","text":"<p>Read data from a remote Google Spreadsheet.</p> Parameter Type Description Default url String Link to the document (\u2018share with anyone having a link\u2019 must be enabled, URL parameters will be removed and corrected automatically). no default streaming boolean Streaming enables reading and writing large Excels files. Warning: Be careful to disable streaming for large datasets (&gt; 10MB), because of high memory consumption. true invalidateCacheAfter Duration Duration until file based cache is invalidated. PT5M linesToSkip int The number of lines to skip in the beginning when reading files. 0 <p>The identifier for this plugin is <code>googlespreadsheet</code>.</p> <p>It can be found in the package <code>com.eccenca.di.gdrive</code>.</p> <p>The dataset needs the document id of a \u201cshare via url\u201d sheet on Google Drive as input. It will automatically correct the URL and add the \u201cexport as xlsx\u201d option to a new URL that will be used to download an Excel Spreadsheet. The download will be cached and treated the same way as an xlsx file in the Excel Dataset.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#caching","title":"Caching","text":"<p>The advanced parameter <code>invalidateCacheAfter</code> allows the user to specify a duration of the file cache after which it is refreshed. A file based cache is created to avoid CAPTCHAs. During the caching and validation of the URL access occurs with random wait times between 1 and 5 seconds. The cache is invalidated after 5 minutes by default.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#neo4j","title":"Neo4j","text":"<p>Neo4j graph</p> Parameter Type Description Default uri String The URL to the Neo4j instance bolt://localhost:7687 user String The Neo4j username for basic authentication. user password PasswordParameter The Neo4j password for basic authentication. PASSWORD_PARAMETER:7LtZjhIrbTu9wze0gA4hPg== nodeLabel String Neo4j label for all entities to be covered by this dataset. When reading, all nodes with this label will be read. When writing, this label will be added to all generated nodes. If the dataset is cleared, only nodes with this label will be deleted. Any clearBeforeExecution boolean If set to true, all nodes with the specified label will be removed, before executing a workflow that writes to this graph. false <p>The identifier for this plugin is <code>neo4j</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.neo4j</code>.</p> <p>Supports reading and writing Neo4j graphs. The following sections outline how graphs are generated and read back.</p> <p>For more information about Neo4j, please refer to the Neo4j documentation.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#nodes","title":"Nodes","text":"<p>For each entity that is written to a Neo4j dataset, a node will be created. A property <code>uri</code> will be added to each generated node, which holds the URI of the original entity. In applications, the URI property should be used instead of the node identifiers, which are auto-generated in Neo4j and do not represent stable URIs.</p> <p>When reading nodes, the entity URIs will be generated based on that property. At the moment, it\u2019s not supported to read nodes that do not provide a <code>uri</code> property.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#labels","title":"Labels","text":"<p>Labels in Neo4j are used to group nodes into sets where all nodes that have a certain label belongs to the same set. Neo4j labels are comparable with classes in RDF (not to be confused with labels in RDF).</p> <p>When writing entities to the Neo4j dataset, the following labels will be added to each generated node:</p> <ul> <li>For each entity type (such as the type set in a mapping), a label will be added to the node in Neo4j.   Since types in eccenca DataIntegration are usually URIs, they will be converted according to the rules further down.</li> <li>The label as configured by the label parameter on the Neo4j dataset itself.   This is typically used to identify all entities that have been written by a certain Neo4j dataset specification in the project.   For instance, if two Neo4j dataset specifications are added to a project - both writing to the same Neo4j database - different labels can be set to distinguish both sets of entities.   In that respect it may be used to model a similar concept as graphs in RDF.</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#relationships","title":"Relationships","text":"<p>A relationship connects two nodes in Neo4j. Hierarchical mappings will generate relationships for all object mappings.</p> <p>Relationships can be addressed with property paths in mappings. At the moment, only paths of length 1 are supported, i.e., it\u2019s not possible to use non-property paths.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#handling-of-uris","title":"Handling of URIs","text":"<p>In eccenca DataIntegration, URIs are typically used to uniquely identify classes and properties. While URIs are central in RDF, Neo4j does allow arbitrary names and does not have any special support for URIs.</p> <p>When generating Neo4j labels, properties and relationships, URIs will be shortened according to the following rules. - If a registered project prefix matches a URI, a name <code>{prefixName}_{localPart}</code> will be generated. For instance, <code>http://xmlns.com/foaf/0.1/name</code> will become <code>foaf_name</code>.   Note that underscores (<code>_</code>) are used instead of colons (<code>:</code>) to separate the namespace and the local name.   The reason is that colons are reserved in the Cypher query language and some tools don\u2019t escape properly and fail on databases that use colons in names. - If no project prefix matches a URI, the URI will be used verbatim. This will look ugly in Neo4j tools, so generally it\u2019s recommended to define prefixes for all used namespaces.</p> <p>When reading generated entities, the URIs of the classes and properties will be reconstructed based on the prefix table of the project. If the prefixes change between writing and reading, different URIs will be generated.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rdf-vs-neo4j-terminology","title":"RDF vs. Neo4j terminology","text":"<p>Neo4j uses a different terminology than RDF or description logic. For users familiar with RDF, the following table shows the correspondent terms for some central concepts. This is meant to help understanding and does not aim to provide a precise mapping as there are semantic differences between Neo4j and RDF.</p> RDF Neo4j resource node class label datatype property property object property relationship graph Do not exist in Neo4j, but labels can be used to mimic graphs.","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel-onedrive-office365","title":"Excel (OneDrive, Office365)","text":"<p>Read data from a remote onedrive or Office365 Spreadsheet.</p> Parameter Type Description Default url String Link to the document (\u2018share with anyone having a link\u2019 must be enabled). no default streaming boolean Streaming enables reading and writing large Excels files. Warning: Be careful to disable streaming for large datasets (&gt; 10MB), because of high memory consumption. true invalidateCacheAfter Duration Duration until file based cache is invalidated. PT5M linesToSkip int The number of lines to skip in the beginning when reading files. 0 <p>The identifier for this plugin is <code>office365preadsheet</code>.</p> <p>It can be found in the package <code>com.eccenca.di.office365</code>.</p> <p>The dataset needs the URL of a \u201cshare via link\u201d sheet on Office 365/OneDrive as input. It will automatically construct a direct download URL, cache the download file handle it like an XLSX file in the Excel Dataset.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#notes","title":"Notes","text":"<p>There are 2 types of URLs that can be shared:</p> <ol> <li>Onedrive links look like <code>https://1drv.ms/x/s!AucULvzmJ-dsdfsfgaIcyWP_XY_G4w?e=yx65uu</code></li> <li>Onedrive (based one sharepoint, for businesses) links look like <code>https://eccencagmbh-my.sharepoint.com/:x:/g/personal/person_eccenca_com/EdEMTEw1dclHiEZXyvy8P4YBit8wSyGsiwU5Kt__sQOZzw</code></li> </ol> <p>The first type should always work as input for this dataset. The second type requires to set up an application in Azure Active Directory. Instructions can be found here: https://github.com/Azure-Samples/ms-identity-msal-java-samples/tree/main/4.%20Spring%20Framework%20Web%20App%20Tutorial/3-Authorization-II/protect-web-api#register-the-service-app-java-spring-resource-api</p> <p>After following the steps access to sharepoint can be setup in the application.conf file for eccenca DataIntegration.</p> <p>Example:</p> <pre><code>com.eccenca.di.office365 = {\n    authority = \"https://login.microsoftonline.com/a0907dd1-f981-4c98-a8b9-1deb27bcf2cc/\"\n    clientId = \"4d14959d-3c62-4f90-a072-a96ca4b3fa9f\"\n    secret = \"Ceb8Q~QkMMV7TBK-ggB3nh22nUnqoDB1KTmkjj\"\n    scope = \"https://graph.microsoft.com/.default\"\n    tenantId = \"a0907dd1-f981-4c98-a8b9-1deb27bcf2cc\"\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#caching_1","title":"Caching","text":"<p>The advanced parameter <code>invalidateCacheAfter</code> allows the user to specify a duration of the file cache after which it is refreshed. A file based cache is created to avoid CAPTCHAs. During the caching and validation of the URL access occurs with random wait times between 1 and 5 seconds. The cache is invalidated after 5 minutes by default.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sparql-endpoint","title":"SPARQL endpoint","text":"<p>Connect to an existing SPARQL endpoint.</p> Parameter Type Description Default endpointURI String The URI of the SPARQL endpoint, e.g., http://dbpedia.org/sparql no default login String Login required for authentication null password PasswordParameter Password required for authentication graph String Only retrieve entities from a specific graph null pageSize int The number of solutions to be retrieved per SPARQL query. 1000 entityList MultilineStringParameter A list of entities to be retrieved. If not given, all entities will be retrieved. Multiple entities are separated by whitespace. pauseTime int The number of milliseconds to wait between subsequent query 0 retryCount int The number of retries if a query fails 3 retryPause int The number of milliseconds to wait until a failed query is retried. 1000 queryParameters String Additional parameters to be appended to every request e.g. &amp;soft-limit=1 empty string strategy Enum The strategy use for retrieving entities: simple: Retrieve all entities using a single query; subQuery: Use a single query, but wrap it for improving the performance on Virtuoso; parallel: Use a separate Query for each entity property. parallel useOrderBy boolean Include useOrderBy in queries to enforce correct order of values. true clearGraphBeforeExecution boolean If set to true this will clear the specified graph before executing a workflow that writes to it. false sparqlTimeout int SPARQL query timeout (select/update) in milliseconds. A value of zero means that the timeout configured via property is used (e.g. configured via silk.remoteSparqlEndpoint.defaults.read.timeout.ms). To overwrite the configured value specify a value greater than zero. 0 <p>The identifier for this plugin is <code>sparqlEndpoint</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.dataset.rdf.datasets</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#distance-measures","title":"Distance Measures","text":"<p>The following distance measures are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#characterbased","title":"Characterbased","text":"<p>Character-based distance measures compare strings on the character level. They are well suited for handling typographical errors.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#is-substring","title":"Is substring","text":"<p>Checks if a source value is a substring of a target value.</p> Parameter Type Description Default reverse boolean Reverse source and target inputs false <p>The identifier for this plugin is <code>isSubstring</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jaro-distance","title":"Jaro distance","text":"<p>String similarity based on the Jaro distance metric.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>jaro</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jaro-winkler-distance","title":"Jaro-Winkler distance","text":"<p>String similarity based on the Jaro-Winkler distance measure.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>jaroWinkler</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalized-levenshtein-distance","title":"Normalized Levenshtein distance","text":"<p>Normalized Levenshtein distance.</p> Parameter Type Description Default minChar char The minimum character that is used for indexing 0 maxChar char The maximum character that is used for indexing z <p>The identifier for this plugin is <code>levenshtein</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#levenshtein-distance","title":"Levenshtein distance","text":"<p>Levenshtein distance. Returns a distance value between zero and the size of the string.</p> Parameter Type Description Default minChar char The minimum character that is used for indexing 0 maxChar char The maximum character that is used for indexing z <p>The identifier for this plugin is <code>levenshteinDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#qgrams","title":"qGrams","text":"<p>String similarity based on q-grams (by default q=2).</p> Parameter Type Description Default q int No description 2 minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>qGrams</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#starts-with","title":"Starts with","text":"<p>Returns success if the first string starts with the second string, failure otherwise.</p> Parameter Type Description Default reverse boolean Reverse source and target values false minLength int The minimum length of the string being contained. 2 maxLength int The potential maximum length of the strings that must match. If the max length is greater  than the length of the string to match, the full string must match. 2147483647 <p>The identifier for this plugin is <code>startsWith</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substring-comparison","title":"Substring comparison","text":"<p>Return 0 to 1 for strong similarity to weak similarity. Based on the paper: Stoilos, Giorgos, Giorgos Stamou, and Stefanos Kollias. \u201cA string metric for ontology alignment.\u201d The Semantic Web-ISWC 2005. Springer Berlin Heidelberg, 2005. 624-637.</p> Parameter Type Description Default granularity String The minimum length of a possible substring match. 3 <p>The identifier for this plugin is <code>substringDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.characterbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#equality","title":"Equality","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#constant","title":"Constant","text":"<p>Always returns a constant similarity value.</p> Parameter Type Description Default value double No description 1.0 <p>The identifier for this plugin is <code>constantDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#string-equality","title":"String equality","text":"<p>Checks for equality of the string representation of the given values. Returns success if string values are equal, failure otherwise. For a numeric comparison of values use the \u2018Numeric Equality\u2019 comparator.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>equality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#greater-than","title":"Greater than","text":"<p>Checks if the source value is greater than the target value.</p> Parameter Type Description Default orEqual boolean Accept equal values false order Enum Per default, if both strings are numbers, numerical order is used for comparison. Otherwise, alphanumerical order is used. Choose a more specific order for improved performance. Autodetect reverse boolean Reverse source and target inputs false <p>The identifier for this plugin is <code>greaterThan</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#inequality","title":"Inequality","text":"<p>Returns success if values are not equal, failure otherwise.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>inequality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#lower-than","title":"Lower than","text":"<p>Checks if the source value is lower than the target value.</p> Parameter Type Description Default orEqual boolean Accept equal values false order Enum Per default, if both strings are numbers, numerical order is used for comparison. Otherwise, alphanumerical order is used. Choose a more specific order for improved performance. Autodetect reverse boolean Reverse source and target inputs false <p>The identifier for this plugin is <code>lowerThan</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-equality","title":"Numeric equality","text":"<p>Compares values numerically instead of their string representation as the \u2018String Equality\u2019 operator does. Allows to set the needed precision of the comparison. A value of 0.0 means that the values must represent exactly the same (floating point) value, values higher than that allow for a margin of tolerance. Example: With a precision of 0.1, the following pairs of values will be considered equal: (1.3, 1.35), (0.0, 0.9999), (0.0, -0.90001), but following pairs will NOT match: (1.2, 1.30001), (1.0, 1.10001), (1.0, 0.89999).</p> Parameter Type Description Default precision double The range of tolerance in floating point number comparisons. Must be 0 or a non-negative number smaller than 1. 0.0 <p>The identifier for this plugin is <code>numericEquality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#relaxed-equality","title":"Relaxed equality","text":"<p>Return success if strings are equal, failure otherwise. Lower/upper case and differences like \u00f6/o, n/\u00f1, c/\u00e7 etc. are treated as equal.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>relaxedEquality</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.equality</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#language","title":"Language","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cjk-reading-distance","title":"CJK reading distance","text":"<p>CJK Reading Distance.</p> Parameter Type Description Default minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>cjkReadingDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.asian</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#korean-phoneme-distance","title":"Korean phoneme distance","text":"<p>Korean phoneme distance.</p> Parameter Type Description Default minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>koreanPhonemeDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.asian</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#korean-translit-distance","title":"Korean translit distance","text":"<p>Transliterated Korean distance.</p> Parameter Type Description Default minChar char No description 0 maxChar char No description z <p>The identifier for this plugin is <code>koreanTranslitDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.asian</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric","title":"Numeric","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#compare-physical-quantities","title":"Compare physical quantities","text":"<p>Computes the distance between two physical quantities. The distance is normalized to the SI base unit of the dimension. For instance for lengths, the distance will be in metres. Comparing incompatible units will yield a validation error.</p> Parameter Type Description Default numberFormat String The IETF BCP 47 language tag, e.g., \u2018en\u2019. en <p>The identifier for this plugin is <code>PhysicalQuantitiesDistance</code>.</p> <p>It can be found in the package <code>com.eccenca.di.measure</code>.</p> <p>SI units and common derived units are supported. The following section lists all supported units. By default, all quantities are normalized to their base unit. For instance, lengths will be normalized to metres.</p> <p>Time</p> <p>Time is expressed in seconds (s). The following alternative units are supported: mo_s, mo_g, a, min, a_g, mo, mo_j, a_j, h, a_t, d.</p> <p>Length</p> <p>Length is expressed in metres (m). The following alternative units are supported: in, nmi, Ao, mil, yd, AU, ft, pc, fth, mi, hd.</p> <p>Mass</p> <p>Mass is expressed in kilograms (kg). The following alternative units are supported: lb, ston, t, stone, u, gr, lcwt, oz, g, scwt, dr, lton.</p> <p>Electric current</p> <p>Electric current is expressed in amperes (A). The following alternative units are supported: Bi, Gb.</p> <p>Temperature</p> <p>Temperature is expressed in kelvins (K). The following alternative units are supported: Cel.</p> <p>Amount of substance</p> <p>Amount of substance is expressed in moles (mol).</p> <p>Luminous intensity</p> <p>Luminous intensity is expressed in candelas (cd).</p> <p>Area</p> <p>Area is expressed in square metres (m\u00b2). The following alternative units are supported: m2, ar, syd, cml, b, sft, sin.</p> <p>Volume</p> <p>Volume is expressed in cubic metres (\u33a5). The following alternative units are supported: st, bf, cyd, cr, L, l, cin, cft, m3.</p> <p>Energy</p> <p>Energy is expressed in joules (J). The following alternative units are supported: cal_IT, eV, cal_m, cal, cal_th.</p> <p>Angle</p> <p>Angle is expressed in radians (rad). The following alternative units are supported: circ, gon, deg, \u2018, \u2018\u2019.</p> <p>Others</p> <ul> <li>1/m, derived units: Ky</li> <li>kg/(m\u00b7s), derived units: P</li> <li>bit/s, derived units: Bd</li> <li>bit, derived units: By</li> <li>Sv</li> <li>N</li> <li>\u03a9, derived units: Ohm</li> <li>T, derived units: G</li> <li>sr, derived units: sph</li> <li>F</li> <li>C/kg, derived units: R</li> <li>cd/m\u00b2, derived units: sb, Lmb</li> <li>Pa, derived units: bar, atm</li> <li>kg/(m\u00b7s\u00b2), derived units: att</li> <li>m\u00b2/s, derived units: St</li> <li>A/m, derived units: Oe</li> <li>kg\u00b7m\u00b2/s\u00b2, derived units: erg</li> <li>kg/m\u00b3, derived units: g%</li> <li>mho</li> <li>V</li> <li>lx, derived units: ph</li> <li>m/s\u00b2, derived units: Gal, m/s2</li> <li>m/s, derived units: kn</li> <li>m\u00b7kg/s\u00b2, derived units: gf, lbf, dyn</li> <li>m\u00b2/s\u00b2, derived units: RAD, REM</li> <li>C</li> <li>Gy</li> <li>Hz</li> <li>H</li> <li>lm</li> <li>W</li> <li>Wb, derived units: Mx</li> <li>Bq, derived units: Ci</li> <li>S</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#date","title":"Date","text":"<p>The distance in days between two dates (\u2018YYYY-MM-DD\u2019 format).</p> Parameter Type Description Default requireMonthAndDay boolean If true, no distance value will be generated if months or days are missing (e.g., 2019-11). If false, missing month or day fields will default to 1. false <p>The identifier for this plugin is <code>date</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#datetime","title":"DateTime","text":"<p>Distance between two date time values (xsd:dateTime format) in seconds.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>dateTime</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#inside-numeric-interval","title":"Inside numeric interval","text":"<p>Checks if a number is contained inside a numeric interval, such as \u20181900 - 2000\u2019.</p> Parameter Type Description Default separator String No description \u2014 <p>The identifier for this plugin is <code>insideNumericInterval</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-similarity","title":"Numeric similarity","text":"<p>Computes the numeric distance between two numbers.</p> Parameter Type Description Default minValue double No description -Infinity maxValue double No description Infinity <p>The identifier for this plugin is <code>num</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geographical-distance","title":"Geographical distance","text":"<p>Computes the geographical distance between two points. Author: Konrad H\u00f6ffner (MOLE subgroup of Research Group AKSW, University of Leipzig)</p> Parameter Type Description Default unit String No description km <p>The identifier for this plugin is <code>wgs84</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tokenbased","title":"Tokenbased","text":"<p>While character-based distance measures work well for typographical errors, there are a number of tasks where token-base distance measures are better suited:</p> <ul> <li>Strings where parts are reordered e.g. \u201cJohn Doe\u201d and \u201cDoe, John\u201d</li> <li>Texts consisting of multiple words</li> </ul>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cosine","title":"Cosine","text":"<p>Cosine Distance Measure.</p> Parameter Type Description Default k int No description 3 <p>The identifier for this plugin is <code>cosine</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#dice-coefficient","title":"Dice coefficient","text":"<p>Dice similarity coefficient.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>dice</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#jaccard","title":"Jaccard","text":"<p>Jaccard similarity coefficient.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>jaccard</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#soft-jaccard","title":"Soft Jaccard","text":"<p>Soft Jaccard similarity coefficient. Same as Jaccard distance but values within an levenhstein distance of \u2018maxDistance\u2019 are considered equivalent.</p> Parameter Type Description Default maxDistance int No description 1 <p>The identifier for this plugin is <code>softjaccard</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#token-wise-distance","title":"Token-wise distance","text":"<p>Token-wise string distance using the specified metric.</p> Parameter Type Description Default ignoreCase boolean No description true metricName String No description levenshtein splitRegex String No description [\\s\\d\\p{Punct}]+ stopwords String No description empty string stopwordWeight double No description 0.01 nonStopwordWeight double No description 0.1 useIncrementalIdfWeights boolean No description false matchThreshold double No description 0.0 orderingImpact double No description 0.0 adjustByTokenLength boolean No description false <p>The identifier for this plugin is <code>tokenwiseDistance</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.distance.tokenbased</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#transformations","title":"Transformations","text":"<p>The following transform and normalization functions are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#combine","title":"Combine","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#concatenate","title":"Concatenate","text":"<p>Concatenates strings from multiple inputs.</p> Parameter Type Description Default glue String Separator to be inserted between two concatenated strings. empty string missingValuesAsEmptyStrings boolean Handle missing values as empty strings. false <p>The identifier for this plugin is <code>concat</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p> <p>Examples</p> <p>Returns [] for parameters [] and input values [].</p> <p>Returns [a] for parameters [] and input values [[a]].</p> <p>Returns [ab] for parameters [] and input values [[a], [b]].</p> <p>Returns [First-Last] for parameters [glue -&gt; -] and input values [[First], [Last]].</p> <p>Returns [First-Second, First-Third] for parameters [glue -&gt; -] and input values [[First], [Second, Third]].</p> <p>Returns [First\u2013Second] for parameters [glue -&gt; -] and input values [[First], [], [Second]].</p> <p>Returns [] for parameters [glue -&gt; -] and input values [[First], [], [Second]].</p> <p>Returns [First\u2013Second] for parameters [glue -&gt; -, missingValuesAsEmptyStrings -&gt; true] and input values [[First], [], [Second]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#concatenate-multiple-values","title":"Concatenate multiple values","text":"<p>Concatenates multiple values received for an input. If applied to multiple inputs, yields at most one value per input. Optionally removes duplicate values.</p> Parameter Type Description Default glue String No description empty string removeDuplicates boolean No description false <p>The identifier for this plugin is <code>concatMultiValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p> <p>Examples</p> <p>Returns [] for parameters [] and input values [].</p> <p>Returns [a] for parameters [] and input values [[a]].</p> <p>Returns [ab] for parameters [] and input values [[a, b]].</p> <p>Returns [axb] for parameters [glue -&gt; x] and input values [[a, b]].</p> <p>Returns [ab, 12] for parameters [] and input values [[a, b], [1, 2]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#merge","title":"Merge","text":"<p>Merges the values of all inputs.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>merge</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.combine</code>.</p> <p>Examples</p> <p>Returns [] for parameters [] and input values [].</p> <p>Returns [a, b, c] for parameters [] and input values [[a, b], [c]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#conditional","title":"Conditional","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#contains-all-of","title":"Contains all of","text":"<p>Accepts two inputs. If the first input contains all of the second input values it returns \u2018true\u2019, else \u2018false\u2019 is returned.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>containsAllOf</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p> <p>Examples</p> <p>Returns [true] for parameters [] and input values [[A, B, C], [A, B]].</p> <p>Returns [false] for parameters [] and input values [[A, B, C], [A, D]].</p> <p>Returns [false] for parameters [] and input values [[A, B, C], [D]].</p> <p>Returns [true] for parameters [] and input values [[A, B, C], [A, B, C]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[A, B, C], []].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[A], [A], [A]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[A]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#contains-any-of","title":"Contains any of","text":"<p>Accepts two inputs. If the first input contains any of the second input values it returns \u2018true\u2019, else \u2018false\u2019 is returned.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>containsAnyOf</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p> <p>Examples</p> <p>Returns [true] for parameters [] and input values [[A, B, C], [A, B]].</p> <p>Returns [true] for parameters [] and input values [[A, B, C], [A, D]].</p> <p>Returns [false] for parameters [] and input values [[A, B, C], [D]].</p> <p>Returns [true] for parameters [] and input values [[A, B, C], [A, B, C]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[A, B, C], []].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[A], [A], [A]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[A]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-contains","title":"If contains","text":"<p>Accepts two or three inputs. If the first input contains the given value, the second input is forwarded. Otherwise, the third input is forwarded (if present).</p> Parameter Type Description Default search String No description no default <p>The identifier for this plugin is <code>ifContains</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p> <p>Examples</p> <p>Returns [this is a match] for parameters [search -&gt; match] and input values [[matching string], [this is a match]].</p> <p>Returns [] for parameters [search -&gt; match] and input values [[different string], [this is a match]].</p> <p>Returns [this is no match] for parameters [search -&gt; match] and input values [[different string], [this is a match], [this is no match]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-exists","title":"If exists","text":"<p>Accepts two or three inputs. If the first input provides a value, the second input is forwarded. Otherwise, the third input is forwarded (if present).</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>ifExists</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p> <p>Examples</p> <p>Returns [yes] for parameters [] and input values [[value], [yes], [no]].</p> <p>Returns [no] for parameters [] and input values [[], [yes], [no]].</p> <p>Returns [] for parameters [] and input values [[value], []].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if-matches-regex","title":"If matches regex","text":"<pre><code>   Accepts two or three inputs.\n   If any value of the first input matches the regex, the second input is forwarded.\n   Otherwise, the third input is forwarded (if present).\n</code></pre> Parameter Type Description Default regex String No description no default negate boolean No description false <p>The identifier for this plugin is <code>ifMatchesRegex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#negate-binary-not","title":"Negate binary (NOT)","text":"<p>Accepts one input, which is either \u2018true\u2019, \u20181\u2019 or \u2018false\u2019, \u20180\u2019 and negates it.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>negateTransformer</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conditional</code>.</p> <p>Examples</p> <p>Returns [1, 0, true, false, true, false] for parameters [] and input values [[0, 1, false, true, False, True]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[falsee, true]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#conversion","title":"Conversion","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#convert-charset","title":"Convert charset","text":"<p>Convert the string from \u201csourceCharset\u201d to \u201ctargetCharset\u201d.</p> Parameter Type Description Default sourceCharset String No description ISO-8859-1 targetCharset String No description UTF-8 <p>The identifier for this plugin is <code>convertCharset</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.conversion</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean-html","title":"Clean HTML","text":"<p>Cleans HTML using a tag white list and allows selection of HTML sections with xPath or cssSelector expressions.  If the tag or attribute white lists are left empty default white lists will be used. The operator takes two inputs: the page HTML and  (optional) the page Url which may be needed to resolve relative links in the page HTML.</p> Parameter Type Description Default tagWhiteList String Tags to keep in the cleaned Text (or reference to a configuration). empty string attributeWhiteList String Tags to keep in the cleaned Text (or reference to a configuration). empty string selectors MultilineStringParameter CSS or XPath queries for selection of content (or reference to a configuration). Comma separated. CssSelectors can be pipe separated for non-sequential execution. no default method Enum Selects use of xPath or css selectors (\u2018xPath\u2019 or \u2018cssSelectors\u2019). xPath <p>The identifier for this plugin is <code>htmlCleaner</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.html</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#date_1","title":"Date","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-date","title":"Parse date","text":"<p>Parses and normalizes dates in different formats.</p> Parameter Type Description Default inputDateFormatId Enum The input date/time format used for parsing the date/time string. w3c Date alternativeInputFormat String An input format string that should be used instead of the selected input format. Java DateFormat string. empty string outputDateFormatId Enum The output date/time format used for parsing the date/time string. w3c Date alternativeOutputFormat String An output format string that should be used instead of the selected output format. Java DateFormat string. empty string <p>The identifier for this plugin is <code>DateTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p> <p>Examples</p> <p>Returns [1999-03-20] for parameters [inputDateFormatId -&gt; German style date format, outputDateFormatId -&gt; w3c Date] and input values [[20.03.1999]].</p> <p>Returns [20.03.1999] for parameters [inputDateFormatId -&gt; w3c Date, outputDateFormatId -&gt; German style date format] and input values [[1999-03-20]].</p> <p>Returns [2017-04-04] for parameters [inputDateFormatId -&gt; common ISO8601, outputDateFormatId -&gt; w3c Date] and input values [[2017-04-04T00:00:00.000+02:00]].</p> <p>Returns [2017-04-04] for parameters [inputDateFormatId -&gt; common ISO8601, outputDateFormatId -&gt; w3c Date] and input values [[2017-04-04T00:00:00+02:00]].</p> <p>Returns [24-Jun-2021 14:50:05 +02:00] for parameters [inputDateFormatId -&gt; common ISO8601, outputDateFormatId -&gt; dateTime with month abbr. (US)] and input values [[2021-06-24T14:50:05.895+02:00]].</p> <p>Returns [24-Dez.-2021 14:50:05 +02:00] for parameters [inputDateFormatId -&gt; dateTime with month abbr. (US), outputDateFormatId -&gt; dateTime with month abbr. (DE)] and input values [[24-Dec-2021 14:50:05 +02:00]].</p> <p>Returns [1999-03-20T20:34.44] for parameters [alternativeInputFormat -&gt; dd.MM.yyyy HH:mm.ss, alternativeOutputFormat -&gt; yyyy-MM-dd\u2019T\u2019HH:mm.ss] and input values [[20.03.1999 20:34.44]].</p> <p>Returns [12:20:00.000] for parameters [inputDateFormatId -&gt; excelDateTime, outputDateFormatId -&gt; xsdTime] and input values [[12:20:00.000]].</p> <p>Returns [\u201301] for parameters [inputDateFormatId -&gt; w3c YearMonth, outputDateFormatId -&gt; w3c Month] and input values [[2020-01]].</p> <p>Returns [\u201431] for parameters [inputDateFormatId -&gt; w3c MonthDay, outputDateFormatId -&gt; w3c Day] and input values [[\u201312-31]].</p> <p>Returns [\u201312-31] for parameters [inputDateFormatId -&gt; w3c Date, outputDateFormatId -&gt; w3c MonthDay] and input values [[2020-12-31]].</p> <p>Fails validation and thus returns [] for parameters [inputDateFormatId -&gt; w3c MonthDay, outputDateFormatId -&gt; w3c Date] and input values [[\u201312-31]].</p> <p>Returns [2020-02-22T16:34:14] for parameters [alternativeInputFormat -&gt; yyyy-MM-dd HHss.SSS, outputDateFormatId -&gt; w3cDateTime] and input values [[2020-02-22 16:34:14.000]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#compare-dates","title":"Compare dates","text":"<p>Compares two dates. Returns 1 if the comparison yields true and 0 otherwise. If there are multiple dates in both sets, the comparator must be true for all dates. For instance, {2014-08-02,2014-08-03} &lt; {2014-08-03} yields 0 as not all dates in the first set are smaller than in the second.</p> Parameter Type Description Default comparator Enum No description &lt; <p>The identifier for this plugin is <code>compareDates</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p> <p>Examples</p> <p>Returns [1] for parameters [comparator -&gt; &lt;] and input values [[2017-01-01], [2017-01-02]].</p> <p>Returns [0] for parameters [comparator -&gt; &lt;] and input values [[2017-01-02], [2017-01-01]].</p> <p>Returns [1] for parameters [comparator -&gt; &gt;] and input values [[2017-01-02], [2017-01-01]].</p> <p>Returns [0] for parameters [comparator -&gt; &gt;] and input values [[2017-01-01], [2017-01-02]].</p> <p>Returns [1] for parameters [comparator -&gt; =] and input values [[2017-01-01], [2017-01-01]].</p> <p>Returns [0] for parameters [comparator -&gt; =] and input values [[2017-01-02], [2017-01-01]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#current-date","title":"Current date","text":"<p>Outputs the current date.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>currentDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#date-to-timestamp","title":"Date to timestamp","text":"<p>Convert an xsd:dateTime to a timestamp. Returns the passed time since the Unix Epoch (1970-01-01).</p> Parameter Type Description Default unit Enum No description milliseconds <p>The identifier for this plugin is <code>datetoTimestamp</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p> <p>Examples</p> <p>Returns [1499117572000] for parameters [] and input values [[2017-07-03T21:32:52Z]].</p> <p>Returns [1499113972000] for parameters [] and input values [[2017-07-03T21:32:52+01:00]].</p> <p>Returns [1499113972] for parameters [unit -&gt; seconds] and input values [[2017-07-03T21:32:52+01:00]].</p> <p>Returns [1499040000000] for parameters [] and input values [[2017-07-03]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration","title":"Duration","text":"<p>Computes the time difference between two data times.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>duration</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration-in-days","title":"Duration in days","text":"<p>Converts an xsd:duration to days.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>durationInDays</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration-in-seconds","title":"Duration in seconds","text":"<p>Converts an xsd:duration to seconds.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>durationInSeconds</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#duration-in-years","title":"Duration in years","text":"<p>Converts an xsd:duration to years.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>durationInYears</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#number-to-duration","title":"Number to duration","text":"<p>Converts a number to an xsd:duration.</p> Parameter Type Description Default unit Enum No description day <p>The identifier for this plugin is <code>numberToDuration</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-date-pattern","title":"Parse date pattern","text":"<p>Parses a date based on a specified pattern, returning an xsd:date.</p> Parameter Type Description Default format String The date pattern used to parse the input values dd-MM-yyyy lenient boolean If set to true, the parser tries to use heuristics to parse dates with invalid fields (such as a day of zero). false <p>The identifier for this plugin is <code>parseDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p> <p>Examples</p> <p>Returns [2015-04-03] for parameters [format -&gt; dd.MM.yyyy] and input values [[03.04.2015]].</p> <p>Returns [2015-04-03] for parameters [format -&gt; dd.MM.yyyy] and input values [[3.4.2015]].</p> <p>Returns [2015-04-03] for parameters [format -&gt; yyyyMMdd] and input values [[20150403]].</p> <p>Fails validation and thus returns [] for parameters [format -&gt; yyyyMMdd, lenient -&gt; false] and input values [[20150000]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#timestamp-to-date","title":"Timestamp to date","text":"<p>Convert a timestamp to xsd:date format. Expects an integer that denotes the passed time since the Unix Epoch (1970-01-01)</p> Parameter Type Description Default format String Custom output format (e.g., \u2018yyyy-MM-dd\u2019). If left empty, a full xsd:dateTime (UTC) is returned. empty string unit Enum No description milliseconds <p>The identifier for this plugin is <code>timeToDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p> <p>Examples</p> <p>Returns [2017-07-03T21:32:52Z] for parameters [] and input values [[1499117572000]].</p> <p>Returns [2017-07-03] for parameters [format -&gt; yyyy-MM-dd] and input values [[1499040000000]].</p> <p>Returns [2017-07-03] for parameters [format -&gt; yyyy-MM-dd, unit -&gt; seconds] and input values [[1499040000]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-after","title":"Validate date after","text":"<p>Validates if the first input date is after the second input date. Outputs the first input if the validation is successful.</p> Parameter Type Description Default allowEqual boolean Allow both dates to be equal. false <p>The identifier for this plugin is <code>validateDateAfter</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p> <p>Examples</p> <p>Fails validation and thus returns [] for parameters [] and input values [[2015-04-02], [2015-04-03]].</p> <p>Returns [2015-04-04] for parameters [] and input values [[2015-04-04], [2015-04-03]].</p> <p>Returns [2015-04-03] for parameters [allowEqual -&gt; true] and input values [[2015-04-03], [2015-04-03]].</p> <p>Fails validation and thus returns [] for parameters [allowEqual -&gt; false] and input values [[2015-04-03], [2015-04-03]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-range","title":"Validate date range","text":"<p>Validates if dates are within a specified range.</p> Parameter Type Description Default minDate String Earliest allowed date in YYYY-MM-DD no default maxDate String Latest allowed data in YYYY-MM-DD no default <p>The identifier for this plugin is <code>validateDateRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-numeric-range","title":"Validate numeric range","text":"<p>Validates if a number is within a specified range.</p> Parameter Type Description Default min double Minimum allowed number no default max double Maximum allowed number no default <p>The identifier for this plugin is <code>validateNumericRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel_1","title":"Excel","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#abs","title":"Abs","text":"<p>Excel ABS(number): Returns the absolute value of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function ABS <p>The identifier for this plugin is <code>Excel_ABS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#acos","title":"Acos","text":"<p>Excel ACOS(number): Returns the inverse cosine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ACOS <p>The identifier for this plugin is <code>Excel_ACOS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#acosh","title":"Acosh","text":"<p>Excel ACOSH(number): Returns the inverse hyperbolic cosine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ACOSH <p>The identifier for this plugin is <code>Excel_ACOSH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#and","title":"And","text":"<p>Excel AND(argument1; argument2 \u2026argument30): Returns TRUE if all the arguments are considered TRUE, and FALSE otherwise.</p> Parameter Type Description Default functionName String The name of the Excel function AND <p>The identifier for this plugin is <code>Excel_AND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#asin","title":"Asin","text":"<p>Excel ASIN(number): Returns the inverse sine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ASIN <p>The identifier for this plugin is <code>Excel_ASIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#asinh","title":"Asinh","text":"<p>Excel ASINH(number): Returns the inverse hyperbolic sine of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ASINH <p>The identifier for this plugin is <code>Excel_ASINH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#atan","title":"Atan","text":"<p>Excel ATAN(number): Returns the inverse tangent of the given number in radians.</p> Parameter Type Description Default functionName String The name of the Excel function ATAN <p>The identifier for this plugin is <code>Excel_ATAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#atan2","title":"Atan2","text":"<p>Excel ATAN2(number_x; number_y): Returns the inverse tangent of the specified x and y coordinates. Number_x is the value for the x coordinate. Number_y is the value for the y coordinate.</p> Parameter Type Description Default functionName String The name of the Excel function ATAN2 <p>The identifier for this plugin is <code>Excel_ATAN2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#atanh","title":"Atanh","text":"<p>Excel ATANH(number): Returns the inverse hyperbolic tangent of the given number. (Angle is returned in radians.)</p> Parameter Type Description Default functionName String The name of the Excel function ATANH <p>The identifier for this plugin is <code>Excel_ATANH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#avedev","title":"Avedev","text":"<p>Excel AVEDEV(number1; number2; \u2026 number_30): Returns the average of the absolute deviations of data points from their mean. Displays the diffusion in a data set. Number_1; number_2; \u2026 number_30 are values or ranges that represent a sample. Each number can also be replaced by a reference.</p> Parameter Type Description Default functionName String The name of the Excel function AVEDEV <p>The identifier for this plugin is <code>Excel_AVEDEV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#average","title":"Average","text":"<p>Excel AVERAGE(number_1; number_2; \u2026 number_30): Returns the average of the arguments. Number_1; number_2; \u2026 number_30 are numerical values or ranges. Text is ignored.</p> Parameter Type Description Default functionName String The name of the Excel function AVERAGE <p>The identifier for this plugin is <code>Excel_AVERAGE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ceiling","title":"Ceiling","text":"<p>Excel CEILING(number; significance; mode): Rounds the given number to the nearest integer or multiple of significance. Significance is the value to whose multiple of ten the value is to be rounded up (.01, .1, 1, 10, etc.). Mode is an optional value. If it is indicated and non-zero and if the number and significance are negative, rounding up is carried out based on that value.</p> Parameter Type Description Default functionName String The name of the Excel function CEILING <p>The identifier for this plugin is <code>Excel_CEILING</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#choose","title":"Choose","text":"<p>Excel CHOOSE(index; value1; \u2026 value30): Uses an index to return a value from a list of up to 30 values. Index is a reference or number between 1 and 30 indicating which value is to be taken from the list. Value1; \u2026 value30 is the list of values entered as a reference to a cell or as individual values.</p> Parameter Type Description Default functionName String The name of the Excel function CHOOSE <p>The identifier for this plugin is <code>Excel_CHOOSE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean","title":"Clean","text":"<p>Excel CLEAN(text): Removes all non-printing characters from the string. Text refers to the text from which to remove all non-printable characters.</p> Parameter Type Description Default functionName String The name of the Excel function CLEAN <p>The identifier for this plugin is <code>Excel_CLEAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#code","title":"Code","text":"<p>Excel CODE(text): Returns a numeric code for the first character in a text string. Text is the text for which the code of the first character is to be found.</p> Parameter Type Description Default functionName String The name of the Excel function CODE <p>The identifier for this plugin is <code>Excel_CODE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#combin","title":"Combin","text":"<p>Excel COMBIN(count_1; count_2): Returns the number of combinations for a given number of objects. Count_1 is the total number of elements. Count_2 is the selected count from the elements. This is the same as the nCr function on a calculator.</p> Parameter Type Description Default functionName String The name of the Excel function COMBIN <p>The identifier for this plugin is <code>Excel_COMBIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cos","title":"Cos","text":"<p>Excel COS(number): Returns the cosine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function COS <p>The identifier for this plugin is <code>Excel_COS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#cosh","title":"Cosh","text":"<p>Excel COSH(number): Returns the hyperbolic cosine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function COSH <p>The identifier for this plugin is <code>Excel_COSH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#count","title":"Count","text":"<p>Excel COUNT(value_1; value_2; \u2026 value_30): Counts how many numbers are in the list of arguments. Text entries are ignored. Value_1; value_2; \u2026 value_30 are values or ranges which are to be counted.</p> Parameter Type Description Default functionName String The name of the Excel function COUNT <p>The identifier for this plugin is <code>Excel_COUNT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#counta","title":"Counta","text":"<p>Excel COUNTA(value_1; value_2; \u2026 value_30): Counts how many values are in the list of arguments. Text entries are also counted, even when they contain an empty string of length 0. If an argument is an array or reference, empty cells within the array or reference are ignored. value_1; value_2; \u2026 value_30 are up to 30 arguments representing the values to be counted.</p> Parameter Type Description Default functionName String The name of the Excel function COUNTA <p>The identifier for this plugin is <code>Excel_COUNTA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#degrees","title":"Degrees","text":"<p>Excel DEGREES(number): Converts the given number in radians to degrees.</p> Parameter Type Description Default functionName String The name of the Excel function DEGREES <p>The identifier for this plugin is <code>Excel_DEGREES</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#devsq","title":"Devsq","text":"<p>Excel DEVSQ(number_1; number_2; \u2026 number_30): Returns the sum of squares of deviations based on a sample mean. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample.</p> Parameter Type Description Default functionName String The name of the Excel function DEVSQ <p>The identifier for this plugin is <code>Excel_DEVSQ</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#even","title":"Even","text":"<p>Excel EVEN(number): Rounds the given number up to the nearest even integer.</p> Parameter Type Description Default functionName String The name of the Excel function EVEN <p>The identifier for this plugin is <code>Excel_EVEN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#exact","title":"Exact","text":"<p>Excel EXACT(text_1; text_2): Compares two text strings and returns TRUE if they are identical. This function is case- sensitive. Text_1 is the first text to compare. Text_2 is the second text to compare.</p> Parameter Type Description Default functionName String The name of the Excel function EXACT <p>The identifier for this plugin is <code>Excel_EXACT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#exp","title":"Exp","text":"<p>Excel EXP(number): Returns e raised to the power of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function EXP <p>The identifier for this plugin is <code>Excel_EXP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fact","title":"Fact","text":"<p>Excel FACT(number): Returns the factorial of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function FACT <p>The identifier for this plugin is <code>Excel_FACT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#false","title":"False","text":"<p>Excel FALSE(): Set the logical value to FALSE. The FALSE() function does not require any arguments.</p> Parameter Type Description Default functionName String The name of the Excel function FALSE <p>The identifier for this plugin is <code>Excel_FALSE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#find","title":"Find","text":"<p>Excel FIND(find_text; text; position): Looks for a string of text within another string. Where to begin the search can also be defined. The search term can be a number or any string of characters. The search is case-sensitive. Find_text is the text to be found. Text is the text where the search takes place. Position (optional) is the position in the text from which the search starts.</p> Parameter Type Description Default functionName String The name of the Excel function FIND <p>The identifier for this plugin is <code>Excel_FIND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#floor","title":"Floor","text":"<p>Excel FLOOR(number; significance; mode): Rounds the given number down to the nearest multiple of significance. Significance is the value to whose multiple of ten the number is to be rounded down (.01, .1, 1, 10, etc.). Mode is an optional value. If it is indicated and non-zero and if the number and significance are negative, rounding up is carried out based on that value.</p> Parameter Type Description Default functionName String The name of the Excel function FLOOR <p>The identifier for this plugin is <code>Excel_FLOOR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fv","title":"Fv","text":"<p>Excel FV(rate; NPER; PMT; PV; type): Returns the future value of an investment based on periodic, constant payments and a constant interest rate. Rate is the periodic interest rate. NPER is the total number of periods. PMT is the annuity paid regularly per period. PV (optional) is the present cash value of an investment. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function FV <p>The identifier for this plugin is <code>Excel_FV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geomean","title":"Geomean","text":"<p>Excel GEOMEAN(number_1; number_2; \u2026 number_30): Returns the geometric mean of a sample. Number_1; number_2; \u2026 number_30 are numerical arguments or ranges that represent a random sample.</p> Parameter Type Description Default functionName String The name of the Excel function GEOMEAN <p>The identifier for this plugin is <code>Excel_GEOMEAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#if","title":"If","text":"<p>Excel IF(test; then_value; otherwise_value): Returns different values based on the test value. Note that in this implementation it will not actually evaluate logical conditions. Then_value is the value that is returned if the test is TRUE. Otherwise_value (optional) is the value that is returned if the test is FALSE.</p> Parameter Type Description Default functionName String The name of the Excel function IF <p>The identifier for this plugin is <code>Excel_IF</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#int","title":"Int","text":"<p>Excel INT(number): Rounds the given number down to the nearest integer.</p> Parameter Type Description Default functionName String The name of the Excel function INT <p>The identifier for this plugin is <code>Excel_INT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#intercept","title":"Intercept","text":"<p>Excel INTERCEPT(data_Y; data_X): Calculates the y-value at which a line will intersect the y-axis by using known x-values and y-values. Data_Y is the dependent set of observations or data. Data_X is the independent set of observations or data. Names, arrays or references containing numbers must be used here. Numbers can also be entered directly.</p> Parameter Type Description Default functionName String The name of the Excel function INTERCEPT <p>The identifier for this plugin is <code>Excel_INTERCEPT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ipmt","title":"Ipmt","text":"<p>Excel IPMT(rate; period; NPER; PV; FV; type): Calculates the periodic amortization for an investment with regular payments and a constant interest rate. Rate is the periodic interest rate. Period is the period for which the compound interest is calculated. NPER is the total number of periods during which annuity is paid. Period=NPER, if compound interest for the last period is calculated. PV is the present cash value in sequence of payments. FV (optional) is the desired value (future value) at the end of the periods. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function IPMT <p>The identifier for this plugin is <code>Excel_IPMT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#irr","title":"Irr","text":"<p>Excel IRR(values; guess): Calculates the internal rate of return for an investment. The values represent cash flow values at regular intervals; at least one value must be negative (payments), and at least one value must be positive (income). Values is an array containing the values. Guess (optional) is the estimated value. If you can provide only a few values, you should provide an initial guess to enable the iteration.</p> Parameter Type Description Default functionName String The name of the Excel function IRR <p>The identifier for this plugin is <code>Excel_IRR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#large","title":"Large","text":"<p>Excel LARGE(data; rank_c): Returns the Rank_c-th largest value in a data set. Data is the cell range of data. Rank_c is the ranking of the value (2nd largest, 3rd largest, etc.) written as an integer.</p> Parameter Type Description Default functionName String The name of the Excel function LARGE <p>The identifier for this plugin is <code>Excel_LARGE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#left","title":"Left","text":"<p>Excel LEFT(text; number): Returns the first character or characters in a text string. Text is the text where the initial partial words are to be determined. Number (optional) is the number of characters for the start text. If this parameter is not defined, one character is returned.</p> Parameter Type Description Default functionName String The name of the Excel function LEFT <p>The identifier for this plugin is <code>Excel_LEFT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ln","title":"Ln","text":"<p>Excel LN(number): Returns the natural logarithm based on the constant e of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function LN <p>The identifier for this plugin is <code>Excel_LN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#log","title":"Log","text":"<p>Excel LOG(number; base): Returns the logarithm of the given number to the specified base. Base is the base for the logarithm calculation.</p> Parameter Type Description Default functionName String The name of the Excel function LOG <p>The identifier for this plugin is <code>Excel_LOG</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#log10","title":"Log10","text":"<p>Excel LOG10(number): Returns the base-10 logarithm of the given number.</p> Parameter Type Description Default functionName String The name of the Excel function LOG10 <p>The identifier for this plugin is <code>Excel_LOG10</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#max","title":"Max","text":"<p>Excel MAX(number_1; number_2; \u2026 number_30): Returns the maximum value in a list of arguments. Number_1; number_2; \u2026 number_30 are numerical values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MAX <p>The identifier for this plugin is <code>Excel_MAX</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#maxa","title":"Maxa","text":"<p>Excel MAXA(value_1; value_2; \u2026 value_30): Returns the maximum value in a list of arguments. Unlike MAX, text can be entered. The value of the text is 0. Value_1; value_2; \u2026 value_30 are values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MAXA <p>The identifier for this plugin is <code>Excel_MAXA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#median","title":"Median","text":"<p>Excel MEDIAN(number_1; number_2; \u2026 number_30): Returns the median of a set of numbers. Number_1; number_2; \u2026 number_30 are values or ranges, which represent a sample. Each number can also be replaced by a reference.</p> Parameter Type Description Default functionName String The name of the Excel function MEDIAN <p>The identifier for this plugin is <code>Excel_MEDIAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mid","title":"Mid","text":"<p>Excel MID(text; start; number): Returns a text segment of a character string. The parameters specify the starting position and the number of characters. Text is the text containing the characters to extract. Start is the position of the first character in the text to extract. Number is the number of characters in the part of the text.</p> Parameter Type Description Default functionName String The name of the Excel function MID <p>The identifier for this plugin is <code>Excel_MID</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#min","title":"Min","text":"<p>Excel MIN(number_1; number_2; \u2026 number_30): Returns the minimum value in a list of arguments. Number_1; number_2; \u2026 number_30 are numerical values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MIN <p>The identifier for this plugin is <code>Excel_MIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mina","title":"Mina","text":"<p>Excel MINA(value_1; value_2; \u2026 value_30): Returns the minimum value in a list of arguments. Here text can also be entered. The value of the text is 0. Value_1; value_2; \u2026 value_30 are values or ranges.</p> Parameter Type Description Default functionName String The name of the Excel function MINA <p>The identifier for this plugin is <code>Excel_MINA</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mirr","title":"Mirr","text":"<p>Excel MIRR(values; investment; reinvest_rate): Calculates the modified internal rate of return of a series of investments. Values corresponds to the array or the cell reference for cells whose content corresponds to the payments. Investment is the rate of interest of the investments (the negative values of the array) Reinvest_rate is the rate of interest of the reinvestment (the positive values of the array).</p> Parameter Type Description Default functionName String The name of the Excel function MIRR <p>The identifier for this plugin is <code>Excel_MIRR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mod","title":"Mod","text":"<p>Excel MOD(dividend; divisor): Returns the remainder after a number is divided by a divisor. Dividend is the number which will be divided by the divisor. Divisor is the number by which to divide the dividend.</p> Parameter Type Description Default functionName String The name of the Excel function MOD <p>The identifier for this plugin is <code>Excel_MOD</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#mode","title":"Mode","text":"<p>Excel MODE(number_1; number_2; \u2026 number_30): Returns the most common value in a data set. Number_1; number_2; \u2026 number_30 are numerical values or ranges. If several values have the same frequency, it returns the smallest value. An error occurs when a value does not appear twice.</p> Parameter Type Description Default functionName String The name of the Excel function MODE <p>The identifier for this plugin is <code>Excel_MODE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#not","title":"Not","text":"<p>Excel NOT(logical_value): Reverses the logical value. Logical_value is any value to be reversed.</p> Parameter Type Description Default functionName String The name of the Excel function NOT <p>The identifier for this plugin is <code>Excel_NOT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#nper","title":"Nper","text":"<p>Excel NPER(rate; PMT; PV; FV; type): Returns the number of periods for an investment based on periodic, constant payments and a constant interest rate. Rate is the periodic interest rate. PMT is the constant annuity paid in each period. PV is the present value (cash value) in a sequence of payments. FV (optional) is the future value, which is reached at the end of the last period. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function NPER <p>The identifier for this plugin is <code>Excel_NPER</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#npv","title":"Npv","text":"<p>Excel NPV(Rate; value_1; value_2; \u2026 value_30): Returns the net present value of an investment based on a series of periodic cash flows and a discount rate. Rate is the discount rate for a period. Value_1; value_2;\u2026 value_30 are values representing deposits or withdrawals.</p> Parameter Type Description Default functionName String The name of the Excel function NPV <p>The identifier for this plugin is <code>Excel_NPV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#odd","title":"Odd","text":"<p>Excel ODD(number): Rounds the given number up to the nearest odd integer.</p> Parameter Type Description Default functionName String The name of the Excel function ODD <p>The identifier for this plugin is <code>Excel_ODD</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#or","title":"Or","text":"<p>Excel OR(logical_value_1; logical_value_2; \u2026logical_value_30): Returns TRUE if at least one argument is TRUE. Returns the value FALSE if all the arguments have the logical value FALSE. Logical_value_1; logical_value_2; \u2026logical_value_30 are conditions to be checked. All conditions can be either TRUE or FALSE. If a range is entered as a parameter, the function uses the value from the range that is in the current column or row.</p> Parameter Type Description Default functionName String The name of the Excel function OR <p>The identifier for this plugin is <code>Excel_OR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#percentile","title":"Percentile","text":"<p>Excel PERCENTILE(data; alpha): Returns the alpha-percentile of data values in an array. Data is the array of data. Alpha is the percentage of the scale between 0 and 1.</p> Parameter Type Description Default functionName String The name of the Excel function PERCENTILE <p>The identifier for this plugin is <code>Excel_PERCENTILE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pi","title":"Pi","text":"<p>Excel PI(): Returns the value of PI to fourteen decimal places.</p> Parameter Type Description Default functionName String The name of the Excel function PI <p>The identifier for this plugin is <code>Excel_PI</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pmt","title":"Pmt","text":"<p>Excel PMT(rate; NPER; PV; FV; type): Returns the periodic payment for an annuity with constant interest rates. Rate is the periodic interest rate. NPER is the number of periods in which annuity is paid. PV is the present value (cash value) in a sequence of payments. FV (optional) is the desired value (future value) to be reached at the end of the periodic payments. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function PMT <p>The identifier for this plugin is <code>Excel_PMT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#poisson","title":"Poisson","text":"<p>Excel POISSON(number; mean; C): Returns the Poisson distribution for the given Number. Mean is the middle value of the Poisson distribution. C = 0 calculates the density function, and C = 1 calculates the distribution.</p> Parameter Type Description Default functionName String The name of the Excel function POISSON <p>The identifier for this plugin is <code>Excel_POISSON</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#power","title":"Power","text":"<p>Excel POWER(base; power): Returns the result of a number raised to a power. Base is the number that is to be raised to the given power. Power is the exponent by which the base is to be raised.</p> Parameter Type Description Default functionName String The name of the Excel function POWER <p>The identifier for this plugin is <code>Excel_POWER</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#ppmt","title":"Ppmt","text":"<p>Excel PPMT(rate; period; NPER; PV; FV; type): Returns for a given period the payment on the principal for an investment that is based on periodic and constant payments and a constant interest rate. Rate is the periodic interest rate. Period is the amortization period. NPER is the total number of periods during which annuity is paid. PV is the present value in the sequence of payments. FV (optional) is the desired (future) value. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function PPMT <p>The identifier for this plugin is <code>Excel_PPMT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#product","title":"Product","text":"<p>Excel PRODUCT(number 1 to 30): Multiplies all the numbers given as arguments and returns the product. Number 1 to number 30 are up to 30 arguments whose product is to be calculated, separated by semi-colons.</p> Parameter Type Description Default functionName String The name of the Excel function PRODUCT <p>The identifier for this plugin is <code>Excel_PRODUCT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#proper","title":"Proper","text":"<p>Excel PROPER(text): Capitalizes the first letter in all words of a text string. Text is the text to be converted.</p> Parameter Type Description Default functionName String The name of the Excel function PROPER <p>The identifier for this plugin is <code>Excel_PROPER</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#pv","title":"Pv","text":"<p>Excel PV(rate; NPER; PMT; FV; type): Returns the present value of an investment resulting from a series of regular payments. Rate defines the interest rate per period. NPER is the total number of payment periods. PMT is the regular payment made per period. FV (optional) defines the future value remaining after the final installment has been made. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period.</p> Parameter Type Description Default functionName String The name of the Excel function PV <p>The identifier for this plugin is <code>Excel_PV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#radians","title":"Radians","text":"<p>Excel RADIANS(number): Converts the given number in degrees to radians.</p> Parameter Type Description Default functionName String The name of the Excel function RADIANS <p>The identifier for this plugin is <code>Excel_RADIANS</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rand","title":"Rand","text":"<p>Excel RAND(): Returns a random number between 0 and 1.</p> Parameter Type Description Default functionName String The name of the Excel function RAND <p>The identifier for this plugin is <code>Excel_RAND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rank","title":"Rank","text":"<p>Excel RANK(value; data; type): Returns the rank of the given Value in a sample. Data is the array or range of data in the sample. Type (optional) is the sequence order, either ascending (0) or descending (1).</p> Parameter Type Description Default functionName String The name of the Excel function RANK <p>The identifier for this plugin is <code>Excel_RANK</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rate","title":"Rate","text":"<p>Excel RATE(NPER; PMT; PV; FV; type; guess): Returns the constant interest rate per period of an annuity. NPER is the total number of periods, during which payments are made (payment period). PMT is the constant payment (annuity) paid during each period. PV is the cash value in the sequence of payments. FV (optional) is the future value, which is reached at the end of the periodic payments. Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period. Guess (optional) determines the estimated value of the interest with iterative calculation.</p> Parameter Type Description Default functionName String The name of the Excel function RATE <p>The identifier for this plugin is <code>Excel_RATE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replace","title":"Replace","text":"<p>Excel REPLACE(text; position; length; new_text): Replaces part of a text string with a different text string. This function can be used to replace both characters and numbers (which are automatically converted to text). The result of the function is always displayed as text. To perform further calculations with a number which has been replaced by text, convert it back to a number using the VALUE function. Any text containing numbers must be enclosed in quotation marks so it is not interpreted as a number and automatically converted to text. Text is text of which a part will be replaced. Position is the position within the text where the replacement will begin. Length is the number of characters in text to be replaced. New_text is the text which replaces text..</p> Parameter Type Description Default functionName String The name of the Excel function REPLACE <p>The identifier for this plugin is <code>Excel_REPLACE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rept","title":"Rept","text":"<p>Excel REPT(text; number): Repeats a character string by the given number of copies. Text is the text to be repeated. Number is the number of repetitions. The result can be a maximum of 255 characters.</p> Parameter Type Description Default functionName String The name of the Excel function REPT <p>The identifier for this plugin is <code>Excel_REPT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#right","title":"Right","text":"<p>Excel RIGHT(text; number): Defines the last character or characters in a text string. Text is the text of which the right part is to be determined. Number (optional) is the number of characters from the right part of the text.</p> Parameter Type Description Default functionName String The name of the Excel function RIGHT <p>The identifier for this plugin is <code>Excel_RIGHT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#roman","title":"Roman","text":"<p>Excel ROMAN(number; mode): Converts a number into a Roman numeral. The value range must be between 0 and 3999; the modes can be integers from 0 to 4. Number is the number that is to be converted into a Roman numeral. Mode (optional) indicates the degree of simplification. The higher the value, the greater is the simplification of the Roman numeral.</p> Parameter Type Description Default functionName String The name of the Excel function ROMAN <p>The identifier for this plugin is <code>Excel_ROMAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#round","title":"Round","text":"<p>Excel ROUND(number; count): Rounds the given number to a certain number of decimal places according to valid mathematical criteria. Count (optional) is the number of the places to which the value is to be rounded. If the count parameter is negative, only the whole number portion is rounded. It is rounded to the place indicated by the count.</p> Parameter Type Description Default functionName String The name of the Excel function ROUND <p>The identifier for this plugin is <code>Excel_ROUND</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#rounddown","title":"Rounddown","text":"<p>Excel ROUNDDOWN(number; count): Rounds the given number. Count (optional) is the number of digits to be rounded down to. If the count parameter is negative, only the whole number portion is rounded. It is rounded to the place indicated by the count.</p> Parameter Type Description Default functionName String The name of the Excel function ROUNDDOWN <p>The identifier for this plugin is <code>Excel_ROUNDDOWN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#roundup","title":"Roundup","text":"<p>Excel ROUNDUP(number; count): Rounds the given number up. Count (optional) is the number of digits to which rounding up is to be done. If the count parameter is negative, only the whole number portion is rounded. It is rounded to the place indicated by the count.</p> Parameter Type Description Default functionName String The name of the Excel function ROUNDUP <p>The identifier for this plugin is <code>Excel_ROUNDUP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#search","title":"Search","text":"<p>Excel SEARCH(find_text; text; position): Returns the position of a text segment within a character string. The start of the search can be set as an option. The search text can be a number or any sequence of characters. The search is not case-sensitive. The search supports regular expressions. Find_text is the text to be searched for. Text is the text where the search will take place. Position (optional) is the position in the text where the search is to start.</p> Parameter Type Description Default functionName String The name of the Excel function SEARCH <p>The identifier for this plugin is <code>Excel_SEARCH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sign","title":"Sign","text":"<p>Excel SIGN(number): Returns the sign of the given number. The function returns the result 1 for a positive sign, \u0096 1 for a negative sign, and 0 for zero.</p> Parameter Type Description Default functionName String The name of the Excel function SIGN <p>The identifier for this plugin is <code>Excel_SIGN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sin","title":"Sin","text":"<p>Excel SIN(number): Returns the sine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function SIN <p>The identifier for this plugin is <code>Excel_SIN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sinh","title":"Sinh","text":"<p>Excel SINH(number): Returns the hyperbolic sine of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function SINH <p>The identifier for this plugin is <code>Excel_SINH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#slope","title":"Slope","text":"<p>Excel SLOPE(data_Y; data_X): Returns the slope of the linear regression line. Data_Y is the array or matrix of Y data. Data_X is the array or matrix of X data.</p> Parameter Type Description Default functionName String The name of the Excel function SLOPE <p>The identifier for this plugin is <code>Excel_SLOPE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#small","title":"Small","text":"<p>Excel SMALL(data; rank_c): Returns the Rank_c-th smallest value in a data set. Data is the cell range of data. Rank_c is the rank of the value (2nd smallest, 3rd smallest, etc.) written as an integer.</p> Parameter Type Description Default functionName String The name of the Excel function SMALL <p>The identifier for this plugin is <code>Excel_SMALL</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sqrt","title":"Sqrt","text":"<p>Excel SQRT(number): Returns the positive square root of the given number. The value of the number must be positive.</p> Parameter Type Description Default functionName String The name of the Excel function SQRT <p>The identifier for this plugin is <code>Excel_SQRT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stdev","title":"Stdev","text":"<p>Excel STDEV(number_1; number_2; \u2026 number_30): Estimates the standard deviation based on a sample. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample based on an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function STDEV <p>The identifier for this plugin is <code>Excel_STDEV</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substitute","title":"Substitute","text":"<p>Excel SUBSTITUTE(text; search_text; new text; occurrence): Substitutes new text for old text in a string. Text is the text in which text segments are to be exchanged. Search_text is the text segment that is to be replaced (a number of times). New text is the text that is to replace the text segment. Occurrence (optional) indicates how many occurrences of the search text are to be replaced. If this parameter is missing, the search text is replaced throughout.</p> Parameter Type Description Default functionName String The name of the Excel function SUBSTITUTE <p>The identifier for this plugin is <code>Excel_SUBSTITUTE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sum","title":"Sum","text":"<p>Excel SUM(number_1; number_2; \u2026 number_30): Adds all the numbers in a range of cells. Number_1; number_2;\u2026 number_30 are up to 30 arguments whose sum is to be calculated. You can also enter a range using cell references.</p> Parameter Type Description Default functionName String The name of the Excel function SUM <p>The identifier for this plugin is <code>Excel_SUM</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumproduct","title":"Sumproduct","text":"<p>Excel SUMPRODUCT(array 1; array 2; \u2026array 30): Multiplies corresponding elements in the given arrays, and returns the sum of those products. Array 1; array 2;\u2026array 30 are arrays whose corresponding elements are to be multiplied. At least one array must be part of the argument list. If only one array is given, all array elements are summed.</p> Parameter Type Description Default functionName String The name of the Excel function SUMPRODUCT <p>The identifier for this plugin is <code>Excel_SUMPRODUCT</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumsq","title":"Sumsq","text":"<p>Excel SUMSQ(number_1; number_2; \u2026 number_30): Calculates the sum of the squares of numbers (totaling up of the squares of the arguments) Number_1; number_2;\u2026 number_30 are up to 30 arguments, the sum of whose squares is to be calculated.</p> Parameter Type Description Default functionName String The name of the Excel function SUMSQ <p>The identifier for this plugin is <code>Excel_SUMSQ</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumx2my2","title":"Sumx2my2","text":"<p>Excel SUMX2MY2(array_X; array_Y): Returns the sum of the difference of squares of corresponding values in two arrays. Array_X is the first array whose elements are to be squared and added. Array_Y is the second array whose elements are to be squared and subtracted.</p> Parameter Type Description Default functionName String The name of the Excel function SUMX2MY2 <p>The identifier for this plugin is <code>Excel_SUMX2MY2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumx2py2","title":"Sumx2py2","text":"<p>Excel SUMX2PY2(array_X; array_Y): Returns the sum of the sum of squares of corresponding values in two arrays. Array_X is the first array whose arguments are to be squared and added. Array_Y is the second array, whose elements are to be added and squared.</p> Parameter Type Description Default functionName String The name of the Excel function SUMX2PY2 <p>The identifier for this plugin is <code>Excel_SUMX2PY2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sumxmy2","title":"Sumxmy2","text":"<p>Excel SUMXMY2(array_X; array_Y): Adds the squares of the variance between corresponding values in two arrays. Array_X is the first array whose elements are to be subtracted and squared. Array_Y is the second array, whose elements are to be subtracted and squared.</p> Parameter Type Description Default functionName String The name of the Excel function SUMXMY2 <p>The identifier for this plugin is <code>Excel_SUMXMY2</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tan","title":"Tan","text":"<p>Excel TAN(number): Returns the tangent of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function TAN <p>The identifier for this plugin is <code>Excel_TAN</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tanh","title":"Tanh","text":"<p>Excel TANH(number): Returns the hyperbolic tangent of the given number (angle in radians).</p> Parameter Type Description Default functionName String The name of the Excel function TANH <p>The identifier for this plugin is <code>Excel_TANH</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#true","title":"True","text":"<p>Excel TRUE(): Sets the logical value to TRUE. The TRUE() function does not require any arguments.</p> Parameter Type Description Default functionName String The name of the Excel function TRUE <p>The identifier for this plugin is <code>Excel_TRUE</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#trunc","title":"Trunc","text":"<p>Excel TRUNC(number; count): Truncates a number to an integer by removing the fractional part of the number according to the precision specified in Tools &gt; Options &gt; OpenOffice.org Calc &gt; Calculate. Number is the number whose decimal places are to be cut off. Count is the number of decimal places which are not cut off.</p> Parameter Type Description Default functionName String The name of the Excel function TRUNC <p>The identifier for this plugin is <code>Excel_TRUNC</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#var","title":"Var","text":"<p>Excel VAR(number_1; number_2; \u2026 number_30): Estimates the variance based on a sample. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing a sample based on an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function VAR <p>The identifier for this plugin is <code>Excel_VAR</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#varp","title":"Varp","text":"<p>Excel VARP(Number_1; number_2; \u2026 number_30): Calculates a variance based on the entire population. Number_1; number_2; \u2026 number_30 are numerical values or ranges representing an entire population.</p> Parameter Type Description Default functionName String The name of the Excel function VARP <p>The identifier for this plugin is <code>Excel_VARP</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#extract","title":"Extract","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#regex-extract","title":"Regex extract","text":"<p>Extracts occurrences of a regex \u201cregex\u201d in a string. If there is at least one capture group, it will return the string of the first capture group instead.</p> Parameter Type Description Default regex String Regular expression no default extractAll boolean If true, all matches are extracted. If false, only the first match is extracted. false <p>The identifier for this plugin is <code>regexExtract</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.extraction</code>.</p> <p>Examples</p> <p>returns the first match Returns [afe123] for parameters [regex -&gt; [a-z]{2,4}123] and input values [[afe123_abc123]].</p> <p>returns all matches, if extractAll = true Returns [afe123, abc123] for parameters [regex -&gt; [a-z]{2,4}123, extractAll -&gt; true] and input values [[afe123_abc123]].</p> <p>returns an empty list if nothing matches Returns [] for parameters [regex -&gt; ^[a-z]{2,4}123] and input values [[abcdef123]].</p> <p>returns the match of the first capture group that matches Returns [abcd] for parameters [regex -&gt; ^([a-z]{2,4})123([a-z]+)] and input values [[abcd123xyz]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#filter","title":"Filter","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#filter-by-length","title":"Filter by length","text":"<p>Removes all strings that are shorter than \u2018min\u2019 characters and longer than \u2018max\u2019 characters.</p> Parameter Type Description Default min int No description 0 max int No description 2147483647 <p>The identifier for this plugin is <code>filterByLength</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#filter-by-regex","title":"Filter by regex","text":"<p>Removes all strings that do NOT match a regex. If \u2018negate\u2019 is true, only strings will be removed that match the regex.</p> Parameter Type Description Default regex String No description no default negate boolean No description false <p>The identifier for this plugin is <code>filterByRegex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-empty-values","title":"Remove empty values","text":"<p>Removes empty values.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeEmptyValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p> <p>Examples</p> <p>Returns [value1, value2] for parameters [] and input values [[value1, , value2]].</p> <p>Returns [] for parameters [] and input values [[, ]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-stopwords-remote-stopword-list","title":"Remove stopwords (remote stopword list)","text":"<p>Removes stopwords from all values. The stopword list is retrieved via a http connection (e.g. https://sites.google.com/site/kevinbouge/stopwords-lists/stopwords_de.txt). Each line in the stopword list contains a stopword. The separator defines a regex that is used for detecting words.</p> Parameter Type Description Default stopWordListUrl String No description no default separator String No description [\\s-]+ <p>The identifier for this plugin is <code>removeRemoteStopwords</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-stopwords","title":"Remove stopwords","text":"<p>Removes stopwords from all values. Each line in the stopword list contains a stopword. The separator defines a regex that is used for detecting words.</p> Parameter Type Description Default stopwordList Resource No description no default separator String No description [\\s-]+ <p>The identifier for this plugin is <code>removeStopwords</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-values","title":"Remove values","text":"<p>Removes values that contain words from a blacklist. The blacklist values are separated with commas.</p> Parameter Type Description Default blacklist String No description no default <p>The identifier for this plugin is <code>removeValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.filter</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geo","title":"Geo","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#retrieve-coordinates","title":"Retrieve coordinates","text":"<p>Retrieves geographic coordinates using Nominatim.</p> Parameter Type Description Default additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>RetrieveCoordinates</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#retrieve-latitude","title":"Retrieve latitude","text":"<p>Retrieves geographic coordinates using Nominatim and returns the latitude.</p> Parameter Type Description Default additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>RetrieveLatitude</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#retrieve-longitude","title":"Retrieve longitude","text":"<p>Retrieves geographic coordinates using Nominatim and returns the longitude.</p> Parameter Type Description Default additionalParameters String Additional URL parameters to be attached to each HTTP search request. Example: \u2018&amp;countrycodes=de&amp;addressdetails=1\u2019. Consult the API documentation for a list of available parameters. empty string <p>The identifier for this plugin is <code>RetrieveLongitude</code>.</p> <p>It can be found in the package <code>com.eccenca.di.geo</code>.</p> <p>Configuration</p> <p>The geocoding service to be queried for searches can be set up in the configuration. The default configuration is as follows:</p> <pre><code>com.eccenca.di.geo = {\n  # The URL of the geocoding service\n  # url = \"https://nominatim.eccenca.com/search\"\n  url = \"https://photon.komoot.de/api\"\n  # url = https://api-adresse.data.gouv.fr/search\n\n  # Additional URL parameters to be attached to all HTTP search requests. Example: '&amp;countrycodes=de&amp;addressdetails=1'.\n  # Will be attached in addition to the parameters set on each search operator directly.\n  searchParameters = \"\"\n\n  # The minimum pause time between subsequent queries\n  pauseTime = 1s\n\n  # Number of coordinates to be cached in-memory\n  cacheSize = 10\n}\n</code></pre> <p>In general, all services adhering to the Nominatim search API should be usable. Please note that when using public services, the pause time should be set to avoid overloading.</p> <p>Logging</p> <p>By default, individual requests to the geocoding service are not logged. To enable logging each request, the following configuration option can be set:</p> <pre><code>logging.level {\n  com.eccenca.di.geo=DEBUG\n}\n</code></pre>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#linguistic","title":"Linguistic","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#nysiis","title":"NYSIIS","text":"<p>NYSIIS phonetic encoding. Provided by the StringMetric library: http://rockymadden.com/stringmetric/.</p> Parameter Type Description Default refined boolean No description true <p>The identifier for this plugin is <code>NYSIIS</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#metaphone","title":"Metaphone","text":"<p>Metaphone phonetic encoding. Provided by the StringMetric library: http://rockymadden.com/stringmetric/.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>metaphone</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize-chars","title":"Normalize chars","text":"<p>Replaces diacritical characters with non-diacritical ones (eg, \u00f6 -&gt; o), plus some specialities like transforming \u00e6 -&gt; ae, \u00df -&gt; ss.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>normalizeChars</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#soundex","title":"Soundex","text":"<p>Soundex algorithm. Provided by the StringMetric library: http://rockymadden.com/stringmetric/.</p> Parameter Type Description Default refined boolean No description true <p>The identifier for this plugin is <code>soundex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#stem","title":"Stem","text":"<p>Stems a string using the Porter Stemmer.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>stem</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.linguistic</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize","title":"Normalize","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-non-alphabetic-characters","title":"Strip non-alphabetic characters","text":"<p>Strips all non-alphabetic characters from a string. Spaces are retained.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>alphaReduce</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#capitalize","title":"Capitalize","text":"<p>Capitalizes the string i.e. converts the first character to upper case. If \u2018allWords\u2019 is set to true, all words are capitalized and not only the first character.</p> Parameter Type Description Default allWords boolean No description false <p>The identifier for this plugin is <code>capitalize</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p> <p>Examples</p> <p>Returns [Capitalize me] for parameters [allWords -&gt; false] and input values [[capitalize me]].</p> <p>Returns [Capitalize Me] for parameters [allWords -&gt; true] and input values [[capitalize me]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#extract-physical-quantity","title":"Extract physical quantity","text":"<p>Extracts physical quantities, such as length or weight values. Values are expected of the form \u2018{Number}{UnitPrefix}{Symbol}\u2019 and are converted to the base unit.</p> <p>Example:</p> <ul> <li>Given a value \u201810km, 3mg\u2019.</li> <li>If the symbol parameter is set to \u2018m\u2019, the extracted value is 10000.</li> <li>If the symbol parameter is set to \u2018g\u2019, the extracted value is 0.001.</li> </ul> Parameter Type Description Default symbol String The symbol of the dimension, e.g., \u2018m\u2019 for meter. empty string numberFormat String The IETF BCP 47 language tag, e.g. \u2018en\u2019. en filter String Only extracts from values that contain the given regex (case-insensitive). empty string index int If there are multiple matches, retrieve the value with the given index (zero-based). 0 <p>The identifier for this plugin is <code>extractPhysicalQuantity</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean-html_1","title":"Clean HTML","text":"<p>Cleans HTML using a tag white list and allows selection of HTML sections with xPath or cssSelector expressions.  If the tag or attribute white lists are left empty default white lists will be used. The operator takes two inputs: the page HTML and  (optional) the page Url which may be needed to resolve relative links in the page HTML.</p> Parameter Type Description Default tagWhiteList String Tags to keep in the cleaned Text (or reference to a configuration). empty string attributeWhiteList String Tags to keep in the cleaned Text (or reference to a configuration). empty string selectors MultilineStringParameter CSS or XPath queries for selection of content (or reference to a configuration). Comma separated. CssSelectors can be pipe separated for non-sequential execution. no default method Enum Selects use of xPath or css selectors (\u2018xPath\u2019 or \u2018cssSelectors\u2019). xPath <p>The identifier for this plugin is <code>htmlCleaner</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.html</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#lower-case","title":"Lower case","text":"<p>Converts a string to lower case.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>lowerCase</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-blanks","title":"Remove blanks","text":"<p>Remove whitespace from a string.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeBlanks</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-duplicates","title":"Remove duplicates","text":"<p>Removes duplicated values, making a value sequence distinct.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeDuplicates</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-parentheses","title":"Remove parentheses","text":"<p>Remove all parentheses including their content, e.g., transforms \u2018Berlin (City)\u2019 -&gt; \u2018Berlin\u2019.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeParentheses</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#remove-special-chars","title":"Remove special chars","text":"<p>Remove special characters (including punctuation) from a string.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>removeSpecialChars</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-uri-prefix","title":"Strip URI prefix","text":"<p>Strips the URI prefix and decodes the remainder. Leaves values unchanged which are not a valid URI.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>stripUriPrefix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p> <p>Examples</p> <p>Returns [value] for parameters [] and input values [[http://example.org/some/path/to/value]].</p> <p>Returns [value] for parameters [] and input values [[urn:scheme:value]].</p> <p>Returns [encoded v\u00e4lue] for parameters [] and input values [[http://example.org/some/path/to/encoded%20v%C3%A4lue]].</p> <p>Returns [value] for parameters [] and input values [[value]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#trim","title":"Trim","text":"<p>Remove leading and trailing whitespaces.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>trim</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#upper-case","title":"Upper case","text":"<p>Converts a string to upper case.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>upperCase</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#fix-uri","title":"Fix URI","text":"<p>Generates valid absolute URIs from the given values. Already valid absolute URIs are left untouched.</p> Parameter Type Description Default uriPrefix String No description urn:url-encoded-value: <p>The identifier for this plugin is <code>uriFix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p> <p>Examples</p> <p>Returns [urn:url-encoded-value:ab] for parameters [] and input values [[ab]].</p> <p>Returns [urn:url-encoded-value:a%26b] for parameters [] and input values [[a&amp;b]].</p> <p>Returns [http://example.org/some/path] for parameters [] and input values [[http://example.org/some/path]].</p> <p>Returns [http://example.org/path?query=some+stuff#hashtag] for parameters [] and input values [[http://example.org/path?query=some+stuff#hashtag]].</p> <p>Returns [urn:valid:uri] for parameters [] and input values [[urn:valid:uri]].</p> <p>Returns [http://www.broken%20domain.com/broken%20weird%20path%20%C3%A4%C3%B6%C3%BC/nice/path/andNowSomeFragment#fragment%C3%A4%C3%B6%C3%BC] for parameters [] and input values [[http://www.broken domain.com/broken weird path \u00e4\u00f6\u00fc/nice/path/andNowSomeFragment#fragment\u00e4\u00f6\u00fc]].</p> <p>Returns [http://domain/#%23path%23] for parameters [] and input values [[http://domain/##path#]].</p> <p>Returns [urn:url-encoded-value:http+%3A+invalid+URI] for parameters [] and input values [[http : invalid URI]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#encode-url","title":"Encode URL","text":"<p>URL encodes the string.</p> Parameter Type Description Default encoding String The character encoding. UTF-8 <p>The identifier for this plugin is <code>urlEncode</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p> <p>Examples</p> <p>Returns [ab] for parameters [] and input values [[ab]].</p> <p>Returns [a%26b] for parameters [] and input values [[a&amp;b]].</p> <p>Returns [http%3A%2F%2Fexample.org%2Fsome%2Fpath] for parameters [] and input values [[http://example.org/some/path]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric_1","title":"Numeric","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#normalize-physical-quantity","title":"Normalize physical quantity","text":"<p>Normalizes physical quantities. Can either convert to a configured unit or to SI base units. For instance for lengths, values will be converted to metres if no target unit is configured. Will output the pure numeric value without the unit. If one input is provided, the physical quantities are parsed from the provided strings of the form \u201c1 km\u201d. If two inputs are provided, the numeric values are parsed from the first input and the units are parsed from the second inputs.</p> Parameter Type Description Default targetUnit String Target unit. Can be left empty to convert to the respective SI base units. empty string numberFormat String The IETF BCP 47 language tag, e.g., \u2018en\u2019. en <p>The identifier for this plugin is <code>PhysicalQuantitiesNormalizer</code>.</p> <p>It can be found in the package <code>com.eccenca.di.measure</code>.</p> <p>SI units and common derived units are supported. The following section lists all supported units. By default, all quantities are normalized to their base unit. For instance, lengths will be normalized to metres.</p> <p>Time</p> <p>Time is expressed in seconds (s). The following alternative units are supported: mo_s, mo_g, a, min, a_g, mo, mo_j, a_j, h, a_t, d.</p> <p>Length</p> <p>Length is expressed in metres (m). The following alternative units are supported: in, nmi, Ao, mil, yd, AU, ft, pc, fth, mi, hd.</p> <p>Mass</p> <p>Mass is expressed in kilograms (kg). The following alternative units are supported: lb, ston, t, stone, u, gr, lcwt, oz, g, scwt, dr, lton.</p> <p>Electric current</p> <p>Electric current is expressed in amperes (A). The following alternative units are supported: Bi, Gb.</p> <p>Temperature</p> <p>Temperature is expressed in kelvins (K). The following alternative units are supported: Cel.</p> <p>Amount of substance</p> <p>Amount of substance is expressed in moles (mol).</p> <p>Luminous intensity</p> <p>Luminous intensity is expressed in candelas (cd).</p> <p>Area</p> <p>Area is expressed in square metres (m\u00b2). The following alternative units are supported: m2, ar, syd, cml, b, sft, sin.</p> <p>Volume</p> <p>Volume is expressed in cubic metres (\u33a5). The following alternative units are supported: st, bf, cyd, cr, L, l, cin, cft, m3.</p> <p>Energy</p> <p>Energy is expressed in joules (J). The following alternative units are supported: cal_IT, eV, cal_m, cal, cal_th.</p> <p>Angle</p> <p>Angle is expressed in radians (rad). The following alternative units are supported: circ, gon, deg, \u2018, \u2018\u2019.</p> <p>Others</p> <ul> <li>1/m, derived units: Ky</li> <li>kg/(m\u00b7s), derived units: P</li> <li>bit/s, derived units: Bd</li> <li>bit, derived units: By</li> <li>Sv</li> <li>N</li> <li>\u03a9, derived units: Ohm</li> <li>T, derived units: G</li> <li>sr, derived units: sph</li> <li>F</li> <li>C/kg, derived units: R</li> <li>cd/m\u00b2, derived units: sb, Lmb</li> <li>Pa, derived units: bar, atm</li> <li>kg/(m\u00b7s\u00b2), derived units: att</li> <li>m\u00b2/s, derived units: St</li> <li>A/m, derived units: Oe</li> <li>kg\u00b7m\u00b2/s\u00b2, derived units: erg</li> <li>kg/m\u00b3, derived units: g%</li> <li>mho</li> <li>V</li> <li>lx, derived units: ph</li> <li>m/s\u00b2, derived units: Gal, m/s2</li> <li>m/s, derived units: kn</li> <li>m\u00b7kg/s\u00b2, derived units: gf, lbf, dyn</li> <li>m\u00b2/s\u00b2, derived units: RAD, REM</li> <li>C</li> <li>Gy</li> <li>Hz</li> <li>H</li> <li>lm</li> <li>W</li> <li>Wb, derived units: Mx</li> <li>Bq, derived units: Ci</li> <li>S Examples</li> </ul> <p>Returns [1000.0] for parameters [] and input values [[1 km]].</p> <p>Returns [0.3048] for parameters [] and input values [[1.0000     ft]].</p> <p>Returns [0.45359237] for parameters [] and input values [[1.0lb]].</p> <p>Returns [1.0] for parameters [] and input values [[1000000000.0 nm]].</p> <p>Returns [-1000000.0] for parameters [] and input values [[-1E6 m]].</p> <p>Returns [1000.5] for parameters [numberFormat -&gt; de] and input values [[1.000,5 m]].</p> <p>Returns [1000.5] for parameters [] and input values [[1,000.5 m]].</p> <p>Returns [0.621371192237334] for parameters [targetUnit -&gt; mi] and input values [[1 km]].</p> <p>Fails validation and thus returns [] for parameters [targetUnit -&gt; m] and input values [[1 kg]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[100.0]].</p> <p>Returns [1000.0] for parameters [] and input values [[1], [km]].</p> <p>Returns [1000.0, 10.0] for parameters [] and input values [[1, 10000], [km, mm]].</p> <p>Fails validation and thus returns [] for parameters [] and input values [[1, 10000, 10], [km, mm]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#aggregate-numbers","title":"Aggregate numbers","text":"<p>Aggregates all numbers in this set using a mathematical operation.</p> Parameter Type Description Default operator String One of \u2018+\u2019, \u2018*\u2019, \u2018min\u2019, \u2018max\u2019, \u2018average\u2019. no default <p>The identifier for this plugin is <code>aggregateNumbers</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#compare-numbers","title":"Compare numbers","text":"<p>Compares the numbers of two sets. Returns 1 if the comparison yields true and 0 otherwise. If there are multiple numbers in both sets, the comparator must be true for all numbers. For instance, {1,2} &lt; {2,3} yields 0 as not all numbers in the first set are smaller than in the second.</p> Parameter Type Description Default comparator Enum No description &lt; <p>The identifier for this plugin is <code>compareNumbers</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#count-values","title":"Count values","text":"<p>Counts the number of values.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>count</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p> <p>Examples</p> <p>Returns [1] for parameters [] and input values [[value1]].</p> <p>Returns [2] for parameters [] and input values [[value1, value2]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#extract-physical-quantity_1","title":"Extract physical quantity","text":"<p>Extracts physical quantities, such as length or weight values. Values are expected of the form \u2018{Number}{UnitPrefix}{Symbol}\u2019 and are converted to the base unit.</p> <p>Example:</p> <ul> <li>Given a value \u201810km, 3mg\u2019.</li> <li>If the symbol parameter is set to \u2018m\u2019, the extracted value is 10000.</li> <li>If the symbol parameter is set to \u2018g\u2019, the extracted value is 0.001.</li> </ul> Parameter Type Description Default symbol String The symbol of the dimension, e.g., \u2018m\u2019 for meter. empty string numberFormat String The IETF BCP 47 language tag, e.g. \u2018en\u2019. en filter String Only extracts from values that contain the given regex (case-insensitive). empty string index int If there are multiple matches, retrieve the value with the given index (zero-based). 0 <p>The identifier for this plugin is <code>extractPhysicalQuantity</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#format-number","title":"Format number","text":"<p>Formats a number according to a user-defined pattern.   The pattern syntax is documented at:   https://docs.oracle.com/javase/8/docs/api/java/text/DecimalFormat.html</p> Parameter Type Description Default pattern String No description no default locale String No description en <p>The identifier for this plugin is <code>formatNumber</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p> <p>Examples</p> <p>Returns [001] for parameters [pattern -&gt; 000] and input values [[1]].</p> <p>Returns [000123.780] for parameters [pattern -&gt; 000000.000] and input values [[123.78]].</p> <p>Returns [123,456.789] for parameters [pattern -&gt; ###,###.###] and input values [[123456.789]].</p> <p>Returns [123.456,789] for parameters [pattern -&gt; ###.###,###, locale -&gt; de] and input values [[123456.789]].</p> <p>Returns [10 apples] for parameters [pattern -&gt; # apples] and input values [[10]].</p> <p>Returns [0010] for parameters [pattern -&gt; 000\u20180\u2019] and input values [[1]].</p> <p>Returns [1] for parameters [pattern -&gt; 0] and input values [[1.0]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#logarithm","title":"Logarithm","text":"<p>Transforms all numbers by applying the logarithm function. Non-numeric values are left unchanged.</p> Parameter Type Description Default base int No description 10 <p>The identifier for this plugin is <code>log</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-operation","title":"Numeric operation","text":"<p>Applies a numeric operation to the values of multiple input operators.  Uses double-precision floating-point numbers for computation.</p> Parameter Type Description Default operator String The operator to be applied to all values. One of \u2018+\u2019, \u2018-\u2018, \u2018*\u2019, \u2018/\u2019 no default <p>The identifier for this plugin is <code>numOperation</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p> <p>Examples</p> <p>Returns [2.0] for parameters [operator -&gt; +] and input values [[1], [1]].</p> <p>Returns [0.0] for parameters [operator -&gt; -] and input values [[1], [1]].</p> <p>Returns [30.0] for parameters [operator -&gt; *] and input values [[5], [6]].</p> <p>Returns [2.5] for parameters [operator -&gt; /] and input values [[5], [2]].</p> <p>Returns [] for parameters [operator -&gt; +] and input values [[1], [no number]].</p> <p>Returns [1.0] for parameters [operator -&gt; *] and input values [[1], []].</p> <p>Returns [3.0] for parameters [operator -&gt; +] and input values [[1, 1], [1]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#numeric-reduce","title":"Numeric reduce","text":"<p>Strip all non-numeric characters from a string.</p> Parameter Type Description Default keepPunctuation boolean No description true <p>The identifier for this plugin is <code>numReduce</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p> <p>Examples</p> <p>Returns [12] for parameters [keepPunctuation -&gt; false] and input values [[some1.2Value]].</p> <p>Returns [1.2] for parameters [keepPunctuation -&gt; true] and input values [[some1.2Value]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parser","title":"Parser","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-date_1","title":"Parse date","text":"<p>Parses and normalizes dates in different formats.</p> Parameter Type Description Default inputDateFormatId Enum The input date/time format used for parsing the date/time string. w3c Date alternativeInputFormat String An input format string that should be used instead of the selected input format. Java DateFormat string. empty string outputDateFormatId Enum The output date/time format used for parsing the date/time string. w3c Date alternativeOutputFormat String An output format string that should be used instead of the selected output format. Java DateFormat string. empty string <p>The identifier for this plugin is <code>DateTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p> <p>Examples</p> <p>Returns [1999-03-20] for parameters [inputDateFormatId -&gt; German style date format, outputDateFormatId -&gt; w3c Date] and input values [[20.03.1999]].</p> <p>Returns [20.03.1999] for parameters [inputDateFormatId -&gt; w3c Date, outputDateFormatId -&gt; German style date format] and input values [[1999-03-20]].</p> <p>Returns [2017-04-04] for parameters [inputDateFormatId -&gt; common ISO8601, outputDateFormatId -&gt; w3c Date] and input values [[2017-04-04T00:00:00.000+02:00]].</p> <p>Returns [2017-04-04] for parameters [inputDateFormatId -&gt; common ISO8601, outputDateFormatId -&gt; w3c Date] and input values [[2017-04-04T00:00:00+02:00]].</p> <p>Returns [24-Jun-2021 14:50:05 +02:00] for parameters [inputDateFormatId -&gt; common ISO8601, outputDateFormatId -&gt; dateTime with month abbr. (US)] and input values [[2021-06-24T14:50:05.895+02:00]].</p> <p>Returns [24-Dez.-2021 14:50:05 +02:00] for parameters [inputDateFormatId -&gt; dateTime with month abbr. (US), outputDateFormatId -&gt; dateTime with month abbr. (DE)] and input values [[24-Dec-2021 14:50:05 +02:00]].</p> <p>Returns [1999-03-20T20:34.44] for parameters [alternativeInputFormat -&gt; dd.MM.yyyy HH:mm.ss, alternativeOutputFormat -&gt; yyyy-MM-dd\u2019T\u2019HH:mm.ss] and input values [[20.03.1999 20:34.44]].</p> <p>Returns [12:20:00.000] for parameters [inputDateFormatId -&gt; excelDateTime, outputDateFormatId -&gt; xsdTime] and input values [[12:20:00.000]].</p> <p>Returns [\u201301] for parameters [inputDateFormatId -&gt; w3c YearMonth, outputDateFormatId -&gt; w3c Month] and input values [[2020-01]].</p> <p>Returns [\u201431] for parameters [inputDateFormatId -&gt; w3c MonthDay, outputDateFormatId -&gt; w3c Day] and input values [[\u201312-31]].</p> <p>Returns [\u201312-31] for parameters [inputDateFormatId -&gt; w3c Date, outputDateFormatId -&gt; w3c MonthDay] and input values [[2020-12-31]].</p> <p>Fails validation and thus returns [] for parameters [inputDateFormatId -&gt; w3c MonthDay, outputDateFormatId -&gt; w3c Date] and input values [[\u201312-31]].</p> <p>Returns [2020-02-22T16:34:14] for parameters [alternativeInputFormat -&gt; yyyy-MM-dd HHss.SSS, outputDateFormatId -&gt; w3cDateTime] and input values [[2020-02-22 16:34:14.000]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-float","title":"Parse float","text":"<p>Parses and normalizes float values.</p> Parameter Type Description Default commaAsDecimalPoint boolean No description false thousandSeparator boolean No description false bracketsForNegative boolean No description false <p>The identifier for this plugin is <code>FloatTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-geo-coordinate","title":"Parse geo coordinate","text":"<p>Parses and normalizes geo coordinates.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>GeoCoordinateParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-geo-location","title":"Parse geo location","text":"<p>Parses and normalizes geo locations like continents, countries, states and cities.</p> Parameter Type Description Default parseTypeId Enum What type of location should be parsed. no default fullStateName boolean Set to true if the full state name should be output instead of the 2-letter code. true <p>The identifier for this plugin is <code>GeoLocationParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-integer","title":"Parse integer","text":"<p>Parses integer values.</p> Parameter Type Description Default commaAsDecimalPoint boolean Use comma as decimal point (uses a point, otherwise) false thousandSeparator boolean Use comma or point to separate digits false <p>The identifier for this plugin is <code>IntegerParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-isin","title":"Parse ISIN","text":"<p>Parses International Securities Identification Numbers (ISIN) values and fails if the String is no valid ISIN.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>IsinParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-skos-term","title":"Parse SKOS term","text":"<p>Parses values from a SKOS ontology.</p> Parameter Type Description Default surfaceFormToRepresentationMapping Map No description no default <p>The identifier for this plugin is <code>SkosTypeParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.discoverer</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#parse-string","title":"Parse string","text":"<p>Parses string values, basically an identity function.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>StringParser</code>.</p> <p>It can be found in the package <code>com.eccenca.di.schema.discovery.parser</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#clean-html_2","title":"Clean HTML","text":"<p>Cleans HTML using a tag white list and allows selection of HTML sections with xPath or cssSelector expressions.  If the tag or attribute white lists are left empty default white lists will be used. The operator takes two inputs: the page HTML and  (optional) the page Url which may be needed to resolve relative links in the page HTML.</p> Parameter Type Description Default tagWhiteList String Tags to keep in the cleaned Text (or reference to a configuration). empty string attributeWhiteList String Tags to keep in the cleaned Text (or reference to a configuration). empty string selectors MultilineStringParameter CSS or XPath queries for selection of content (or reference to a configuration). Comma separated. CssSelectors can be pipe separated for non-sequential execution. no default method Enum Selects use of xPath or css selectors (\u2018xPath\u2019 or \u2018cssSelectors\u2019). xPath <p>The identifier for this plugin is <code>htmlCleaner</code>.</p> <p>It can be found in the package <code>com.eccenca.di.plugins.html</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replace_1","title":"Replace","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#excel-map","title":"Excel map","text":"<p>Replaces values based on a map of values read from a file in Open XML format (XLSX). The XLSX file may contain several sheets of the form:</p> <p>mapFrom,mapTo , \u2026 and more</p> <p>An empty string can be created in Excel and alternatives by inserting =\u201d\u201d in the input line of a cell.</p> <p>If there are multiple values for a single key, all values will be returned for the given key.</p> <p>Note that the mapping table will be cached in memory. If the Excel file is updated (even while transforming), the map will be reloaded within seconds.</p> Parameter Type Description Default excelFile Resource Excel file inside the resources directory containing one or more sheets with mapping tables. no default sheetName String The sheet that contains the mapping table or empty if the first sheet should be taken. empty string skipLines int How many rows to skip before reading the mapping table. By default the expected header row is skipped. 1 strict boolean If set to true the operator throws validation errors for values it cannot map. If set to false it will output them unchanged. true conflictStrategy Enum The strategy how to cope with map conflicts when in strict-mode. Current strategies are to retain the values and to remove them. retain <p>The identifier for this plugin is <code>excelMap</code>.</p> <p>It can be found in the package <code>com.eccenca.di.excel</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#map","title":"Map","text":"<p>Replaces values based on a map of values.</p> Parameter Type Description Default map Map A map of values no default default String Default if the map defines no value no default <p>The identifier for this plugin is <code>map</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p> <p>Examples</p> <p>Returns [Value1] for parameters [map -&gt; Key1:Value1,Key2:Value2, default -&gt; Undefined] and input values [[Key1]].</p> <p>Returns [Undefined] for parameters [map -&gt; Key1:Value1,Key2:Value2, default -&gt; Undefined] and input values [[Key1X]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#map-with-default","title":"Map with default","text":"<p>Takes two inputs. Tries to map the first input based on the map of values parameter config. If the input value is not found in the map, it takes the value of the second input. The indexes of the mapped value and the default value match. If there are less default values than values to map, the last default value is replicated to match the count.</p> Parameter Type Description Default map Map A map of values no default <p>The identifier for this plugin is <code>mapWithDefaultInput</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#regex-replace","title":"Regex replace","text":"<p>Replace all occurrences of a regex \u201cregex\u201d with \u201creplace\u201d in a string.</p> Parameter Type Description Default regex String No description no default replace String No description no default <p>The identifier for this plugin is <code>regexReplace</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#replace_2","title":"Replace","text":"<p>Replace all occurrences of a string \u201csearch\u201d with \u201creplace\u201d in a string.</p> Parameter Type Description Default search String No description no default replace String No description no default <p>The identifier for this plugin is <code>replace</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.replace</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#selection","title":"Selection","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#coalesce-first-non-empty-input","title":"Coalesce (first non-empty input)","text":"<p>Forwards the first non-empty input, i.e. for which any value(s) exist. A single empty string is considered a value.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>coalesce</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.selection</code>.</p> <p>Examples</p> <p>Returns [] for parameters [] and input values [[], [], []].</p> <p>Returns [] for parameters [] and input values [[], []].</p> <p>Returns [] for parameters [] and input values [].</p> <p>Returns [first] for parameters [] and input values [[], [first], [second]].</p> <p>Returns [first A, first B] for parameters [] and input values [[], [first A, first B], [second]].</p> <p>Returns [first] for parameters [] and input values [[first], [second]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#regex-selection","title":"Regex selection","text":"<pre><code>  This transformer takes 3 inputs.\n  The first input should have exactly one value that should be passed out again untouched.\n  The second input has at least two Regex values - two in order to make sense.\n  The third input should have exactly one value which is checked against the regexes.\n\n  The result of the transformer is a sequence with the same length of number of regexes.\n  For the output value (of the first input) is set to each position in this sequence where\n  the related regex also matched.\n\n  If oneOnly is true only the position of the &lt;strong&gt;first&lt;/strong&gt; matching regex will be set to the output value.\n</code></pre> Parameter Type Description Default oneOnly boolean No description false <p>The identifier for this plugin is <code>regexSelect</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.selection</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sequence","title":"Sequence","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#count-values_1","title":"Count values","text":"<p>Counts the number of values.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>count</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.numeric</code>.</p> <p>Examples</p> <p>Returns [1] for parameters [] and input values [[value1]].</p> <p>Returns [2] for parameters [] and input values [[value1, value2]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#get-value-by-index","title":"Get value by index","text":"<p>Returns the value found at the specified index. Fails or returns an empty result depending on failIfNoFound is set or not.        Please be aware that this will work only if the data source supports some kind of ordering like XML or JSON. This        is probably not a good idea to do with RDF models.</p> <pre><code>   If emptyStringToEmptyResult is true then instead of a result with an empty String, an empty result is returned.\n</code></pre> Parameter Type Description Default index int No description no default failIfNotFound boolean No description false emptyStringToEmptyResult boolean No description false <p>The identifier for this plugin is <code>getValueByIndex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.sequence</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#sequence-values-to-indexes","title":"Sequence values to indexes","text":"<p>Transforms the sequence of values to their respective indexes in the sequence.   Example:    - (\u201ca\u201d, \u201cb\u201d, \u201cc\u201d) becomes (0, 1, 2)</p> <p>If there is more than one input, the values are numbered from the first input on and continued for the next inputs.   Applied against an RDF source the order might not be deterministic.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>toSequenceIndex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.sequence</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substring","title":"Substring","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-postfix","title":"Strip postfix","text":"<p>Strips a postfix of a string.</p> Parameter Type Description Default postfix String No description no default <p>The identifier for this plugin is <code>stripPostfix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p> <p>Examples</p> <p>Returns [value] for parameters [postfix -&gt; Postfix] and input values [[valuePostfix]].</p> <p>Returns [Value] for parameters [postfix -&gt; Postfix] and input values [[Value]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-prefix","title":"Strip prefix","text":"<p>Strips a prefix of a string.</p> Parameter Type Description Default prefix String No description no default <p>The identifier for this plugin is <code>stripPrefix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p> <p>Examples</p> <p>Returns [Value] for parameters [prefix -&gt; prefix] and input values [[prefixValue]].</p> <p>Returns [ValueWithoutPrefix] for parameters [prefix -&gt; prefix] and input values [[ValueWithoutPrefix]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#strip-uri-prefix_1","title":"Strip URI prefix","text":"<p>Strips the URI prefix and decodes the remainder. Leaves values unchanged which are not a valid URI.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>stripUriPrefix</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p> <p>Examples</p> <p>Returns [value] for parameters [] and input values [[http://example.org/some/path/to/value]].</p> <p>Returns [value] for parameters [] and input values [[urn:scheme:value]].</p> <p>Returns [encoded v\u00e4lue] for parameters [] and input values [[http://example.org/some/path/to/encoded%20v%C3%A4lue]].</p> <p>Returns [value] for parameters [] and input values [[value]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#substring_1","title":"Substring","text":"<p>Returns a substring between \u2018beginIndex\u2019 (inclusive) and \u2018endIndex\u2019 (exclusive). If \u2018endIndex\u2019 is 0 (default), it is ignored and the entire remaining string starting with \u2018beginIndex\u2019 is returned. If \u2018endIndex\u2019 is negative, -endIndex characters are removed from the end.</p> Parameter Type Description Default beginIndex int The beginning index, inclusive. 0 endIndex int The end index, exclusive. Ignored if set to 0, i.e., the entire remaining string starting with \u2018beginIndex\u2019 is returned. If negative, -endIndex characters are removed from the end 0 stringMustBeInRange boolean If true, only strings will be accepted that are within the start and end indices, throwing a validating error if an index is out of range. true <p>The identifier for this plugin is <code>substring</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p> <p>Examples</p> <p>Returns [a] for parameters [beginIndex -&gt; 0, endIndex -&gt; 1] and input values [[abc]].</p> <p>Returns [c] for parameters [beginIndex -&gt; 2, endIndex -&gt; 3] and input values [[abc]].</p> <p>Returns [] for parameters [beginIndex -&gt; 3, endIndex -&gt; 3] and input values [[abc]].</p> <p>Fails validation and thus returns [c] for parameters [beginIndex -&gt; 2, endIndex -&gt; 4] and input values [[abc]].</p> <p>Returns [c] for parameters [beginIndex -&gt; 2, endIndex -&gt; 4, stringMustBeInRange -&gt; false] and input values [[abc]].</p> <p>Returns [] for parameters [beginIndex -&gt; 10, endIndex -&gt; 20, stringMustBeInRange -&gt; false] and input values [[abc]].</p> <p>Returns [ab] for parameters [beginIndex -&gt; 0, endIndex -&gt; -1] and input values [[abc]].</p> <p>Returns [bc] for parameters [beginIndex -&gt; 1, endIndex -&gt; 0] and input values [[abc]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#trim_1","title":"Trim","text":"<p>Remove leading and trailing whitespaces.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>trim</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.normalize</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#until-character","title":"Until character","text":"<p>Extracts the substring until the character given.</p> Parameter Type Description Default untilCharacter char No description no default <p>The identifier for this plugin is <code>untilCharacter</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.substring</code>.</p> <p>Examples</p> <p>Returns [ab] for parameters [untilCharacter -&gt; c] and input values [[abcde]].</p> <p>Returns [abab] for parameters [untilCharacter -&gt; c] and input values [[abab]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#template","title":"Template","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#evaluate-template_1","title":"Evaluate template","text":"<p>Evaluates a template. Input values can be addressed using the variables \u2018input1\u2019, \u2018input2\u2019, etc.</p> Parameter Type Description Default template MultilineStringParameter The template no default language Enum The template language. Currently, Jinja is supported. Jinja <p>The identifier for this plugin is <code>TemplateTransformer</code>.</p> <p>It can be found in the package <code>com.eccenca.di.templating.operators</code>.</p> <p>Examples</p> <p>Returns [Hello John Doe,</p> <p>How are you today?] for parameters [template -&gt; Hello {{input1}} {{input2}},</p> <p>How are you today?] and input values [[John], [Doe]].</p> <p>Fails validation and thus returns [] for parameters [template -&gt; Hello {{badVariable}} {{input1}}] and input values [[John], [Doe]].</p> <p>Fails validation and thus returns [] for parameters [template -&gt; Hello {{input01}}] and input values [].</p> <p>Fails validation and thus returns [] for parameters [template -&gt; Hello {{input1}}] and input values [].</p> <p>Returns [Hello AB] for parameters [template -&gt; Hello {{input1}}] and input values [[A, B]].</p> <p>Returns [Hello Bob, Eve, how are you doing?] for parameters [template -&gt; Hello {% for value in input1 %}{{value}}, {% endfor %}how are you doing?] and input values [[Bob, Eve]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tokenization","title":"Tokenization","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#camel-case-tokenizer","title":"Camel case tokenizer","text":"<p>Tokenizes a camel case string. That is it splits strings between a lower case characted and an upper case character.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>camelcasetokenizer</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.tokenization</code>.</p> <p>Examples</p> <p>Returns [camel, Case, String] for parameters [] and input values [[camelCaseString]].</p> <p>Returns [nocamelcase] for parameters [] and input values [[nocamelcase]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#tokenize","title":"Tokenize","text":"<p>Tokenizes all input values.</p> Parameter Type Description Default regex String The regular expression used to split values. \\s <p>The identifier for this plugin is <code>tokenize</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.tokenization</code>.</p> <p>Examples</p> <p>Returns [Hello, World] for parameters [] and input values [[Hello World]].</p> <p>Returns [.175, .050] for parameters [regex -&gt; ,] and input values [[.175,.050]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validation","title":"Validation","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-after_1","title":"Validate date after","text":"<p>Validates if the first input date is after the second input date. Outputs the first input if the validation is successful.</p> Parameter Type Description Default allowEqual boolean Allow both dates to be equal. false <p>The identifier for this plugin is <code>validateDateAfter</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p> <p>Examples</p> <p>Fails validation and thus returns [] for parameters [] and input values [[2015-04-02], [2015-04-03]].</p> <p>Returns [2015-04-04] for parameters [] and input values [[2015-04-04], [2015-04-03]].</p> <p>Returns [2015-04-03] for parameters [allowEqual -&gt; true] and input values [[2015-04-03], [2015-04-03]].</p> <p>Fails validation and thus returns [] for parameters [allowEqual -&gt; false] and input values [[2015-04-03], [2015-04-03]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-date-range_1","title":"Validate date range","text":"<p>Validates if dates are within a specified range.</p> Parameter Type Description Default minDate String Earliest allowed date in YYYY-MM-DD no default maxDate String Latest allowed data in YYYY-MM-DD no default <p>The identifier for this plugin is <code>validateDateRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-number-of-values","title":"Validate number of values","text":"<p>Validates that the number of values lies in a specified range.</p> Parameter Type Description Default min int Minimum allowed number of values 0 max int Maximum allowed number of values 1 <p>The identifier for this plugin is <code>validateNumberOfValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p> <p>Examples</p> <p>Returns [value1] for parameters [min -&gt; 0, max -&gt; 1] and input values [[value1]].</p> <p>Fails validation and thus returns [] for parameters [min -&gt; 0, max -&gt; 1] and input values [[value1, value2]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-numeric-range_1","title":"Validate numeric range","text":"<p>Validates if a number is within a specified range.</p> Parameter Type Description Default min double Minimum allowed number no default max double Maximum allowed number no default <p>The identifier for this plugin is <code>validateNumericRange</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#validate-regex","title":"Validate regex","text":"<p>Validates if all values match a regular expression.</p> Parameter Type Description Default regex String regular expression \\w* <p>The identifier for this plugin is <code>validateRegex</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.validation</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#value","title":"Value","text":"","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#constant_1","title":"Constant","text":"<p>Generates a constant value.</p> Parameter Type Description Default value String The constant value to be generated empty string <p>The identifier for this plugin is <code>constant</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#constant-uri","title":"Constant URI","text":"<p>Generates a constant URI.</p> Parameter Type Description Default value Uri The constant URI to be generated <p>The identifier for this plugin is <code>constantUri</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#current-date_1","title":"Current date","text":"<p>Outputs the current date.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>currentDate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.date</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#dataset-parameter","title":"Dataset parameter","text":"<p>Reads a meta data parameter from a dataset in Corporate Memory. If authentication is enabled, workbench.superuser must be configured.</p> Parameter Type Description Default project ProjectReference The project of the dataset. cmem dataset TaskReference The dataset the meta data parameter is read from. no default key String No description no default lang String No description empty string <p>The identifier for this plugin is <code>datasetParameter</code>.</p> <p>It can be found in the package <code>com.eccenca.di.workflow.operators.datasetParam</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#default-value","title":"Default Value","text":"<p>Generates a default value, if the input values are empty. Forwards any non-empty values.</p> Parameter Type Description Default value String The default value to be generated, if input values are empty default <p>The identifier for this plugin is <code>defaultValue</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p> <p>Examples</p> <p>Returns [input value] for parameters [] and input values [[input value]].</p> <p>Returns [default value] for parameters [value -&gt; default value] and input values [[]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#empty-value","title":"Empty value","text":"<p>Generates an empty value value.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>emptyValue</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#random-number","title":"Random number","text":"<p>Generates a set of random numbers.</p> Parameter Type Description Default min double The smallest number that could be generated. 0.0 max double The largest number that could be generated. 100.0 minCount int The minimum number of values to generate in each set. 1 maxCount int The maximum number of values to generate in each set. 1 <p>The identifier for this plugin is <code>randomNumber</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#read-parameter","title":"Read parameter","text":"<p>Reads a parameter from a Java Properties file.</p> Parameter Type Description Default resource Resource The Java properties file to read the parameter from. no default parameter String The name of the parameter. no default <p>The identifier for this plugin is <code>readParameter</code>.</p> <p>It can be found in the package <code>org.silkframework.plugins.transformer.value</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#uuid","title":"UUID","text":"<p>Generates UUIDs. If no input value is provided, a random UUID (type 4) is generated using a cryptographically strong pseudo random number generator. If input values are provided, a name-based UUID (type 3) is generated for each input value. Each input value will generate a separate UUID. For building a UUID from multiple inputs, the Concatenate operator can be used.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>uuid</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.transformer.value</code>.</p> <p>Examples</p> <p>Returns [cee963a2-8f70-3e97-b51a-85ef732e66dd] for parameters [] and input values [[input value]].</p> <p>Returns [690802dd-a317-335f-807c-e4e1e32b7b5b, 925cbd7f-377b-3fbd-8f4c-ca41529b74ad] for parameters [] and input values [[\u00fc\u00f6\u00e4!, \u00ea\u00e9\u00e8]].</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#aggregations","title":"Aggregations","text":"<p>The following aggregation functions are available:</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#average_1","title":"Average","text":"<p>Computes the weighted average.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>average</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#geometric-mean","title":"Geometric mean","text":"<p>Compute the (weighted) geometric mean.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>geometricMean</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#handle-missing-values","title":"Handle missing values","text":"<p>Generates a default similarity score, if no similarity score is provided (e.g., due to missing values). Using this operator can have a performance impact, since it lowers the efficiency of the underlying computation.</p> Parameter Type Description Default defaultValue double The default value to be generated, if no similarity score is provided. Must be a value between -1 (inclusive) and 1 (inclusive). \u20181\u2019 represents boolean true and \u2018-1\u2019 represents boolean false. -1.0 <p>The identifier for this plugin is <code>handleMissingValues</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#or_1","title":"Or","text":"<p>At least one input score must be within the threshold. Selects the maximum score.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>max</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#and_1","title":"And","text":"<p>All input scores must be within the threshold. Selects the minimum score.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>min</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#negate","title":"Negate","text":"<p>Negates the result of the input comparison. A single input is expected. Using this operator can have a performance impact, since it lowers the efficiency of the underlying computation.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>negate</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#euclidian-distance","title":"Euclidian distance","text":"<p>Calculates the Euclidian distance.</p> <p>This plugin does not require any parameters. The identifier for this plugin is <code>quadraticMean</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/dataintegration/plugin-reference/#scale","title":"Scale","text":"<p>Scales the result of the first input. All other inputs are ignored.</p> Parameter Type Description Default factor double All input similarity values are multiplied with this factor. 1.0 <p>The identifier for this plugin is <code>scale</code>.</p> <p>It can be found in the package <code>org.silkframework.rule.plugins.aggegrator</code>.</p> <ol> <li> <p>Hive 1.2.1 is ODPi runtime compliant\u00a0\u21a9</p> </li> </ol>","tags":["Reference"]},{"location":"deploy-and-configure/configuration/datamanager/","title":"DataManager","text":"<p>This page describes how to configure eccenca DataManager.</p> <p>It is intended for system administrators, who are responsible for installing, configuring, maintaining and supporting the deployment of DataManager.</p> <p>eccenca DataManager is a single-page JavaScript application, which means the application consists of a single HTML page which loads all needed web resources in the browser after loading the page itself.</p> <p>In the context of DataManager, these web resources are:</p> <ul> <li>The application including its configuration (<code>app*.js</code>, <code>config.js</code>)</li> <li>Styles (<code>*.css</code>)</li> <li>Web fonts for typography as well as for icons (<code>*.woff</code>, <code>*.ttf</code>, <code>*.eot</code>)</li> <li>Images (e.g. logos) (<code>*.png</code>, <code>*.svg</code>)</li> </ul> <p>DataManager communicates with different API endpoints in order to retrieve and manipulate data.</p> <p>The features of DataManager include:</p> <ul> <li>Dataset Manager to create and update datasets and its meta data</li> <li>Vocabulary Manager to install and remove Vocabulary descriptions</li> <li>Data browser to explore and manage graph-based data</li> <li>Taxonomy Editor to manage and create SKOS based taxonomies</li> <li>Query editor to query graph-based data via SPARQL queries</li> <li>Access control</li> <li>Compliance of W3C standards such as\u00a0RDF,\u00a0Linked Data\u00a0and\u00a0SPARQL</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/datamanager/account-settings-module/","title":"Account Settings module","text":"<p>Configuration property:\u00a0<code>modules.accountSettings</code>\u00a0| Scope: app-wide and per workspace</p> <p>DataManager provides a menu item for editing the account settings, which needs to be enabled in keycloak.</p> <ul> <li>js.config.modules.accountSettings<ul> <li>url</li> <li>enable</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.accountSettings.enable true no none boolean js.config.modules.accountSettings.url <code>http://docker.local/auth/realms/cmem/account/?referer={{REFERRER}}&amp;referrer_uri={{REFERRER_URI}}</code> yes if enable is true none url string with placeholders:\u00a0{{REFERRER}},\u00a0{{<code>REFERRER_URI}}</code>"},{"location":"deploy-and-configure/configuration/datamanager/admin-module/","title":"Admin module","text":"<p>Configuration property:\u00a0<code>modules.administration</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Admin module of DataManager is used to manage user access rights.</p> <ul> <li>js.config.modules.administration<ul> <li>enable</li> <li>accessConditions<ul> <li>graph</li> </ul> </li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.administration.enable true no none boolean <p>Set this property to true to enable the Admin module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>, all other settings of\u00a0<code>modules.administration</code>\u00a0are skipped.</p> Property Default Required Conflicts with Valid values js.config.modules.administration.accessConditions.graph none yes none string (URI) <p>Set this property to define the graph of saved access conditions.</p>"},{"location":"deploy-and-configure/configuration/datamanager/admin-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules:\nadministration:\nenable: true\naccessConditions:\ngraph: \"urn:elds-backend-access-conditions-graph\"\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/api-endpoints/","title":"Api endpoints","text":"<p>Configuration property:\u00a0<code>api</code>\u00a0| Scope: app-wide and per workspace</p> <p>DataManager provides the option to define which endpoints should be used for SPARQL and SPARQL Update requests.</p> <ul> <li>js.config.api<ul> <li>sparql</li> <li>sparqlUpdate</li> <li>defaultTimeout</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.api.sparql \u2018/proxy/:endpointId/sparql\u2019 yes none string <p>Use this property to define the default endpoint for all SPARQL requests.</p> <p>Info</p> <p>When a relative path is set, the base url will be added automatically. The placeholder\u00a0<code>:endpointId</code>\u00a0will be set according to the used workspace defined at\u00a0<code>js.config.workspaces[id].backend.endpointId</code>.</p> Property Default Required Conflicts with Valid values js.config.api.sparqlUpdate \u2018/proxy/:endpointId/update\u2019 yes none string <p>Use this property to define the default endpoint for all SPARQL Update requests.</p> <p>Info</p> <p>When a relative path is set, the base url will be added automatically. The placeholder\u00a0<code>:endpointId</code>\u00a0will be set according to the used workspace defined at\u00a0<code>js.config.workspaces[id].backend.endpointId</code>.</p> Property Default Required Conflicts with Valid values js.config.api.defaultTimeout 60000 no none number <p>Set this property to limit the timeout (in milliseconds) for requesting data in the tables of DataManager.</p>"},{"location":"deploy-and-configure/configuration/datamanager/api-endpoints/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.api:\nsparql: /proxy/:endpointId/sparql\nsparqlUpdate: /proxy/:endpointId/update\ndefaultTimeout: 60000\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/app-presentation/","title":"App presentation","text":"<p>Configuration property:\u00a0<code>appPresentation</code>\u00a0| Scope: app-wide and per workspace</p> <p>DataManager provides the option to customize the visual presentation.</p> <ul> <li>js.config.appPresentation<ul> <li>faviconUrl</li> <li>windowTitle</li> <li>logoUrl</li> <li>headerName</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.appPresentation.faviconUrl none no none string (URL) <p>Use this property to define a custom favicon that is displayed in the browser tab. Pictures can be an URL or a\u00a0Data URL.</p> Property Default Required Conflicts with Valid values js.config.appPresentation.windowTitle \u2018eccenca DataManager\u2019 no none string <p>Use this property to define a custom browser tab name.</p> Property Default Required Conflicts with Valid values js.config.appPresentation.logoUrl none no none string (URL) <p>Use this property to define a custom logo that is shown in the Module bar. Images can be a an URL or a\u00a0Data URL.</p> Property Default Required Conflicts with Valid values js.config.appPresentation.headerName <p>Use this property to define a custom name that is shown in the Module bar.</p>"},{"location":"deploy-and-configure/configuration/datamanager/app-presentation/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.appPresentation:\nfaviconUrl: https://example.com/example/favicon.png\nwindowTitle: Datamanager\nlogoUrl: https://example.com/example/logo.png\nheaderName: Datamanager\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/application-logging-with-logback/","title":"Application logging with Logback","text":"<p>Logging for eccenca DataManager can also be configured with\u00a0Logback, which, for example, allows a more granular control on file rolling strategies. For further information on configuration options, refer to the\u00a0Logback\u2019s Configuration\u00a0manual section and the Spring Boot\u2019s Configure Logback for logging manual section.</p> Property Default Required Conflicts with Valid values logging.configuration none no none string (file path) <p>Use this property to specify where the Logback configuration is located.</p>"},{"location":"deploy-and-configure/configuration/datamanager/application-logging-with-logback/#configuration-example","title":"Configuration example","text":"<pre><code>logging:\nconfiguration: \"${ELDS_HOME}/etc/datamanager/logback.xml\"\n</code></pre> <p>The following example\u00a0<code>logback.xml</code>\u00a0file defines a rolling file strategy where files are rotated on a time base (1 day) with a limit of 7 files, which means that the logging files contain a log history of a maximum of 1 week.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration&gt;\n&lt;appender name=\"TIME_BASED_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n&lt;file&gt;/opt/elds/var/log/datamanager.log&lt;/file&gt;\n&lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt;\n&lt;!-- daily rollover, history for 1 week --&gt;\n&lt;fileNamePattern&gt;/opt/elds/var/log/datamanger.%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;\n&lt;maxHistory&gt;7&lt;/maxHistory&gt;\n&lt;/rollingPolicy&gt;\n&lt;encoder&gt;\n&lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;\n&lt;/encoder&gt;\n&lt;/appender&gt;\n&lt;logger name=\"com.eccenca\" level=\"INFO\"&gt;\n&lt;appender-ref ref=\"TIME_BASED_FILE\" /&gt;\n&lt;/logger&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/dataset-module/","title":"Dataset module","text":"<p>Configuration property:\u00a0<code>modules.datasets</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Dataset module of DataManager is used to manage datasets and their attached resources.</p> <ul> <li>js.config.modules.datasets<ul> <li>enable</li> <li>startWith</li> <li>graphUrl</li> <li>dataintegration<ul> <li>url</li> <li>project</li> </ul> </li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.datasets.enable false no none boolean <p>Set this property to\u00a0<code>true</code>\u00a0to enable the Dataset module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>\u00a0, all other settings of\u00a0<code>modules.datasets</code>\u00a0are skipped. To use the module you also need to have read access to the graphs specified in\u00a0<code>js.config.modules.vocabulary.graphUrl</code>and\u00a0<code>js.config.shacl.shapesGraph</code>\u00a0as well as the access control action\u00a0<code>urn:eccenca:di</code>.</p> Property Default Required Conflicts with Valid values js.config.modules.datasets.startWith false no none boolean <p>Set this property to\u00a0<code>true</code>\u00a0to load this module as default one after login.</p> <p>Info</p> <p>If more than one module has defined<code>startWith: true</code>\u00a0the top most module in the navigation bar will be set as default.</p> Property Default Required Conflicts with Valid values js.config.modules.datasets.graphUrl none yes none string (URI) <p>Use this property to define the target graph for read and write operations.</p> Property Default Required Conflicts with Valid values js.config.modules.datasets.dataintegration.url none yes none string (URL) <p>Use this property to define the URL of DataIntegration that is needed for dataset workflows.</p> Property Default Required Conflicts with Valid values js.config.modules.datasets.dataintegration.project none yes none string <p>Use this property to define the name of the DataIntegration project.</p>"},{"location":"deploy-and-configure/configuration/datamanager/dataset-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules:\ndatasets:\nenable: true\nstartWith: false\ngraphUrl: https://example.com/example/datasets/\ndataintegration:\nurl: https://example.com/dataintegration/\nproject: your_project_name\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/default-configuration/","title":"Default Configuration","text":"<p>The following configuration shows the default configuration of DataManager.</p> <pre><code>js.config.workspaces:\ndefault:\nname: Eccenca Vocabulary Service\nauthorization:\ntype: anonymous\nbackend:\ntype: dataplatform\nurl: https://vocab.eccenca.com/\nendpointId: default\njs.config.appPresentation:\nwindowTitle: eccenca DataManager\nheaderName: DataManager\njs.config.shacl:\nshapesGraph: https://vocab.eccenca.com/shacl/\njs.config.resourceTable:\ntimeoutDownload: 600000\njs.config.api:\nsparql: /proxy/:endpointId/sparql\nsparqlQueryBase64Encoded: false\nsparqlUpdate: /proxy/:endpointId/update\ndefaultTimeout: 60000\njs.config.errorPages:\ngraphAccess:\ntitle: Unauthorized User\nmessage: You are not authorized to use this workspace.\nmoduleAccess:\ntitle: No module accessible\nmessage: You have no access to any module.\nworkspaceAccess:\ntitle: Workspace access problem\nmessage: You are logged in successfully but you do not have enough permissions. Please contact your administrator.\njs.config.titleHelper:\nproperties:\n- http://www.w3.org/ns/shacl#name\n- http://www.w3.org/2004/02/skos/core#prefLabel\n- http://xmlns.com/foaf/0.1/name\n- http://purl.org/dc/elements/1.1/title\n- http://purl.org/dc/terms/title\n- http://www.w3.org/2000/01/rdf-schema#label\nlanguages:\n- en\n- ''\njs.config.userPermissions:\nallowCreateWorkspace: true\nallowSelectWorkspace: true\njs.config.modules.task:\nenable: true\njs.config.modules.administration:\nenable: true\naccessConditions:\ngraph: urn:elds-backend-access-conditions-graph\njs.config.modules.datasets:\nenable: false\njs.config.modules.explore:\nenable: true\nshacl:\nallowCopy: false\nshowGraphInfo: false\nuseSaveApi: false\ngraphlist:\ndefaultGraph: ''\nhideSearch: false\nnavigation:\ndefaultClass: ''\nitemsPerPage: 10\ntopQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT ?resource ?hasChildren\n{{FROM}}\nWHERE {\n{\n?resource rdfs:subClassOf owl:Thing .\n} UNION {\n?r a ?resource .\nFILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } .\n} UNION {\n?resource a owl:Class\nFILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } .\n}\nOPTIONAL{\n?child rdfs:subClassOf ?resource .\nFILTER(isIRI(?child)) .\n}\nFILTER(isIRI(?resource)) .\nBIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren)\n}\nsubQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT ?resource ?hasChildren\n{{FROM}}\nWHERE {\n?resource rdfs:subClassOf {{RESOURCE}} .\nOPTIONAL{\n?child rdfs:subClassOf ?resource .\nFILTER(isIRI(?child)) .\n}\nFILTER(isIRI(?resource)) .\nBIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren)\n}\nsearchQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\nSELECT ?resource ?hasChildren\n{{FROM}}\nWHERE {\n{\n?resource rdfs:subClassOf+ owl:Thing\n} UNION {\n?r a ?resource .\n} UNION {\n?resource a owl:Class\n}\nOPTIONAL{\n?child rdfs:subClassOf ?resource .\nFILTER(isIRI(?child)) .\n}\nOPTIONAL {\n?resource rdfs:label ?label1 .\n}\nOPTIONAL {\n?resource skos:prefLabel ?label2 .\n}\nBIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren)\nFILTER(isIRI(?resource)) .\nFILTER(\nregex(str(?resource),\"{{QUERY}}\",\"i\") ||\nregex(str(?label1),\"{{QUERY}}\",\"i\") ||\nregex(str(?label2),\"{{QUERY}}\",\"i\")\n)\n}\nlistQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nSELECT ?instance\n{{FROM}}\nWHERE {\n{\n?instance a {{RESOURCE}} .\nFILTER isIRI(?instance) .\n} UNION {\n?class rdfs:subClassOf+ {{RESOURCE}} .\n?instance a ?class .\nFILTER isIRI(?instance).\n}\n}\noverallSearchQuery: |\nSELECT ?resource\n{{FROM}}\nWHERE {\n?resource ?p0 ?label.\nFILTER (!isBLANK(?resource)).\nFILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))).\n}\ndetails:\nproperties:\nenable: true\nusage:\nenable: true\nreferences:\nenable: true\nturtle:\nenable: true\nstatistics:\nenable: true\nsunburst:\nenable: true\nvisualization:\nenable: true\nwebvowlConfig:\nfilter:\nliterals: true\nrelations: true\nsolitarySubclasses: true\nclassDisjointness: true\nsetOperators: true\ndegreeOfCollapsing: true\nmode:\ndynamicLabelWidth: true\npickAndPin: true\nnodeScaling: true\ncompactNotation: true\ncolorExternals: true\nexport:\njson: true\nsvg: true\ngravity:\nclassDistance: true\ndataTypeDistance: true\nreset: true\npause: true\nsearch: true\njs.config.modules.thesaurus:\nenable: true\njs.config.modules.query:\nenable: true\ngraph: https://ns.eccenca.com/data/queries/\ntimeout: 600000\njs.config.modules.vocabulary:\nenable: false\njs.config.modules.gdprsearch:\nenable: false\njs.config.modules.linkrules:\nenable: false\njs.config.modules.annotation:\nenable: false\njs.config.modules.search:\nenable: false\njs.config.modules.tracking:\nenable: false\njs.config.modules.reports:\nenable: false\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/easynav-module/","title":"EasyNav module","text":"<p>Configuration property:\u00a0<code>modules.easynav</code>\u00a0| Scope: app-wide and per workspace</p> <p>The EasyNav module of DataManager is used for visual graph data exploration.</p> <ul> <li>js.config.modules.easynav<ul> <li>enable</li> <li>startWith</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.easynav.enable false no none boolean <p>Use this property to enable the EasyNav module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>\u00a0, all other settings of\u00a0<code>modules.easynav</code>\u00a0are skipped.</p> Property Default Required Conflicts with Valid values js.config.modules.easynav.startWith false no none boolean <p>Use this property to load the EasyNav module as after login.</p> <p>Info</p> <p>If more than one module has defined\u00a0<code>startWith: true</code>\u00a0the top most module in the navigation bar will be set as default.</p>"},{"location":"deploy-and-configure/configuration/datamanager/error-pages/","title":"Error pages","text":"<p>Configuration property:\u00a0<code>errorPages</code>\u00a0| Scope: app-wide and per workspace</p> <p>DataManager provides the option to customize special user errors.</p> <ul> <li>js.config.errorPages<ul> <li>graphAccess<ul> <li>title</li> <li>message</li> </ul> </li> <li>moduleAccess<ul> <li>title</li> <li>message</li> </ul> </li> <li>workspaceAccess<ul> <li>title</li> <li>message</li> </ul> </li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.errorPages.graphAccess.title \u2018Unauthorized User\u2019 no none string <p>Use this property to define a custom title that is displayed if a user does not have read access to any graph of the selected workspace.</p> Property Default Required Conflicts with Valid values js.config.errorPages.graphAccess.message \u2018You are not authorized to use this workspace.\u2019 no none string <p>Use this property to define a custom message that is displayed if a user does not have read access to any graph of the selected workspace.</p> Property Default Required Conflicts with Valid values js.config.errorPages.moduleAccess.title \u2018No module accessible\u2019 no none string <p>Use this property to define a custom title that is displayed if a user does not have permission to access any module of DataManager.</p> Property Default Required Conflicts with Valid values js.config.errorPages.moduleAccess.message \u2018You have no access to any module.\u2019 no none string <p>Use this property to define a custom message that is displayed if a user does not have permission to access any module of DataManager.</p> Property Default Required Conflicts with Valid values js.config.errorPages.workspaceAccess.title \u2018Workspace access problem\u2019 no none string <p>Use this property to define a custom title that is displayed if the workspace is not accessible.</p> Property Default Required Conflicts with Valid values js.config.errorPages.workspaceAccess.message \u2018The configured workspace is not accessible.\u2019 no none string <p>Use this property to define a custom message that is displayed if the workspace is not accessible.</p>"},{"location":"deploy-and-configure/configuration/datamanager/error-pages/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.errorPages:\ngraphAccess:\ntitle: Unauthorized User\nmessage: You are not authorized to use this workspace.\nmoduleAccess:\ntitle: No module accessible\nmessage: You have no access to any module.\nworkspaceAccess:\ntitle: Workspace access problem\nmessage:: The configured workspace is not accessible.\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/explore-module/","title":"Explore module","text":"<p>Configuration property:\u00a0<code>modules.explore</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Explore module of DataManager is used for graph data exploration.</p> <ul> <li>js.config.modules.explore<ul> <li>enable</li> <li>startWith</li> <li>overallSearchQuery</li> <li>mapServer<ul> <li>url</li> <li>ext</li> </ul> </li> <li>graphlist<ul> <li>defaultGraph</li> <li>hideSearch</li> <li>whiteList</li> </ul> </li> <li>navigation<ul> <li>defaultClass</li> <li>topQuery</li> <li>subQuery</li> <li>searchQuery</li> <li>listQuery</li> <li>itemsPerPage</li> </ul> </li> <li> <p>details</p> <ul> <li>properties<ul> <li>enable</li> </ul> </li> <li>usage<ul> <li>enable</li> </ul> </li> <li>references<ul> <li>enable</li> </ul> </li> <li>turtle<ul> <li>enable</li> </ul> </li> <li> <p>history</p> </li> <li> <p>enable</p> </li> <li> <p>statistics</p> <ul> <li>enable</li> <li>sunburst<ul> <li>enable</li> </ul> </li> <li>visualization</li> </ul> </li> <li>enable</li> <li>webvowlConfig<ul> <li>filter<ul> <li>literals</li> <li>relations</li> <li>solitarySubclasses</li> <li>classDisjointness</li> <li>setOperators</li> <li>degreeOfCollapsing</li> </ul> </li> <li>mode<ul> <li>dynamicLabelWidth</li> <li>pickAndPin</li> <li>nodeScaling</li> <li>compactNotation</li> <li>colorExternals</li> </ul> </li> <li>export<ul> <li>json</li> <li>svg</li> </ul> </li> <li>gravity<ul> <li>classDistance</li> <li>dataTypeDistance</li> </ul> </li> <li>reset</li> <li>pause</li> <li>search</li> <li>externalTools</li> </ul> </li> <li>toolX<ul> <li>enable</li> <li>tabname</li> <li>iframeUrlTemplate</li> </ul> </li> </ul> </li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.explore.enable true no none boolean <p>Use this property to enable the Explore module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>\u00a0, all other settings of\u00a0<code>modules.explore</code>\u00a0are skipped.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.startWith false no none boolean <p>Use this property to load the Explore module as default one after login.</p> <p>Info</p> <p>If more than one module has defined\u00a0<code>startWith: true</code>\u00a0the top most module in the navigation bar will be set as default.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.overallSearchQuery see below no none string (query <p>Use this property to define a custom query for the search field provided in the Module bar.</p> <pre><code># default query\njs.config.modules.explore.overallSearchQuery: |\nSELECT DISTINCT ?resource ?_resource\n{{FROM}}\nWHERE {\n?resource ?p0 ?label.\nFILTER (!isBLANK(?resource)).\nBIND (?resource as ?_resource) .\nFILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))).\n}\n</code></pre> <p>Info</p> <p>The placeholder\u00a0<code>{{QUERY}}</code>\u00a0is replaced with the search string entered by the user. A placeholder\u00a0<code>{{FROM}}</code>\u00a0can be used to insert the currently selected graph URI.</p> Property Default Required Conflicts with Valid values <code>js.config.modules.explore.mapServer.url</code> none no none string (URI) <p>Extension of the tiles as provided by OpenMapTiles Map Server</p> <p>Info</p> <p>It works together with\u00a0<code>js.config.modules.explore.mapServer.ext</code>, and only if it is set.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.mapServer.ext none no none string (URI) <p>The service url as provided by OpenMapTiles Map Server. If not defined, the wikimedia server is used.</p> <p>Info</p> <p>It works together with\u00a0<code>js.config.modules.explore.mapServer.url</code>, and only if it is set.</p> <p>This is how\u00a0<code>mapServer.ext</code>\u00a0and\u00a0<code>mapServer.url</code>\u00a0are used in your configuration:</p> <pre><code>js.config.modules.explore:\nmapServer:\nurl: 'https://osm.your-host.com/styles/osm-bright'\next: 'png'\n</code></pre> Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.defaultGraph none no none string (file extension) <p>Use this property to define a graph URI the user is allowed to work on.</p> <p>Info</p> <p>If the property is set the\u00a0Graph box\u00a0is hidden.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.hideSearch true no none boolean <p>Set this property to true to hide the Search field in the Graph box</p> Property Default Required Conflicts with Valid values js.config.modules.explore.graphlist.whiteList none no none list of strings (query) <p>Use this property to specify a list of graphs the user can see.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.defaultClass none no none string (URI) <p>Use this property to setup a default class.</p> <p>Info</p> <p>It works together with\u00a0<code>defaultGraph</code>, and only if it is set.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.topQuery see below no none string (query) <p>Use this property to specify a custom query that defines which top level classes of a graph are displayed.</p> <pre><code># default query\njs.config.modules.explore.navigation.topQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT  ?resource ?hasChildren\n{{FROM}}\nWHERE {\n{\n?resource rdfs:subClassOf owl:Thing .\n} UNION {\n?r a ?resource .\nFILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } .\n} UNION {\n?resource a owl:Class\nFILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } .\n}\nOPTIONAL{\n?child rdfs:subClassOf ?resource .\nFILTER(isIRI(?child)) .\n}\nFILTER(isIRI(?resource)) .\nBIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren)\nFILTER (!regex(\nSTR(?resource),\n\"^http://(www.w3.org/(2002/07/owl|2000/01/rdf-schema|1999/02/22-rdf-syntax-ns))#\",\n\"i\"\n))\n}\n</code></pre> <p>Info</p> <p>A placeholder\u00a0<code>{{FROM}}</code>\u00a0can be used to insert the currently selected graph URI. The\u00a0<code>{{FROM}}</code>placeholder will be resolved to\u00a0<code>FROM &lt;graphUri&gt;</code>.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.subQuery see below no none string (query) <p>Use this property to specify a custom query that defines which subclasses of top level classes of a graph are displayed.</p> <pre><code># default query\njs.config.modules.explore.navigation.subQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nSELECT ?resource ?hasChildren\n{{FROM}}\nWHERE {\n?resource rdfs:subClassOf {{RESOURCE}} .\nOPTIONAL{\n?child rdfs:subClassOf ?resource .\nFILTER(isIRI(?child)) .\n}\nFILTER(isIRI(?resource)) .\nBIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren)\n}\n</code></pre> <p>Info</p> <p>The placeholder\u00a0<code>{{RESOURCE}}</code>\u00a0is replaced with the selected parent class. A placeholder\u00a0<code>{{FROM}}</code>\u00a0can be used to insert the currently selected graph URI. The\u00a0<code>{{FROM}}</code>\u00a0placeholder will be resolved to\u00a0<code>FROM &lt;graphUri&gt;</code>.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.searchQuery see below no none string (query) <p>Use this property to specify a custom query that defines which classes of a graph are displayed when the user uses the\u00a0Search\u00a0field in the\u00a0Navigation box.</p> <pre><code># default query\njs.config.modules.explore.navigation.searchQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\nSELECT  ?resource ?hasChildren\n{{FROM}}\nWHERE {\n{\n?resource rdfs:subClassOf+ owl:Thing\n} UNION {\n?r a ?resource .\n} UNION {\n?resource a owl:Class\n}\nOPTIONAL{\n?child rdfs:subClassOf ?resource .\nFILTER(isIRI(?child)) .\n}\nOPTIONAL {\n?resource rdfs:label ?label1 .\n}\nOPTIONAL {\n?resource skos:prefLabel ?label2 .\n}\nBIND(IF(BOUND(?child), \"hasChildren\", \"noChildren\") AS ?hasChildren)\nFILTER(isIRI(?resource)) .\nFILTER(\nregex(str(?resource),\"{{QUERY}}\",\"i\") || regex(str(?label1),\"{{QUERY}}\",\"i\") || regex(str(?label2),\"{{QUERY}}\",\"i\")\n)\n}\n</code></pre> <p>Info</p> <p>The placeholder {{QUERY}} is replaced with the search string. A placeholder\u00a0<code>{{GRAPH}}</code>\u00a0can be used for insert currently selected graph URI (will be resolved to\u00a0<code>&lt;graphUri&gt;</code>\u00a0).</p> Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.listQuery see below no none string (query) <p>Use this property to specify a custom query that defines which resources are displayed that are type of a selected class of a graph.</p> <pre><code># default query\njs.config.modules.explore.navigation.listQuery: |\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nSELECT ?instance\n{{FROM}}\nWHERE {\n{\n?instance a {{RESOURCE}} .\nFILTER isIRI(?instance) .\n} UNION {\n?class rdfs:subClassOf+ {{RESOURCE}} .\n?instance a ?class .\nFILTER isIRI(?instance).\n}\n}\n</code></pre> <p>Info</p> <p>The placeholder\u00a0<code>{{RESOURCE}}</code>\u00a0is replaced by the selected resource URI. A placeholder\u00a0<code>{{GRAPH}}</code>can be used for insert currently selected graph URI (will be resolved to\u00a0<code>&lt;graphUri&gt;</code>\u00a0).</p> Property Default Required Conflicts with Valid values js.config.modules.explore.navigation.itemsPerPage 15 no none number <p>Use this property to specify the number of items shown per page of navigation.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.properties.enable true no none boolean <p>Use this property to enable the\u00a0<code>properties</code>\u00a0tab of DataManager</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.usage.enable true no none boolean <p>Use this property to enable the\u00a0<code>usage</code>\u00a0tab of DataManager.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.references.enable true no none boolean <p>Use this property to enable the\u00a0<code>references</code>\u00a0tab of DataManager.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.turtle.enable true no none boolean <p>Use this property to enable the\u00a0<code>turtle</code>\u00a0tab of DataManager.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.history.enable true no none boolean <p>Use this property to enable the\u00a0History\u00a0tab of DataManager.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.statistics.enable true no none boolean <p>Use this property to enable the\u00a0<code>statistic</code>\u00a0tab of DataManager.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.statistics.sunburst.enable true no none boolean <p>Sunburst is the visualization element in\u00a0<code>statistic</code>\u00a0tab.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.enable true no none boolean <p>Use this property to enable the\u00a0<code>visualization</code>\u00a0tab of DataManager.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.literals true no none boolean <p>Use this property to enable the literals filter in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.relations true no none boolean <p>Use this property to enable the relations filter in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.solitarySubclasses true no none boolean <p>Use this property to enable the solitary subclasses filter in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.classDisjointness true no none boolean <p>Use this property to enable the class disjointness filter in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.setOperators true no none boolean <p>Use this property to enable the set operators filter in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.filter.degreeOfCollapsing true no none boolean <p>Use this property to enable the degree of collapsing function in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.dynamicLabelWidth true no none boolean <p>Use this property to enable the dynamic label width mode in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.pickAndPin true no none boolean <p>Use this property to enable the pick and pin mode in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.nodeScaling true no none boolean <p>Use this property to enable the node scaling mode in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.compactNotation true no none boolean <p>Use this property to enable the compact notation mode in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.mode.colorExternals true no none boolean <p>Use this property to enable the color externals mode in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.export.json true no none boolean <p>Use this property to enable the feature export as json in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.export.svg true no none boolean <p>Use this property to enable the feature export as svg in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.gravity.classDistance true no none boolean <p>Use this property to enable the class distance option in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.gravity.dataTypeDistance true no none boolean <p>Use this property to enable the dataType distance option in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.reset true no none boolean <p>Use this property to enable the reset function in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.pause true no none boolean <p>Use this property to enable the pause function in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.details.visualization.webvowlConfig.search true no none boolean <p>Use this property to enable the search function in the OWL viewer.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.enable false no none boolean <p>The externTools section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame.</p> <p>Use this property to enable or disable one specific external tool configuration.</p> <pre><code>js.config.modules.explore.externalTools.toolX.enable: true\n</code></pre> Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.tabname none yes none string <p>The\u00a0<code>externTools</code>\u00a0section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame.</p> <p>Use this property to name the tab for the external tool.</p> Property Default Required Conflicts with Valid values js.config.modules.explore.externalTools.toolX.iframeUrlTemplate none yes none string (URL) <p>The\u00a0<code>externTools</code>\u00a0section can be used to configure one or more external tools which will be integrated as additional tabs in a resource detail view. The tool is then presented in the content of an iFrame. In addition to that, a JSON representation of the presented resource is send via postmate to the running application inside of the iFrame.</p> <p>Use this property to specify the URL which will be loaded in the iFrame inside of the new application tab.</p> <pre><code>js.config.modules.explore.externalTools.toolX.iframeUrlTemplate:\n\"http://example.org/app/{{RESOURCE}}\"\n</code></pre> <p>Info</p> <p>The placeholder\u00a0<code>{{RESOURCE}}</code>\u00a0is replaced by the selected resource URI. The placeholder\u00a0<code>{{RESOURCELABEL}}</code>\u00a0is replaced with the titleHelper generated label of the resource.</p>"},{"location":"deploy-and-configure/configuration/datamanager/explore-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules.explore:\nenable: true\nstartWith: true\noverallSearchQuery: |\nSELECT DISTINCT ?resource ?_resource\n{{FROM}}\nWHERE {\n?resource ?p0 ?label.\nFILTER (!isBLANK(?resource)).\nBIND (?resource as ?_resource) .\nFILTER (contains (lcase(str(?label)), lcase(\"{{QUERY}}\"))).\n}\ngraphlist:\ndefaultGraph: ''\nhideSearch: true\nnavigation:\ntopQuery: |\nSELECT DISTINCT ?resource\nWHERE {\n?r a ?resource .\nFILTER NOT EXISTS { ?resource rdfs:subClassOf ?super } .\n}\nsubQuery: |\nSELECT DISTINCT ?resource\nWHERE {\n?r a ?resource .\n?resource rdfs:subClassOf {{RESOURCE}} .\n}\nsearchQuery: |\nSELECT DISTINCT ?resource\nWHERE {\n?r a ?resource .\n?resource rdfs:label ?label .\nFILTER(contains(?label, \"{{QUERY}}\")) .\n}\nlistQuery: |\nSELECT DISTINCT ?resource\nWHERE {\n{\n?resource a {{RESOURCE}} .\n} UNION {\n?class rdfs:subClassOf+ {{RESOURCE}} .\n?resource a ?class .\n}\n}\ndetails:\nproperties:\nenable: true\nusage:\nenable: true\nreferences:\nenable: true\nturtle:\nenable: true\nstatistics:\nenable: true\nsunburst:\nenable: true\nvisualization:\nenable: true\nwebvowlConfig:\nfilter:\nliterals: true\nrelations: true\nsolitarySubclasses: true\nclassDisjointness: true\nsetOperators: true\ndegreeOfCollapsing: true\nmode:\ndynamicLabelWidth: true\npickAndPin: true\nnodeScaling: true\ncompactNotation: true\ncolorExternals: true\nexport:\njson: true\nsvg: true\ngravity:\nclassDistance: true\ndataTypeDistance: true\nreset: true\npause: true\nsearch: true\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/query-module/","title":"Query module","text":"<p>Configuration property:\u00a0<code>modules.query</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Query module of DataManager is used to query rdf data directly from store.</p> <ul> <li>js.config.modules.query<ul> <li>enable</li> <li>graph</li> <li>startWith</li> <li>timeout</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.query.enable true no none boolean <p>Set this property to\u00a0<code>true</code>\u00a0to enable the Query module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>, all other settings of modules.query are skipped. To use the module you also need to have read access to the graphs specified in\u00a0<code>js.config.modules.vocabulary.graph</code>\u00a0and\u00a0<code>js.config.shacl.shapesGraph</code>\u00a0as well as the access control action\u00a0<code>urn:eccenca:QueryUserInterface</code>\u00a0.</p> Property Default Required Conflicts with Valid values js.config.modules.query.graph none yes none string (URI) <p>Use this property to define the target graph for read and write operations.</p> Property Default Required Conflicts with Valid values js.config.modules.query.startWith false no none boolean <p>Set this property to true to load this module as the default one after login.</p> <p>Info</p> <p>If more than one module is defined\u00a0<code>startWith: true</code>\u00a0the top most module in the navigation bar will be set as default.</p> Property Default Required Conflicts with Valid values js.config.modules.query.timeout 600000 no none number <p>Set this property to limit the timeout (in milliseconds) for requesting manual queries in Query Module.</p>"},{"location":"deploy-and-configure/configuration/datamanager/query-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules:\nquery:\nenable: true\nstartWith: false\ngraph: \"https://ns.eccenca.com/data/queries/\"\ntimeout: 600000\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/resource-table-configuration/","title":"Resource Table configuration","text":"<p>Configuration property:\u00a0<code>resourceTable</code></p> <p>DataManager provides the option to configure the Resource Tables.</p> <ul> <li>js.config.resourceTable<ul> <li>timeoutDownload</li> <li>pagination<ul> <li>limit</li> </ul> </li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.resourceTable.timeoutDownload 600000 no none number <p>Set this property to limit the timeout (in milliseconds) for requesting a file to download in the tables of Query Module.</p> Property Default Required Conflicts with Valid values js.config.resourceTable.<code>pagination.limit</code> 25 no none number <p>Set this property to limit the default pagination limit for any Resource Table.</p>"},{"location":"deploy-and-configure/configuration/datamanager/resource-table-configuration/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.shacl:\nshapesGraph: \"https://vocab.eccenca.com/shacl/\"\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/shacl-configuration/","title":"Shacl configuration","text":"<p>Configuration property:\u00a0<code>shacl</code></p> <p>DataManager provides the option to configure shacl.</p> <ul> <li>js.config.shacl<ul> <li>shapesGraph</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.shacl.shapesGraph none yes none string (URL) <p>Define in which graph shacl shapes exists.</p>"},{"location":"deploy-and-configure/configuration/datamanager/shacl-configuration/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.shacl:\nshapesGraph: \"https://vocab.eccenca.com/shacl/\"\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/task-module/","title":"Task module","text":"<p>Configuration property:\u00a0<code>modules.task</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Task module of DataManager is used to offer direct user action interfaces for viewing or manipulating specific data.</p> <ul> <li>js.config.modules.task<ul> <li>enable</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.task.enable true no none boolean <p>Set this property to\u00a0<code>true</code>\u00a0to enable the Task module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>, all other settings of\u00a0<code>modules.task</code>\u00a0are skipped.</p>"},{"location":"deploy-and-configure/configuration/datamanager/task-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules:\n  task:\n    enable: true\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/thesaurus-module/","title":"Thesaurus module","text":"<p>Configuration property:\u00a0<code>modules.thesaurus</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Thesaurus module of DataManager is used to manage thesauri or taxonomies with SKOS vocabularies.</p> <ul> <li>js.config.modules.thesaurus<ul> <li>enable</li> <li>startWith</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.thesaurus.enable false no none boolean <p>Set this property to true to enable the Thesaurus module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>, all other settings of\u00a0<code>modules.thesaurus</code>\u00a0are skipped. To use the module you also need to have read access to the graph\u00a0<code>js.config.shacl.shapesGraph</code>\u00a0as well as the access control action\u00a0<code>urn:eccenca:ThesaurusUserInterface</code>\u00a0.</p> Property Default Required Conflicts with Valid values js.config.modules.thesaurus.startWith false no none boolean <p>Set this property to true to load this module as the default one after login.</p> <p>Info</p> <p>If more than one module has defined\u00a0<code>startWith: true</code>\u00a0the top most module in the navigation bar will be set as default.</p>"},{"location":"deploy-and-configure/configuration/datamanager/thesaurus-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules:\nthesaurus:\nenable: true\nstartWith: false\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/title-helper/","title":"Title helper","text":"<p>Configuration property:\u00a0<code>titleHelper</code>\u00a0| Scope: app-wide and per workspace</p> <p>DataManager provides the option to define which labels of properties are displayed.</p> <ul> <li>js.config.titleHelper<ul> <li>properties</li> <li>languages</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.titleHelper.properties see below yes, if\u00a0<code>titleHelper.languages</code>\u00a0is set none list of strings <p>Use this property to define an array of properties used for fetching titles. The default value is:</p> <pre><code>[\n'http://www.w3.org/2004/02/skos/core#prefLabel',\n'http://xmlns.com/foaf/0.1/name',\n'http://purl.org/dc/elements/1.1/title',\n'http://purl.org/dc/terms/title',\n'http://www.w3.org/2000/01/rdf-schema#label'\n]\n</code></pre> Property Default Required Conflicts with Valid values js.config.titleHelper.languages [\u2018en\u2019] no none list of strings <p>Use this property to define an array of languages used for fetching titles.</p>"},{"location":"deploy-and-configure/configuration/datamanager/title-helper/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.titleHelper:\nproperties:\n- \"http://xmlns.com/foaf/0.1/name\"\n- \"http://www.w3.org/2000/01/rdf-schema#label\"\nlanguages:\n- en\n- de\n</code></pre> <p>Info</p> <p>The order how labels in different languages are displayed is determined by the rank of\u00a0<code>properties</code>\u00a0combined with all given\u00a0<code>languages</code>\u00a0, default language english (if not already set) and the path property without any language tag.</p> <p>In this example ordering is set as following:</p> <pre><code>- http://xmlns.com/foaf/0.1/name@en\n- http://xmlns.com/foaf/0.1/name@de\n- http://xmlns.com/foaf/0.1/name\n- http://www.w3.org/2000/01/rdf-schema#label@en\n- http://www.w3.org/2000/01/rdf-schema#label@de\n- http://www.w3.org/2000/01/rdf-schema#label.\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/user-permissions/","title":"User permissions","text":"<p>Configuration property:\u00a0<code>userPermissions</code></p> <p>DataManager provides the option to define user rights on the Workspace page.</p> <ul> <li>js.config.userPermissions<ul> <li>allowCreateWorkspace</li> <li>allowSelectWorkspace</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.userPermissions.allowCreateWorkspace true no none boolean <p>When this property is set to\u00a0<code>true</code>,\u00a0the user has the right to manually create new workspaces on the Workspace page.</p> Property Default Required Conflicts with Valid values js.config.userPermissions.allowSelectWorkspace true no none boolean <p>When this property is set to\u00a0<code>true</code>,\u00a0the user has the right to manually select an existing workspace. This property can only be used when the\u00a0<code>js.config.userPermissions.allowCreateWorkspace</code>\u00a0property is set\u00a0<code>true</code>.</p>"},{"location":"deploy-and-configure/configuration/datamanager/user-permissions/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.userPermissions:\nallowCreateWorkspace: true\nallowSelectWorkspace: true\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/vocabs-module/","title":"Vocabs module","text":"<p>Configuration property:\u00a0<code>modules.vocabulary</code>\u00a0| Scope: app-wide and per workspace</p> <p>The Vocabs module of DataManager is used to manage available vocabularies.</p> <ul> <li>js.config.modules.vocabulary<ul> <li>enable</li> <li>startWith</li> <li>graphUrl</li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.modules.vocabulary.enable false no none boolean <p>Set this property to true to enable the Vocabs module of DataManager.</p> <p>Info</p> <p>If this property is set to\u00a0<code>false</code>, all other settings of\u00a0<code>modules.vocabulary</code>\u00a0are skipped. To use the module you also need to have read access to the graph specified in\u00a0<code>js.config.modules.vocabulary.graphUrl</code> and the access control action\u00a0<code>urn:eccenca:VocabularyUserInterface</code>\u00a0.</p> Property Default Required Conflicts with Valid values js.config.modules.vocabulary.startWith false no none boolean <p>Set this property to true to load this module as the default one after login.</p> <p>Info</p> <p>If more than one module has defined\u00a0<code>startWith: true</code>\u00a0the top most module in the navigation bar will be set as default.</p> Property Default Required Conflicts with Valid values js.config.modules.vocabulary.graphUrl none yes none string (URI) <p>Use this property to define the target graph for read and write operations.</p>"},{"location":"deploy-and-configure/configuration/datamanager/vocabs-module/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.modules:\nvocabulary:\nenable: true\nstartWith: false\ngraphUrl: https://example.com/example/vocabs/\n</code></pre>"},{"location":"deploy-and-configure/configuration/datamanager/workspaces/","title":"Workspaces","text":"<p>Configuration property:\u00a0<code>workspaces</code>\u00a0| Scope: app-wide only</p> <p>DataManager provides the option to define pre-configured workspaces a user can select for login.</p> <ul> <li>js.config.workspaces<ul> <li>id<ul> <li>name</li> <li> <p>authorization</p> <ul> <li>type</li> <li>logoutRedirectUrl         -   backend</li> <li>type</li> <li>url</li> <li>endpointId         -   DIWorkspace</li> <li>enable</li> <li>url</li> </ul> </li> </ul> </li> </ul> </li> </ul> Property Default Required Conflicts with Valid values js.config.workspaces[id].name none yes none string <p>Use this property to define a descriptive name for the workspace that the user sees when selecting a workspace.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.type none yes none string <p>Use this property to define the type of authorization.</p> <p>Info</p> <p>Currently, possible values are anonymous and oauth2 . If oauth2 is selected, you need to configure more options as described in section\u00a0Authorization.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.logoutRedirectUrl none no none string (URL) <p>Use this property to define a page other than the DataManager page that a user is redirected to after logout.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.type none yes none string <p>Use this property to define the type of the data backend.</p> <p>Info</p> <p>Currently, the only possible value is\u00a0<code>dataplatform</code>.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.url none yes none string (URL) <p>Use this property to define the base URL of the data backend.</p> <p>Info</p> <p>For\u00a0<code>js.config.workspaces[id].backend.type: dataplatform</code>\u00a0the base URL must not contain \u201c<code>/</code>\u201d at the end.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].backend.endpointId none yes none string <p>Use this property to define the identifier of a specific SPARQL endpoint at the data backend.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].DIWorkspace.enable none yes none true / false <p>Use this property to enable/disable Data Integration menu item on navigation menu.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].DIWorkspace.url none yes none string <p>Use this property to define the url where Data Integration is accessible.</p>"},{"location":"deploy-and-configure/configuration/datamanager/workspaces/#configuration-example","title":"Configuration example","text":"<pre><code>js.config.workspaces:\n# definition for workspace 1\ndefault:\nname: 'Default Workspace'\nauthorization:\ntype: 'oauth2'\noauth2:\nclientId: 'eldsClient'\ngrantType: 'implicit'\nauthorizeUrl: 'https://&lt;dataplatform_uri&gt;/oauth/authorize'\nbackend:\ntype: 'dataplatform'\nurl: 'https://&lt;dataplatform_uri&gt;'\nendpointId: 'default'\nDIWorkspace:\nenable: true\nurl: /dataintegration/workspace-beta\n# definition for workspace 2\notherSpace:\nname: 'Another Workspace'\nauthorization:\ntype: 'anonymous'\nbackend:\ntype: 'dataplatform'\nurl: '&lt;dataplatform_uri&gt;'\nendpointId: 'default'\nDIWorkspace:\nenable: true\nurl: /dataintegration/workspace-beta\n</code></pre> <p>Most configuration options that are app-wide applied can be overruled by specific workspace configurations. To overrule an app-wide configuration include the specific configuration option in the workspace definition as shown in the example below:</p> <pre><code>js.config.workspaces:\n# definition for workspace 1\ndefault:\nname: 'Default Workspace'\nauthorization:\ntype: 'anonymous'\nbackend:\ntype: 'dataplatform'\nurl: 'https://&lt;dataplatform_uri&gt;'\nendpointId: 'default'\n# overwrites app-wide appPresentation configuration for this workspace refer to chapter App presentation\nappPresentation:\nwindowTitle: 'Example Name'\njs.config.appPresentation:\nfaviconUrl: https://example.com/example/favicon.png\nwindowTitle: DataManager\nlogoUrl: https://example.com/example/logo.png\nheaderName: Datamanager\n</code></pre> <p>In the example above, the properties\u00a0<code>js.config.workspaces and js.config.appPresentation</code>\u00a0are configured. The property\u00a0<code>js.config.appPresentation.windowTitle</code>\u00a0in\u00a0<code>js.config.workspaces</code>\u00a0changes the title of\u00a0DataManager\u00a0from \u2018DataManager\u2018 to \u2018Example Name\u2018 for this specific workspace. Users logged\u00a0in to this workspace see\u00a0Example Name\u00a0as window title, because the workspace configuration overrules the app-wide configuration.</p>"},{"location":"deploy-and-configure/configuration/datamanager/workspaces-authorization/","title":"Workspaces Authorization","text":"<p>DataManager provides several types of authorization.</p> <ul> <li>js.config.workspaces<ul> <li>id<ul> <li>authorization<ul> <li>type</li> <li>oauth2<ul> <li>grantType</li> <li>authorizeUrl</li> <li>clientId</li> <li>tokenUrl</li> <li>clientSecret</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>If you want to use OAuth2, you need to set\u00a0<code>authorization.type</code>\u00a0to\u00a0<code>oauth2</code>\u00a0and specify further configuration parameters within the\u00a0<code>oauth2</code>\u00a0parameter:</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.grantType none yes, if authorization.type is\u00a0<code>oauth2</code> none <code>implicit</code>\u00a0or\u00a0<code>authorization_code</code> <p>Use this property to define the OAuth2 workflow. Depending on the chosen value you need to configure different properties:</p> <ul> <li><code>implicit</code>\u00a0:\u00a0<code>authorizeURL</code>\u00a0and\u00a0<code>clientId</code></li> <li><code>authorization_code</code>\u00a0:\u00a0<code>authorizeUrl</code>\u00a0,\u00a0<code>clientId</code>\u00a0,\u00a0<code>tokenUrl</code>\u00a0, and as optional property\u00a0<code>clientSecret</code></li> </ul> <p>Info</p> <p>It is recommended to use the implicit workflow, as DataManager is a client application.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.authorizeUrl none yes, if\u00a0<code>authorization.type</code>\u00a0is\u00a0<code>oauth2</code> none string (URL) <p>Use this property to define the authorization endpoint URL of the OAuth2 authorization server.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.clientId none yes, if\u00a0<code>authorization.type</code>\u00a0is\u00a0<code>oauth2</code> none string <p>Use this property to define the client identifier issued by the OAuth2 authorization server.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.tokenUrl none yes, if<code>authorization.oauth2.grantType</code>\u00a0is \u2018authorization_code\u2018 none string (URL) <p>Use this property to define the authorization token endpoint URL of the OAuth2 authorization server.</p> <p>Info</p> <p>This property is only needed for\u00a0<code>js.config.workspaces[id].authorization.oauth2.grantType=authorization_code</code>.</p> Property Default Required Conflicts with Valid values js.config.workspaces[id].authorization.oauth2.clientSecret none yes, if\u00a0<code>authorization.oauth2.grantType</code>\u00a0is \u2018authorization_code\u2018 none string <p>Use this property to define a passphrase for OAuth2 client authorization. Usually, this property is not required. It must only be set if the authorization server expects a client secret.</p>"},{"location":"deploy-and-configure/configuration/datamanager/workspaces-authorization/#configuration-example","title":"Configuration example","text":"<p>The two examples below show how a backend configuration can look like, for example for an eccenca DataPlatform using different authorization options.</p> <p>DataPlatform configuration with anonymous access:</p> <pre><code>js.config.workspaces:\ndefault:\nname: 'Default Workspace (anonymous)'\nauthorization:\ntype: 'anonymous'\nbackend:\ntype: 'dataplatform'\nurl: 'https://&lt;dataplatform_uri&gt;'\nendpointId: 'default'\n</code></pre> <p>DataPlatform configuration with implicit OAuth2 workflow:</p> <pre><code>js.config.workspaces:\ndefault:\nname: 'Default endpoint (oauth-implicit)'\nauthorization:\ntype: 'oauth2'\noauth2:\nclientId: 'eldsClient'\ngrantType: 'implicit'\nauthorizeUrl: 'https://&lt;dataplatform_uri&gt;/oauth/authorize'\nbackend:\ntype: 'dataplatform'\nurl: 'https://&lt;dataplatform_uri&gt;'\nendpointId: 'default'\n</code></pre> <p>DataPlatform with authorization code OAuth2 workflow:</p> <pre><code>js.config.workspaces:\ndefault:\nname: 'Default endpoint (oauth-implicit)'\nauthorization:\ntype: 'oauth2'\noauth2:\nclientId: 'eldsClient'\ngrantType: 'authorization_code'\nauthorizeUrl: 'https://&lt;dataplatform_uri&gt;/oauth/authorize'\ntokenUrl: 'https://&lt;dataplatform_uri&gt;/oauth/token'\nclientSecret: 'secret'\nbackend:\ntype: 'dataplatform'\nurl: 'https://&lt;dataplatform_uri&gt;'\nendpointId: 'default'\n</code></pre>"},{"location":"deploy-and-configure/configuration/dataplatform/","title":"DataPlatform","text":"<p>This manual describes how to install and set up eccenca DataPlatform. It is intended for system administrators, who are responsible for installing, configuring, maintaining and supporting the deployment of DataPlatform. To use this manual, system administrators should have knowledge about Linux (Ubuntu) and the installation environment on which DataPlatform is deployed.</p> <p>The following subsections describe different configuration topics in detail. Every subsection is presented with a property key overview and a details section with additional explanations:</p> <ul> <li>DataPlatform general configuration</li> <li>OAuth specific configuration</li> <li>Triple Store specific configuration<ul> <li>Ontotext GraphDB</li> <li>HTTP</li> <li>In-Memory</li> <li>AWS Neptune</li> <li>Stardog</li> <li>Openlink Virtuoso</li> </ul> </li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/dataplatform/application-full/","title":"General","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#license","title":"License","text":"<p>By default, DataPlatform is subject to the eccenca free Personal, Evaluation and Development License Agreement (PEDAL), a license intended for non-commercial usage. When your delivery includes a dedicated license file, you have to configure DataPlatform to enable your license.  To change the default configuration, you have several options. If the properties under license are not provided the default license included (PEDAL) is used.</p> <p>In case a dedicated license file is used, different configuration options can overwrite each other. The license is read in the following sequence:</p> <ol> <li>license.key property</li> <li>license.file property</li> <li>license.asc file in the same folder, where the application is started from (in Standalone Mode)</li> <li>Fallback to eccenca free Personal, Evaluation and Development License Agreement (PEDAL)</li> </ol> <p>Property: license.key</p> <p>Use this property to specify the license key as a YAML multiline string value of the license.key property.</p> <pre><code>key: |\n-----BEGIN PGP MESSAGE-----\n...\n...\n-----END PGP MESSAGE-----\n</code></pre> Category Value Default none Required false Valid values PGP Key (Message) Conflicts with license.file Environment LICENSE_KEY <p>Property: license.file</p> <p>Use this property to specify the location of the license file</p> Category Value Default none Required false Valid values location of the license file Conflicts with license.key Environment LICENSE_FILE"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#general-platform-settings-for-dataplatform","title":"General platform settings for DataPlatform","text":"<p>This section provides general configuration settings.</p>"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#caching","title":"Caching","text":"<p>DataPlatform provides caching support which is enabled by default with an in-memory Caffeine cache.</p> <p>Property: spring.cache.type</p> <p>Use this property to define the type of cache to use. The default type (INFINISPAN) provides a cache based on infinispan which can be further configured under the custom properties \u201cspring.cache.infinispan\u201d</p> <p>To disable caching, set the type to NONE (not recommended).</p> Category Value Default INFINISPAN Required true Valid values INFINISPAN, NONE Environment SPRING_CACHE_TYPE <p>Property: spring.cache.infinispan.mode</p> Category Value Default LOCAL Required false Valid values string Environment SPRING_CACHE_INFINISPAN_MODE <p>Multipart upload limits config You may need to set the following parameter values to 2048MB for implementations that cannot handle large requests</p> <p>Property: spring.servlet.multipart.max-file-size</p> <p>Use this property to define the maximum size of an uploaded file in number of bytes. Values can use the suffixed \u201cMB\u201d or \u201cKB\u201d (e.g. \u20181024MB\u2019).</p> <p>Note: If DataPlatform is deployed in a Servlet container, make sure to also configure support for large file sizes.</p> Category Value Default 4096MB Required false Valid values string Environment SPRING_SERVLET_MULTIPART_MAX_FILE_SIZE <p>Property: spring.servlet.multipart.max-request-size</p> <p>Use this property to define the maximum size of HTTP request in number of bytes. Values can use the suffixed \u201cMB\u201d or \u201cKB\u201d (e.g. \u20181024MB\u2019).</p> Category Value Default 4096MB Required false Valid values string Environment SPRING_SERVLET_MULTIPART_MAX_REQUEST_SIZE <p>Spring Cloud configuration</p> <p>Property: spring.cloud.config.enabled</p> Category Value Default false Required false Valid values string Environment SPRING_CLOUD_CONFIG_ENABLED <p>Spring Sleuth Zipkin exporter configuration (tracing)</p> <p>Property: spring.zipkin.base-url</p> Category Value Default http://localhost:9411/ Required false Valid values string Environment SPRING_ZIPKIN_BASE_URL <p>Property: spring.zipkin.enabled</p> <p>Whether the export to zipkin is enabled</p> Category Value Default false Required false Valid values string Environment SPRING_ZIPKIN_ENABLED <p>The service name for DataPlatform under which the traces are stored</p> <p>Property: spring.zipkin.service.name</p> Category Value Default DP Required false Valid values string Environment SPRING_ZIPKIN_SERVICE_NAME <p>Spring Cloud Sleuth distributed tracing. This generated IDs for queries/updates and all operations in DataPlatform. Write to tracing exporter s. spring.zipkin.</p> <p>Property: spring.sleuth.enabled</p> <p>Whether tracing is enabled. If not then IDs for i.e. queries are generated via UUID mechanism. Backend store \u201cneptune\u201d is not compatible with tracing enabled.</p> Category Value Default true Required false Valid values string Environment SPRING_SLEUTH_ENABLED"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#openapi-specification-and-swagger-ui","title":"OpenAPI Specification and Swagger UI","text":"<p>You can activate endpoints to expose an OpenAPI compliant specification of the available DataPlatform APIs. Developers can make use of this information to understand the API and to bootstrap client integration code.</p> <p>The servers URLs can be customized by setting the environment variable OPENAPI_SERVER_URLS on the machine or in the docker container that runs DataManager:</p> <pre><code>export OPENAPI_SERVER_URLS=\"https://my-custom.domain.com:443/dataplatform\"\n</code></pre> <p>Configuration example:</p> <pre><code>springdoc:\nswagger-ui:\nenabled: true\napi-docs:\nenabled: true\n</code></pre> <p>Property: springdoc.api-docs.enabled</p> <p>Use this property to enable and expose endpoint that provide the OpenAPI compliant specification of the DataPlatform APIs. The following endpoints will become available when this option is set to true:</p> <ul> <li>/v3/api-docs</li> <li>/v3/api-docs.yaml</li> <li>/v3/api-docs/swagger-config</li> </ul> Category Value Default false Required false Valid values boolean Environment SPRINGDOC_API_DOCS_ENABLED <p>Property: springdoc.swagger-ui.enabled</p> <p>Use this property to enable and expose a Swagger UI browser interface that can be used to explore and interact with the APIs. The following endpoints will become available when this option is set to true:</p> <ul> <li>/swagger-ui.html</li> </ul> Category Value Default false Required false Valid values boolean Environment SPRINGDOC_SWAGGER_UI_ENABLED"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#cross-origin-resource-sharing-cors","title":"Cross-origin resource sharing (CORS)","text":"<p>DataPlatform supports Cross-origin resource sharing (CORS).</p> <p>Configuration example:</p> <pre><code>http:\ncors:\nallowedOrigins:\n- http://example.org\n- https://example.com\n</code></pre> <p>Property: http.cors.allowedOrigins</p> <p>Use this property to define the list of allowed origins. The values must be either specific origins, e.g. http://example.org, or * for all origins.</p> Category Value Default [*] Required false Valid values list of strings Environment HTTP_CORS_ALLOWEDORIGINS <p>Property: http.cors.allowedMethods</p> <p>Use this property to define the list of allowed HTTP methods. The special value * allows all methods.</p> Category Value Default [OPTIONS, HEAD, GET, POST, PUT, DELETE, PATCH] Required false Valid values list of strings (of OPTIONS, HEAD, GET, POST,  PUT, DELETE, PATCH) Environment HTTP_CORS_ALLOWEDMETHODS <p>Property: http.cors.allowedHeaders</p> <p>Use this property to define the list of allowed HTTP headers. The special value * may be used to allow all headers.</p> Category Value Default [Authorization, X-Requested-With, Content-Type, Content-Length, ETag] Required false Valid values list of strings Environment HTTP_CORS_ALLOWEDHEADERS <p>Property: http.cors.exposedHeaders</p> <p>Use this property to define the list of headers that an actual response might have and can be exposed.</p> Category Value Default [WWW-Authenticate, Link, ETag] Required false Valid values list of strings Environment HTTP_CORS_EXPOSEDHEADERS <p>Property: http.cors.allowCredentials</p> <p>Use this property to define whether the browser should send credentials, such as cookies along with cross domain requests.</p> Category Value Default false Required false Valid values boolean Environment HTTP_CORS_ALLOWCREDENTIALS <p>Property: http.cors.maxAge</p> <p>Use this property to define how long in seconds the response from a pre-flight request can be cached by clients.</p> Category Value Default 3600 Required false Valid values non-negative integer Environment HTTP_CORS_MAXAGE"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#http-client-settings","title":"HTTP client settings","text":"<p>Java 11 HTTP client settings for HTTP access to the backend store.</p> <p>Property: httpclient.connectionPoolSize</p> <p>The maximum number of connections to keep in the HTTP/1.1 keep alive cache. A value of 0 means that the cache is unbounded</p> Category Value Default 200 Required false Valid values string Environment HTTPCLIENT_CONNECTIONPOOLSIZE <p>Property: httpclient.keepalive.timeout</p> <p>The number of seconds to keep idle HTTP/1.1 connections alive in the keep alive cache</p> Category Value Default 1200 Required false Valid values string Environment HTTPCLIENT_KEEPALIVE_TIMEOUT"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#authorization","title":"Authorization","text":"<p>DataPlatform supports authorization of RDF named graphs and actions. Authorization for clients and/or users is specified by the access conditions model which is described in section Access conditions. You can configure root access for a specific group of users who are given unrestricted access regardless of the defined access conditions. Refer to section Root Access for more information.</p> <p>Property: authorization.rootAccess</p> <p>Use this property to enable or disable root access. DataPlatform allows root access for a specific administrator group (see property authorization.abox.adminGroup). You can toggle root access using the property authorization.rootAccess. Regardless of the access conditions declared in the access conditions model (see Access conditions), all members of the administrator group are permitted to read and write all graphs of all endpoints and are allowed to perform all actions.</p> <p>For example, the following configuration grants root access to any user in the group admins:</p> <pre><code>authorization:\nrootAccess: true\nabox:\nadminGroup: admins\n</code></pre> Category Value Default true Required false Valid values boolean Environment AUTHORIZATION_ROOTACCESS <p>Use the following configuration options to specify options for collecting user information</p> <p>Property: authorization.userInfoGraph.active</p> <p>Use this property to enable/disable collection of user information of logged-in users</p> Category Value Default true Required false Valid values string Environment AUTHORIZATION_USERINFOGRAPH_ACTIVE <p>Property: authorization.userInfoGraph.ignored-account-names</p> <p>Logins of the following account names are not collected</p> Category Value Default [service-account-cmem-service-account] Required false Valid values string Environment AUTHORIZATION_USERINFOGRAPH_IGNORED_ACCOUNT_NAMES <p>Use the following configuration options to specify values used by DataPlatform when working with RDF data, such as default URIs and prefixes.</p> <p>Property: authorization.abox.adminGroup</p> <p>Use this property to configure the group that gets root access if enabled (see section Root access).</p> Category Value Default elds-admins Required false Valid values string Environment AUTHORIZATION_ABOX_ADMINGROUP <p>Property: authorization.abox.publicGroup</p> <p>Use this property to configure the URI of the public user group (see section Public access). Note: If you change this property, you also need to change existing URI descriptions and existing access conditions.</p> Category Value Default urn:elds-backend-public-group Required false Valid values string Environment AUTHORIZATION_ABOX_PUBLICGROUP <p>Property: authorization.abox.anonymousUser</p> <p>Use this property to configure the URI of the public user (see section Public access). Note: If you change this property, you also need to change existing URI descriptions and existing access conditions.</p> Category Value Default urn:elds-backend-anonymous-user Required false Valid values string Environment AUTHORIZATION_ABOX_ANONYMOUSUSER <p>Property: authorization.abox.prefix</p> <p>Use this property to set the namespace of URIs created by DataPlatform. Note: If you change this property, you also need to change the corresponding shape definitions for access conditions (more precisely, the URI template), as well as existing URI descriptions and existing access conditions.</p> Category Value Default https://ns.eccenca.com/ Required false Valid values string Environment AUTHORIZATION_ABOX_PREFIX"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#access-conditions","title":"Access conditions","text":"<p>IMPORTANT: The following properties are deprecated and have no function anymore!</p> <p>Property: authorization.abox.accessConditions.url</p> <p>DEPRECATED Use this property to set the URL of the access conditions model file. This can be either a remote (http://\u2026) or a local (file:\u2026) .rdf file. Refer to section Access conditions for more information on the access conditions model.</p> Category Value Default none Required false Valid values string Environment AUTHORIZATION_ABOX_ACCESSCONDITIONS_URL <p>Property: authorization.abox.accessConditions.graph</p> <p>DEPRECATED Use this property to set the graph containing the access conditions model. Note: If you change this property, you also need to change the corresponding shape definitions for access conditions (more precisely, the UI SPARQL queries).</p> Category Value Default urn:elds-backend-access-conditions-graph Required false Valid values string Conflicts with url Environment AUTHORIZATION_ABOX_ACCESSCONDITIONS_GRAPH"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#sparql-endpoints","title":"SPARQL endpoints","text":"<p>SPARQL endpoints declare how DataPlatform connects to a SPARQL-capable store or service. This includes stores that are capable of reading and writing RDF such as Virtuoso as well as read-only services like remote SPARQL HTTP endpoints (e.g. DBpedia).</p> <p>With the default configuration, DataPlatform uses an in-memory database. This means, that no persistent storage is available, unless a store supporting data persistence is configured.</p> <p>The following example showcases a setup in which for each Resource all rdfs:label, Literals with language es, then en and in the end those without a language are evaluated.  If nothing matches here, skos:prefLabel is examined in the same way</p> <pre><code>proxy:\nendpointIds:\n- my_stardog\nlabelProperties:\n- \"http://www.w3.org/2000/01/rdf-schema#label\"\n- \"http://www.w3.org/2004/02/skos/core#prefLabel\"\nlanguagePreferences:\n- \"es\"\n- \"en\"\n- \"\"\n</code></pre> <p>Property: proxy.defaultBaseIri</p> <p>Base IRI for this Corporate Memory instance. If not set falls back to environment variable DEPLOY_BASE_URL, further fallback to https://fallback.eccenca.com/</p> Category Value Default https://fallback.eccenca.com/ Required false Valid values URI Environment PROXY_DEFAULTBASEIRI <p>Property: proxy.labelProperties</p> <p>Use this property to specify which RDF properties should be used to provide label values when matching IRIs against a search term during rewriting SELECT-queries. Note: This configuration property affects modification of SELECT-queries for search triggered by the search-string query parameter. Results of SELECT-queries when the resolveLabels property is set to LABELS</p> Category Value Default [http://www.w3.org/2004/02/skos/core#prefLabel, http://www.w3.org/2000/01/rdf-schema#label, http://purl.org/dc/terms/title, http://www.w3.org/ns/shacl#name] Required false Valid values list of Properties Environment PROXY_LABELPROPERTIES <p>Property: proxy.descriptionProperties</p> <p>Use this property to specify which RDF properties should be used to provide description values when matching IRIs against a search term during rewriting SELECT-queries. Note: This configuration property affects modification of SELECT-queries for search triggered by the search-string query parameter. Results of SELECT-queries when the resolveLabels property is set to LABELS</p> Category Value Default [http://purl.org/dc/terms/description, http://www.w3.org/2000/01/rdf-schema#comment] Required false Valid values list of Properties Environment PROXY_DESCRIPTIONPROPERTIES <p>Property: proxy.languagePreferences</p> <p>Specifies base language preferences for this instance. </p> <p>Note: This configuration property affects results of SELECT-queries when the resolveLabels property is set to LABELS.</p> Category Value Default [de, en, ] Required false Valid values string Environment PROXY_LANGUAGEPREFERENCES <p>Property: proxy.languagePreferencesAnyLangFallback</p> <p>Allows the fallback to ignoring the languagePreferences, in case none of the configured match the data.</p> Category Value Default true Required false Valid values string Environment PROXY_LANGUAGEPREFERENCESANYLANGFALLBACK <p>Property: proxy.maxCBDDepth</p> <p>The Concise Boundary Description is used for viewing and editing resoures. By default up to a max of 5 Blank nodes are traversed for calculation. Increasing the max fetch will support deeper constructs, but will also add to loading time.</p> Category Value Default 5 Required false Valid values string Environment PROXY_MAXCBDDEPTH <p>Property: proxy.shapedMaxValueCount</p> <p>Maximum Values for shaped Resources When a resource is shaped by shacl forms, shapedMaxValueCount limits the number of values returned per <code>shacl:PropertyShape</code>. The default needs to be larger than the DataManager setting for for \u2018propertyLimit\u2019, which is up to 25. Changing this value allows custom endpoints to fetch more data. Increasing this value will increase response time</p> Category Value Default 26 Required false Valid values string Environment PROXY_SHAPEDMAXVALUECOUNT <p>Property: proxy.cacheExpiration</p> <p>Cache Expiration - Caches in DataPlatform have a default expiration time which can be set</p> Category Value Default PT30M Required false Valid values ISO 8601 duration format string i.e. PT30M, PT1D Environment PROXY_CACHEEXPIRATION <p>Property: proxy.cacheSelectiveInvalidation</p> <p>Indicates whether the DataPlatform caches should selectively invalidate based upon the result of the done operations (insofar as determinable) or not</p> Category Value Default true Required false Valid values boolean Environment PROXY_CACHESELECTIVEINVALIDATION <p>Property: proxy.queryMonitorMaxMemoryInMb</p> <p>Maximum memory entries in the query monitor can take in the heap space. The actual usage is an estimated usage only!</p> Category Value Default 30 Required false Valid values Value in MB Environment PROXY_QUERYMONITORMAXMEMORYINMB <p>Property: proxy.fetchValuesStrategy</p> <p>Value Fetch Strategy Determines how the Knowledge Graph is walked for values for specific resources. Used for resolving titles &amp; comments and loading shaped resources. - RESOURCE_IN_VALUES uses a SPARQL <code>VALUES (?resource ) { (:resource1)(:resource2)}</code> - FILTER_ONLY Uses SPARQL uses a SPARQL <code>FILTER (?resource in (:resource1, :resource2))</code></p> Category Value Default RESOURCE_IN_VALUES Required false Valid values RESOURCE_IN_VALUES, FILTER_ONLY Environment PROXY_FETCHVALUESSTRATEGY"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#syncing-graph-via-git-repositories","title":"Syncing graph via git repositories","text":"<p>DataPlatform can sync graphs between git repositories and the backend store. Changes of graphs in the backend are transferred to the git repository on each update / write of the graph. Changes of the graph in the git repository are synchronized to the store on a scheduled basis.</p> <p>Only HTTP git repositories with basic authentication can be used. A local public bare repository reachable from DataPlatform can be used in the DataPlatform configuration (for testing purposes).</p> <p>For details how to provide the correct git authentication refer to https://www.codeaffine.com/2014/12/09/jgit-authentication/.</p> <p>Note</p> <p>All properties need to be written as camel case (e.g. \u201cgitSync\u201d), hyphens as separators must not be used.</p> <p>An example git DataPlatform configuration using a gitlab git repository looks like:</p> <pre><code>gitSync:\nenabled: true\nremoteUrl: https://gitlab-ci-token:abcMyCiTokenxy5@gitlab.example.com/username/gitsync.git\nuser: username\npassword: abcMyCiTokenxy5\nbranch: master\nscheduledPullCron: \"0 */5 * * * *\"\n</code></pre> <p>Property: gitSync.enabled</p> <p>Activates / Deactivates git graph sync feature</p> Category Value Default false Required false Valid values boolean Environment GITSYNC_ENABLED <p>Property: gitSync.dataFolder</p> <p>The folder inside the repositories where Corporate Memory places the synchronized files</p> Category Value Default data Required false Valid values string Environment GITSYNC_DATAFOLDER <p>Property: gitSync.remoteUrl</p> <p>A remote git repository (http, local) - configured http repositories in graph configuration take precedence over this</p> Category Value Default none Required false Valid values HTTP or local repository which can be reached from DataPlatform Environment GITSYNC_REMOTEURL <p>Property: gitSync.branch</p> <p>The main branch on which the git sync takes place - the sync may create new branches on conflict. The branch must exist before using the feature.</p> Category Value Default main Required false Valid values An existing branch in the repository Environment GITSYNC_BRANCH <p>Property: gitSync.user</p> <p>The git username for simple user/password authentification - may be empty for local repository (s. remoteUrl) w/o authentification</p> Category Value Default none Required false Valid values Existing git repository user Environment GITSYNC_USER <p>Property: gitSync.password</p> <p>The git password for simple user/password authentification - may be empty for local repository (s. remoteUrl) w/o authentification</p> Category Value Default none Required false Valid values Existing git repository password Environment GITSYNC_PASSWORD <p>Property: gitSync.committerName</p> <p>The committer name which appears in the commit message on system commits</p> Category Value Default eccenca DataPlatform Required false Valid values string Environment GITSYNC_COMMITTERNAME <p>Property: gitSync.committerEmail</p> <p>The committer email which appears in the commit message  on system commits</p> Category Value Default info@eccenca.com Required false Valid values string Environment GITSYNC_COMMITTEREMAIL <p>Property: gitSync.scheduledPullCron</p> <p>Schedules Pull Frequency - Configured git repositories for sync are pulled regularly to check for external updates of synchronized graphs. This setting sets the frequency of the pull.</p> Category Value Default 0 /30 *  * * Required false Valid values Cron setting according to https://docs.spring.io/spring-framework/docs/current/reference/html/integration.html#scheduling-cron-expression Environment GITSYNC_SCHEDULEDPULLCRON"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#application-logging","title":"Application logging","text":"<p>By default, DataPlatform only logs to the console. You can change the log level or configure logging into a file.</p> <p>There are multiple levels of logging you can choose from that are explained in the table below.</p> <p>Available log levels are: TRACE, DEBUG, INFO, WARN, ERROR, FATAL and OFF. The default root log level is WARN. It is also possible to set the log level per package. Per default only console output is activated. To enable file output, specify a log file (auto-rotating, 10Mb file size). Possible log settings for specific modules: Query Logging: com.eccenca.elds.backend.sparql.query.logging: DEBUG</p> <p>The levels can also be configured on runtime via the loggers HTTP endpoint as described in section Application loggers of the Developer Manual.</p> <pre><code>logging:\nlevel:\nroot: WARN\ncom.eccenca.elds.backend: DEBUG\norg.springframework: INFO\nfile: /var/logs/dataplatform.log\n</code></pre> <p>Use these properties to specify where you want to store your logging file. Specifying a file leads to both, logging to standard output and the file. File output creates an auto-rotating file with 10 MB file size each.</p> <p>Property: logging.file.name</p> <p>Log file name (for instance, <code>myapp.log</code>). Names can be an exact location or relative to the current directory.</p> Category Value Default none Required false Valid values string (file name) - empty to disable file output Environment LOGGING_FILE_NAME <p>Property: logging.file.path</p> <p>Location of the log file. For instance, <code>/var/log</code>.</p> Category Value Default none Required false Valid values string (file path) - empty to disable file output Environment LOGGING_FILE_PATH <p>Property: logging.config</p> <p>Logging for DataPlatform can also be configured with Logback, which, for example, allows a more granular control on file rolling strategies. For further information on configuration options, refer to the Logback\u2019s Configuration manual section and the Spring Boot\u2019s Configure Logback for Logging manual section.</p> <p>Use this property to specify where the Logback configuration is located.</p> <pre><code>logging:\nconfiguration: ELDS_HOME/etc/dataplatform/logback.xml\n</code></pre> Category Value Default none Required false Valid values string (file path) Environment LOGGING_CONFIG <p>Property: logging.level.audit</p> Category Value Default INFO Required false Valid values string Environment LOGGING_LEVEL_AUDIT <p>Property: logging.level.com.eccenca.elds.backend</p> Category Value Default INFO Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND <p>Property: logging.level.org.springframework</p> Category Value Default WARN Required false Valid values string Environment LOGGING_LEVEL_ORG_SPRINGFRAMEWORK <p>Property: logging.level.com.eccenca.elds.backend.webapp.web.filter.SimpleCorsFilter</p> Category Value Default WARN Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_WEBAPP_WEB_FILTER_SIMPLECORSFILTER <p>Property: logging.level.com.eccenca.elds.backend.webapp.web.GlobalControllerExceptionHandler</p> Category Value Default TRACE Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_WEBAPP_WEB_GLOBALCONTROLLEREXCEPTIONHANDLER <p>Property: logging.level.com.eccenca.elds.backend.stardog.StardogTemplate</p> Category Value Default INFO Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_STARDOG_STARDOGTEMPLATE <p>Property: logging.level.com.eccenca.elds.backend.cache.logging</p> Category Value Default WARN Required false Valid values string Environment LOGGING_LEVEL_COM_ECCENCA_ELDS_BACKEND_CACHE_LOGGING"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#audit-trail-logging","title":"Audit trail logging","text":"<p>DataPlatform is able to log the access of each user to named graphs in form of an audit trail log under the logger name audit.</p> <pre><code>auditTrail:\nenabled: true\nauditedGraphs:\n- \"example.org/data\"\n- \"aksw.org\"\n</code></pre> <p>Property: audit-trail.enabled</p> <p>Use this property to enable logging of read and write access to every graph access. If auditTrail.auditedGraphs is specified, only those graphs are logged. Note: If audit trail logging is enabled, RDF upload over the Graph Store Protocol interface is limited to triple formats. Any attempt to upload a quad format results in an HTTP 415 error.</p> Category Value Default false Required false Valid values boolean Environment AUDIT_TRAIL_ENABLED <p>Property: audit-trail.auditedGraphs</p> <p>Use this property to specify graphs whose read and write access you want to be logged. Omit this value to log access to all graphs.</p> Category Value Default none Required false Valid values List of graph IRIs Environment AUDIT_TRAIL_AUDITEDGRAPHS <p>Limits the size of the query response</p> <p>Property: sparql.query.limit</p> Category Value Default 100000 Required false Valid values string Environment SPARQL_QUERY_LIMIT"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#embedded-tomcat","title":"Embedded Tomcat","text":"<p>The URL under which DataPlatform is accessible has the following form: PROTOCOL://HOST:PORT/CONTEXT_PATH where:</p> <pre><code>- PROTOCOL: http or https depending on SSL configuration (see section SSL support)\n- HOST: The hostname pointing to the server where DataPlatform is installed\n- PORT: The TCP port where the embedded server is available (see the property server.port)\n- CONTEXT_PATH: The context path under which DataPlatform is available (see the property server.servlet.contextPath)\n</code></pre> <pre><code>server:\nport: 9090\nservlet:\ncontextPath: /dataplatform\n</code></pre> <p>Property: server.port</p> <p>Use this property to set the TCP port where the embedded server is available.</p> Category Value Default 9090 Required false Valid values integer Environment SERVER_PORT <p>Property: server.servlet.contextPath</p> <p>Use this property to define the context path under which DataPlatform is available. If this property is provided, use a leading slash.</p> Category Value Default none Required false Valid values string Environment SERVER_SERVLET_CONTEXTPATH <p>Tomcat servlet settings</p> <p>Property: server.servlet.session.cookie.same-site</p> Category Value Default none Required false Valid values string Environment SERVER_SERVLET_SESSION_COOKIE_SAME_SITE"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#https-support-for-standalone-mode","title":"HTTPS support for standalone mode","text":"<p>If DataPlatform is executed in standalone mode (see Standalone), the embedded servlet container can be configured to support one-way (server certification) or two-way (server and client certification) SSL. A KeyStore is required for one-way SSL and both a KeyStore as well as a TrustStore are required for two-way SSL.</p> <p>Refer to the Oracle documentation to see how to create KeyStore and TrustStore files.</p> <p>Configuration example:</p> <pre><code>server:\nssl:\nkey-store: ./key-store.jks\nkey-store-password: jks-password\nclient-auth: NEED\n</code></pre> <p>Property: server.ssl.key-store</p> <p>Use this property to define the path to the KeyStore used for one-way or two-way SSL authentication.</p> <p>In case of two-way authentication, a TrustStore must also be configured. This configuration must be provided as Java system properties either directly in the execution command or as part of the JAVA_TOOL_OPTIONS environment variable, e.g.:</p> <pre><code>JAVA_TOOL_OPTIONS=-Djavax.net.ssl.trustStore=path_to_trust_store.jks -Djavax.net.ssl.trustStorePassword=trust_store_password (ADD TO EXISTING JAVA_TOOL_OPTIONS)\n</code></pre> Category Value Default none Required false Valid values string Environment SERVER_SSL_KEY_STORE <p>Property: server.ssl.key-store-password</p> <p>Use this property to set the password to unlock the KeyStore used for one-way or two-way SSL authentication.</p> Category Value Default none Required false Valid values string Environment SERVER_SSL_KEY_STORE_PASSWORD <p>Property: server.ssl.client-auth</p> <p>Use this property to define the client identification policy.</p> <p>If WANT is set, client identification is optional. If NEED is set, client identification is mandatory, so unauthenticated clients are refused.</p> Category Value Default none Required false Valid values none, NEED, WANT Environment SERVER_SSL_CLIENT_AUTH"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#https-support-for-proxy-deployment","title":"HTTPS support for proxy deployment","text":"<p>If DataPlatform is running behind a proxy server (e.g. Apache) then you must use all of the following properties to enforce HTTPS.</p> <p>Configuration recommendation:</p> <pre><code>  server:\ntomcat:\nremoteIpHeader: x-forwarded-for\nprotocolHeader: x-forwarded-proto\nsecurity:\nrequireSsl: true\n</code></pre> <p>Note: This configuration recommendation provides settings for headers most commonly used by proxies. Make sure to add all three properties in order to enforce HTTPS.</p> <p>Property: server.tomcat.remoteIpHeader</p> <p>Use this property to set the request header which is required to identify the originating IP address of the client connecting to DataPlatform through an HTTP proxy.</p> Category Value Default none Required false Valid values string Environment SERVER_TOMCAT_REMOTEIPHEADER <p>Property: server.tomcat.protocolHeader</p> <p>Use this property to set the request header which is required to identify the originating protocol of an HTTP request through an HTTP proxy.</p> Category Value Default none Required false Valid values string Environment SERVER_TOMCAT_PROTOCOLHEADER <p>Property: server.tomcat.max-swallow-size</p> Category Value Default -1 Required false Valid values string Environment SERVER_TOMCAT_MAX_SWALLOW_SIZE"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#scheduler-for-asynchronous-operations","title":"Scheduler for asynchronous operations","text":"<p>Schedulers Configuration (Thread Pools) for asynchronous operations like background file uploads</p> <p>Property: scheduler.bulkLoadPoolSize</p> <p>Bulk upload Pool Size - Limits how many (bulk/large) uploads via GSP / bulk load can be run in parallel in file upload.</p> Category Value Default 1 Required false Valid values integer Environment SCHEDULER_BULKLOADPOOLSIZE <p>Property: scheduler.analyticalPoolSize</p> <p>Limits how many analytical requests can be run in parallel. Analytical requests  can have longer runtimes than retrieval requests.</p> Category Value Default 10 Required false Valid values integer Environment SCHEDULER_ANALYTICALPOOLSIZE"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#asynchronous-file-uploads","title":"Asynchronous file uploads","text":"<p>Files can be asynchronously uploaded to the backend store in multiple steps which include an analysis of the uploaded file.  Please s. API documentation under /api/upload/ for further information.</p> <p>Property: files.maxStorageSingleFileSizeMb</p> <p>Maximum size of one stored file (as uploaded i.e. can also be compressed size) Value in Mb</p> Category Value Default 3000 Required false Valid values string Environment FILES_MAXSTORAGESINGLEFILESIZEMB <p>Property: files.minStorageTempSpaceLeftMb</p> <p>Minimum storage space left on temp device of DataPlatform for file uploads Value in Mb</p> Category Value Default 3000 Required false Valid values string Environment FILES_MINSTORAGETEMPSPACELEFTMB <p>Property: files.maintenanceExpirationDuration</p> <p>Cron setting for housekeeping / maintenance job Stored files and saved analysis will be deleted if older than maintenanceExpirationDuration</p> Category Value Default P1D Required false Valid values string Environment FILES_MAINTENANCEEXPIRATIONDURATION"},{"location":"deploy-and-configure/configuration/dataplatform/application-full/#store-configuration","title":"Store configuration","text":"<p>Store properties for connecting to a triple store backend. Please see specific sections in documentation for each backend.</p> <p>Property: store.type</p> <p>One of the supported types of backends DataPlatform can connect to</p> Category Value Default none Required true Valid values MEMORY, HTTP, GRAPHDB, STARDOG, VIRTUOSO, NEPTUNE Environment STORE_TYPE <p>Property: store.owlImportsResolution</p> <p>Use this property to enable OWL imports resolution</p> Category Value Default true Required false Valid values string Environment STORE_OWLIMPORTSRESOLUTION <p>Property: store.authorization</p> <p>Strategies to realize authorization for an RDF endpoint</p> Category Value Default none Required false Valid values NONE, REWRITE_FROM Environment STORE_AUTHORIZATION <p>Property: store.queryTimeoutGeneral</p> <p>Query timeout as duration which is active if no timeout in request has been set</p> Category Value Default PT1H Required false Valid values ISO 8601 duration format Environment STORE_QUERYTIMEOUTGENERAL"},{"location":"deploy-and-configure/configuration/dataplatform/application-graphdb-full/","title":"GraphDB","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-graphdb-full/#configuration-for-connecting-to-graphdb-backend","title":"Configuration for connecting to GraphDB backend","text":"<p>Configuration example:</p> <p>This example configures a connection with HTTPS to a remote graphdb store (https://remote:7200) using the workbench import directory which is shared with the GraphDB instance. The repository will be created on startup of CMEM.</p> <pre><code>store:\ntype: graphdb\nowlImportsResolution: true\nauthorization: REWRITE_FROM\ngraphdb:\nhost: \"remote\"\nport: 7200\nssl-enabled: true\nrepository: \"newRepository\"\nusername: \"admin\"\npassword: \"admin\"\nimportDirectory: \"/shared/mount\"\nuseDirectTransfer: false\ncreateRepositoryOnStartup: true\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cgraphdb\u201d</p> Category Value Default graphdb Required true Valid values GRAPHDB Environment STORE_TYPE"},{"location":"deploy-and-configure/configuration/dataplatform/application-graphdb-full/#specific-settings-for-graphdb","title":"Specific settings for GraphDB","text":"<p>Property: store.graphdb.host</p> <p>The host of the GraphDB database</p> Category Value Default none Required true Valid values string Environment STORE_GRAPHDB_HOST <p>Property: store.graphdb.port</p> <p>The port of the GraphDB database</p> Category Value Default 7200 Required false Valid values integer Environment STORE_GRAPHDB_PORT <p>Property: store.graphdb.ssl-enabled</p> <p>Whether SSL is enabled or not (http vs. https)</p> Category Value Default false Required false Valid values boolean Environment STORE_GRAPHDB_SSL_ENABLED <p>Property: store.graphdb.repository</p> <p>The name of the repository to connect to</p> Category Value Default cmem Required false Valid values string Environment STORE_GRAPHDB_REPOSITORY <p>Property: store.graphdb.username</p> <p>The user name to connect with</p> Category Value Default user Required true Valid values string Environment STORE_GRAPHDB_USERNAME <p>Property: store.graphdb.password</p> <p>The credentials of the given user</p> Category Value Default password Required true Valid values string Environment STORE_GRAPHDB_PASSWORD <p>Property: store.graphdb.importDirectory</p> <p>Import directory to be utilized in the \u201cworkbench import with shared folder\u201d approach. Not relevant when useDirectTransfer is true. Must be set when useDirectTransfer is false.</p> Category Value Default none Required false Valid values string Environment STORE_GRAPHDB_IMPORTDIRECTORY <p>Property: store.graphdb.useDirectTransfer</p> <p>Set to true to use the native Graph Store API endpoint. Set to false to use the GraphDB workbench import. The import directory must be set then.</p> Category Value Default true Required false Valid values boolean Environment STORE_GRAPHDB_USEDIRECTTRANSFER <p>Property: store.graphdb.create-repository-on-startup</p> <p>Whether to create the given repository on startup if it does not exist</p> Category Value Default true Required false Valid values boolean Environment STORE_GRAPHDB_CREATE_REPOSITORY_ON_STARTUP <p>Property: store.graphdb.gdbBaseIndex</p> <p>The iri of the lucene index to be used for searches. If the default index is used, Dataplatform syncs the index with the configured \u2018proxy.labelProperties\u2019</p> Category Value Default http://www.ontotext.com/connectors/lucene/instance#cmembaseindex Required false Valid values Valid URI of lucene index Environment STORE_GRAPHDB_GDBBASEINDEX <p>Property: store.graphdb.graphDbChangeTrackingActive</p> <p>Whether to make use of GraphDB change tracking during SPARQL updates (s. https://graphdb.ontotext.com/documentation/10.0/change-tracking.html). This setting is relevant in regards to selectively evicting DP caches depending on the outcome of the SPARQL update s. also proxy.cacheSelectiveInvalidation</p> Category Value Default true Required false Valid values boolean Environment STORE_GRAPHDB_GRAPHDBCHANGETRACKINGACTIVE <p>Property: store.graphdb.graphDbChangeTrackingMaxQuadMemory</p> <p>Maximum amount of quads of change tracking result which will be loaded in memory</p> Category Value Default 1000 Required false Valid values int Environment STORE_GRAPHDB_GRAPHDBCHANGETRACKINGMAXQUADMEMORY"},{"location":"deploy-and-configure/configuration/dataplatform/application-http-full/","title":"HTTP","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-http-full/#configuration-for-connecting-to-arbitrary-sparql-http-backend","title":"Configuration for connecting to arbitrary SPARQL HTTP backend","text":"<p>Use the following set of properties to connect to arbitrary HTTP SPARQL services.</p> <p>Configuration example:</p> <pre><code>store:\ntype: http\nauthorization: REWRITE_FROM\nhttp:\nquery-endpoint-url: \"http://localhost:7200/repositories/cmem\"\nupdate-endpoint-url: \"http://localhost:7200/repositories/cmem/statements\"\ngraph-store-endpoint-url: \"http://localhost:7200/repositories/cmem/rdf-graphs/service\"\nusername: \"user\"\npassword: \"password\"\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201chttp\u201d</p> Category Value Default http Required true Valid values HTTP Environment STORE_TYPE <p>Property: store.authorization</p> Category Value Default REWRITE_FROM Required false Valid values string Environment STORE_AUTHORIZATION <p>Specific settings for HTTP. At least query and update endpoints must be provided.</p> <p>Property: store.http.query-endpoint-url</p> <p>Use this property to configure the endpoint to which SPARQL 1.1 queries are sent.</p> Category Value Default http://localhost:7200/repositories/cmem Required true Valid values string Environment STORE_HTTP_QUERY_ENDPOINT_URL <p>Property: store.http.update-endpoint-url</p> <p>Use this property to configure the endpoint to which SPARQL 1.1 updates are sent.</p> Category Value Default http://localhost:7200/repositories/cmem/statements Required true Valid values string Environment STORE_HTTP_UPDATE_ENDPOINT_URL <p>Property: store.http.graph-store-endpoint-url</p> <p>Use this property to configure the endpoint to SPARQL 1.1 Graph Store Protocol requests are sent.</p> Category Value Default http://localhost:7200/repositories/cmem/rdf-graphs/service Required false Valid values string Environment STORE_HTTP_GRAPH_STORE_ENDPOINT_URL <p>Property: store.http.username</p> <p>Basic authentication is used if this parameter is provided.</p> Category Value Default user Required false Valid values string Environment STORE_HTTP_USERNAME <p>Property: store.http.password</p> <p>Basic authentication is used if this parameter is provided.</p> Category Value Default password Required false Valid values string Environment STORE_HTTP_PASSWORD <p>Property: store.http.graphListQuery</p> <p>Defines how the raw list of graphs is retrieved, and therefore which graphs are visible to the system. Graph must be bound to variable ?g !</p> Category Value Default SELECT distinct ?g {graph ?g {?s ?p ?o}} Required false Valid values Valid SPARQL query with bound variable \u201cg\u201d Environment STORE_HTTP_GRAPHLISTQUERY"},{"location":"deploy-and-configure/configuration/dataplatform/application-inmemory-full/","title":"In-Memory","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-inmemory-full/#configuration-for-connecting-to-internal-memory-backend","title":"Configuration for connecting to internal memory backend","text":"<p>You can configure a in-memory SPARQL backend. Based on Jena Models, in-memory backends do not provide persistent storage.  Hence, shutting down a DataPlatform configured with an in-memory backend deletes your data and therefore you should use it only for testing purposes.</p> <p>Configuration example:</p> <p>This example configures an in-memory store which initializes with the triples contained in the given file.</p> <pre><code>store:\ntype: memory\nauthorization: REWRITE_FROM\nmemory:\nfiles:\n- \"/data/data.trig\" </code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cmemory\u201d</p> Category Value Default memory Required true Valid values MEMORY Environment STORE_TYPE <p>Property: store.authorization</p> Category Value Default REWRITE_FROM Required false Valid values string Environment STORE_AUTHORIZATION <p>Specific settings for in-memory backend</p> <p>Property: store.memory.files</p> <p>list of files in file URI scheme</p> Category Value Default none Required false Valid values A list of files Environment STORE_MEMORY_FILES"},{"location":"deploy-and-configure/configuration/dataplatform/application-neptune-full/","title":"Neptune","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-neptune-full/#configuration-for-connecting-to-neptune-backend","title":"Configuration for connecting to Neptune backend","text":"<p>Configuration example:</p> <p>This example configures a connection to a neptune instance in the AWS region eu-central-1. Authentication is enabled  so it is assumed that CMEM runs on a EC2 VM with configured role for authentication to neptune. Files (uncompressed) greater than 100MB are uploaded via S3 based bulk loader. The S3 bucket is accessed in this case via an access point which is configured here. The EC2 role CMEM runs under has write access to the bucket. One of the role the neptune cluster runs under is configured in this setting and has read access to the bucket. On bulk load the loading runs parallel in the setting HIGH which causes higher cpu load but better performance.</p> <pre><code>store:\ntype: neptune\nauthorization: REWRITE_FROM\nneptune:\nhost: \"neptune123.eu-central-1.neptune.amazonaws.com\"\nport: 8182\naws:\nregion: \"eu-central-1\"\nauthEnabled: true\ns3:\nbucketNameOrAPAlias: \"nap1-nnipjzugs1ar45n11n316mzagiw6heuc1a-s3alias\"\niamRoleArn: \"arn:aws:iam::887770733838:role/NeptuneLoadFromS3\"\nbulkLoadThresholdInMb: 100\nbulkLoadParallelism: HIGH\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cneptune\u201d</p> Category Value Default neptune Required true Valid values NEPTUNE Environment STORE_TYPE <p>Configuration of a neptune instance connection.</p> <p>Property: store.neptune.host</p> <p>The name of one of the writer endpoints of the neptune instance.</p> Category Value Default none Required true Valid values string Environment STORE_NEPTUNE_HOST <p>Property: store.neptune.port</p> <p>The port of the neptune instance.</p> Category Value Default 8182 Required false Valid values integer Environment STORE_NEPTUNE_PORT <p>Settings for the connection to the Amazon Cloud</p> <p>Property: store.neptune.aws.region</p> <p>The region where the neptune instance is located i.e. \u201ceu-central-1\u201d s. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions</p> Category Value Default eu-central-1 Required true Valid values One of the AWS regions Environment STORE_NEPTUNE_AWS_REGION <p>Property: store.neptune.aws.authEnabled</p> <p>Whether the neptune instance is configured with enabled IAM authentication. In case of enabled authentication the credentials need to be accessible to the JVM of the dataplatform. Deployment on EC2 and assigning a role to the VM is sufficient. Other ways to achieve this are described in https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html</p> Category Value Default true Required true Valid values boolean Environment STORE_NEPTUNE_AWS_AUTHENABLED <p>Settings for S3 bucket connection and upload of large files to the neptune instance. The neptune store blocks all HTTP requests with size &gt;150MB. To upload larger files a graph file is temporarily stored in a S3 bucket and uploaded via Neptune Bulk Loader. The S3 bucket needs to be in the same region as the neptune cluster. For more information s. https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html. If no S3 upload is necessary then the limit of 150 MB on HTTPS uploads apply for neptune. The whole section can be left out of the configuration.</p> <p>Property: store.neptune.s3.bucketNameOrAPAlias</p> <p>The name of the bucket or access point -&gt; the role CMEM runs under needs write access to the bucket</p> Category Value Default none Required false Valid values string Environment STORE_NEPTUNE_S3_BUCKETNAMEORAPALIAS <p>Property: store.neptune.s3.iamRoleArn</p> <p>The name of the role the neptune loader accesses the bucket -&gt; the role needs read access to the bucket s. https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html</p> Category Value Default none Required false Valid values string Environment STORE_NEPTUNE_S3_IAMROLEARN <p>Property: store.neptune.s3.bulkLoadThresholdInMb</p> <p>The threshold on uncompressed graph data when bulk upload is applied with a maximum / default of 150 MB</p> Category Value Default 150 Required false Valid values string Environment STORE_NEPTUNE_S3_BULKLOADTHRESHOLDINMB <p>Property: store.neptune.s3.bulkLoadParallelism</p> <p>The degree of parallelism (CPU) for the neptune loader, possible values are LOW, MEDIUM, HIGH, OVERSUBSCRIBE, default of HIGH</p> Category Value Default HIGH Required false Valid values LOW, MEDIUM, HIGH, OVERSUBSCRIBE Environment STORE_NEPTUNE_S3_BULKLOADPARALLELISM"},{"location":"deploy-and-configure/configuration/dataplatform/application-oauth-full/","title":"OAuth","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-oauth-full/#authentication","title":"Authentication","text":"<p>Access to DataPlatform resources is restricted using OAuth 2.0.</p>"},{"location":"deploy-and-configure/configuration/dataplatform/application-oauth-full/#configuration-example","title":"Configuration example","text":"<pre><code>spring:\nsecurity:\noauth2:\nresourceserver:\nanonymous: true\njwt:\nissuerUri: http://keycloak/auth/realms/cmem\n</code></pre>"},{"location":"deploy-and-configure/configuration/dataplatform/application-oauth-full/#oauth-20-resource-server","title":"OAuth 2.0 Resource Server","text":"<p>In order to protect access to it\u2019s resources, DataPlatform acts as an OAuth 2.0 resource server accepting and responding to a protected resource request using a JSON Web Token (JWT).</p> <p>The OAuth 2.0 specification as well as the JSON Web Token specification don\u2019t define any mandatory claims to be contained in a JWT access token. However, if the property spring.security.oauth2.resourceserver.jwt.issuer-uri is set, the iss (issuer) claim is required to be contained in the JWT. It\u2019s value must be equal to the configured issuer URI. Additionally, in order to identify the requesting principal, either the username claim or the clientId claim must be contained in the JWT.</p> <p>Property: spring.security.oauth2.resourceserver.anonymous</p> <p>Use this property to allow anonymous access to protected resources.</p> Category Value Default false Required false Valid values boolean Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_ANONYMOUS <p>Property: spring.security.oauth2.resourceserver.jwt.issuerUri</p> <p>Use this property to specify the URI that an OpenID Connect Provider asserts as its Issuer Identifier if it supports OpenID Connect discovery. If this property is set, the iss (issuer) claim is required to be contained in the JWT. The value of the claim has to be the same value as the configured issuer URI.</p> <p>Note: If the authorization server is down when DataPlatform queries it (given appropriate timeouts), then startup will fail. Also, if the authorization server doesn\u2019t support the Provider Configuration endpoint, or if DataPlatform must be able to start up independently from the authorization server, use the property jwk-set-uri instead.</p> Category Value Default http://docker.localhost/auth/realms/cmem Required false Valid values URI to OpenID Connect Provider Conflicts with spring.security.oauth2.resourceserver.jwt.jwkSetUri Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_ISSUERURI <p>Property: spring.security.oauth2.resourceserver.jwt.jwkSetUri</p> <p>Use this property to specify the JSON Web Key URI to use to verify the JWT token.</p> Category Value Default none Required false Valid values URI to OpenID Connect Provider Conflicts with spring.security.oauth2.resourceserver.jwt.issuerUri Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWKSETURI <p>Use the following configuration options to specify the claims conveyed by a JWT used to access protected resources of DataPlatform. If nothing is configured, a default configuration is provided with the following configuration</p> <p>Property: spring.security.oauth2.resourceserver.jwt.claims.username</p> <p>Use this property to specify the claim providing the account name of the user accessing a protected resource.</p> Category Value Default preferred_username Required false Valid values string Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_CLAIMS_USERNAME <p>Property: spring.security.oauth2.resourceserver.jwt.claims.groups</p> <p>Use this property to specify the claim identifying the roles (authorities) of the user accessing a protected resource.</p> Category Value Default groups Required false Valid values string Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_CLAIMS_GROUPS <p>Property: spring.security.oauth2.resourceserver.jwt.claims.clientId</p> <p>Use this property to specify the claim providing the OAuth 2.0 client ID to which a token was issued.</p> Category Value Default clientId Required false Valid values string Environment SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_CLAIMS_CLIENTID"},{"location":"deploy-and-configure/configuration/dataplatform/application-oauth-full/#oauth-20-client-configuration","title":"OAuth 2.0 client configuration","text":"<p>In order to protect access to it\u2019s resources, DataPlatform acts as an OAuth 2.0 Client which provides authentication its own clients by means of a session cookie. For this type of authentication a JSON Web Token (JWT)  is not necessary. The registration which is configured is named \u201ckeycloak\u201d and provides a login page redirecting to a keycloak backend. For specific customizations please s. https://docs.spring.io/spring-security/reference/servlet/oauth2/client/index.html</p> <p>One authentication backend is configured named \u2018keycloak\u2019. The login page is accessible under \u2018{basepath}/oauth2/authorization/keycloak\u2019</p> <p>Property: spring.security.oauth2.client.registration.keycloak.client-id</p> Category Value Default dataintegration Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_CLIENT_ID <p>Property: spring.security.oauth2.client.registration.keycloak.authorization-grant-type</p> Category Value Default authorization_code Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_AUTHORIZATION_GRANT_TYPE <p>Property: spring.security.oauth2.client.registration.keycloak.client-authentication-method</p> Category Value Default basic Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_CLIENT_AUTHENTICATION_METHOD <p>Property: spring.security.oauth2.client.registration.keycloak.redirectUri</p> Category Value Default {baseUrl}/login/oauth2/code/{registrationId} Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_REDIRECTURI <p>Property: spring.security.oauth2.client.registration.keycloak.scope</p> Category Value Default [openid, profile, email] Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_SCOPE <p>Property: spring.security.oauth2.client.registration.keycloak.provider.keycloak.issuer-uri</p> Category Value Default http://docker.localhost/auth/realms/cmem Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_PROVIDER_KEYCLOAK_ISSUER_URI <p>Property: spring.security.oauth2.client.registration.keycloak.provider.keycloak.user-name-attribute</p> Category Value Default preferred_username Required false Valid values string Environment SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_KEYCLOAK_PROVIDER_KEYCLOAK_USER_NAME_ATTRIBUTE"},{"location":"deploy-and-configure/configuration/dataplatform/application-stardog-full/","title":"Stardog","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-stardog-full/#configuration-for-connecting-to-stardog-backend","title":"Configuration for connecting to Stardog backend","text":"<p>Configuration example:</p> <p>This example configures a connection with HTTPS to a remote stardog store (https://remote:5820). All SPARQL updates have a  timout of 5 minutes configured.</p> <pre><code>store:\ntype: stardog\nowlImportsResolution: true\nauthorization: REWRITE_FROM\nstardog:\nhost: \"remote\"\nport: 5820\nssl-enabled: true\nrepository: \"stardog\"\nusername: \"admin\"\npassword: \"admin\"\nupdateTimeoutInMilliseconds: 300000\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cstardog\u201d</p> Category Value Default stardog Required true Valid values STARDOG Environment STORE_TYPE <p>Specific settings for Stardog</p> <p>Property: store.stardog.host</p> <p>The host of the Stardog database</p> Category Value Default none Required false Valid values string Environment STORE_STARDOG_HOST <p>Property: store.stardog.port</p> <p>The port of the Stardog database</p> Category Value Default 5820 Required false Valid values integer Environment STORE_STARDOG_PORT <p>Property: store.stardog.ssl-enabled</p> <p>Whether SSL is enabled or not (http vs. https)</p> Category Value Default false Required false Valid values boolean Environment STORE_STARDOG_SSL_ENABLED <p>Property: store.stardog.repository</p> <p>The name of the repository to connect to</p> Category Value Default none Required false Valid values string Environment STORE_STARDOG_REPOSITORY <p>The user name to connect with</p> <p>Property: store.stardog.password</p> <p>The credentials of the given user</p> Category Value Default none Required false Valid values string Environment STORE_STARDOG_PASSWORD <p>Property: store.stardog.updateTimeoutInMilliseconds</p> <p>Use this property to set the upper bound for update operation execution time. If an update request consists of multiple update operations, the timeout applies to each update operation individually. To support this, the Stardog server must be properly configured.</p> Category Value Default 3600000 Required false Valid values string Environment STORE_STARDOG_UPDATETIMEOUTINMILLISECONDS"},{"location":"deploy-and-configure/configuration/dataplatform/application-virtuoso-full/","title":"Virtuoso","text":""},{"location":"deploy-and-configure/configuration/dataplatform/application-virtuoso-full/#configuration-for-connecting-to-virtuoso-backend","title":"Configuration for connecting to Virtuoso backend","text":"<p>Configuration example:</p> <p>This example configures a connection with HTTPS to a remote Virtuoso store (https://remote:8080). </p> <pre><code>store:\ntype: virtuoso\nauthorization: REWRITE_FROM\nvirtuoso:\nhost: \"remote\"\nssl-enabled: true\nport: 8080\nusername: \"admin\"\npassword: \"admin\"\ndatabasePort: 1111\n</code></pre> <p>Property: store.type</p> <p>The type of the store must be set to \u201cvirtuoso\u201d</p> Category Value Default virtuoso Required true Valid values VIRTUOSO Environment STORE_TYPE <p>Specific settings for Virtuoso</p> <p>Property: store.virtuoso.host</p> <p>The host of the Virtuoso server</p> Category Value Default none Required false Valid values string Environment STORE_VIRTUOSO_HOST <p>Property: store.virtuoso.port</p> <p>The HTTP port of the Virtuoso server</p> Category Value Default 8080 Required false Valid values integer Environment STORE_VIRTUOSO_PORT <p>Property: store.virtuoso.databasePort</p> <p>The database port of the Virtuoso server for direct access to the JDBC database</p> Category Value Default 1111 Required false Valid values integer Environment STORE_VIRTUOSO_DATABASEPORT <p>Property: store.virtuoso.ssl-enabled</p> <p>Whether SSL is enabled or not (http vs. https)</p> Category Value Default false Required false Valid values string Environment STORE_VIRTUOSO_SSL_ENABLED <p>Property: store.virtuoso.username</p> <p>The user name to connect with</p> Category Value Default none Required false Valid values string Environment STORE_VIRTUOSO_USERNAME <p>Property: store.virtuoso.password</p> <p>The credentials of the given user</p> Category Value Default none Required false Valid values string Environment STORE_VIRTUOSO_PASSWORD"},{"location":"deploy-and-configure/configuration/docker-orchestration/","title":"Docker Orchestration","text":"","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#introduction","title":"Introduction","text":"<p>This page describes the configuration for the docker-compose based orchestration.</p> <p>The Docker Orchestration (hereafter simply orchestration) is configured via environment files. In this document we provide an overview on how the environment files are loaded, how to modify the configuration inside those files and available configuration parameters.</p>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#environment-files-loading-sequence","title":"Environment Files: Loading Sequence","text":"<p>The environment files are supplied in the\u00a0CONFIGFILE environment variable to the make targets inside the orchestration. For example, in\u00a0Scenario: Single Node Cloud Installation\u00a0we have created a <code>prod.env</code> environment file and created the Corporate Memory instance using <code>prod.env</code> configuration:</p> <pre><code>$ CONFIGFILE=environments/prod.env make clean-pull-start-bootstrap\n</code></pre> <p>When you run <code>make clean-pull-start-bootstrap</code> target, the Makefile will evaluate and export the environment variables from the <code>environments/default.env</code>, your <code>${CONFIGFILE}</code> or <code>environments/config.env</code> and <code>environments/scripted-env.mk</code>:</p> <pre><code>$ cat Makefile\n...\ninclude ${CONFIGFILE_BASE_DIRECTORY}/default.env\ninclude ${CONFIGFILE}\ninclude ${CONFIGFILE_BASE_DIRECTORY}/scripted-env.mk\nexport\n...\n</code></pre> <p>The files are loaded exactly in this order and the later env files will overwrite the environment variables from the former env files. In other words, your <code>${CONFIGFILE}</code> will have precedence over <code>environments/default.env</code>. While <code>environments/scripted-env.mk</code> has precedence over both\u00a0<code>environments/default.env</code> and your <code>${CONFIGFILE}</code>.</p>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#configuring-docker-orchestration","title":"Configuring Docker Orchestration","text":"<p>To configure the orchestration according to your requirements, you need simply to create a file inside <code>environments/</code> directory and set the necessary variables there. For example, to replicate the minimum configuration from <code>config.env</code>, you can do the following:</p> <pre><code>$ echo \"create empty environments/prod.env file\"\n$ touch environments/prod.env\n$ echo \"inject necessary variables into the prod.env\"\n$ echo \"CMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" &gt;&gt; environments/prod.env\n$ echo \"STARDOG_PASSWORD=admin\" &gt;&gt; environments/prod.env\n$ echo \"TRUSTSTOREPASS=Aimeik5Ocho5riuC\" &gt;&gt; environments/prod.env\n</code></pre> <p>This configuration will be sufficient to run the orchestration locally as described in\u00a0Scenario: Local Installation:</p> <pre><code>$ CONFIGFILE=environments/prod.env make clean-pull-start-bootstrap\n</code></pre>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#available-configuration-variables","title":"Available Configuration Variables","text":"<p>All available configuration environment variables are listed in <code>environments/default.env</code> file. In this section we describe the default value and purpose of each of those variables.</p>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#docker-settings","title":"Docker Settings","text":"Variable Default Value Description Docker DOCKER_CMD_ADD (optional) Additional command line arguments to be supplied to docker-compose such as\u00a0\u2013tlscacert,\u00a0\u2013tlscert,\u00a0\u2013tlskey or\u00a0\u2013tlsverify ECC_HOST (internal)","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#deployment-settings","title":"Deployment Settings","text":"Variable Default Value Description DEPLOYPROTOCOL http Deploy protocol: http or https DEPLOYHOST docker.localhost Deploy host such as docker.localhost or corporate-memory.example.com PORT 80 Port for apache2 to listen on, for SSL configuration see section below. DEST $(dir $(abspath Makefile)) Directory where the orchestration is located (by default resolves to the directory where this Makefile is located) APACHE_BASE_FILE docker-compose.apache2-exposed.yml docker-compose extension file for apache2, see SSL configuration section below for an example APACHE_CONFIG default.conf Apache2 virtual host configuration SSLCONF ssl.default.conf Apache2 virtual host configuration for SSL setup HTTP_PORT 80 APACHE_HTTP_PORT is used as a standard port 80 in SSL setup LETSENCRYPT_MAIL administration@eccenca.com email to be used when requesting letsencrypt certificates DATAINTEGRATION_BASE_FILE docker-compose.dataintegration-base.yml docker-compose extension file for DataIntegration,\u00a0see SSL configuration section below for an example TRUSTSTOREPASS (empty) Truststore password, see self-signed certificates configuration section below for an example","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#project-settings","title":"Project Settings","text":"Variable Default Value Description BOOTSTRAP false \u201cfalse\u201d or \u201ctrue\u201d, indicates whether to load the Corporate Memory bootstrap data PROJECT_NAME_SUFFIX (empty) (optional) will append to the docker-compose project environment variable\u00a0COMPOSE_PROJECT_NAME","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#datamanager-settings","title":"DataManager Settings","text":"Variable Default Value Description DATAMANAGER_CONFIG_WORKSPACES_DEFAULT_NAME CMEM Orchestration Name of the default DataManager workspace DATAMANAGER_CONFIG_APPPRESENTATION_HEADERNAME eccenca Corporate Memory DataManager header name DATAMANAGER_CONFIG_APPPRESENTATION_WINDOWTITLE eccenca Corporate Memory DataManager windows title","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#dataplatform-settings","title":"DataPlatform Settings","text":"Variable Default Value Description AUTHORIZATION_ABOX_PREFIX http://eccenca.com/ ABox prefix defines a prefix for access control lists, changing this can break authorization in the Corporate Memory instance AUTHORIZATION_ABOX_ADMINGROUP elds-admins Default admin group for the Corporate Memory users DATAPLATFORM_CONTEXTPATH /dataplatform Context path for the dataplatform, meaning that dataplatform will run under <code>http://dataplatform.host/dataplatform</code> DATAPLATFORM_JAVA_TOOL_OPTIONS -Xms512m -Xmx2g Java options, modify to increase memory allocation","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#dataintegration-settings","title":"DataIntegration Settings","text":"Variable Default Value Description DATAINTEGRATION_EXECUTOR LocalExecutionManager DataIntegration\u00a0execution.manager.plugin parameter, see\u00a0DataIntegration\u00a0manual for more details INTERNAL_BASE_URL ${DEPLOYPROTOCOL}://${DEPLOYHOST} Used for\u00a0DATAPLATFORM_URL and\u00a0OAUTH_TOKEN_URL\u00a0variables DATAPLATFORM_URL ${INTERNAL_BASE_URL}${DATAPLATFORM_CONTEXTPATH} DataIntegration eccencaDataPlatform.url\u00a0parameter,\u00a0see\u00a0DataIntegration\u00a0manual for more details OAUTH_AUTHORIZATION_URL ${DEPLOY_BASE_URL}/auth/realms/cmem/protocol/openid-connect/auth DataIntegration\u00a0oauth.authorizationUrl parameter,\u00a0see\u00a0DataIntegration\u00a0manual for more details OAUTH_TOKEN_URL ${INTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/token DataIntegration oauth.tokenUrl\u00a0parameter,\u00a0see\u00a0DataIntegration\u00a0manual for more details DATAINTEGRATION_PRODUCTION_CONFIG_FILE /opt/cmem/eccenca-DataIntegration/dist/etc/dataintegration/conf/production.conf Path to DataIntegration production.conf,\u00a0for injecting production related parameters, like encryption keys DATAINTEGRATION_JAVA_TOOL_OPTIONS -Xmx4g Java options, modify to increase memory allocation","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#keycloak-settings","title":"Keycloak Settings","text":"Variable Default Value Description PROXY_ADDRESS_FORWARDING false Keycloak proxy forwarding, necessary for SSL configuration KEYCLOAK_AUTH_URL_INTERNAL <code>http://keycloak:8080/auth/realms/cmem/protocol/openid-connect/token</code> (internal) used in scripts/utils.sh to restore DataIntegration projects CMEM_SERVICE_ACCOUNT_CLIENT_ID (internal) used in scripts/utils.sh to restore DataIntegration projects","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#stardog-settings","title":"Stardog Settings","text":"Variable Default Value Description STARDOG_SEARCHINDEX_ENABLE true Enable or disable stardog search index STARDOG_SERVER_JAVA_ARGS -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g Java options, modify to increase memory allocation","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#component-versions","title":"Component Versions","text":"Variable Default Value Description DI_VERSION develop DataIntegration docker image version to be used DP_VERSION develop DataPlatform docker image version to be used DM_VERSION develop DataManager docker image version to be used APACHE2_VERSION v2.6.0 Apache2 docker image version to be used KEYCLOAK_VERSION v6.0.1-2 Keycloak docker image version to be used POSTGRES_VERSION 11.5-alpine Postgresql docker image version to be used STARDOG_VERSION v7.2.0-1 Stardog docker image version to be used","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#ssl-configuration-with-letsencrypt-example","title":"SSL Configuration with Letsencrypt Example","text":"<p>A complete example on how to deploy the Corporate Memory instance on Hetzner with Letsencrypt certificates is described in\u00a0Scenario: Single Node Cloud Installation</p> <pre><code>#!/bin/bash\nCMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nSTARDOG_PASSWORD=admin\n# change DEPLOYHOST to your own value! the one you have configured in your DNS\nDEPLOYHOST=corporate-memory.eccenca.dev\nPROXY_ADDRESS_FORWARDING=true\nDATAINTEGRATION_JAVA_TOOL_OPTIONS=-Xmx4g\nDATAPLATFORM_JAVA_TOOL_OPTIONS=-Xms2g -Xmx4g\nSTARDOG_SERVER_JAVA_ARGS=-Xms2g -Xmx2g -XX:MaxDirectMemorySize=4g\n\n# letsencrypt:\nSSLCONF=ssl.letsencrypt.conf\n# change MAIL to your own value! use a common system administration mailbox here\nLETSENCRYPT_MAIL=administration@eccenca.com\n\nDI_VERSION=v20.03\nDP_VERSION=v20.03\nDM_VERSION=v20.03\nAPACHE2_VERSION=v2.6.0\nKEYCLOAK_VERSION=v6.0.1-2\nPOSTGRES_VERSION=11.5-alpine\nSTARDOG_VERSION=v7.2.0-1\n\n#################################\n# Do NOT CHANGE these settings. #\n# ###############################\n# NOTE:\n#  - these settings differ from http setup but should not be altered\n#\nDEPLOYPROTOCOL=https\nPORT=443\nAPACHE_BASE_FILE=docker-compose.apache2-ssl.yml\nAPACHE_CONFIG=default.ssl.conf\nPROXY_ADDRESS_FORWARDING=true\n</code></pre>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/docker-orchestration/#ssl-configuration-with-self-signed-certificates-example","title":"SSL Configuration with Self-Signed Certificates Example","text":"<pre><code>#!/bin/bash\nCMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nSTARDOG_PASSWORD=admin\nTRUSTSTOREPASS=Aimeik5Ocho5riuC\n\n# Set this to your deployhost\nDEPLOYHOST=corporate.memory\nDATAINTEGRATION_BASE_FILE=docker-compose.dataintegration-ssl.yml\n\nDI_VERSION=v20.03\nDP_VERSION=v20.03\nDM_VERSION=v20.03\nAPACHE2_VERSION=v2.6.0\nKEYCLOAK_VERSION=v6.0.1-2\nPOSTGRES_VERSION=11.5-alpine\nSTARDOG_VERSION=v7.2.0-1\n\n#################################\n# Do NOT CHANGE these settings. #\n# ###############################\n# NOTE:\n#  - these settings differ from http setup but should not be altered\n#\nDEPLOYPROTOCOL=https\nPORT=443\nAPACHE_BASE_FILE=docker-compose.apache2-ssl.yml\nAPACHE_CONFIG=default.ssl.conf\nPROXY_ADDRESS_FORWARDING=true\n</code></pre>","tags":["Configuration","Docker"]},{"location":"deploy-and-configure/configuration/keycloak/","title":"Keycloak","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#introduction","title":"Introduction","text":"<p>This page documents important steps in order to configure Keycloak as an authentication backend for Corporate Memory. The screenshots displayed in this documentation were taken from Keycloak v20 using the <code>keycloak.v2</code> theme.</p> <p>Info</p> <p>You do not need these instruction in case you followed the documentation on\u00a0Scenario: Local Installation\u00a0or\u00a0Scenario: Single Node Cloud Installation\u00a0(in this case, everything was done automatically). However, in case you need to integrate Corporate Memory with an existing Keycloak, this page may help you. Please also have the\u00a0Keycloak - Server Administration Guide\u00a0ready\u00a0</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#realm-configuration","title":"Realm configuration","text":"<p>Warning</p> <p>A realm can be im-/exported. However, exported realms will not contain user credentials. So be aware not losing data.</p> <p>To create a realm, use the drop down menu for choosing a realm on the left side.</p> <ul> <li>Create a realm <code>cmem</code><ul> <li>Select Realm settings</li> <li>General tab:</li> <li>Change HTML Display name to\u00a0<code>&lt;span class=\"ecc-logo\"&gt;&lt;/span&gt;Corporate Memory</code></li> <li>Themes tab</li> <li>Switch realm\u2019s login theme to\u00a0<code>eccenca</code></li> <li>Switch realm\u2019s account theme to\u00a0<code>eccenca</code></li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#client-configuration","title":"Client configuration","text":"<p>There are two (three) different kinds of clients used by Corporate Memory:</p> <ul> <li>One client is used by DM/DP/DI to authenticate a user for using the UI (usually named <code>cmem</code>).</li> <li>The other client is for using the command line client as a technical user (usually named <code>cmem-service-account</code>).   Depending on the environment, there might be an other use case when running background schedules, then a third client, also as technical user, might be useful.</li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#add-clients-by-importing-the-json-exports","title":"Add clients by importing the JSON exports","text":"<p>Add a client named\u00a0<code>cmem</code> by select clients, then create client. The client described below can also be imported. Please download the file below, then select Import client. For the <code>cmem-service-account</code> client you have to edit the file and replace the secret or regenerate the secret in keycloak after the import.</p> <p>Available files:</p> <ul> <li>client configuration for using the ui (<code>cmem</code>)</li> <li>client configuration with credentials for technical account (<code>cmem-service-account</code>)</li> </ul> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#create-client-cmem-manually-for-web-interface","title":"Create client <code>cmem</code> manually (for web interface)","text":"<p>This client is intended for the usage with DataManager, Dataplatform and DataIntegration (user login):</p> <p></p> <ul> <li>Client type: OpenID Connect</li> <li>Client ID: i.e. <code>cmem</code>, you need to remember this and use this later</li> <li>Name and Description: fill as you like</li> <li>Select Next</li> <li>Client authentication: Off</li> <li>Authorization: Off</li> <li>Enable\u00a0Standard Flow Enabled\u00a0(enables OAuth 2.0 Authorization Code Flow)</li> <li>Before v23.1:<ul> <li>Additionally enable\u00a0Implicit Flow Enabled</li> </ul> </li> <li>Save</li> </ul> <p></p> <p>The dialog above closes and you land on the configuration page of this client:</p> <ul> <li>Valid redirect URIs: Add the correct URL pattern (wildcard\u00a0<code>http://example.org/*</code>\u00a0works) to\u00a0<code>Valid Redirect URIs</code>\u00a0(<code>*</code>\u00a0for testing purposes can be used as well)</li> <li>Switch the Tabs to Client scopes and select the first scope (i.e.: <code>cmem-dedicated</code>)</li> </ul> <p> </p> <ul> <li>Configure a new mapper (Client -&gt; Client Scopes -&gt; Add / Select Client Scope -&gt; Add Mapper)<ul> <li>select Mapper Type\u00a0User Client Role</li> <li>Name <code>groups</code></li> <li>Token Claim Name <code>groups</code></li> <li>Disable\u00a0Full group path</li> <li>Disable\u00a0Add to ID token</li> <li>Enable\u00a0Add to access token</li> <li>Enable\u00a0Add to user info</li> </ul> </li> <li>Save</li> </ul> <p></p> <ul> <li>In Corporate Memory configuration until v22.2:<ul> <li>Configure this client ID under\u00a0<code>js.config.workspaces.default.authorization.oauth2.clientId</code>\u00a0in DataManager\u2019s configuration file (Datamanager needs implicit flow)</li> <li>Configure  this client ID under\u00a0<code>oauth.clientId = \"cmem\"</code>\u00a0in DataManager\u2019s configuration file (Dataintegration needs standard flow)</li> </ul> </li> <li>In Corporate Memory configuration from v23.1:<ul> <li>Configure this client ID in the environments with the name <code>OAUTH_CLIENT_ID</code> in <code>/environments/config.env</code></li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#create-a-client-manually-technical-account","title":"Create a client manually (Technical Account)","text":"<p>This client is intended for internal use by DataIntegration (scheduler super-user) and data import purposes (cmemc)</p> <ul> <li>Client type: OpenID Connect</li> <li>Client ID: i.e. <code>cmem-service-account</code>, you need to remember this and use this later</li> <li>Name and Description: fill as you like</li> <li>Select Next</li> <li>Client authentication: On</li> <li>Authorization: Off</li> <li>Authentication flow: only enable <code>Service accounts roles</code>, the rest can be disabled</li> <li> <p>Save</p> </li> <li> <p>Go to\u00a0Credentials\u00a0and configure\u00a0Client Id and Secret, copy the client secret for later usage</p> </li> </ul> <p> </p> <ul> <li>Go to\u00a0Roles\u00a0and add the\u00a0<code>elds-admins</code>\u00a0role</li> <li>Select Action and Add associated roles</li> <li>Select Filter by client then</li> </ul> <p> </p> <ul> <li>Go to\u00a0Service Account Roles -&gt; Client Roles (<code>cmem-service-account</code>)\u00a0and add the\u00a0<code>elds-admins</code>\u00a0role to\u00a0Assigned Roles<ul> <li>no Realm roles needed beforehand</li> </ul> </li> <li>Switch the Tabs to Client scopes and select the first scope (i.e.: <code>cmem-service-account-dedicated</code>)</li> </ul> <p> </p> <ul> <li>Configure a new mapper (Client -&gt; Client Scopes -&gt; Add / Select Client Scope -&gt; Add Mapper)</li> <li>select Mapper Type\u00a0<code>User Client Role</code><ul> <li>Name <code>roles</code></li> <li>Token Claim Name <code>groups</code></li> <li>Enable Add to ID token</li> <li>Enable\u00a0Add to access token</li> <li>Enable\u00a0Add to user info</li> </ul> </li> <li>Save</li> </ul> <p></p> <ul> <li>Go to tab Service account roles</li> <li>Select the link in the center To manage detail and group mappings, click on the username service-account-YOUR_CLIENT_ID</li> </ul> <p></p> <ul> <li>Go to tab Role mapping and select Assign role</li> <li>Change the filter to Filter by clients and select the new Client ID, i.e <code>cmem-service-account</code></li> </ul> <p> </p> <ul> <li>In Corporate Memory configuration:<ul> <li>If DataIntegration schedulers are required, configure this client id and secret under the properties <code>workbench.superuser.client</code>\u00a0and\u00a0<code>workbench.superuser.clientSecret</code>\u00a0in DataIntegration\u2019s configuration file or</li> <li>in docker-compose-orchestration you can edit this in the environment as:     <pre><code>  CMEM_SERVICE_ACCOUNT_CLIENT_ID=cmem-service-account\n  CMEM_SERVICE_ACCOUNT_CLIENT_SECRET=YourSecret\n  DATAINTEGRATION_CMEM_SERVICE_CLIENT=cmem-service-account\n  DATAINTEGRATION_CMEM_SERVICE_CLIENT_SECRET=YourSecret\n</code></pre></li> <li>in helm this value is defined by:     <pre><code>  DATAINTEGRATION_CMEM_SERVICE_CLIENT_SECRET: {{ .Values.global.cmemClientSecret }}\nDATAINTEGRATION_CMEM_SERVICE_CLIENT: {{ .Values.global.cmemClientId }}\n</code></pre></li> <li>For cmemc you can configure this with <code>OAUTH_CLIENT_ID</code> and <code>OAUTH_CLIENT_SECRET</code>.</li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#groups-configuration","title":"Groups configuration","text":"<ul> <li>Go to\u00a0Groups and add the following groups:</li> <li><code>elds-admins</code></li> <li>Any groups provided by your user management system (e.g. LDAP) that must be recognized/mapped by Keycloak</li> <li>In Corporate Memory docker orchestration,\u00a0<code>local-users</code>,\u00a0<code>local-admins</code></li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/#users-configuration","title":"Users configuration","text":"<ul> <li>This applies to the\u00a0Docker Orchestration, for other setups consult the\u00a0Keycloak manual.</li> <li>Go to\u00a0<code>Users</code></li> <li>Add the following users and assign their groups respectively (for each user go to credentials, add password and disable\u00a0<code>Temporary</code>)</li> <li><code>user:user</code><ul> <li>groups:\u00a0<code>local-users</code></li> </ul> </li> <li><code>admin:admin</code><ul> <li>groups:\u00a0<code>local-admin</code></li> </ul> </li> </ul>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/","title":"Changing Passwords and Keys","text":"<p>This page describes how to change passwords and keys for a new deployment (esp. in the context of a\u00a0Single Node Cloud Installation).</p> <p>Assuming your instance runs at\u00a0<code>https://cmem.example.com/</code>\u00a0in a default installation Keycloak is deployed at\u00a0<code>https://cmem.example.com/auth</code>\u00a0(this may vary depending on your setup).</p> <p>This is your starting page:</p> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#change-credentials-of-keycloak-admin-account","title":"Change credentials of Keycloak admin account","text":"<p>To change the admin user\u2019s password go to \u201cAdministration Console\u201d and login with username/password admin/admin</p> <p>In the upper right corner go to \u201cManage account\u201d</p> <p></p> <p>In the \u201cAccount Security\u201d field go to \u201cSigning In\u201d</p> <p></p> <p>Click on update in \u201cBasic Authentication\u201d to set a new admin password for :</p> <p></p> <p>Set a new password.</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#change-credentials-of-cmem-service-account","title":"Change\u00a0credentials of cmem-service-account","text":"<p>Make sure the realm Cmem is selected, go to Clients in left sidebar and edit\u00a0cmem-service-account:</p> <p></p> <p>Switch to \u201cCredentials\u201d tab and press \u201cRegenerate Secret\u201d Button.</p> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#change-credentials-of-user-accounts","title":"Change credentials of user accounts","text":"<p>In default configuration, there are two users: user and admin. Both are configured with different groups to have different permissions inside Corporate Memory.</p> <p>To change the default passwords, select the Cmem Realm and open Users in the left sidebar:</p> <p></p> <p></p> <p>Then, select \u201cView all users\u201d and choose an account you want to change the password for (we start with admin)</p> <p></p> <p>Here you can change the password and unselect Temporary. Then press \u201cReset Password\u201d</p> <p></p> <p>Now proceed with the other account(s).</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/change-passwords-and-keys/#persisting","title":"Persisting","text":"<p>Warning</p> <p>This step is applicable only in case your deployment is based on\u00a0the\u00a0Single Node Cloud Installation.</p> <p>In order to persist this setup go back to your terminal inside the installation directory.</p> <p>The following make targets will create a database dump, store it in\u00a0<code>data/backups/keycloak/latest.sql</code>\u00a0and replace the initial database dump\u00a0<code>conf/postgres/docker-entrypoint-initdb.d/keycloak_db.sql</code>.</p> <pre><code>make keycloak-backup keycloak-restore\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/","title":"Configure Corporate Memory with an external Keycloak","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#introduction","title":"Introduction","text":"<p>Maybe you already operate a central Keycloak deployment in your infrastructure or you want to deploy multiple stages of Corporate Memory with a single Keycloak. Very often this results a Keycloak which is deployed in a different domain than your Corporate Memory, i.e. <code>cmem.example.com</code> and <code>keycloak.example.com</code>. For this scenario, this page give some hints.</p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#configuration-in-keycloak","title":"Configuration in Keycloak","text":"<p>When using a Keycloak in a different domain, you have to allow this domain in the Keycloak settings:</p> <ul> <li>In Realm Settings, go to Security defenses tab<ul> <li><code>X-Frame-Options</code> need to be cleared</li> <li>The <code>Content-Security-Policy</code> header needs to be defined for allowing the framing of the login mask of Keycloak for the deployment <code>frame-src &lt;https://cmem.example.com/&gt;;</code></li> </ul> </li> <li>In Clients go to i.e. <code>cmem</code> client<ul> <li>add <code>https://cmem.example.com/*</code> to Valid redirect URIs</li> </ul> </li> </ul> <p></p> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#configuration-in-corporate-memory","title":"Configuration in Corporate Memory","text":"","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#environments","title":"Environments","text":"<p>When running the Corporate Memory docker orchestration, you can configure the Keycloak through editing <code>environments/config.env</code>. Then just add the variables below. You can get those from the <code>.well-known</code> url from your instance, e.g. <code>https://keycloak.example.com/auth/realms/cmem/.well-known/openid-configuration</code>:</p> <pre><code>OAUTH_AUTHORIZATION_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/auth\nOAUTH_TOKEN_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/token\nOAUTH_JWK_SET_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/certs\nOAUTH_USERINFO_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/userinfo\nOAUTH_LOGOUT_REDIRECT_URL=${EXTERNAL_BASE_URL}/auth/realms/cmem/protocol/openid-connect/logout?redirect_uri=${EXTERNAL_BASE_URL}\nOAUTH_CLIENT_ID=cmem\n</code></pre> <p></p>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#dataintegration-optional","title":"Dataintegration (optional)","text":"<p>By default, Dataintegration is configured through environments. However you can also edit this in Dataintegration\u2019s config file <code>dataintegration.conf</code>:</p> <pre><code>oauth.clientId = ${OAUTH_CLIENT_ID}\noauth.authorizationUrl = ${OAUTH_AUTHORIZATION_URL}\noauth.tokenUrl = ${OAUTH_TOKEN_URL}\noauth.logoutRedirectUrl = ${OAUTH_LOGOUT_REDIRECT_URL}\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#dataplatform-optional","title":"Dataplatform (optional)","text":"<p>By default, Dataplatform is configured through environments. However you can also edit this in Dataplatform\u2019s config file <code>application.yml</code>:</p> <pre><code>spring.security.oauth2:\nresourceserver:\nanonymous: \"${DATAPLATFORM_ANONYMOUS}\"\njwt:\njwk-set-uri: \"${OAUTH_JWK_SET_URL}\"\nclient:\nregistration:\nkeycloak:\nclient-id: \"${OAUTH_CLIENT_ID}\"\nauthorization-grant-type: \"authorization_code\"\nclient-authentication-method: \"basic\"\nredirectUri: \"${DEPLOY_BASE_URL: 'http://localhost' }/dataplatform/login/oauth2/code/{registrationId}\"\nscope: # openid is mandatory as spring somehow does not add it to the userinfo request\n- openid\n- profile\n- email\nprovider:\nkeycloak:\njwk-set-uri: \"${OAUTH_JWK_SET_URL}\"\nauthorization-uri: \"${OAUTH_AUTHORIZATION_URL}\"\ntoken-uri: \"${OAUTH_TOKEN_URL}\"\nuser-info-uri: \"${OAUTH_USERINFO_URL}\"\nuser-name-attribute: \"preferred_username\"\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#cmemc","title":"cmemc","text":"<p>In cmemc you also need to change the Keycloak cmemc tries to authenticate before connecting to Corporate Memory. You have to add this:</p> <pre><code>KEYCLOAK_BASE_URI=https://keycloak.example.com/\nKEYCLOAK_REALM_ID=cmem\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/keycloak/using-external-keycloak/#helm-charts-optional","title":"Helm charts (optional)","text":"<p>In the helm charts, we assumed you deploy Keycloak by official charts, either via operator, or via helm charts. In either way you can configure the base realm path in the value section.</p> <pre><code>  # This is the base Keycloak realm url, e.g. https://cmem.example.com/auth/realms/cmem\n.Values.global.keycloakIssuerUrl: https://keycloak.example.com/auth/realms/cmem\n.Values.global.oauthClientId: cmem\n</code></pre>","tags":["Configuration","Security","Keycloak"]},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/","title":"Label Resolution and Full-Text Search","text":""},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#introduction","title":"Introduction","text":"<p>Label resolution translates resource identifiers (URI/IRI) into human readable labels. This resolution and, by extension, the full text search is configurable for different scenarios.</p>"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#configuration","title":"Configuration","text":"<p>eccenca DataPlatform offers three configuration options:\u00a0<code>labelProperties</code>(line 2)\u00a0<code>languagePreferences</code>\u00a0(line 5) and\u00a0<code>languagePreferencesAnyLangFallback</code>\u00a0(line 8).</p> <pre><code>proxy:\nlabelProperties: # (1)\n- \"http://www.w3.org/2000/01/rdf-schema#label\"\n- \"http://www.w3.org/2004/02/skos/core#prefLabel\"\nlanguagePreferences: #(2)\n- \"en\"\n- \"\"\nlanguagePreferencesAnyLangFallback: true #(3)\n</code></pre> <p>These properties define not only which properties and languages should be considered, but also the precedence of languages and properties over each other.</p> <p>The retrieval process can be simplified to the following procedure:</p> <ul> <li>First, when determining the label for a resource, the language is evaluated, then the property is considered.</li> <li>Consequently, for a resource in the default case:<ol> <li>An english\u00a0value for\u00a0<code>rdfs:label</code>\u00a0is searched.</li> <li>A literal of the property\u00a0<code>rdfs:label</code>\u00a0without a language tag is searched (which is why there is an entry\u00a0<code>\"\"</code>).</li> <li>An english value of\u00a0<code>skos:prefLabel</code>\u00a0is searched.</li> <li>A literal of the property\u00a0<code>skos:prefLabel</code>\u00a0without a language tag is searched.</li> <li>If nothing is found, DataPlatform tries to create a prefixed URI, otherwise the last segment of the resource identifier is used.</li> </ol> </li> </ul> <p>Additionally, in case more than one label could be retrieved, for example by conflicting values, the alphabetically first entry is used.</p>"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#example","title":"Example","text":"<p>How labels are resolved is best explained using these default settings and some examples.</p> <pre><code>:Resource1 rdfs:label \"Leipzig\"@en.\n:Resource2 :someOtherProperty \"Berlin\"@en.\n:Resource3 rdfs:label \"Stuttgart\"@de\n:Resource4 rdfs:label \"Hanover\"@en\n:Resource4 rdfs:label \"Another Label for Hanover\"@en\n</code></pre> <ul> <li>For\u00a0<code>**:Resource1**</code>\u00a0the label will be\u00a0<code>Leipzig</code> as the english\u00a0<code>rdfs:label</code> will be retrieved.</li> <li>For\u00a0<code>**:Resource2\u00a0**</code> the label cannot be retrieved from the Knowledge Graph since no known property is used. Hence the fallback.</li> <li>For :Resource3 the label will be retrieved as\u00a0<code>Stuttgart</code>, if the (3)\u00a0<code>languagePreferencesAnyLangFallback</code>\u00a0is\u00a0<code>true</code>.</li> <li>While there is a well-known property used, none of the used languages match. Using the fallback, the alphabetically first match is retrieved in this case.</li> <li>For\u00a0<code>:Resource4</code> multiple label candidates could be determined.</li> <li>In this case,\u00a0<code>Another Label for Hanover</code>\u00a0is retrieved as it is the first value in the alphanumerical comparison.</li> </ul>"},{"location":"deploy-and-configure/configuration/label-resolution-and-full-text-search/#client-api","title":"Client API","text":"<p>The label resolution functionality can also be used by client systems. This functionality is exposed as an\u00a0API endpoint\u00a0(<code>&lt;dp_url&gt;/api/explore/title</code>).</p>"},{"location":"deploy-and-configure/configuration/production-ready-settings/","title":"Production-Ready Settings","text":"<p>If you plan to deploy\u00a0Corporate Memory\u00a0in a publically accessible environment you need to take care about some final configuration steps.</p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#restrict-redirect-urls","title":"Restrict Redirect URLs","text":"<p>As stated in the Keycloak Server Administration Guide:</p> <p>Make your registered redirect URIs as specific as possible. Registering vague redirect URIs for Authorization Code Flows may allow malicious clients to impersonate another client with broader access.</p> <p>Corporate Memory uses the following clients to authenticate against keycloak. For each client, you have to adjust the Valid Redirect URIs field.</p> <ul> <li>datamanager</li> <li>dataintegration</li> </ul> <p>Go to\u00a0<code>Cmem-Realm</code>\u00a0\u2192\u00a0<code>Clients</code>\u00a0\u2192\u00a0<code>datamanger / dataintegration</code>\u00a0and enter your deploy URL, e.g., <code>https://cmem.example.net/*</code>.</p> <p></p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#password-policies","title":"Password Policies","text":"<p>If you create users in Keycloak, make sure these users have strong passwords. To enforce this, setting up password policies can help.</p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#cookie-settings","title":"Cookie Settings","text":"<p>In Keycloak you should enforce the secure flag for keycloak cookies. Go to\u00a0<code>Cmem-Realm</code>\u00a0\u2192 <code>Realm Settings</code>\u00a0\u2192 <code>Login</code>\u00a0and change Require SSL to <code>all requests</code>. If you are running without SSL, you will no longer be able to log in to Corporate Memory.</p> <p>Once this is done, make sure DataPlatform and DataIntegration use\u00a0<code>HTTPS</code>\u00a0to connect to Keycloak. See the usage of\u00a0<code>DATAPLATFORM_AUTH_URL</code>,\u00a0<code>OAUTH_AUTHORIZATION_URL</code> and\u00a0<code>OAUTH_TOKEN_URL</code>.</p> <p></p>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#cors-settings","title":"CORS Settings","text":"","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#dataplatform","title":"DataPlatform","text":"<p>DataPlatform uses\u00a0<code>http.cors.allowedOrigins *</code>\u00a0as the default setting. It is recommended to correctly set the values for the following headers:</p> <ul> <li><code>Access-Control-Allow-Origin</code>:\u00a0 specifies which domains can access a\u00a0site\u2019s resources. For example, if ABC Corp. has domains\u00a0<code>ABC.com</code>\u00a0and\u00a0<code>XYZ.com</code>, then its developers can use this header to securely grant\u00a0<code>XYZ.com</code>\u00a0access to ABC.com\u2019s resources.</li> <li><code>Access-Control-Allow-Methods</code>: specifies which HTTP request methods (<code>GET</code>, <code>PUT</code>, <code>DELETE</code>, etc.) can be used to access resources. This\u00a0header lets developers further enhance security by specifying what methods are valid when XYZ accesses ABC\u2019s resources.</li> </ul> <p>Detailed configuration options can be found\u00a0here.</p> <p>This is an example section from DataPlatform <code>application.yml</code>:</p> <pre><code>## Cross-Origin Resource Sharing (CORS) settings\nhttp:\ncors:\nallowedOrigins:\n- \"https://cmem.example.net\"\nallowedMethods:\n- \"OPTIONS\"\n- \"HEAD\"\n- \"GET\"\n- \"POST\"\n- \"PUT\"\n- \"DELETE\"\n- \"PATCH\"\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/production-ready-settings/#dataintegration","title":"DataIntegration","text":"<p>DataIntegration uses\u00a0<code>cors.config.allowOrigins *</code>\u00a0as the default setting.\u00a0 It is recommended to correctly set the value for the <code>Access-Control-Allow-Origin</code>\u00a0header. It specifies which domains can access a\u00a0site\u2019s resources. For example, if ABC Corp. has the domains\u00a0<code>ABC.com</code>\u00a0and\u00a0<code>XYZ.com</code>, you can use this header to securely grant\u00a0<code>XYZ.com</code>\u00a0access to <code>ABC.com</code>\u2018s resources. Detailed configuration options can be found\u00a0here.</p> <p>This is an example section from <code>dataintegration.conf</code>:</p> <pre><code>## Cross-Origin Resource Sharing (CORS) settings\n# CORS configuration ###\ncors.enabled = true\n# List of domains that are allowed to do requests.\n# Wildcard '*' means \"All domains\".\ncors.config.allowOrigins = \"*\"\n# Support cookies, auth etc. for the configured domain under allowOrigins.\n# If set to true, allowOrigins must not have '*' configured.\ncors.config.allowCredentials = false\n</code></pre>","tags":["Configuration","Security"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/","title":"Quad Store Configuration","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#virtuoso-setup","title":"Virtuoso setup","text":"<p>This section only covers setup options that have a direct impact on the interoperability between DataPlatform and Virtuoso. There are far more options which can significantly improve the overall performance. For an overview of all configuration files and options of Virtuoso refer to the document\u00a0Database Server Administration\u00a0of the official product documentation. Ensure to add the suggested parameters to the corresponding subsections of the configuration file before starting the server.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#compatibility","title":"Compatibility","text":"<ul> <li>Virtuoso 7.2.4.2\u00a0- DataPlatform is compatible with\u00a0Virtuoso 7.2.4.2. Compatibility with other versions is not guaranteed.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#configuration","title":"Configuration","text":"<p>The following options apply to the Virtuoso configuration file\u00a0<code>virtuoso.ini</code>. Check these options before starting DataPlatform and adjust them if necessary.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#ssl-support","title":"SSL support","text":"<p>Mandatory configuration if\u00a0<code>sparqlEndpoints.virtuoso[i].sslEnabled=true</code>. The server must have a valid certificate, which must be trusted by the system where DataPlatform runs. In this case, the\u00a0<code>sparqlEndpoints.virtuoso[i].port</code>\u00a0property must point to the\u00a0<code>SSLServerPort</code>.</p> <pre><code>[Parameters]\nSSLServerPort      = 2111\n;; Make sure that &lt;PATH_TO_CERTS_DIR&gt; is contained by DirsAllowed\nSSLCertificate     = &lt;PATH_TO_CERTS_DIR&gt;/public-cert.pem\nSSLPrivateKey      = &lt;PATH_TO_CERTS_DIR&gt;/private-key.pem\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#transaction-size-limit-optional","title":"Transaction size limit (optional)","text":"<p>This option has impact on the maximum file size which can be uploaded using the SPARQL Graph Store endpoint and executing update queries.\\ The maximum value you can set for this option is\u00a0<code>2147483000</code>\u00a0bytes.</p> <pre><code>[Parameters]\nTransactionAfterImageLimit = &lt;size_limit_in_bytes&gt; # default 50000000\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#database-buffers","title":"Database buffers","text":"<p>These options determine the amount of RAM used by Virtuoso to cache database files. To choose the appropriate values consult the\u00a0<code>virtuoso.ini</code>\u00a0file.</p> <pre><code>[Parameters]\nNumberOfBuffers          = &lt;size&gt;\nMaxDirtyBuffers          = &lt;size&gt;\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#resultset-max-rows","title":"ResultSet max rows","text":"<p>This setting is used to limit the number of rows in a result. The effective limit is the lowest value of this setting and the SPARQL query\u00a0<code>LIMIT</code>\u00a0clause value (if present). The exact value is situation-related and depends on the size of the datasets and the consequential number of returned rows. We recommend to set this option to\u00a0<code>0</code>\u00a0to disable the limitation.</p> <pre><code>[SPARQL]\nResultSetMaxRows = 0 # default 10000\n</code></pre>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#stardog-setup","title":"Stardog setup","text":"<p>This section only covers limitations and options which have a direct impact on the interoperability between DataPlatform and Stardog. For an overview of all configuration options of Stardog refer to the official\u00a0Stardog documentation. Make sure to add the parameters described in this section to the\u00a0<code>stardog.properties</code> configuration file\u00a0before starting the server.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#compatibility_1","title":"Compatibility","text":"<ul> <li>Stardog 7.2.1 -DataPlatform is compatible with\u00a0Stardog version 7.1.1. Compatibility with newer versions is not guaranteed.</li> <li>Stardog 6.2.3 (deprecated) -\u00a0DataPlatform is compatible with\u00a0Stardog version 6.2.3.</li> </ul> <p>!!! note     Support for 6.2.3 is deprecated and will be removed in later DataPlatform releases.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#configuration_1","title":"Configuration","text":"<ul> <li>Search enabled\\     DataPlatform relies on the Stardog Semantic Search, which has to be enabled by setting:</li> <li><code>search.enabled=true</code>\\         You can set this property using either Stardog Studio or the\u00a0<code>stardog-admin</code>\u00a0commands.\\         Refer to the\u00a0Stardog documentation\u00a0for more detailed information.</li> <li>Server side named graph security\\     If the\u00a0<code>PROVISIONED</code>\u00a0access control strategy is used for the configured endpoint, you have to set the property\u00a0<code>security.named.graphs=true</code>\u00a0for the configured database as explained in the\u00a0Stardog documentation. Additionally, the following properties are required:</li> <li><code>password.length.max</code>: For the provisioned mode to work properly. This property should have a value of at least 64.</li> <li> <p><code>password.regex</code>: The default value configured in Stardog is not compatible with the passwords generated by DataPlatform. The regex should be\u00a0<code>[\\\\w+\\\\/=]+</code></p> </li> <li> <p>SSL support\\     Mandatory configuration if\u00a0<code>sparqlEndpoints.stardog[i].sslEnabled=true</code>. The server must have a valid certificate which must be trusted by the system where DataPlatform runs. In this case, the\u00a0<code>sparqlEndpoints.stardog[i].port</code>\u00a0property must point to the SSL port (which default value is\u00a0<code>5821</code>).\\     Consult the\u00a0Configuring Stardog to use SSL\u00a0section of Stardog\u2019s manual for more information on the topic.</p> </li> <li>Query timeout override\\     In order to allow DataPlatform to override the query timeout for individual queries, you have to ensure that the property\u00a0<code>query.timeout.override.enabled</code>\u00a0for the database is set to\u00a0<code>true</code>\u00a0(which is the default).\\     Consult the\u00a0Configuring Query Management\u00a0section of Stardog\u2019s manual for further information.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#limitations","title":"Limitations","text":"<ul> <li>Quad format upload\\     The Graph Store Protocol implementation for Stardog does not support uploading of RDF quad data (TriG, N-Quads).</li> <li>Initial connection\\     The first request to DataPlatform can take several seconds due to connection startup to the Stardog server.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#caveats","title":"Caveats","text":"","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#datatype-canonicalization","title":"Datatype canonicalization","text":"<p>By default Stardog performs canonicalization of XSD integer datatypes as explained in the\u00a0Stardog documentation. That means that the datatype of all literals declaring a sub-datatype of\u00a0<code>xsd:integer</code>\u00a0is generalized on load or insert:</p> original datatype of literal new datatype of canonicalized literal <code>xsd:int</code> <code>xsd:integer</code> <code>xsd:short</code> <code>xsd:integer</code> <code>xsd:byte</code> <code>xsd:integer</code> <code>xsd:nonNegativeInteger</code> <code>xsd:integer</code> <code>xsd:positiveInteger</code> <code>xsd:integer</code> <code>xsd:nonPositiveInteger</code> <code>xsd:integer</code> <code>xsd:negativeInteger</code> <code>xsd:integer</code> <code>xsd:unsignedLong</code> <code>xsd:integer</code> <code>xsd:unsignedInt</code> <code>xsd:integer</code> <code>xsd:unsignedShort</code> <code>xsd:integer</code> <code>xsd:unsignedByte</code> <code>xsd:integer</code> <p>For example, when ingesting the following RDF data</p> <pre><code>@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\nxsd:long &lt;urn:example:value&gt; \"42\"^^xsd:long .\nxsd:int &lt;urn:example:value&gt; \"42\"^^xsd:int .\nxsd:short &lt;urn:example:value&gt; \"42\"^^xsd:short .\nxsd:byte &lt;urn:example:value&gt; \"42\"^^xsd:byte .\nxsd:nonNegativeInteger &lt;urn:example:value&gt; \"42\"^^xsd:nonNegativeInteger .\nxsd:positiveInteger &lt;urn:example:value&gt; \"42\"^^xsd:positiveInteger .\nxsd:nonPositiveInteger &lt;urn:example:value&gt; \"-42\"^^xsd:nonPositiveInteger .\nxsd:negativeInteger &lt;urn:example:value&gt; \"-42\"^^xsd:negativeInteger .\nxsd:unsignedLong &lt;urn:example:value&gt; \"42\"^^xsd:unsignedLong .\nxsd:unsignedInt &lt;urn:example:value&gt; \"42\"^^xsd:unsignedInt .\nxsd:unsignedShort &lt;urn:example:value&gt; \"42\"^^xsd:unsignedShort .\nxsd:unsignedByte &lt;urn:example:value&gt; \"42\"^^xsd:unsignedByte .\n</code></pre> <p>in a Stardog database with\u00a0<code>index.literals.canonical</code>\u00a0set to\u00a0<code>true</code>\u00a0(default), it will be stored as</p> <pre><code>@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\nxsd:long &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:int &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:short &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:byte &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:nonNegativeInteger &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:positiveInteger &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:nonPositiveInteger &lt;urn:example:value&gt; \"-42\"^^xsd:integer .\nxsd:negativeInteger &lt;urn:example:value&gt; \"-42\"^^xsd:integer .\nxsd:unsignedLong &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:unsignedInt &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:unsignedShort &lt;urn:example:value&gt; \"42\"^^xsd:integer .\nxsd:unsignedByte &lt;urn:example:value&gt; \"42\"^^xsd:integer .\n</code></pre> <p>If data is to be copied between CMEM setups backed by Stardog, the\u00a0<code>index.literals.canonical</code>\u00a0options of the corresponding databases must be set to the same value on both setups.</p> <p>It is recommended to only turn this canonicalization off when it is strictly necessary, due to a negative impact on database performance without it.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#graphdb-setup","title":"GraphDB setup","text":"<p>This section covers only limitations and options which have a direct impact on the interoperability between DataPlatform and GraphDB. For an overview of all configuration options of GraphDB refer to the official\u00a0GraphDB documentation.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#compatibility_2","title":"Compatibility","text":"<ul> <li>GraphDB 9.2.0 -\u00a0DataPlatform is compatible with\u00a0GraphDB version 9.2.0. Compatibility with newer versions is not guaranteed.</li> </ul>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/quad-store-configuration/#configuration_2","title":"Configuration","text":"<p>No specific configuration changes are needed in order to run with GraphDB.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/reverse-proxy/","title":"Reverse Proxy","text":"<p>A reverse proxy forwards all requests from the users to the called service and returns the result to the users. Corporate Memory is tested with an Apache HTTP server as a reverse proxy.</p> <p>Reverse proxy is a necessary component in the Corporate Memory deployment. It enables you to:</p> <ul> <li>define routes for all the components within one domain name</li> <li>expose only ports 80 and 443 to the outside network, all other communication would be performed in the internal network</li> <li>ease configuration and management of the SSL certificates</li> </ul> <p>This also enables you to activate the\u00a0Linked Data delivery mode\u00a0of DataPlatform. The Linked Data delivery mode is able to serve Linked Data that uses the same namespace as the configured domain name as resolvable URIs including content negotiation.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/reverse-proxy/#example-configuration-for-apache-http-server","title":"Example Configuration for Apache HTTP server","text":"<p>apache configuration template</p> <pre><code>&lt;VirtualHost *:80&gt;\n    ServerName corporate-memory.example.com\n    Redirect permanent / https://corporate-memory.example.com/\n&lt;/Virtualhost&gt;\n&lt;VirtualHost *:443&gt;\n\n    ServerName corporate-memory.example.com\n    ServerAlias www.corporate-memory.example.com\n    ProxyPreserveHost On\n\n    ProxyPass /auth https://keycloak.host/auth retry=0\n    ProxyPassReverse /auth https://keycloak.host/auth\n\n    ProxyPass /dataplatform https://dataplatform.host/dataplatform retry=0\n    ProxyPassReverse /dataplatform https://dataplatform.host/dataplatform\n\n    RewriteEngine  on\n    RewriteRule    \"^/dataintegration$\"  \"/dataintegration/\" [R]\n\n    RewriteCond %{HTTP:Upgrade} =websocket [NC]\n    RewriteRule \"/dataintegration/(.*)\" wss://dataintegration.host/dataintegration/$1 [P,L]\n    RewriteCond %{HTTP:Upgrade} !=websocket [NC]\n    RewriteRule \"/dataintegration/(.*)\" https://dataintegration.host/dataintegration/$1 [P,L]\n\n    ProxyPassReverse /dataintegration https://dataintegration.host/dataintegration\n\n    ProxyPass / https://datamanager.host/ retry=0\n    ProxyPassReverse / https://datamanager.host/\n\n    # https://github.com/gitlabhq/gitlabhq/issues/8924\n    AllowEncodedSlashes NoDecode\n\n    # Network timeout in seconds for proxied requests (default 300)\n    # http://serverfault.com/questions/500467/apache2-proxy-timeout/583266\n    ProxyTimeout 1200\n\n    #######################\n    # SSL\n    #######################\n    SSLEngine on\n    SSLCertificateFile /etc/apache2/ssl/ssl-bundle.crt\n    SSLCertificateKeyFile /etc/apache2/ssl/cert.key\n    SSLCertificateChainFile /etc/apache2/ssl/cert.cabundle\n&lt;/VirtualHost&gt;\n</code></pre> <p>Information about the runtime environment which is used to run DataPlatform, DataIntegration and DataManager, is hidden.</p> <p>Note</p> <p>Keep in mind that you have to adjust the location paths/URLs in the DataManager and DataIntegration configuration files.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/configuration/reverse-proxy/#linked-data-delivery-mode","title":"Linked Data delivery mode","text":"<p>The Linked Data delivery mode is able to serve data that uses the same namespace as the configured domain name as resolvable URIs including content negotiation.</p> <p>Therefore you can use the following template (e.g.:\u00a0https://corporate-memory.example.com):</p> <ul> <li>https://dataplatform.corporate-memory.example.com\u00a0(DataPlatform)</li> <li>https://dataplatform.corporate-memory.example.com/vocabulary/example/\u00a0(a custom vocabulary)</li> <li>with HTTPS enforcement (recommended)</li> </ul> <p>apache sample config for linked data delivery</p> <pre><code>&lt;VirtualHost *:80&gt;\n  ServerName dataplatform.corporate-memory.example.com\n  Redirect permanent / https://dataplatform.corporate-memory.example.com/\n&lt;/Virtualhost&gt;\n\n&lt;VirtualHost *:443&gt;\n\n  ServerName dataplatform.corporate-memory.example.com\n\n  RewriteEngine On\n  ProxyPreserveHost On\n\n  Define reverse_url\\\n  \"http://DATAPLATFORM_SERVICE_HOST:8080/proxy/default/graph?owlImportsResolution=false&amp;graph=\"\n\n  # Return complete graphs for URIs ending with slash\n  RewriteCond %{REQUEST_METHOD} ^(.*)\n  RewriteRule \"^/vocabulary/(.+)/$\"\\\n    \"${reverse_url}%{REQUEST_SCHEME}://%{HTTP_HOST}%{REQUEST_URI}\"\\\n    [P]\n\n  # Everything else is just plain proxied\n  RewriteRule \"^/(.*)$\" \"http://DATAPLATFORM_SERVICE_HOST:8080/$1\" [P]\n\n  # Reverse proxy only needed for login via browser (e.g. with DataManager)\n  ProxyPassReverse \"/\" \"http://DATAPLATFORM_SERVICE_HOST:8080/\"\n\n  # ssl-config\n  SSLEngine on\n  SSLCertificateFile /etc/apache2/ssl/ssl-bundle.crt\n  SSLCertificateKeyFile /etc/apache2/ssl/cert.key\n  SSLCertificateChainFile /etc/apache2/ssl/cert.cabundle\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note</p> <p>Keep in mind, that the vocabulary namespace is using\u00a0<code>https</code>\u00a0as protocol, too.</p>","tags":["Configuration"]},{"location":"deploy-and-configure/installation/","title":"Installation","text":"<p>This page describes proven deployment scenarios for eccenca Corporate Memory.</p> <p>All Corporate Memory components are distributed as Docker images and can be obtained from eccenca\u2019s Artifactory service. To run them you need a Docker enabled Linux server. In addition to that, eccenca provides distribution archives for all components which contain configuration examples (YAML) as well as JAR/WAR artifacts.</p>"},{"location":"deploy-and-configure/installation/#operating-systems-os","title":"Operating Systems (OS)","text":"<p>Corporate Memory is tested on Ubuntu 18.04 (backward compatible with 16.04 and 14.04) and RHEL 7.7.</p> <p>Special note on RHEL SELinux Support: there is no limitation for RedHat SELinux. We recommend to keep the SELinux in\u00a0enforced\u00a0mode. You can keep the default setting of the\u00a0<code>/etc/selinux/config</code>\u00a0file.</p> sample config /etc/selinux/config<pre><code># This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=enforcing\n# SELINUXTYPE= can take one of three values:\n#     targeted - Targeted processes are protected,\n#     minimum - Modification of targeted policy. Only selected processes are protected.\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n</code></pre>"},{"location":"deploy-and-configure/installation/#docker-compose-based-orchestration-deployment","title":"Docker-compose based Orchestration deployment","text":"<p>Docker Compose\u00a0is a convenient way to provision several Docker containers locally for development setups or on remote servers for single node setups.</p> <p>eccenca is heavily using Docker Compose for all kinds of internal and customer deployments. For more details on how to use docker-compose based orchestration refer to\u00a0Scenario: Local Installation\u00a0and\u00a0Scenario: Single Node Cloud Installation.</p>"},{"location":"deploy-and-configure/installation/#dataintegration","title":"DataIntegration","text":""},{"location":"deploy-and-configure/installation/#running-on-a-spark-cluster","title":"Running on a Spark Cluster","text":"<p>eccenca DataIntegration supports the execution of DataIntegration workflows in a cluster environment with Apache Spark.</p>"},{"location":"deploy-and-configure/installation/#prerequisites","title":"Prerequisites","text":"<p>For the execution of DataIntegration in a Spark cluster the following software components from the Hadoop eco-system are recommended:</p> <ul> <li>Scala 2.11 or 2.10</li> <li>Apache Spark 2.1.2 (compiled for Scala 2.11)</li> <li>Apache Hadoop 2.7 (HDFS)</li> <li>Apache Hive 1.2, with a relational data bases as meta store (e.g. Derby)</li> </ul> <p>Recent versions of the following Hadoop distributions are generally supported as well:</p> <ul> <li>Hortonworks (HDP 2.5)</li> <li>Cloudera (CDH 5.8)</li> <li>Oracle Big Data Lite (4.6)</li> <li>Microsoft HDInsight (based on HDP)</li> </ul>"},{"location":"deploy-and-configure/installation/#installation_1","title":"Installation","text":"<p>A Spark application can run in three different modes:</p> <ul> <li>local mode</li> <li>client mode</li> <li>cluster mode</li> </ul> <p>The local mode is for running Spark applications on one local machine. In the client mode the DataIntegration application will run outside of the cluster and create Spark Jobs to be executed in the cluster at run time. The cluster mode requires that the application using Spark runs completely in the cluster and is managed by the software running on the cluster (e.g. Spark, Apache Yarn, Mesos). DataIntegration supports local mode (for testing), client mode (for production, only with clusters managed by Spark) or cluster mode on Yarn (for production, integrates best with other distributed applications).</p> <p>When running DataIntegration in a cluster, the same installation procedure and prerequisites apply as for the local installation. The application can be installed outside the cluster or on any cluster node. A number of configuration options have to be set to be able to connect to and use a Spark cluster. The necessary configuration options are described in\u00a0DataIntegration.</p>"},{"location":"deploy-and-configure/installation/#dataplatform","title":"DataPlatform","text":""},{"location":"deploy-and-configure/installation/#scaling","title":"Scaling","text":"<p>Run multiple DataPlatform instances with the same configuration to enable high-availability and/or high-performance setups.</p>"},{"location":"deploy-and-configure/installation/#prerequisites_1","title":"Prerequisites","text":"<p>For running multiple DataPlatform instances the following prerequisites apply:</p> <ul> <li>The same application configuration properties must be used by all scaled instances.</li> <li>If access control for any SPARQL endpoint is active, a shared Redis cache used by all DataPlatform instances is required.</li> </ul>"},{"location":"deploy-and-configure/installation/#limitations","title":"Limitations","text":"<p>When running multiple DataPlatform instances it is not possible to use a shared Virtuoso backend with provisioned access control active.</p>"},{"location":"deploy-and-configure/installation/#troubleshooting","title":"Troubleshooting","text":"<p>In case DataPlatform failed to start, check the logs for error messages pointing to faulty parameters in the configuration. Since not every faulty behavior is apparent from reading the logs, the following checks can help you to verify the configuration:</p> <ul> <li>Check the\u00a0<code>http(s)://&lt;servername:port&gt;/actuator/health/</code>\u00a0endpoint to verify if the SPARQL proxy service endpoints are configured properly.</li> </ul> <p>Note: Refer to the\u00a0Spring documentation\u00a0on how to set active profiles.</p>"},{"location":"deploy-and-configure/installation/#plugins","title":"Plugins","text":"<p>In some cases DataPlatform needs to be extended with plugins. Extensions are necessary when drivers cannot be included due to licensing restrictions or when plugins are delivered separately.</p> <p>In this case, you have to update the .war file of DataPlatform by placing the plugin .jar files in the same directory, or by stating the path via the configuration option.</p> <p>To include plugins that are located in the same directory as the\u00a0<code>eccenca-DataPlatform.war</code>\u00a0file, execute the .war file with the option\u00a0<code>-u</code>\u00a0or\u00a0<code>--update-war</code>:</p> <pre><code># with plugins located in the same folder as the WAR file\njava -jar ${JAVA_TOOL_OPTIONS} eccenca-DataPlatform.war --update-war\n</code></pre> <p>If the plugins to be included are not located in the same folder as the .war file, you can specify a directory containing the plugins as the argument of the\u00a0<code>-u</code>\u00a0or\u00a0<code>--update-war</code>\u00a0option.</p> <pre><code>java -jar ${JAVA_TOOL_OPTIONS} eccenca-DataPlatform.war -u /data/plugins\n</code></pre> <p>The last command repackages the\u00a0<code>eccenca-DataPlatform.war</code>\u00a0by including all plugins (.jar) located in the specified directory.</p> <p>Note: Make sure that only the\u00a0<code>eccenca-DataPlatform.war</code>\u00a0file is in the directory since multiple .war files can cause problems.</p> <p>Note: During the update procedure, the directory\u00a0<code>WEB-INF</code>\u00a0is created. Due to security concerns the update mechanism does not delete this directory. You can delete it after the update process is finished.</p>"},{"location":"deploy-and-configure/installation/migrating-stores/","title":"Migrating Stores","text":""},{"location":"deploy-and-configure/installation/migrating-stores/#sizing-and-deployment","title":"Sizing and Deployment","text":"<ul> <li>size and deploy of the new store (refer to the capacity planning / sizing considerations, refer to the docker container / orchestration)</li> <li>store specific config (e.g. search-all-graphs in SD)</li> </ul>"},{"location":"deploy-and-configure/installation/migrating-stores/#transferring-data-and-configuration","title":"Transferring Data and Configuration","text":"<ul> <li>backing-up / exporting and restore / import of graphs, DI-projects, configuration (if any)<ul> <li>graphs<ul> <li>blacklisting the DI projects graphs</li> </ul> </li> <li>config<ul> <li>DM<ul> <li>stardog text match support (this is a DM parameter!)</li> <li>search queries</li> <li>navigation</li> </ul> </li> </ul> </li> <li>DP<ul> <li>configure the resp. store</li> </ul> </li> <li>DI<ul> <li>nothing to do \u2026 just duplicate / copy the configuration as-is</li> <li><code>cmemc admin workspace export / import</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"deploy-and-configure/installation/migrating-stores/#test-and-validation","title":"Test and Validation","text":"<ul> <li>best practice:<ul> <li>run all (SELECT) queries in the query catalog and compare results (e.g. with <code>cmemc</code>)<ul> <li>theoretically this could also be applied to INSERT queries (by re-writing into SELECTS in case you want / need to omit altering your graphs)</li> </ul> </li> <li>count all triples in all graphs on both instances before/after export/import (<code>cmemc graph count --all</code>)</li> </ul> </li> </ul>"},{"location":"deploy-and-configure/installation/migrating-stores/#optimizing-your-setup","title":"Optimizing Your Setup","text":"<ul> <li>optimizing customization (e.g. queries in SHAPES; DI; DM-config)<ul> <li>\u201ctextmatch\u201d / \u201clucene\u201d queries need to be migrated (a query can be helpful to find these queries\u2026)</li> <li>performance comparisons could be automated via <code>cmemc query replay</code><ul> <li>identify query that won\u2019t run or run slow</li> </ul> </li> </ul> </li> <li>general query best practices<ul> <li>\u2192 query optimization guide<ul> <li>use <code>VALUE</code> instead of <code>FILTER (?x IN (...))</code> (esp. on GDB)</li> </ul> </li> </ul> </li> </ul>"},{"location":"deploy-and-configure/installation/scenario-local-installation/","title":"Introduction","text":"<p>This page describes a docker-compose based orchestration running on your local machine and accessible via browser.</p> <p>The code examples in this section assumes that you have POSIX-compliant shell (linux, macOS or WSL for Windows).</p>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#requirements","title":"Requirements","text":"<ul> <li>Access credentials to eccenca Artifactory and eccenca Docker Registry \u2192\u00a0contact us to get yours</li> <li>docker\u00a0and\u00a0docker-compose\u00a0(v1) installed locally</li> <li>git\u00a0installed locally</li> <li>At least 4 CPUs and 12GB of RAM (recommended: 16GB) dedicated to docker</li> </ul>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#setup-check-installation-environment","title":"Setup &amp; Check Installation Environment","text":"<p>Download the\u00a0Corporate Memory docker orchestration\u00a0from eccenca Artifactory.</p> <p>Open a terminal window, create a directory, copy and extract docker orchestration there.</p> <pre><code># create eccenca-corporate-memory directory in your ${HOME} and set as a working dir\n\n$ mkdir ${HOME}/eccenca-corporate-memory &amp;&amp; cd ${HOME}/eccenca-corporate-memory\n\n# cp Corporate Memory docker orchestration distribution in the local directory\n# Change VERSION to the version you have downloaded e.g. v20.03\n\n$ cp ${HOME}/Downloads/cmem-orchestration-VERSION.zip ./\n$ unzip cmem-orchestration-VERSION.zip\n$ rm cmem-orchestration-VERSION.zip\n$ git init &amp;&amp; git add . &amp;&amp; git commit -m \"stub\"\n</code></pre> <p>Check your local environment:</p> <pre><code># run the following command (without $) to check your docker server version, should be at least 19.03\n# to have the current security patches, always update your docker version to the latest one\n\n$ docker info | grep -i version\nServer Version: 19.03.8\n# check docker-compose version, should be at least 1.25.0\n# update to the latest version if necessary\n\n$ docker-compose --version\ndocker-compose version 1.25.4, build 8d51620a\n# login into eccenca docker registry\n\n$ docker login docker-registry.eccenca.com\nUsername: yourusername\nPassword:\nLogin Succeeded\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#installation","title":"Installation","text":"<p>To install Corporate Memory, you need to modify your local hosts file (located in /etc/hosts), minimal configuration is as follows:</p> <pre><code>##\n# Host Database\n#\n# localhost is used to configure the loopback interface\n# when the system is booting.  Do not change this entry.\n##\n127.0.0.1 localhost\n127.0.0.1 docker.localhost\n127.0.0.1 corporate.memory\n</code></pre> <p>Corporate Memory uses Ontotext GraphDB triple store as default backend. Graphdb is available as free version and does not requires a license. If you have a license for graphdb you can copy the file to the <code>license</code>folder inside Corporate Memory\u2019s root folder.</p> <pre><code>$ cp YOUR_SE_LICENSE_FILE ${HOME}/cmem-orchestration-VERSION/licenses/graphdb-se.license\n# or\n$ cp YOUR_EE_LICENSE_FILE ${HOME}/cmem-orchestration-VERSION/licenses/graphdb-ee.license\n</code></pre> <p>Then change the file <code>environments/config.env</code> to use the correct version:</p> <pre><code>###############################\n# Stores                      #\n###############################\n# default: graphdb-se\n#DP_STORE=graphdb-ee\nDP_STORE=graphdb-free\n</code></pre> <p>Run the command to clean workspace, pull the images, start the Corporate Memory instance and load initial data:</p> <pre><code>$ cd ${HOME}/cmem-orchestration-VERSION\n\n# Pulling the images will take time\n$ make clean-pull-start-bootstrap\n</code></pre> <p>You should see the output as follows:</p> <pre><code>/usr/local/bin/docker-compose kill\n/usr/local/bin/docker-compose stop\n/usr/local/bin/docker-compose down --volumes --remove-orphans\nRemoving network dockerlocal_default\nRemoving volume dockerlocal_stardog\n/usr/local/bin/docker-compose rm -v --force\nNo stopped containers\nPulling apache2         ... done\nPulling datamanager     ... done\nPulling store           ... done\nPulling postgres        ... done\nPulling keycloak        ... done\nPulling dataplatform    ... done\nPulling dataintegration ... done\nPulling cmemc           ... done\n/usr/local/bin/docker-compose  up -d\nCreating network \"dockerlocalhost_default\" with the default driver\nCreating volume \"dockerlocalhost_postgres_volume\" with default driver\nCreating volume \"dockerlocalhost_import_volume\" with default driver\nCreating volume \"dockerlocalhost_store_volume\" with default driver\nCreating dockerlocalhost_store_1       ... done\nCreating dockerlocalhost_apache2_1     ... done\nCreating dockerlocalhost_datamanager_1 ... done\nCreating dockerlocalhost_postgres_1    ... done\nCreating dockerlocalhost_cmemc_1       ... done\nCreating dockerlocalhost_keycloak_1    ... done\nCreating dockerlocalhost_dataplatform_1    ... done\nCreating dockerlocalhost_dataintegration_1 ... done\n/home/user/cmem-orchestration-v22.2.2//scripts/waitForSuccessfulStart.dist.sh\nWaiting for healthy orchestration................... done\nRemove existing bootstrap data from triple store and import shipped data from DP\nchmod a+r conf/cmemc/cmemc.ini\ndocker run -i --rm --network dockerlocalhost_default --env \"OAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\" --volume /home/user/cmem-orchestration-v22.2.2/data:/data --volume /home/user/cmem-orchest\nration-v22.2.2/conf/cmemc/cmemc.ini:/config/cmemc.ini docker-registry.eccenca.com/eccenca-cmemc:v22.2 -c cmem admin store bootstrap --import\nUpdate or import bootstrap data ... done\nmake[1]: Leaving directory '/home/ttelleis/aztest/cmem-orchestration-v22.2.2'\nCMEM-Orchestration successfully started with store graphdb-free.\nPlease open http://docker.localhost for validation.\nRun make logs to see log output\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-local-installation/#initial-login-test","title":"Initial Login / Test","text":"<p>Open your browser and navigate to\u00a0http://docker.localhost</p> <p></p> <p>Click CONTINUE WITH LOGIN and use one of these default accounts:</p> account password description <code>admin</code> <code>admin</code> Is member of the global admin group (can see and do anything) <code>user</code> <code>user</code> Is member of the local user group (can not change access conditions or see internal graphs) <p></p> <p>After successful login, you will see Corporate Memory interface. You can now proceed to the\u00a0Getting Started\u00a0section.</p>"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/","title":"Scenario: RedHat Enterprise Linux 7","text":""},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#introduction","title":"Introduction","text":"<p>This page describes a docker-compose based orchestration running on RedHat Enterprise Linux 7 (RHEL 7) inside a VirtualBox virtual machine.</p>"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#requirements","title":"Requirements","text":"<ul> <li>Virtualbox\u00a0and\u00a0vagrant\u00a0installed locally</li> <li>Terminal with ssh client installed locally</li> <li>POSIX-compatible command line interface (Linux, macOS or WSL for Windows)</li> </ul>"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#provisioning","title":"Provisioning","text":"<p>Create a working directory for this scenario and inside the working directory\u00a0<code>Vagrantfile</code>\u00a0with the following contents:</p> <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\nVagrant.configure(\"2\") do |config|\nconfig.vbguest.auto_update = false\nconfig.vbguest.no_remote = true\nconfig.vm.box = \"generic/rhel7\"\nconfig.ssh.private_key_path = File.expand_path('~/.vagrant.d/insecure_private_key')\nconfig.ssh.insert_key = false\nconfig.vm.define \"rhel7\" do |rhel7|\nrhel7.vm.network \"private_network\", ip: \"10.10.10.10\"\nrhel7.vm.hostname = \"rhel7.eccenca.local\"\nrhel7.vm.provider \"virtualbox\" do |dpvm|\ndpvm.memory = 10240\ndpvm.cpus = 4\nend\n  end\nend\n</code></pre> <p>Spin up the virtual machine:</p> <pre><code>$ vagrant up\nBringing machine 'rhel7' up with 'virtualbox' provider...\n==&gt; rhel7: Importing base box 'generic/rhel7'...\n==&gt; rhel7: Matching MAC address for NAT networking...\n==&gt; rhel7: Checking if box 'generic/rhel7' is up to date...\n==&gt; rhel7: A newer version of the box 'generic/rhel7' for provider 'virtualbox' is\n==&gt; rhel7: available! You currently have version '1.9.18'. The latest is version\n==&gt; rhel7: '2.0.6'. Run `vagrant box update` to update.\n==&gt; rhel7: Setting the name of the VM: rhel7_rhel7_1587731923819_11065\n==&gt; rhel7: Clearing any previously set network interfaces...\n==&gt; rhel7: Preparing network interfaces based on configuration...\n    rhel7: Adapter 1: nat\n    rhel7: Adapter 2: hostonly\n==&gt; rhel7: Forwarding ports...\n    rhel7: 22 (guest) =&gt; 2222 (host) (adapter 1)\n==&gt; rhel7: Running 'pre-boot' VM customizations...\n==&gt; rhel7: Booting VM...\n==&gt; rhel7: Waiting for machine to boot. This may take a few minutes...\n    rhel7: SSH address: 127.0.0.1:2222\n    rhel7: SSH username: vagrant\n    rhel7: SSH auth method: private key\n==&gt; rhel7: Machine booted and ready!\n==&gt; rhel7: Checking for guest additions in VM...\n    rhel7: The guest additions on this VM do not match the installed version of\n    rhel7: VirtualBox! In most cases this is fine, but in rare cases it can\n    rhel7: prevent things such as shared folders from working properly. If you see\n    rhel7: shared folder errors, please make sure the guest additions within the\n    rhel7: virtual machine match the version of VirtualBox you have installed on\n    rhel7: your host and reload your VM.\n    rhel7:\n    rhel7: Guest Additions Version: 5.2.30 r130521\n    rhel7: VirtualBox Version: 6.0\n==&gt; rhel7: Setting hostname...\n==&gt; rhel7: Configuring and enabling network interfaces...\n</code></pre> <p>Now you can connect to the virtual machine using\u00a0<code>~/.vagrant.d/insecure_private_key</code>\u00a0ssh key:</p> <pre><code># add vagrant ssh key to your keychain\nssh-add ~/.vagrant.d/insecure_private_key\n\n# connect to the VM\nssh vagrant@10.10.10.10\n</code></pre> <p>Info</p> <p>For username:password in curl command use the credentials to access eccenca Artifactory and docker registry.</p> <p>Install the necessary software\u00a0Inside the virtual machine and download the Corporate Memory orchestration from\u00a0releases.eccenca.com:</p> <pre><code># switch to superuser\nsudo su\n\n# Register your RHEL instance\nsubscription-manager register\nexport POOL_ID=$(subscription-manager list --available | grep \"Pool ID:\" | cut -d':' -f 2 | tr -d '[:space:]')\nsubscription-manager attach --pool=${POOL_ID}\n# enable RHEL repositories\nsubscription-manager repos --enable=rhel-7-server-rpms\nsubscription-manager repos --enable=rhel-7-server-extras-rpms\nsubscription-manager repos --enable=rhel-7-server-optional-rpms\n\n# install and start docker\nyum install docker device-mapper-libs device-mapper-event-libs\nsystemctl start docker.service\nsystemctl enable docker.service\n\n# install docker-compose\ncurl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /bin/docker-compose\nchmod +x /bin/docker-compose\n\n# Install necessary system utilities\nyum install unzip git jq\n\n# get corporate memory orchestration package\ncurl -u username https://releases.eccenca.com/docker-orchestration/cmem-orchestration-v21.11.5.zip &gt; cmem-orchestration.zip\nunzip cmem-orchestration.zip\nrm cmem-orchestration.zip\nmv cmem-orchestration-v* /opt/corporate-memory\ncd /opt/corporate-memory\ngit init &amp;&amp; git add README.md &amp;&amp; git commit -m \"init\"\n# give docker daemon access to /opt/corporate-memory directory\nchcon -Rt svirt_sandbox_file_t /opt/corporate-memory\n</code></pre> <p>Create\u00a0<code>/opt/corporate-memory/environments/prod.env</code>\u00a0file with the following contents:</p> <pre><code>#!/bin/bash linenums=\"1\"\nCMEM_SERVICE_ACCOUNT_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nSTARDOG_PASSWORD=admin\nTRUSTSTOREPASS=Aimeik5Ocho5riuC\nDEPLOYHOST=corporate.memory\n\nDI_VERSION=v20.03\nDP_VERSION=v20.03\nDM_VERSION=v20.03\nAPACHE2_VERSION=v2.6.0\nKEYCLOAK_VERSION=v6.0.1-2\nPOSTGRES_VERSION=11.5-alpine\nSTARDOG_VERSION=v7.2.0-1\n\nDATAINTEGRATION_JAVA_TOOL_OPTIONS=-Xmx2g\nDATAPLATFORM_JAVA_TOOL_OPTIONS=-Xms1g -Xmx2g\nSTARDOG_SERVER_JAVA_ARGS=-Xms1g -Xmx1g -XX:MaxDirectMemorySize=2g\n\nDEPLOYPROTOCOL=https\nPORT=443\nAPACHE_BASE_FILE=docker-compose.apache2-ssl.yml\nDATAINTEGRATION_BASE_FILE=docker-compose.dataintegration-ssl.yml\nAPACHE_CONFIG=default.ssl.conf\nPROXY_ADDRESS_FORWARDING=true\n</code></pre> <p>Login into eccenca docker registry:</p> <pre><code>docker login docker-registry.eccenca.com\n</code></pre> <p>Provide a stardog license or request a trial license:</p> <pre><code># check validity of your license\n$ make stardog-license-check\ndocker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin\nThe license is invalid: java.io.EOFException\nmake: *** [custom.dist.Makefile:5: stardog-license-check] Error 1\n# request stardog trial license\n$ make stardog-license-request\ndocker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license request --force --output /data/stardog-license-key.bin\nThank you for downloading Stardog.\nA valid license was not found in /data.\nWould you like to download a trial license from Stardog (y/N)? y\nContacting Stardog..............\nPlease provide a valid email address to start your 60-day trial (we may occasionally contact you with Stardog news):  ivan.ermilov@eccenca.com\nContacting license server...................\nEmail validated. You now have a 60-day Stardog trial license. Starting Stardog...\n                                                         %\u2584,\n                                                       \u2591\u2591\u0393\u256c\u2580\u2580\u2588\u2593\u2563\u2310\n                                                      \u2584\u2593\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2568\u2593\n                          .\u2310\u2310.                     .\u00bd\u2593\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2580\u2584\n                     \u2310\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393\u00ab\u2310              \u2264\u2591\u2593\u2588\u2588\u2588\u2593\u2593\u258c\u2584\u2591\u2591\u2591\u2591\u2591\u2593\u2592\u2588\u0393\u2310\n                .\u00bb\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2265\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2590\u2588\u2584\u2559\u2591\u2591\u2265\u2591\u2591\u2265[\u00bb.\n             \u250c\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2593\u2593\u2593\u258c\u258c\u258c\u258c\u2593\u2593\u2588\u2593\u2310\n          .\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2592\u2592\u2593\u2593\u2592\u2592\u2592\u2591\u2591\u2591\u2591\u255f\u2588\u2588\u2588\u2588\u2588\u2588\u2559 \u2514\u2588b  \u2588\u2588\u2588\u2588\u2580\u2580\u2592\u2588\u2588\u2588\u2588\u2588\u258c\n \u0393    .\u2229\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588`   \u2559   \u255f\u2588\u2593\u2229  \u2588\u2588\u2588\u2580\u2588\u2588\u258c\n\u251c\u2591, .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2591\u256b\u2588\u2588\u2588\u2588\u2588\u2588        \u2590\u2588    \u2588\u2588    \u2559\n\u251c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588          ,  '  \u2584\n \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2555    \u2590\u2588\u2584   \u2588\u2588\n.\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2584\u2591\u2591\u2591\u2591\u2591\u2584\u2563\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2588\u2588\u2591\u2591\u2563   \u2588\u2588\u2584\u2584\u2563\u2592\u2592\u258c\u2584\u2584\n \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591,  \u2514\u2559\u2580\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2566  \u2588\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u258c\n \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591       \u2559\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2593\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2580\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u256c\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n  '\u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2584\u2584`          \u2514\u2580\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2551\u2588\u2588\u2593\u2584\u2591\u2580\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u0393 \"\u2559\u2591\u2591\u2591\u2591\u2580\u2580\n    \u2559\u2591\u2591\u2584\u2563\u2593\u2593\u2588\u2588\u2580\u2580               \u2559\u2580\u2588\u2588\u2588\u258c\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2562\u2588\u2588\u2588\u2588\u2588\u2310   \u2559\u2591\u2591\u2591\u2591\u2591\u2591\u2591\n    \u2514\u2563\u2588\u2588\u2588\u2580\u2580\u2514                     \u2559\u2580\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2588\u2580       '\"\"`\n                                 .\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2560\u2593\u2588\u2588\u2588\u2580\u00b2\n                                \u00ab\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2563\u2588\u2588\u2588\u2580\n                               \u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2534\u2580\u2580\u2559\n                            .\u0393\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229`\n                \u00e1\u2580\u2580\u2555\u2584#\u258c\u2580\u2580\u2591\u2265\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2559\u2229\"\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2229`\n\u2514\u2591\u2591\u2591\u2591\u2559\u2229`\nThank you!\n\n# check the license again\n$ make stardog-license-check\ndocker run -it --rm --name stardog-license-check -v data:/data -v /opt/corporate-memory//conf/stardog/stardog-license-key.bin:/data/stardog-license-key.bin docker-registry.eccenca.com/complexible-stardog:v7.2.0-1 stardog-admin license info /data/stardog-license-key.bin\nLicensee: Stardog Trial User (ivan.ermilov@eccenca.com), Stardog Union\nVersion: Stardog *\nType:  Trial\nIssued:  Mon Mar 30 10:47:17 GMT 2020\nExpiration: 59 days\nSupport: The license does not include maintenance.\nQuantity: 3\n</code></pre> <p>Finally deploy the Corporate Memory instance:</p> <pre><code># create local truststore\nCONFIGFILE=environments/prod.env make buildTrustStore\n\n# start and bootstrap Corporate Memory\nCONFIGFILE=environments/prod.env make clean-pull-start-bootstrap\n</code></pre> <p>You have successfully deployed a Corporate Memory instance.</p>"},{"location":"deploy-and-configure/installation/scenario-redhat-enterprise-linux-7/#access-corporate-memory-instance","title":"Access Corporate Memory Instance","text":"<p>On your localhost where you are running VirtualBox, modify /etc/hosts file:</p> <pre><code>echo \"10.10.10.10 corporate.memory\" &gt;&gt; /etc/hosts\n</code></pre> <p>Open your browser and navigate to\u00a0[https://corporate.memory]https://corporate.memory</p> <p></p> <p>Click CONTINUE WITH LOGIN and use one of these default accounts:</p> account password description <code>admin</code> <code>admin</code> Is member of the global admin group (can see and do anything) <code>user</code> <code>user</code> Is member of the local user group (can not change access conditions or see internal graphs) <p></p> <p>After successful login, you will see Corporate Memory interface. You can now proceed to the\u00a0Getting Started\u00a0section.</p>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/","title":"Scenario: Single Node Cloud Installation","text":""},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#introduction","title":"Introduction","text":"<p>This page describes a docker-compose based orchestration running on a server instance accessible publicly via browser (SSL enabled via letsencrypt).</p>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#requirements","title":"Requirements","text":"<ul> <li>ssh access to a server instance\u00a0(Debian 11) with a public IP address</li> <li>A resolvable domain name to this server</li> <li>Terminal with ssh client installed locally</li> <li>An eccenca partner account for the docker registry as well as the release artifact area</li> </ul>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#server-provisioning","title":"Server Provisioning","text":"<p>In this step, you install necessary software on the server and execute the following commands as root:</p> <pre><code>$ apt-get update\n\n# install ntp and set timezone\n$ apt-get install -y ntp\n$ timedatectl set-timezone Europe/Berlin\n\n# install needed packages\n$ apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common gnupg lsb-release gettext zip unzip git make vim\n\n# install docker\n$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n$ echo   \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n$ apt-get update\n$ apt-get install docker-ce docker-ce-cli containerd.io\n\n# (optional) add a user to docker group\n# usermod -a -G docker admin\n\n# install docker-compose\n$ curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n$ chmod +x /usr/local/bin/docker-compose\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#installation","title":"Installation","text":"<p>Info</p> <p>For username and password in curl command use the credentials to access eccenca Artifactory and docker registry.</p> <p>Connect to the server and navigate to the directory with the Corporate Memory docker orchestration:</p> <pre><code># login to the eccenca docker registry\n$ docker login docker-registry.eccenca.com\n\n# download the Corporate Memory orchestration distribution\n$ cd /opt\n$ curl -u username https://releases.eccenca.com/docker-orchestration/latest.zip &gt; cmem-orchestration.zip\n\n# unzip the orchestration and move the unzipped directory to /opt/cmem-orchestration\n$ unzip cmem-orchestration.zip\n$ rm cmem-orchestration.zip\n$ mv cmem-orchestration-v* /opt/cmem-orchestration\n\n# configure git in order to commit changes to the orchestration\n$ cd /opt/cmem-orchestration\n$ git config --global user.email \"you@example.com\" &amp;&amp; git init &amp;&amp; git add . &amp;&amp; git commit -m \"stub\"\n</code></pre> <p>The Corporate Memory docker orchestration is configured with environment files.</p> <p>You will need to create an environment file at\u00a0<code>/opt/cmem-orchestration/environments/prod.env</code>. For now, you can use the provided file\u00a0<code>config.ssl-letsencrypt.env</code>\u00a0as a template.</p> <p>Info</p> <p>You need to change the lines with\u00a0DEPLOYHOST and\u00a0LETSENCRYPT_MAIL to your actual values.</p> <pre><code>$ cd /opt/cmem-orchestration/environments\n$ cp config.ssl-letsencrypt.env prod.env\n\n# change DEPLOYHOST and LETSENCRYPT_MAIL values\n$ vi prod.env\n</code></pre> <p>In addition that, you need to remove the default config and link it to your prod.env</p> <pre><code>$ cd /opt/cmem-orchestration/environments\n\n$ rm config.env\n$ ln -s prod.env config.env\n</code></pre> <p>To see all available configuration options refer to\u00a0Docker Orchestration configuration\u00a0page.</p> <p>Next, request SSL certificates from\u00a0letsencrypt\u00a0service:</p> <pre><code>$ cd /opt/cmem-orchestration\n$ make letsencrypt-create\n</code></pre> <p>Change\u00a0<code>CMEM_BASE_URI</code> according to your <code>DEPLOYHOST</code>.</p> <pre><code># update cmemc configuration\n$ rm conf/cmemc/cmemc.ini\n$ cat &lt;&lt;EOF &gt; conf/cmemc/cmemc.ini\n[cmem]\nCMEM_BASE_URI=https://corporate-memory.eccenca.dev/\nOAUTH_GRANT_TYPE=client_credentials\nOAUTH_CLIENT_ID=cmem-service-account\nOAUTH_CLIENT_SECRET=c8c12828-000c-467b-9b6d-2d6b5e16df4a\nEOF\n</code></pre> <p>Finally deploy the Corporate Memory instance:</p> <pre><code>$ make clean-pull-start-bootstrap\n$ make tutorials-import\n</code></pre> <p>Optional: you can install cmem as a systemd service for this use these commands as root oder sudo:</p> <pre><code>$ cp /opt/cmem-orchestration/conf/systemd/cmem-orchestration.service /etc/systemd/system\n$ systemctl enable cmem-orchestration\n$ systemctl start cmem-orchestration\n</code></pre>"},{"location":"deploy-and-configure/installation/scenario-single-node-cloud-installation/#validation-and-finalisation","title":"Validation and Finalisation","text":"<p>Open your browser and navigate to the host you have created in DNS server, e.g.\u00a0<code>https://corporate-memory.eccenca.dev</code></p> <p>Click CONTINUE WITH LOGIN and use one of these default accounts:</p> account password description <code>admin</code> <code>admin</code> Is member of the global admin group (can see and do anything) <code>user</code> <code>user</code> Is member of the local user group (can not change access conditions or see internal graphs) <p></p> <p>After successful login, you will see Corporate Memory interface. You can now proceed to the\u00a0 Getting Started\u00a0section.</p> <p>Do not forget to change the passwords of your deployment, especially if it is available from the public internet. For this, take a look at\u00a0Change Passwords and Keys.</p>"},{"location":"deploy-and-configure/requirements/","title":"Requirements","text":"<p>This page lists software and hardware requirements for eccenca Corporate Memory deployments. For a general overview of a deployment setup please refer to the\u00a0System Architecture.</p>"},{"location":"deploy-and-configure/requirements/#minimal-setup","title":"Minimal Setup","text":"<p>A minimal single-node deployment for testing/evaluation purposes means:</p> <ul> <li>no memory consuming linking and transformation workflows,</li> <li>nearly no concurrent users.</li> </ul> <p>Depending on how much RAM is dedicated to the triple store, Knowledge Graphs up to several million triples can be built and served.</p> <ul> <li>Operating System / Hardware<ul> <li>Bare metal server or VM with Debian based linux OS (see\u00a0Installation\u00a0for details)</li> <li>16 GB RAM</li> <li>100 GB free disk space (10 GB for docker images + data + logs over time)</li> <li>docker and docker-compose (we deliver an orchestration including all needed components)</li> </ul> </li> </ul> <p>For an example of a single-node installation refer to the following scenarios:</p> <ul> <li>Scenario: Local Installation</li> <li>Scenario: RedHat Enterprise Linux 7</li> <li>Scenario: Single Node Cloud Installation</li> </ul>"},{"location":"deploy-and-configure/requirements/#typical-setup","title":"Typical Setup","text":"<p>In a typical deployment all components are installed on separate VMs (nodes). Therefore, six separate VMs are required.</p> <p>The following numbers are based on existing customer deployments running Knowledge Graphs up to 300 million triples with 40 concurrent users.</p> Component CPU Memory eccenca DataPlatform &gt;= 4 cores<sup>1</sup> &gt;= 8 GB RAM eccenca DataIntegration &gt;= 4 cores<sup>2</sup> &gt;= 8 GB RAM<sup>2</sup> eccenca DataManager 2 cores &gt;= 2 GB RAM Triple / Quad Store &gt;= 4 cores<sup>1</sup> &gt;= 8 GB RAM<sup>3</sup> Keycloak incl. PostgeSQL<sup>4</sup> 2 cores &gt;= 4 GB RAM Proxy Server<sup>4</sup> &gt;= 2 cores<sup>1</sup> &gt;= 2 GB RAM"},{"location":"deploy-and-configure/requirements/#clients","title":"Clients","text":""},{"location":"deploy-and-configure/requirements/#browser-web-client","title":"Browser / Web Client","text":"<p>We support all (LTS/ESR) versions of the below listed browsers that are actively supported be the respective publishers:</p> <ul> <li>Microsoft Edge &gt; v88.0</li> <li>Google Chrome or Chromium &gt; v92.0</li> <li>Firefox &gt; v78.0</li> </ul> <p>Note</p> <p>Internet Explorer 11 as well as Safari Browser are not officially supported. IE11 is reported not to work.</p>"},{"location":"deploy-and-configure/requirements/#command-line-client-cmemc","title":"Command Line Client (cmemc)","text":"<p>For cmemc, currently Python 3.9 is supported, but Python 3.10 is reported to work as well.</p> <p>There is also a docker image available.</p> <ol> <li> <p>Needs to be scaled with concurrent users.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Depends on the DataIntegration workflows.\u00a0\u21a9\u21a9</p> </li> <li> <p>Needs to be scaled with the amount of triples.\u00a0\u21a9</p> </li> <li> <p>In cloud deployments, this could / will be a cloud service.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"deploy-and-configure/system-architecture/","title":"System Architecture","text":"<p>This page describes the overall system architecture of eccenca Corporate Memory and its components.</p> <p></p> <p>eccenca Corporate Memory consists of five core components:</p> <ul> <li>(2)\u00a0eccenca DataIntegration</li> <li>(3)\u00a0eccenca DataManager,</li> <li>(4)\u00a0eccenca DataPlatform,</li> <li>(8)\u00a0Keycloak, and</li> <li>(12)\u00a0cmemc (Corporate Memory Control)</li> </ul> <p>DataIntegration (2) is the Corporate Memory component which enables integration of datasets into a single consistent knowledge graph. Datasets in their original format are mapped to RDF schemata and then linked to and persisted into a knowledge graph. The data integration is performed semi-automatically based on domain-specific integration rules and vocabularies (OWL ontologies). Corporate Memory supports multiple kinds of source integration data sources (6) such as SQL databases or files of different formats. These files can be processed with DataIntegration either locally or on a remote Spark cluster (7).</p> <p>DataManager (3) is a single-page JavaScript application which enables creating and managing knowledge graphs based on established W3C standards. It is a generic data browser suitable to edit, explore and query the created knowledge graph. DataManager provides convenient options to create specific data views by using\u00a0Shapes Constraint Language\u00a0(SHACL).</p> <p>DataPlatform (4) is a semantic middleware application which provides a unified access to semantic graph data. Additionally, DataPlatform manages authorization of the users according to the access control lists defined in the Triple Store. The knowledge graph is stored in a quad store (5) connected to DataPlatform. This can either be a physical store like\u00a0Complexible Stardog,\u00a0GraphDB,\u00a0Virtuoso\u00a0or a remotely accessible SPARQL 1.1 compliant HTTP endpoint.</p> <p>Keycloak (8) provides authentication. Keycloak can act as an authentication broker for already existing, external OpenId Connect or SAML infrastructures (9). In addition to that, Keycloak supports a wide variety of internal user management configuration scenarios and the option to connect to an external LDAP server for user and group synchronization (10). Keycloak uses the embedded Java-based relational database H2 as a default to store its configuration data. However, it is highly recommended to\u00a0use a relational database\u00a0(11) for production use instead. Refer to the\u00a0Keycloak manual\u00a0for further information on possible setups.</p> <p>cmemc\u00a0(12) (Corporate Memory Control) is the eccenca Corporate Memory Command Line Interface (CLI). cmemc is intended for System Administrators and Linked Data Experts who wants to automate and remote control activities on Corporate Memory.</p>"},{"location":"develop/","title":"Develop","text":"<p>API documentation and programming recipes.</p> <p> Intended audience: Software Developers and Linked Data Experts</p> <ul> <li> <p> Java</p> <p>Accessing Graphs with Java Applications covers how to connect to Corporate Memory using a Java program.</p> </li> <li> <p> Python</p> <p>For Python developers, we offer a Plugin SDK as well as an API for accessing and manipulating Corporate Memory Instances (cmem-cmempy).</p> </li> <li> <p> OpenAPI specification</p> <p>API Specifications are available for</p> <ul> <li>DataIntegration (Build) and</li> <li>DataPlatform (Explore/Consume).</li> </ul> </li> </ul>"},{"location":"develop/accessing-graphs-with-java-applications/","title":"Accessing Graphs with Java Applications","text":"","tags":["Java"]},{"location":"develop/accessing-graphs-with-java-applications/#introduction","title":"Introduction","text":"<p>This short recipe covers how to connect to Corporate Memory using a Java program. Such program can connect to Corporate Memory at any time autonomously, independently of whether a user is logged in or not.</p>","tags":["Java"]},{"location":"develop/accessing-graphs-with-java-applications/#java-example","title":"Java Example","text":"<p>This example assumes that there is a Corporate Memory instance runnning at <code>http://docker.localhost</code>, and the programmer has access to its files. The process is very simple:</p> <ol> <li>Obtain a Bearer token.<ol> <li>Go to the file <code>cmem-orchestration/environments/config.env</code>, and get the client secret from variable <code>CMEM_SERVICE_ACCOUNT_CLIENT_SECRET</code>.</li> <li>With the client secret, connect to to the OpenID endpoint to obtain the Bearer token.</li> </ol> </li> <li>Use the Bearer token to connect to Corporate Memory, and, for example, execute a query.</li> </ol> <p>The following code provides a simple implementation of the process:</p> JavaCMEMHTTPClient.java<pre><code>package com.eccenca.cmem.client;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.apache.http.HttpEntity;\nimport org.apache.http.HttpResponse;\nimport org.apache.http.NameValuePair;\nimport org.apache.http.client.ClientProtocolException;\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClientBuilder;\nimport org.apache.http.message.BasicNameValuePair;\nimport org.apache.http.util.EntityUtils;\nimport org.json.JSONObject;\npublic class HTTPClient {\npublic static void main(String[] args) throws ClientProtocolException, IOException {\n// We assume that the Corporate Memory instance is running at docker.localhost\nString openidConnectEndpoint = \"http://docker.localhost/auth/realms/cmem/protocol/openid-connect/token\";\n// Get the client secret to obtain the bearer token from file cmem-orchestration/environments/config.env,\n// variable CMEM_SERVICE_ACCOUNT_CLIENT_SECRET\nString clientSecret = \"...\";\n// Create an HTTP Client\nCloseableHttpClient client = HttpClientBuilder.create().build();\n// POST request to obtain the bearer token for later authorization\nHttpPost httpPostToken = new HttpPost(openidConnectEndpoint);\nhttpPostToken.setHeader(\"Content-type\", \"application/x-www-form-urlencoded\");\nList &lt; NameValuePair &gt; params = new ArrayList &lt; NameValuePair &gt; ();\nparams.add(new BasicNameValuePair(\"grant_type\", \"client_credentials\"));\nparams.add(new BasicNameValuePair(\"client_id\", \"cmem-service-account\"));\nparams.add(new BasicNameValuePair(\"client_secret\", clientSecret));\nhttpPostToken.setEntity(new UrlEncodedFormEntity(params));\n// Parse the JSON response to obtain the bearer token\nHttpResponse httpResponseToken = client.execute(httpPostToken);\nHttpEntity httpEntity = httpResponseToken.getEntity();\nString responseBody = EntityUtils.toString(httpEntity);\nJSONObject obj = new JSONObject(responseBody);\nString bearerToken = \"Bearer \" + obj.getString(\"access_token\");\n// POST request to query the default SPARQL endpoint with the bearer token obtained above\nHttpPost httpPostQuery = new HttpPost(\"http://docker.localhost/dataplatform/proxy/default/sparql\");\nhttpPostQuery.setHeader(\"Accept\", \"application/sparql-results+json\");\nhttpPostQuery.setHeader(\"Content-type\", \"application/x-www-form-urlencoded\");\nhttpPostQuery.setHeader(\"Authorization\", bearerToken);\nfinal ArrayList &lt; NameValuePair &gt; postParameters = new ArrayList &lt; NameValuePair &gt; ();\npostParameters.add(new BasicNameValuePair(\"query\", \"SELECT * WHERE {?s ?p ?o} LIMIT 10\"));\nhttpPostQuery.setEntity(new UrlEncodedFormEntity(postParameters));\n// The response (variable responseBodyQuery bellow) should have some bindings:\n//  {\n//     \"head\": {\n//       \"vars\": [ \"s\" , \"p\" , \"o\" ]\n//     } ,\n//     \"results\": {\n//       \"bindings\": [\nHttpResponse httpResponseQuery = client.execute(httpPostQuery);\nHttpEntity httpEntityQuery = httpResponseQuery.getEntity();\nString responseBodyQuery = EntityUtils.toString(httpEntityQuery);\n}\n}\n</code></pre>","tags":["Java"]},{"location":"develop/cmempy-python-api/","title":"cmempy - Python API","text":"","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#introduction","title":"Introduction","text":"<p>cmempy is a Python API wrapper around the eccenca Corporate Memory HTTP APIs which can be used to rapidly script processes which interact with Corporate Memory. cmempy is also the underlying Python module which powers the cmemc - Command Line Interface.</p>","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#installation","title":"Installation","text":"<p>cmempy is published as an Apache 2 licensed open source python package at pypi.org, hence you are able to install it with a simple pip command:</p> <pre><code>$ pip install cmem-cmempy\n</code></pre>","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#configure-a-connection","title":"Configure a Connection","text":"<p>The used Corporate Memory connection is configured by providing environment variables similar to cmemc (Environment based Configuration).</p> <p>These environment variables can be created and changed in your code or used from the process which executes your python code (e.g. your shell). If you have a working cmemc file based configuration setup already you can export the environment to your shell using cmemc config eval.</p> <p>The following table lists all processed environment variables:</p> Variable Description Default Value CMEM_BASE_URI Base URL of your Corporate Memory http://docker.localhost DI_API_ENDPOINT Data Integration API endpoint CMEM_BASE_URI/dataintegration DP_API_ENDPOINT Data Platform API endpoint CMEM_BASE_URI/dataplatform OAUTH_TOKEN_URI OAuth 2.0 Token endpoint CMEM_BASE_URI/auth/realms/cmem/protocol/openid-connect/token OAUTH_GRANT_TYPE OAuth 2.0 grant type (password or client_credentials) client_credentials OAUTH_USER Username to retrieve the token admin OAUTH_PASSWORD Password to retrieve the token secret OAUTH_CLIENT_ID OAuth 2.0 client id cmem-service-account OAUTH_CLIENT_SECRET OAuth 2.0 client secret secret SSL_VERIFY Verify SSL certs for API requestsv True REQUESTS_CA_BUNDLE Path to the CA Bundle file (.pem) Internal path to included CA bundle","tags":["API","Python"]},{"location":"develop/cmempy-python-api/#commented-example","title":"Commented Example","text":"<p>Here is a commented code example how to configure and use cmempy.</p> <p>The example demonstrates, how to execute SPARQL queries on DataPlatform, as well as how to work with the DataIntegration workspace and retrieve workflow status information:</p> example_usage.py<pre><code>\"\"\"Basic example, how to use cmempy\"\"\"\nfrom os import environ\nfrom cmem.cmempy.workspace.projects.project import get_projects\nfrom cmem.cmempy.workflow import get_workflows\nfrom cmem.cmempy.workspace.activities.taskactivity import get_activity_status\nfrom cmem.cmempy.queries import SparqlQuery\n# setup the environment for the connection to Corporate Memory\nenviron[\"CMEM_BASE_URI\"] = \"http://docker.local\"\nenviron[\"OAUTH_GRANT_TYPE\"] = \"client_credentials\"\nenviron[\"OAUTH_CLIENT_ID\"] = \"cmem-service-account\"\nenviron[\"OAUTH_CLIENT_SECRET\"] = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n# this query simply lists 5 resource subjects from the triple store\nQUERY_TEXT = \"SELECT DISTINCT ?s WHERE {?s ?p ?o} LIMIT 5\"\n# the default result is a JSON structure according to the W3C standard\n# seeAlso: https://www.w3.org/TR/sparql11-results-json/\nresults = SparqlQuery(QUERY_TEXT).get_results()\nprint(results)\n# loop over project descriptions\nfor project in get_projects():\nproject_id = project[\"name\"]\nprint(\"Project: {}:\".format(project_id))\n# loop over workflow ids for a project\nfor workflow_id in get_workflows(project_id):\n# get the status object of a specific workflow\nstatus = get_activity_status(project_id, workflow_id)\nmessage = status[\"message\"]\nprint(\"- Workflow: {} ({}):\".format(workflow_id, message))\n</code></pre> <p>Starting this script should result in an output similar to this:</p> <pre><code>$ python example_usage.py\n{\n  \"head\": {\n    \"vars\": [ \"s\" ]\n  } ,\n  \"results\": {\n    \"bindings\": [\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"https://vocab.eccenca.com/dsm/\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"https://vocab.eccenca.com/dsm/ThesaurusProject\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"http://www.w3.org/2002/07/owl#\" }\n      } ,\n      {\n        \"s\": { \"type\": \"uri\" , \"value\": \"http://www.w3.org/2000/01/rdf-schema#\" }\n      }\n    ]\n  }\n}\nProject: cmem:\n- Workflow: my-workflow (Idle):\n</code></pre>","tags":["API","Python"]},{"location":"develop/dataintegration-apis/","title":"DataIntegration APIs","text":"<p>The latest OpenAPI specification is available at https://releases.eccenca.com/OpenAPI/.</p> <p>You can (re)view it with the redoc web UI or the petstore web UI</p>","tags":["API"]},{"location":"develop/dataintegration-apis/#introduction","title":"Introduction","text":"<p>eccenca DataIntegration APIs can be used to control, initiate and setup all task and activities related to the \u2605 Build step (such as datasets, transformations, linking tasks etc.).</p>","tags":["API"]},{"location":"develop/dataintegration-apis/#media-types","title":"Media Types","text":"<p>The default media type of most responses is application/json. Other possible response media types can be reached by changing the Accept header of the request.</p> <p>Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method.</p> <p>Dependent on the specific API, eccenca DataIntegration works with the following application media types which correspond to the following specification documents:</p> Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/xml XML Media Types application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/problem+json Problem Details for HTTP APIs","tags":["API"]},{"location":"develop/dataplatform-apis/","title":"DataPlatform APIs","text":"<p>The latest OpenAPI specification is available at https://releases.eccenca.com/OpenAPI/.</p> <p>You can (re)view it with the redoc web UI or the petstore web UI</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#introduction","title":"Introduction","text":"<p>eccenca DataPlatform APIs can be used to import, export, query and extract information from graphs as well as to check access conditions.</p> <p>This section describes common characteristics and features of all provided APIs.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#media-types","title":"Media Types","text":"<p>The default media type of most responses is <code>application/json</code>. Other possible response media types can be reached by changing the Accept header of the request. Alternatively, the desired response media type can be expressed in the request URI.</p> <p>Possible values of this HTTP header field are API dependent and listed as part of the specific HTTP method.</p> <p>Dependent on the specific API, eccenca DataPlatform works with the following application media types which correspond to the following specification documents:</p> Media Type Specification Document application/x-www-form-urlencoded HTML 4.01 Specification, Forms application/json The JavaScript Object Notation (JSON) Data Interchange Format application/ld+json JSON-LD 1.0 text/turtle RDF 1.1 Turtle - Terse RDF Triple Language application/n-triples RDF 1.1 N-Triples - A line-based syntax for an RDF graph application/rdf+xml RDF 1.1 XML Syntax application/n-quads RDF 1.1 N-Quads application/trig RDF 1.1 TriG application/sparql-query SPARQL 1.1 Query Language application/sparql-update SPARQL 1.1 Update application/sparql-results+json SPARQL 1.1 Query Results JSON Format application/sparql-results+xml SPARQL Query Results XML Format (Second Edition) text/csv SPARQL 1.1 Query Results CSV and TSV Formats text/tab-separated-values SPARQL 1.1 Query Results CSV and TSV Formats application/vnd.openxmlformats-officedocument.spreadsheetml.sheet Microsoft Office Excel (.xlsx) format application/problem+json Problem Details for HTTP APIs","tags":["API"]},{"location":"develop/dataplatform-apis/#media-type-request-by-uri","title":"Media type request by URI","text":"<p>The desired response media type can be requested by adding a format query parameter. The parameter value is interpreted as a media type abbreviation that is expanded to a proper media type string. eccenca DataPlatform maps the following abbreviations to media types it supports:</p> Abbreviation Media Type rdf application/rdf+xml ttl text/turtle jsonld application/ld+json nt application/n-triples trig application/trig nq application/n-quads srj application/sparql-results+json srx application/sparql-results+xml csv text/csv tsv text/tab-separated-values <p>Thus, for example, a request to <code>/proxy/default/graph</code> with the Accept header value <code>text/turtle</code> and a request to <code>/proxy/default/graph?format=ttl</code> express the same intent for the media type of the response.</p> <p>If both this format query parameter and an Accept header is present in a request, the parameter value takes precedence.</p> <p>Usage of media type request by URI can be useful to create browser links that will express an intent for the media type of the response.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#security-schemes","title":"Security Schemes","text":"<p>The default security scheme is OAuth 2.0. However, this can be changed in the configuration.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#sparql-result-set-streaming","title":"SPARQL result set streaming","text":"<p>The SPARQL proxy pipes the results of SPARQL queries directly from the underlying data endpoint to the request client. This however does not always apply for CONSTRUCT queries.</p> <p>The result of a CONSTRUCT query is a set of statements. RDF graph serialization formats tend to group the information for compactness - e.g. in Turtle, all statements for a subject are written together - avoiding subject and subject-predicate repetition, for which it is necessary to have the complete result set at disposal.</p> <p>Therefore, before sending the result to the request client, the complete result is loaded and then serialized. This creates a potential danger whenever a large result set is build and could lead to overload of the server.</p> <p>There is however one serialization format (the N-Triples format) which is streaming friendly and that should always be used whenever large result sets are expected.</p>","tags":["API"]},{"location":"develop/dataplatform-apis/#sparql-default-graph-rdf-dataset","title":"SPARQL default graph &amp; RDF dataset","text":"","tags":["API"]},{"location":"develop/dataplatform-apis/#default-graph","title":"Default graph","text":"<p>The definition of the RDF dataset of a query in the SPARQL 1.1 specification leads to problems regarding the default graph of a SPARQL service. On one hand it is defined that:</p> <p>A SPARQL query is executed against an RDF dataset which represents a collection of graphs. An RDF dataset comprises one graph, the default graph, which does not have a name, and zero or more named graphs, where each named graph is identified by an IRI.</p> <p>Furthermore, it says:</p> <p>A SPARQL query may specify the dataset to be used for matching by using the FROM clause and the FROM NAMED clause to describe the RDF dataset. If a query provides such a dataset description, then it is used in place of any dataset that the query service would use if no dataset description is provided in a query. The RDF dataset may also be specified in a SPARQL protocol request, in which case the protocol description overrides any description in the query itself. A query service may refuse a query request if the dataset description is not acceptable to the service.</p> <p>The FROM and FROM NAMED keywords allow a query to specify an RDF dataset by reference; they indicate that the dataset should include graphs that are obtained from representations of the resources identified by the given IRIs (i.e. the absolute form of the given IRI references). The dataset resulting from a number of FROM and FROM NAMED clauses is:</p> <ul> <li>a default graph consisting of the RDF merge of the graphs referred to in the FROM clauses, and</li> <li>a set of (IRI, graph) pairs, one from each FROM NAMED clause.</li> </ul> <p>If there is no FROM clause, but there is one or more FROM NAMED clauses, then the dataset includes an empty graph for the default graph.</p> <p>That means the default graph of a SPARQL service cannot be explicitly referenced in the RDF dataset of a SPARQL query using FROM / FROM NAMED.</p> <p>For this reason, DataPlatform does not allow the manipulation of the service\u2019s default graph.</p> <p>To enforce this policy, the following restriction applies to incoming SPARQL 1.1 Update queries:</p> <ul> <li>Update queries (INSERT DATA, DELETE DATA and DELETE/INSERT) targeted against the service\u2019s default graph will not be accepted by returning an HTTP 400 Bad Request status code.</li> </ul>","tags":["API"]},{"location":"develop/dataplatform-apis/#default-rdf-dataset","title":"Default RDF dataset","text":"<p>The interpretation of the RDF dataset of a query differs between various SPARQL service implementations (as shown here).</p> <p>In the case a query declares no RDF dataset, DataPlatform uses the following default RDF dataset declaration to provide a uniform behavior for all supported SPARQL services:</p> <ul> <li>The default graph is the union (RDF Merge graph) of all named graphs the user is allowed to access.</li> <li>The set of named graphs contains all named graphs the user is allowed to access.</li> </ul>","tags":["API"]},{"location":"develop/dataplatform-apis/#http-error-responses","title":"HTTP error responses","text":"<p>The default format for HTTP error responses is compliant with RFC 7807 Problem Details for HTTP APIs. An HTTP error response contains a JSON object that provides at least two fields:</p> <ul> <li><code>title</code>: A short, human-readable summary of the problem type.</li> <li><code>detail</code>: A human-readable explanation specific to this occurrence of the problem.</li> </ul> <p>The following optional non-standard fields may also be set:</p> <ul> <li><code>status</code>: The HTTP status code for this occurrence of the problem.</li> <li><code>cause</code>: The cause for this occurrence of the problem. It contains at least the same elements as specified previously, such as <code>title</code> and <code>detail</code>.</li> </ul> <p>The following example shows an HTTP response containing JSON problem details using the <code>application/problem+json</code> media type:</p> <pre><code>HTTP/1.1 500\nContent-Type: application/problem+json\n{\n\"title\": \"Internal Server Error\",\n\"status\": 500,\n\"detail\": \"Database server 'Stardog' unavailable\",\n\"cause\": {\n\"title\": \"Internal Server Error\",\n\"status\": 500,\n\"detail\": \"Connection refused (Connection refused)\"\n}\n}\n</code></pre>","tags":["API"]},{"location":"develop/python-plugins/","title":"Python Plugins","text":"<p>Beginning from version 22.1, we support the extension of DataIntegration with build plugins.</p> <p>The following pages give an overview about this feature:</p> <ul> <li> <p> Installation and Usage</p> <p>Intended for Linked Data Experts and Deployment Engineers, this page outlines how to install and use existing python plugins.</p> </li> <li> <p> Development</p> <p>Intended for Developers, this page gives an overview on the plugin concepts and how to start developing your own plugins.</p> </li> <li> <p> Setup and Configuration</p> <p>Intended for Deployment Engineers, this page discusses setup and configuration issues.</p> </li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/","title":"Python Plugin Development","text":"","tags":["Python"]},{"location":"develop/python-plugins/development/#introduction","title":"Introduction","text":"<p>Python plugins are small software projects which extend the functionality of eccenca Corporate Memory. They have its own release cycle and are not included in the main software. Python plugins can can be installed and uninstalled during runtime.</p> <p>In order to support the development of python plugins, we published a base package as well as a project template. Please have a look at these projects to get started.</p> <p>This page gives an overview of the concepts you need to understand in order to develop plugins.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#base-package","title":"Base Package","text":"<p><code>cmem-plugin-base</code> is a Python library that provides a set of base classes for developing plugins for the eccenca Corporate Memory (CMEM) platform. These base classes provide a consistent interface for defining new plugins, handling configuration, and communicating with the DataIntegration of CMEM.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin-types","title":"Plugin Types","text":"<p>The following plugin types are defined.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#workflow-plugins","title":"Workflow Plugins","text":"<p>A workflow plugin implements a new operator (task) that can be used within a workflow. A workflow plugin may accept an arbitrary list of inputs and optionally returns a single output.</p> <p>The lifecycle of a workflow plugin is as follows:</p> <ul> <li>The plugin will be instantiated once the workflow execution reaches the respective plugin.</li> <li>The <code>execute</code> function is called and gets the results of the ingoing operators as input.</li> <li>The output is forwarded to the next operator.</li> </ul> <p>The following depiction shows a task of the plugin My Workflow Plugin. The task has two connected incoming tasks and one connected outgoing task.</p> <p></p> <p>The corresponding source code of the plugin is listed below.</p> workflow.py<pre><code>from typing import Sequence\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext\nfrom cmem_plugin_base.dataintegration.description import PluginParameter, Plugin\nfrom cmem_plugin_base.dataintegration.entity import Entities\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\n@Plugin(label=\"My Workflow Plugin\")\nclass MyWorkflowPlugin(WorkflowPlugin):\n\"\"\"My Workflow Plugin\"\"\"\ndef execute(\nself, inputs: Sequence[Entities], context: ExecutionContext\n) -&gt; Entities:\nreturn inputs[0]\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#transform-plugins","title":"Transform Plugins","text":"<p>A transform plugin can be used in transform and linking rules. It accepts an arbitrary number of inputs and returns an output. Each input as well as the output consists of a sequence of values.</p> <p>The image below shows a value transformation that uses the My Transform Plugin plugin. The plugin splits the input string into a list of words and forwards only the last one.</p> <p></p> <p>The corresponding source code of the plugin is listed below.</p> transform.py<pre><code>from typing import Sequence\nfrom cmem_plugin_base.dataintegration.description import PluginParameter, Plugin\nfrom cmem_plugin_base.dataintegration.plugins import TransformPlugin\n@Plugin(label=\"My Transform Plugin\")\nclass MyTransformPlugin(TransformPlugin):\n\"\"\"My Transform Plugin\"\"\"\ndef transform(self, inputs: Sequence[Sequence[str]]) -&gt; Sequence[str]:\nfor item in inputs:\nreturn item[0].split(\" \")[-1]\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin","title":"Plugin","text":"<p>The <code>@Plugin</code> decorator is used to mark a Python class as a Workflow/Transform Operator plugin. This decorator takes several parameters that provide information about the plugin, such as the <code>label</code>, <code>plugin_id</code>, and <code>description</code>. It also allows for defining a list of <code>parameters</code> that can be used to customize the plugin behavior. The <code>documentation</code> parameter can be used to provide additional information about the plugin that will be displayed in the Workflow Editor.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin-parameter","title":"Plugin Parameter","text":"<p>The <code>PluginParameter</code> represents a parameter that can be used to customize a Workflow/Transform Operator plugin\u2019s behavior.</p> <p>The <code>PluginParameter</code> class can be instantiated multiple times within a <code>@Plugin</code> decorator to create a list of <code>parameters</code> that can be used to customize the plugin\u2019s behavior. The <code>param_type</code> parameter can be set to a specific parameter type class that extends the <code>ParameterType</code> base class to validate user input and provide additional functionality.</p> <p>The <code>PluginParameter</code> has several parameters that can be specified when initializing an instance:</p> <ul> <li><code>name</code>: The name of the parameter. This is a required parameter and must be specified.</li> <li><code>label</code>: A visible label of the parameter. This is an optional parameter and can be left blank. If left blank, the name of the parameter will be used as the label.</li> <li><code>description</code>: A visible description of the parameter. This is an optional parameter and can be left blank.</li> <li><code>param_type</code>: Optionally overrides the parameter type. Usually, this does not have to be set manually as it will be inferred from the plugin automatically.</li> <li><code>default_value</code>: The parameter default value (optional). If not specified, it will be inferred from the plugin automatically.</li> <li><code>advanced</code>: A boolean flag indicating whether or not this is an advanced parameter that can only be changed in the advanced section. This is an optional parameter and defaults to False.</li> <li><code>visible</code>: A boolean flag indicating whether or not the parameter will be displayed to the user in the UI. This is an optional parameter and defaults to True.</li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/#parameter-type","title":"Parameter Type","text":"<p>The <code>ParameterType</code> class is a generic class that serves as the base for all other parameter types. It has methods for converting parameter values to and from strings, and for providing auto-completion suggestions.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#concrete-parameter-types","title":"Concrete Parameter Types","text":"<p>There are several concrete parameter types defined in the module, including <code>StringParameterType</code>, <code>IntParameterType</code>, <code>FloatParameterType</code>, <code>BoolParameterType</code>, <code>PluginContextParameterType</code>, and <code>EnumParameterType</code>. These correspond to different types of parameters that a plugin might use, such as strings, integers, and boolean values.</p> <p>Each concrete parameter type implements the <code>from_string</code> and <code>to_string</code> methods for converting parameter values to and from strings.</p> <p><code>EnumParameterType</code> also takes an additional argument in its constructor to specify the enumeration type it represents.</p> <p>Concrete Parameters Initialization</p> <pre><code>\"\"\"Concrete Parameters Example\"\"\"\nfrom enum import Enum\nfrom typing import Sequence\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext\nfrom cmem_plugin_base.dataintegration.description import Plugin, PluginParameter\nfrom cmem_plugin_base.dataintegration.entity import (\nEntities,\n)\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\nfrom cmem_plugin_base.dataintegration.types import EnumParameterType\nclass Animal(Enum):\n\"\"\"Animal Enum\"\"\"\nCAT = 1\nDOG = 2\nHORSE = 3\nLION = 4\n@Plugin(\nlabel=\"Concrete Parameters Example\",\ndescription=\"Use of concrete parameters and set default values\",\ndocumentation=\"\"\"\n- `value_int`: A parameter of Integer Type\n- `value_float`: A parameter of Integer Type\n- `value_str`: A Parameter of String Type\n- `value_bool`: A Parameter of Boolean Type\n- `value_enum`: A Parameter of Enum Type\n\"\"\",\nparameters=[\nPluginParameter(\nname=\"value_int\",\nlabel=\"Integer\",\ndescription=\"A parameter of Integer Type\",\ndefault_value=10,\n),\nPluginParameter(\nname=\"value_float\",\nlabel=\"Float\",\ndescription=\"A parameter of Integer Type\",\ndefault_value=5.0,\n),\nPluginParameter(\nname=\"value_str\",\nlabel=\"String\",\ndescription=\"A Parameter of String Type\",\ndefault_value=\"eccenca Developer\",\n),\nPluginParameter(\nname=\"value_bool\",\nlabel=\"Boolean\",\ndescription=\"A Parameter of Boolean Type\",\ndefault_value=True,\n),\nPluginParameter(\nname=\"value_enum\",\nlabel=\"Enum\",\ndescription=\"A Parameter of Enum Type \",\nparam_type=EnumParameterType(enum_type=Animal),\ndefault_value=Animal.CAT,\n),\n],\n)\nclass ConcreteParameters(WorkflowPlugin):\n\"\"\"Example Workflow Plugin: Random Values\"\"\"\ndef __init__(\nself,\nvalue_int: int = 10,\nvalue_float: float = 5.0,\nvalue_str: str = \"eccenca Developer\",\nvalue_bool: bool = True,\nvalue_enum: Animal = Animal.CAT,\n) -&gt; None:\nself.value_int = value_int\nself.value_float = value_float\nself.value_str = value_str\nself.value_bool = value_bool\nself.value_enum = value_enum\ndef execute(self, inputs: Sequence[Entities], context: ExecutionContext) -&gt; None:\nself.log.info(\"Concrete Parameters Example\")\nself.log.info(f\"{self.value_int}\")\nself.log.info(f\"{self.value_float}\")\nself.log.info(f\"{self.value_str}\")\nself.log.info(f\"{self.value_bool}\")\nself.log.info(f\"{self.value_int}\")\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#dataintegration-parameter-types","title":"DataIntegration Parameter Types","text":"<p>In addition to concrete parameter types, the base package offers some special types that are derived from data integration. These special types include Password, Dataset, Multiline, Choice Type, and others. These types are provided to enhance the development of plugins and offer greater flexibility when creating custom parameters.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#choice-parametertype","title":"Choice ParameterType","text":"<p><code>ChoiceParameterType</code> that represents a parameter type with a pre-defined set of choices. It allows users to select from the available choices using autocompletion, and provides labels for each of the choices. Example</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#dataset-parametertype","title":"Dataset ParameterType","text":"<p><code>DatasetParameterType</code> can be used as a parameter type for plugins that require dataset input. It provides autocompletion suggestions based on the user\u2019s query terms and allows filtering datasets by dataset type (csv, json, etc.) in the current project. It also returns the label of the selected dataset as its value. Example</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#graph-parametertype","title":"Graph ParameterType","text":"<p><code>GraphParameterType</code> that represents a parameter type for selecting a knowledge graph. It provides autocompletion suggestions for available graphs, based on various filtering criteria, such as whether to show DI project graphs or system resource graphs, and which classes the graphs should belong to.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#multiline-parametertype","title":"Multiline ParameterType","text":"<p><code>MultilineStringParameterType</code> is used to represent a multiline string parameter type in a DataIntegration, which allows multiline text entry from users. Example</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#password-parametertype","title":"Password ParameterType","text":"<p><code>PasswordParameterType</code> is a parameter type that can be used in plugins to handle password strings. When a <code>password</code> is entered by a user, this parameter type will encrypt and store the password so that it cannot be viewed by the user or stored in plain text. Example</p> <p>These are some examples of special type parameters, and you can find more for your plugin development here.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#autocomplete-method","title":"AutoComplete Method","text":"<p>The <code>ParameterType</code> class also defines an <code>autocomplete()</code>, which can be used to provide auto-completion suggestions for a parameter value. The <code>EnumParameterType</code>is an example of a parameter type that uses auto-completion suggestions. This is a method that is designed to assist with autocompleting values when querying terms. It is a special method that requires attention as it plays an important role in providing suggestions for autocomplete functionality.</p> <p><code>autocomplete()</code> takes in three parameters: <code>query_terms</code>, <code>depend_on_parameter_values</code>, and <code>context</code>. It returns a list of <code>Autocompletion</code> objects, which represent the possible auto-completion results.</p> <ul> <li> <p>The <code>query_terms</code> parameter is a list of lower case conjunctive search terms. These are the search terms that the user has entered, and the <code>auto-completion()</code> will attempt to find results that match all of them.</p> </li> <li> <p>The <code>depend_on_parameter_values</code> parameter is a list of values for the parameters that the <code>auto-completion()</code> depends on. These values will be used to generate the auto-completion results. The type of each parameter value is the same as in the init method, which means that if a <code>password</code> parameter is specified, the type of the parameter value will be of <code>Password</code> Type.</p> </li> <li> <p>The <code>context</code> parameter represents the <code>PluginContext</code> in which the auto-completion is requested. This could be, for example, the context of a specific plugin, or the context of the entire system.</p> </li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/#autocompletion","title":"Autocompletion","text":"<p>The method returns a list of <code>Autocompletion</code> objects, which represent the possible auto-completion results. Each <code>Autocompletion</code> object has two attributes: value and label.</p> <ul> <li>The <code>value</code> attribute represents the value to which the parameter value should be set.</li> <li>The <code>label</code> attribute is an optional label that a human user would see instead.</li> </ul> <p>Note</p> <p><code>autocomplete()</code> should be modified to generate actual auto-completion results based on the input parameters.</p> <p>Example</p> <p><pre><code>class KaggleSearch(StringParameterType):\n\"\"\"Kaggle Search Type\"\"\"\nautocompletion_depends_on_parameters: list[str] = [\"username\", \"api_key\"]\n# auto complete for values\nallow_only_autocompleted_values: bool = True\n# auto complete for labels\nautocomplete_value_with_labels: bool = True\ndef autocomplete(\nself,\nquery_terms: list[str],\ndepend_on_parameter_values: list[Any],\ncontext: PluginContext,\n) -&gt; list[Autocompletion]:\nauth(depend_on_parameter_values[0], depend_on_parameter_values[1].decrypt())\nresult = []\nif len(query_terms) != 0:\ndatasets = search(query_terms=query_terms)\nfor dataset in datasets:\nslug = get_slugs(str(dataset))\nresult.append(\nAutocompletion(\nvalue=f\"{slug.owner}/{slug.name}\",\nlabel=f\"{slug.owner}/{slug.name}\",\n)\n)\nresult.sort(key=lambda x: x.label)  # type: ignore\nreturn result\nif len(query_terms) == 0:\nlabel = \"Search for kaggle datasets\"\nresult.append(Autocompletion(value=\"\", label=f\"{label}\"))\nresult.sort(key=lambda x: x.label)  # type: ignore\nreturn result\n</code></pre> The <code>KaggleSearch</code> class is a <code>StringParameterType</code> that allows users to search for Kaggle datasets. It inherits from the <code>StringParameterType</code> class and overrides its <code>autocomplete()</code> to provide autocompletion(of type <code>Autocompletion</code>) for search results.</p> <p>The <code>autocomplete()</code> method uses the search function to search for datasets on Kaggle and returns a list of <code>Autocompletion</code> objects representing the search results.</p> <p>The <code>autocompletion_depends_on_parameters</code> attribute of the KaggleSearch class is a list of strings that specifies which parameter values this autocomplete method depends on. In this case, it depends on the values of the <code>username</code> and <code>api_key</code> parameters in order to authenticate the Kaggle API.</p> <p>If the <code>query_terms</code> list is empty, it returns a single Autocompletion object with an empty value and a label prompting the user to search for Kaggle datasets.</p> <p>The <code>allow_only_autocompleted_values</code> attribute is set to <code>True</code>, which means that the user can only select values from the autocomplete suggestions. The autocomplete_value_with_labels attribute is set to <code>True</code>, which means that the autocomplete suggestions include both values and human-readable labels.</p> <p>This code block is derived from cmem-plugin-kaggle</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#context-objects","title":"Context Objects","text":"<p>The cmem-plugin-base package describes context objects, which are passed to the plugin depending on the executed method.</p> <p></p>","tags":["Python"]},{"location":"develop/python-plugins/development/#basic-understanding","title":"Basic Understanding","text":"<p>Context objects have been introduced to provide a way to access context-dependent functionalities during plugin creation, update, or execution.</p> <p>These context objects allow accessing various useful functionalities such as the current OAuth token, updating the execution report for workflows, DI version, and current project details.</p> <p>Note</p> <p>Having a basic understanding of context objects and their functionalities can help developers effectively use them to create and execute plugins in DataIntegration.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#system-context","title":"System Context","text":"<p>SystemContext can be used to obtain important system information. It has three methods: di_version, encrypt, and decrypt. The <code>di_version()</code> returns the version of the running DataIntegration instance. The encrypt and decrypt methods can be used to secure values using a secret key that is configured in the system. Overall, the SystemContext is useful when needing to obtain system information or encrypt/decrypt values in a secure manner.</p> <p>Example</p> <ul> <li> <p>Password Parameter Type: it is used to decrypt an encrypted password value.</p> </li> <li> <p>Example of decrypting the key.</p> </li> </ul>","tags":["Python"]},{"location":"develop/python-plugins/development/#user-context","title":"User Context","text":"<p>UserContext can be used to obtain information about the user that is interacting with the system. It has three methods: <code>user_uri()</code>, <code>user_label()</code>, and <code>token()</code>. The <code>user_uri()</code> returns the URI of the user, which can be used to identify them uniquely. The <code>user_label()</code> returns the name of the user, which can be used for display purposes. The <code>token()</code> retrieves the OAuth token for the user, which can be used to authenticate requests made on behalf of the user.</p> <p>Example</p> <p>Here: The UserContext is used to set up the cmempy user access before accessing the resource from the dataset.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#task-context","title":"Task Context","text":"<p>TaskContext can be used to obtain information about the project and task that an object is part of. The <code>project_id()</code> returns the identifier of the project, which can be used to retrieve information about the project or to associate the object with the project. The <code>task_id()</code> returns the identifier of the task, which can be used to retrieve information about the task or to associate the object with the task. This information can be used for various purposes, such as retrieving additional metadata about the project or task, or associating the object with the project or task in order to perform specific operations.</p> <p>Example</p> <p>Here is an example of how the context object can be used to retrieve the <code>project_id</code> of the current <code>task</code>.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#execution-report","title":"Execution Report","text":"<p>ExecutionReport is used to provide insight into the execution of a workflow operator. It contains important information such as the number of <code>entities</code> that have been processed, a short label and <code>description</code> of the executed operation, a <code>summary</code> table representing the summary of the report, any warnings or user-friendly messages that occurred during execution, and an <code>error</code> message in case a fatal error occurred.</p> <p>ExecutionReport is used by workflow operators to generate execution reports. This information can be used for various purposes, such as providing insight into the performance of the operator, identifying any warnings or errors that occurred during execution, and stopping the workflow execution in case a fatal error occurred. The information contained in the ExecutionReport can also be displayed in real-time in the user interface.</p> <p>Example</p> <p>Here, the execution report with the number of successful messages sent by the producer and a summary of Kafka statistics. It also includes information about the operation performed (i.e., write) and a description of the operation (i.e., messages sent), making it easier to track the progress of the process.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#report-context","title":"Report Context","text":"<p>ReportContext is used to pass context information into workflow plugins that may generate a report during execution. It contains a single method called update that can be called repeatedly during operator execution to update the current execution report. <code>update()</code> takes an instance of the ExecutionReport as input and updates the current report with the information contained in the ExecutionReport. This allows plugins to generate reports that can be used for various purposes, such as providing insight into the performance of the plugin, identifying any warnings or errors that occurred during execution, and stopping the workflow execution in case a fatal error occurred.</p> <p>Example</p> <p>Here: While producing messages in Kafka, the message count is constantly updated.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#plugin-context","title":"Plugin Context","text":"<p>PluginContext provides important context information during plugin creation or update. It has three attributes: system, user, and project_id.</p> <p>The <code>system</code> attribute is of type SystemContext and contains general system information. The <code>user</code> attribute is of type UserContext and contains information about the user. The <code>project_id</code> attribute contains the identifier of the project that contains or will contain the plugin.</p> <p>Note</p> <p>After creation, the plugin may be updated or executed by another user.</p> <p>Example</p> <p>All parameters here use the <code>PluginContext</code>, to get the context in which the param type is requested.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#execution-context","title":"Execution Context","text":"<p>ExecutionContext combines context objects that are available during plugin execution. It contains four attributes:</p> <ul> <li><code>system</code>: An instance of the SystemContext, which provides general system information.</li> <li><code>user</code>: An optional instance of the UserContext, which provides information about the user that issued the plugin execution.</li> <li><code>task</code>: An instance of the TaskContext, which provides metadata about the executed plugin.</li> <li><code>report</code>: An instance of the ReportContext, which allows to update the execution report.</li> </ul> <p>The ExecutionContext is used to provide context information to plugins during execution, enabling plugins to access information about the environment in which they are running, the user who initiated the execution, and the task being executed. The ReportContext attribute allows plugins to generate and update reports during execution.</p> <p>Note</p> <p>Here, The ExecutionContext is only available to the WorkflowPlugins and provides information and resources related to the execution environment. Plugins can use it to access/update information that may impact the report, such as logging and configuration data.</p> <p>Example</p> <p>Here is an example of how the context object can be used to retrieve the <code>project_id</code> of the current <code>task</code> and the <code>user</code> associated with the current execution.</p>","tags":["Python"]},{"location":"develop/python-plugins/development/#entities","title":"Entities","text":"<p>An <code>entity</code> is a structure to describe data objects which are passed around in workflows from one task to another task. An entity is identified by a <code>uri</code> and holds <code>values</code>, i.e., a sequence of string sequences (= a list of multi-value fields).</p> <p>Multiple entities are handled as <code>entities</code> objects, which have an attached <code>schema</code> to it.</p> <p>A <code>schema</code> contains of <code>path</code> descriptions and is identified by a <code>type_uri</code>.</p> <p>The following image shows these terms and their relationships. (1)</p> <ol> <li>The concrete implementation details of entities can be found in the entity module of the cmem-plugin-base package.</li> </ol> <p></p> Class Description <code>Entities</code> Holds a collection of entities and their schema <code>Entity</code> An Entity can represent an instance of any given concept. <code>EntitySchema</code> An entity schema that represents the type of uri and list of paths <code>EntityPath</code> A path in a schema","tags":["Python"]},{"location":"develop/python-plugins/development/#producing-entities","title":"Producing Entities","text":"<p>The following section shows the source code of a Produce Entities plugin, which is commented below.</p> entities-producer.py<pre><code>\"\"\"Entities Producer\"\"\"\nimport uuid\nfrom secrets import token_urlsafe\nfrom typing import Sequence\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext, ExecutionReport\nfrom cmem_plugin_base.dataintegration.description import Plugin, PluginParameter\nfrom cmem_plugin_base.dataintegration.entity import (\nEntities,\nEntity,\nEntitySchema,\nEntityPath,\n)\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\n@Plugin(\nlabel=\"Produce Entities\",\ndescription=\"Generates random values of X rows a Y values.\",\ndocumentation=\"\"\"\nThis example workflow operator generates random values as Entities.\nThe values are generated in X rows a Y values. Both parameter can be specified:\n- 'number_of_entities': How many rows do you need.\n- 'number_of_values': How many values per row do you need.\n\"\"\",\nparameters=[\nPluginParameter(\nname=\"number_of_entities\",\nlabel=\"Entities (Rows)\",\ndescription=\"How many rows will be created per run.\",\ndefault_value=\"10\",\n),\nPluginParameter(\nname=\"number_of_values\",\nlabel=\"Values (Columns)\",\ndescription=\"How many values are created per entity / row.\",\ndefault_value=\"5\",\n),\n],\n)\nclass EntitiesProducer(WorkflowPlugin):\n\"\"\"Entities Producer Plugin\"\"\"\ndef __init__(self, number_of_entities: int = 2, number_of_values: int = 2) -&gt; None:\nif number_of_entities &lt; 1:\nraise ValueError(\"Entities (Rows) needs to be a positive integer.\")\nif number_of_values &lt; 1:\nraise ValueError(\"Values (Columns) needs to be a positive integer.\")\nself.number_of_entities = number_of_entities\nself.number_of_values = number_of_values\ndef execute(\nself, inputs: Sequence[Entities], context: ExecutionContext\n) -&gt; Entities:\nself.log.info(\"Start creating random values.\")\nself.log.info(f\"Config length: {len(self.config.get())}\")\nentities_counter = 0\nvalue_counter = 0\nentities = []\nfor _ in range(self.number_of_entities):\nentity_uri = f\"urn:uuid:{str(uuid.uuid4())}\"\nentities_counter += 1\nvalues = []\nfor _ in range(self.number_of_values):\nvalues.append([token_urlsafe(16)])\nvalue_counter += 1\ncontext.report.update(\nExecutionReport(\nentity_count=entities_counter,\noperation=\"wait\",\noperation_desc=\"entities generated\",\n)\n)\nentities.append(Entity(uri=entity_uri, values=values))\npaths = []\nfor path_no in range(self.number_of_values):\npath_uri = f\"https://entities.org/vocab/RandomValuePath/{path_no}\"\npaths.append(EntityPath(path=path_uri))\nschema = EntitySchema(\ntype_uri=\"https://entities.org/vocab/RandomValueRow\",\npaths=paths,\n)\nself.log.info(\nf\"Happy to serve {entities_counter} entities with {value_counter} values.\"\n)\ncontext.report.update(\nExecutionReport(\nentity_count=entities_counter,\noperation=\"wait\",\noperation_desc=\"entities generated\",\nsummary=[\n(\"No. of entities\", f\"{entities_counter}\"),\n(\"No. of values\", f\"{value_counter}\"),\n],\n)\n)\nreturn Entities(entities=entities, schema=schema)\n</code></pre> <p>Code explanation:</p> <ol> <li>Provide a label, description and short documentation for the plugin. (#17-27)</li> <li>Define the parameters of the plugin. Here, two parameters are defined, where one specifies the number of <code>rows</code> and the other acthe number of <code>columns</code>. (#24-41)</li> <li>Intialise the parameters of the plugin. Additionally, you can validate and raise exceptions from <code>init()</code>. (#46-54)</li> <li>To return Entities we have to create a list of <code>entities</code> and its <code>schema</code>. As a first step, declare entities as an empty list. (#62)</li> <li>As previously mentioned, each <code>Entity</code> should have a <code>URI</code> and it can have sequence of <code>values</code>. Here, a list of entities is created with random UUIDs based on rows and values are created based on columns. After each entity is created it is appended to the entities list. (#64-78)</li> <li>To generate a <code>schema</code> (which is of type <code>EntitySchema</code>), which should have a <code>type_uri</code> and a sequence of <code>paths</code>, define an empty list of paths. (#79)</li> <li>Based on the columns, each unique path is appended to the paths list. Once all paths are added, the schema is updated with <code>type_uri</code> and <code>paths</code> respectively. (#80-86)</li> <li>Once the entities and the schema are generated you can return them. (#101)</li> <li>Update plugin logs using <code>PluginLogger</code> which is available as a default logger. (#87-91)</li> <li>To make your plugin more user-friendly you can use the Context API <code>report.update()</code> to update the workflow report. (#91-100)</li> </ol>","tags":["Python"]},{"location":"develop/python-plugins/development/#consuming-entities","title":"Consuming Entities","text":"<p>Consuming entities in a workflow plugin means that you process at least one <code>entities</code> object from the <code>inputs</code> list.</p> <p>The following code shows a plugin which loops through all inputs and counts all entities and its values.</p> entities-consumer.py<pre><code>\"\"\"Consume Entities\"\"\"\nfrom typing import Sequence\nfrom cmem_plugin_base.dataintegration.context import ExecutionContext, ExecutionReport\nfrom cmem_plugin_base.dataintegration.description import Plugin\nfrom cmem_plugin_base.dataintegration.entity import Entities\nfrom cmem_plugin_base.dataintegration.plugins import WorkflowPlugin\n@Plugin(\nlabel=\"Consume Entities\",\ndescription=\"Reads random values of X rows a Y values.\",\ndocumentation=\"\"\"\nThis example workflow operator reads random values.\n\"\"\",\n)\nclass EntitiesConsumer(WorkflowPlugin):\n\"\"\"Entities Consumer\"\"\"\ndef execute(self, inputs: Sequence[Entities], context: ExecutionContext):\nentities_counter = 0\nvalue_counter = 0\nfor item in inputs:\nfor entity in item.entities:\nentities_counter += 1\nfor _ in entity.values:\nvalue_counter += 1\ncontext.report.update(\nExecutionReport(\nentity_count=entities_counter,\noperation=\"wait\",\noperation_desc=\"entities received\",\nsummary=[\n(\"No. of entities\", f\"{entities_counter}\"),\n(\"No. of values\", f\"{value_counter}\"),\n],\n)\n)\n</code></pre> <p>Code explanation:</p> <ol> <li><code>inputs</code> from the workflow is a sequence of <code>Entities</code> with each item from the input having a list of entities and each entity having values. The entities and values are seperately counted. (#22-26)</li> <li>Once the counting is done, the workflow report is updated with the total number of entities and values as the summary. (#27-37)</li> </ol>","tags":["Python"]},{"location":"develop/python-plugins/development/#configuration","title":"Configuration","text":"<p>Plugins can have an application-wide configuration which cannot be changed on runtime and applies to all instances of this plugin.</p> <p>This plugin configuration is provided in the <code>self.config</code> PluginConfig object of the plugin. The <code>get</code> method of this object returns a JSON string of the configuration.</p> <p>Plugin configurations use the <code>plugin_id</code> as a config path in <code>dataintegration.conf</code>.</p> Example plugin configuration<pre><code>plugins.python.&lt;plugin_id&gt; = {\n    key1 = \"value1\"\n    key2 = \"value2\"\n}\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/development/#logging","title":"Logging","text":"<p>Logging should be done with the PluginLogger, which is available as <code>self.log</code> in all plugins.</p> <pre><code>self.log.info(\"Successfully executed Workflow Plugin\")\n</code></pre> <p>On runtime, this logger will be replaced with a JVM based logging function feeding the plugin logs to the normal DataIntegration log stream. This JVM-based logger will prefix all plugin logs with <code>plugins.python.&lt;plugin id&gt;</code>.</p>","tags":["Python"]},{"location":"develop/python-plugins/installation/","title":"Installation and Usage of Python Plugins","text":"<p>Plugins are a released as parts of Python packages. They can but do not need to be open-source and published on pypi.org (a widely used Python Package Index). One package can contain of multiple plugins.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/installation/#installation","title":"Installation","text":"<p>If you want to install a python plugin package, you need to use cmemc\u2019s admin workspace python command group.</p> <p>The following shell commands demonstrate the basic workflow:</p> Install a plugin package from pypi.org:<pre><code>$ cmemc admin workspace python install cmem-plugin-graphql\nInstall package cmem-plugin-graphql ... done\n</code></pre> List installed plugins:<pre><code>$ cmemc admin workspace python list-plugins\nID                                 Type            Label\n---------------------------------  --------------  -------------\ncmem_plugin_graphql-GraphQLPlugin  WorkflowPlugin  GraphQL query\n</code></pre> <p>You can get a list of all installed python packages: (1)</p> <ol> <li>This list contains all installed packages in the python environment, not just your plugin packages.</li> </ol> List all installed python packages:<pre><code>$ cmemc admin workspace python list\nName                Version\n------------------  -----------\ncertifi             2022.5.18.1\ncharset-normalizer  2.0.12\ncmem-cmempy         22.1.1\ncmem-plugin-base    1.2.0\nidna                3.3\nisodate             0.6.1\njep                 4.0.2\npip                 20.3.4\npyparsing           3.0.9\nrdflib              6.1.1\nrequests            2.27.1\nrequests-toolbelt   0.9.1\nsetuptools          52.0.0\nsix                 1.16.0\nurllib3             1.26.9\nwheel               0.34.2\n</code></pre> <p>You also can (un-)install packages in a specific version or from a source distribution file. Please have a look at the admin workspace python command group for a complete documentation of the package / plugin commands.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/installation/#usage","title":"Usage","text":"<p>Depending on the plugin type, an installed plugin can appear in different parts of the Build workbench:</p> <p></p> <p>Workflow Plugins are listed in the Create new item dialog in the Task category. From there, you can create a task in your project and use it in a workflow.</p> <p></p> <p>Transform Plugins are listed in the sidebar of the Value formula editor and the Linking editor in the  Transform tab. Drag and drop it on the canvas, and connect it with ingoing and / or outgoing links to other elements.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/installation/#known-issues","title":"Known Issues","text":"<p>Sharing the <code>PYTHONPATH</code> via NFS</p> <p>The <code>PYTHONPATH</code> is the location where the installed python plugins and their dependencies are stored for DataIntegration. When you share this path via NFS, there are issues with open file locks which will sometimes break the plugin installation requests. A workaround for this is to install or upgrade plugins right after rebooting DataIntergration (and before you start a workflow that uses a python plugin). We currently advise against using NFS on this path directly. The problem is known by the python community but there is no fix or workaround available yet.</p>","tags":["Python","Plugin"]},{"location":"develop/python-plugins/setup/","title":"Setup and Configuration","text":"<p>This section describes which backend components are needed on the DataIntegration server.</p> <p>Info</p> <p>When using the official eccenca docker images, setup and configuration is already done.</p>","tags":["Python"]},{"location":"develop/python-plugins/setup/#configuration","title":"Configuration","text":"<p>The following DataIntegration configuration section describes how to setup and enable the Python Plugin system.</p> <pre><code>#################################################\n# Plugin Configuration\n#################################################\n\n# this (optional) file can be used to hold python plugin specific configuration\ninclude \"python-plugins.conf\"\n\ncom.eccenca.di.scripting = {\n  python = {\n    PythonPluginRegistry = {\n      # Python plugins will only be loaded if 'enabled' is set to true.\n      enabled = true\n\n      # Plugins will only be loaded below the following base package.\n      basePackage = \"cmem\"\n    }\n\n    PythonPackageManager = {\n      # Python package installer executable.\n      # pipExecutable = \"pip\"\n      pipExecutable = \"cmem-pip-wrapper.sh\"\n    }\n  }\n}\n</code></pre>","tags":["Python"]},{"location":"develop/python-plugins/setup/#python-interpreter","title":"Python Interpreter","text":"<p>An installation of the CPython distribution (at least version 3.3) is required. Although other distributions, such as Anaconda, should work as well, only CPython is officially supported.</p>","tags":["Python"]},{"location":"develop/python-plugins/setup/#java-embedded-python-jep","title":"Java Embedded Python (Jep)","text":"<p>The Jep package needs to be installed.</p> <p>The libraries contained in the Jep module need to be accessible from the Java Virtual Machine running DataIntegration. This can be achieved by setting an environment variable to the directory path where the Jep module is located:</p> <ul> <li> Linux: set <code>LD_LIBRARY_PATH</code>.</li> <li> OS X: set <code>DYLD_LIBRARY_PATH</code>.</li> <li> Windows: set <code>PATH</code>.</li> </ul> <p>For alternative installation methods, visit </p>","tags":["Python"]},{"location":"explore-and-author/","title":"Explore and Author","text":""},{"location":"explore-and-author/#explore-and-author","title":"Explore and Author","text":"<p>In the Explore section you will learn how Corporate Memory allows you to interact with your Enterprise Knowledge Graph. All relevant modules and functionalities are described. You will also learn how we make use of\u00a0SHACL Shapes\u00a0in order to\u00a0customize the way how you can interact with your data.</p> <p> Intended audience: Linked Data Experts and Domain Experts</p> <ul> <li> <p> Graph Exploration</p> <p>This module provides a generic and extensible RDF data browser and editor.</p> </li> <li> <p> Vocabulary Catalog</p> <p>This module allows for managing vocabularies in Corporate Memory that are accessible for the user.</p> </li> <li> <p> Thesauri Management</p> <p>The Thesaurus module provides a user interface to create, browse and edit thesaurus resources and general taxonomical data modeled in SKOS.</p> </li> <li> <p> Query Module</p> <p>The\u00a0Query\u00a0module provides a user interface to store, describe, search and edit SPARQL queries.</p> </li> </ul>"},{"location":"explore-and-author/easynav-module/","title":"EasyNav Module","text":"<p>Info</p> <p>EasyNav is a preview feature, which is disabled by default. Preview features can be unstable.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#introduction","title":"Introduction","text":"<p>This feature allows for the visual exploration of Knowledge Graphs. It allows to save and share explorations. Furthermore, sophisticated individual search settings (filter presets) can be created and configured per workspace.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#usage","title":"Usage","text":"<p>If enabled, content of Knowledge Graphs can be explored in a visual way, rendering nodes and edges and allowing the user to expand along the relationships between the nodes.</p> <p>Start using <code>EasyNav</code> by selecting the respective module entry in the main navigation.</p> <p></p> <p>At the module welcome screen the user can either load a saved visualization of start searching for an initial node / resource by providing a search term.</p> <p>Note</p> <p>The graph selection drop-down might or might not be visible depending the existence of an (optional) <code>EasyNav Module</code> configuration. In case no specific module configuration exists or non has not has been set for the current workspace the graph selection will be shown. A <code>EasyNav Module</code> configuration pre-configures a graph. Thus, the dropdown will not be shown if such has been configured for the current workspace.</p> <p></p> <p>Enter a search term to populate the result list. Click a result to start the visual graph exploration.</p> <p></p> <p>The exploration starts with the selected node (or a saved exploration). The nodes can further be expanded along the relationships that exist to other resources. Therefore, click the node expansion button on the right side of a node (the point where the arrows originate in the screenshot below).</p> <p></p> <p>Any expanded resource / node can be added to the current exploration by double-clicking the node. Clicking anywhere on the empty canvas will close the relationship dialog and retain the added nodes and their relationships only.</p> <p></p> <p>Click  on a node to see literal values related to this resource  closes the details again.</p> <p><code>Save</code> allows to save an exploration,  will start a new exploration while  allows to open any previously saved exploration.</p> <p></p> <p>The <code>Visualization catalog</code> dialog shows the saved exploration and allows to  open,  delete or to  copy the link to the exploration.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#setup","title":"Setup","text":"<p>This feature needs to be initially enabled in the DataManager configuration file (<code>application.yml</code>). See configuration of EasyNav module for details how to enable it.</p> <p>Without further (workspace) specific configuration the feature can be used asking for the graph that shall be explored every time a new exploration is started.</p> <p>Optionally a <code>EasyNav Module</code> configuration can be created to provide a fixed graph selection and search filter settings.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#create-a-easynav-module-configuration","title":"Create a EasyNav Module Configuration","text":"<p>In the <code>Knowledge Graphs</code> module navigate to the <code>CMEM Configuration</code> graph.</p> <p>Select the class <code>EasyNav Module</code> and <code>Create a new \"EasyNav Module\"</code>.</p> <p></p> <p>Provide a <code>Name</code> for your configuration and select the <code>Default Graph</code> which contains the nodes you want to explore visually. This graph can of course be an integration graph.</p> <p><code>Search Configuration</code> is optional but a powerful feature to create predefined search filter/facets. If want to use this capability select existing <code>Search Configuration</code>s in the drop down or create stubs for the configurations you want to setup.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#set-the-easynav-module-in-the-workspace-configuration","title":"Set the EasyNav Module in the Workspace configuration","text":"<p>After creating the <code>EasyNav Module</code> configuration it need to be selected in workspace configuration(s) that shall be using it.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#create-a-search-configuration","title":"Create a Search Configuration","text":"<p>Follow the stub link from creating a new configuration in the <code>Module</code> dialog. Then click edit to provide the necessary details.</p> <p></p> <p>At least a <code>Name</code> and <code>Search Weight</code> need to be specified. The weight can be used to boost the results of one search configuration over another in case multiple <code>Search Configuration</code>s are used.</p> <p><code>Graph Resource Patter</code> are a topic on its own and explained here.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/#technical-background","title":"Technical Background","text":"<p><code>Search Configuration</code>s will be cumulatively executed when search terms are provided. Which means each additional <code>Search Configuration</code> increases the time to produce results.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/","title":"GraphResourcePattern","text":"<p>This chapter specifies the JSON object used to provide a EasyNav Search (query) configuration. The <code>GraphResourcePattern</code> is part of an optional advanced configuration of the EasyNav module. It can be used to provide search filter / facets in order to tailor the search results presented to the end user.</p> <p>The <code>GraphResourcePattern</code> object reference is provided in different ways depending on your preferences:</p> Type ScriptJSON Schemajson-schema-for-humans <pre><code>export type GraphResourcePattern = {\npaths: Path[];\npathFilters?: PathVariableFilter[];\n};\nexport type Path = {\nsubjectVarName?: string;\nobjectVarName?: string;\ninverted: boolean;\npredicate: string;\n};\nexport type PathVariableFilter = {\nvarname?: string;\nvarIsAnyOneOfResource?: string[];\nvarIsAnyOneOfLiteral?: Literal[];\nisNoneOfResource?: string[];\nisNoneOfLiteral?: Literal[];\nliteralFilters?: PathVariableLiteralFilter[];\n};\nexport type Literal = {\nvalue: string;\nlang?: string;\ndatatype?: string;\n};\nexport type PathVariableLiteralFilter = {\noperation:\n| \"GreaterThan\"\n| \"LessThan\"\n| \"GreaterEqualsThan\"\n| \"LessEqualThan\"\n| \"NotEquals\"\n| \"Contains\"\n| \"Regex\";\nvalue: Literal;\n};\n</code></pre> <pre><code>{\n\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\"title\" : \"GraphResourcePattern\",\n\"description\" : \"Description of the GraphResourcePattern JSON data structure\",\n\"properties\": {\n\"pathFilters\": {\n\"items\": {\n\"properties\": {\n\"isNoneOfLiteral\": {\n\"items\": {\n\"properties\": {\n\"datatype\": {\n\"type\": \"string\"\n},\n\"lang\": {\n\"type\": \"string\"\n},\n\"value\": {\n\"type\": \"string\"\n}\n},\n\"type\": \"object\"\n},\n\"type\": \"array\"\n},\n\"isNoneOfResource\": {\n\"items\": {\n\"type\": \"string\"\n},\n\"type\": \"array\"\n},\n\"literalFilters\": {\n\"items\": {\n\"properties\": {\n\"operation\": {\n\"enum\": [\n\"Contains\",\n\"GreaterEqualsThan\",\n\"GreaterThan\",\n\"LessEqualThan\",\n\"LessThan\",\n\"NotEquals\",\n\"Regex\"\n],\n\"type\": \"string\"\n},\n\"value\": {\n\"properties\": {\n\"datatype\": {\n\"type\": \"string\"\n},\n\"lang\": {\n\"type\": \"string\"\n},\n\"value\": {\n\"type\": \"string\"\n}\n},\n\"type\": \"object\"\n}\n},\n\"type\": \"object\"\n},\n\"type\": \"array\"\n},\n\"varIsAnyOneOfLiteral\": {\n\"items\": {\n\"properties\": {\n\"datatype\": {\n\"type\": \"string\"\n},\n\"lang\": {\n\"type\": \"string\"\n},\n\"value\": {\n\"type\": \"string\"\n}\n},\n\"type\": \"object\"\n},\n\"type\": \"array\"\n},\n\"varIsAnyOneOfResource\": {\n\"items\": {\n\"type\": \"string\"\n},\n\"type\": \"array\"\n},\n\"varname\": {\n\"type\": \"string\"\n}\n},\n\"type\": \"object\"\n},\n\"type\": \"array\"\n},\n\"paths\": {\n\"items\": {\n\"properties\": {\n\"inverted\": {\n\"type\": \"boolean\"\n},\n\"objectVarName\": {\n\"type\": \"string\"\n},\n\"predicate\": {\n\"type\": \"string\"\n},\n\"subjectVarName\": {\n\"type\": \"string\"\n}\n},\n\"type\": \"object\"\n},\n\"type\": \"array\"\n}\n},\n\"type\": \"object\"\n}\n</code></pre> <p>The following section are a human friendly representation generated using json-schema-for-humans</p> <p>Title: GraphResourcePattern</p> Type <code>object</code> Required No Additional properties [Not allowed] Defined in #/definitions/GraphResourcePattern <p>Description: Description of the GraphResourcePattern JSON data structure</p> Property Pattern Type Deprecated Definition Title/Description - pathFilters No array No - - + paths No array No - - <p>An concrete example object is shown here:</p> <p>Example <code>GraphResourcePattern</code></p> <pre><code>{\n\"paths\": [\n{\n\"subjectVarName\": \"subResource\",\n\"objectVarName\": \"resource\",\n\"predicate\": \"http://example.com/vocab/hasParent\"\n},\n{\n\"subjectVarName\": \"resource\",\n\"predicate\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\n\"objectVarName\": \"class\"\n}\n],\n\"pathFilters\": [\n{\n\"varname\": \"class\",\n\"varIsAnyOneOfResource\": [\n\"http://example.com/vocab/Company\"\n]\n}\n]\n}\n</code></pre> <p>A valid configuration must use a <code>subjectVarName</code> called <code>resource</code>. This is the binding that yields results.</p> <p>This configuration produces the following result, it only shows results where:</p> <ul> <li><code>resource</code> is of type <code>http://example.com/vocab/Company</code></li> <li>a <code>subResource</code> exists which is related to <code>resource</code> via the <code>http://example.com/vocab/hasParent</code> property</li> </ul>","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#pathfilters","title":"pathFilters","text":"Type <code>array</code> Required No Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description PathVariableFilter -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#pathvariablefilter","title":"PathVariableFilter","text":"Type <code>object</code> Required No Additional properties [Not allowed] Defined in #/definitions/PathVariableFilter Property Pattern Type Deprecated Definition Title/Description - isNoneOfLiteral No array No - - - isNoneOfResource No array of string No - - - literalFilters No array No - - - varIsAnyOneOfLiteral No array No - - - varIsAnyOneOfResource No array of string No - - - varname No string No - -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#isnoneofliteral","title":"isNoneOfLiteral","text":"Type <code>array</code> Required No Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description Literal -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#literal","title":"Literal","text":"Type <code>object</code> Required No Additional properties [Not allowed] Defined in #/definitions/Literal Property Pattern Type Deprecated Definition Title/Description - datatype No string No - - - lang No string No - - + value No string No - -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#datatype","title":"datatype","text":"Type <code>string</code> Required No","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#lang","title":"lang","text":"Type <code>string</code> Required No","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#value","title":"value","text":"Type <code>string</code> Required Yes","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#isnoneofresource","title":"isNoneOfResource","text":"Type <code>array of string</code> Required No Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description isNoneOfResource items -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#isnoneofresource-items","title":"isNoneOfResource items","text":"Type <code>string</code> Required No","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#literalfilters","title":"literalFilters","text":"Type <code>array</code> Required No Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description PathVariableLiteralFilter -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#pathvariableliteralfilter","title":"PathVariableLiteralFilter","text":"Type <code>object</code> Required No Additional properties [Not allowed] Defined in #/definitions/PathVariableLiteralFilter Property Pattern Type Deprecated Definition Title/Description + operation No enum (of string) No - - + value No object No Same as pathFilters_items_isNoneOfLiteral_items -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#operation","title":"operation","text":"Type <code>enum (of string)</code> Required Yes <p>Must be one of:</p> <ul> <li>\u201cGreaterThan\u201d</li> <li>\u201cLessThan\u201d</li> <li>\u201cGreaterEqualsThan\u201d</li> <li>\u201cLessEqualThan\u201d</li> <li>\u201cNotEquals\u201d</li> <li>\u201cContains\u201d</li> <li>\u201cRegex\u201d</li> </ul>","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#value_1","title":"value","text":"Type <code>object</code> Required Yes Additional properties [Not allowed] Same definition as pathFilters_items_isNoneOfLiteral_items","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#varisanyoneofliteral","title":"varIsAnyOneOfLiteral","text":"Type <code>array</code> Required No Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description Literal -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#literal_1","title":"Literal","text":"Type <code>object</code> Required No Additional properties [Not allowed] Same definition as pathFilters_items_isNoneOfLiteral_items","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#varisanyoneofresource","title":"varIsAnyOneOfResource","text":"Type <code>array of string</code> Required No Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description varIsAnyOneOfResource items -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#varisanyoneofresource-items","title":"varIsAnyOneOfResource items","text":"Type <code>string</code> Required No","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#varname","title":"varname","text":"Type <code>string</code> Required No","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#paths","title":"paths","text":"Type <code>array</code> Required Yes Array restrictions Min items N/A Max items N/A Items unicity False Additional items False Tuple validation See below Each item of this array must be Description Path -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#path","title":"Path","text":"Type <code>object</code> Required No Additional properties [Not allowed] Defined in #/definitions/Path Property Pattern Type Deprecated Definition Title/Description + inverted No boolean No - - - objectVarName No string No - - + predicate No string No - - - subjectVarName No string No - -","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#inverted","title":"inverted","text":"Type <code>boolean</code> Required Yes","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#objectvarname","title":"objectVarName","text":"Type <code>string</code> Required No","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#predicate","title":"predicate","text":"Type <code>string</code> Required Yes","tags":["Reference"]},{"location":"explore-and-author/easynav-module/GraphResourcePattern/#subjectvarname","title":"subjectVarName","text":"Type <code>string</code> Required No <p>Generated using json-schema-for-humans on 2022-11-10 at 21:15:54 +0100</p>","tags":["Reference"]},{"location":"explore-and-author/embedding-services-via-the-integrations-module/","title":"Embedding Services via the Integrations Module","text":"<p>A DataManager module is available that can be used to embed / integrate other web-services in Corporate Memory. The module can be used and configured globally or individually per workspace configuration.</p> <p></p>","tags":["Dashboards"]},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#activation-and-configuration-in-datamanager","title":"Activation and configuration in DataManager","text":"<p>In order to use it you need to add respective configuration section(s) into the DataManager\u00a0<code>application.yml</code>\u00a0configuration file.</p> <p>This is a sample for a global configuration that enables the module and shows two services:</p> <pre><code>js.config.modules.integrations:\nname: \"INTEGRATIONS\"\nenable: true\ntabs:\n-\nname: \"Service One\"\nurl: \"https://one.eccenca.com/service-one\"\n-\nname: \"Service Two\"\nurl: \"https://two.eccenca.com/service-two\"\n</code></pre> <p>The\u00a0<code>name</code>\u00a0properties can be customized. Important is that the module is enabled (\u201c<code>enable: true</code>\u201d) in at least one place (globally or in a certain workspace) in order to be shown.</p> <p>Warning</p> <p>In case your Corporate Memory Instance is served via HTTPS, no HTTP services can be used due to browser security limitations.</p> <p>You can redefine all or parts of the configuration per workspace, e.g. in order to disable the module in a specific workspace add \u201c<code>modules.integrations.enable: false</code>\u201d to the configuration of the respective workspace.</p> <p>A restart of DataManager will be required in order for the configuration change to become effective.</p>","tags":["Dashboards"]},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#link-configuration-in-dataintegration","title":"Link Configuration in DataIntegration","text":"<p>The (module) link configuration in DataIntegration is managed in its own configuration. Thus, the following snippet from a\u00a0<code>dataintegration.conf</code>\u00a0 shows how to add the \u201cINTEGRATINOS\u201d link to the DataIntegrations menu:</p> <pre><code>eccencaDataManager.moduleLinks = [\n{\npath = \"explore\"\ndefaultLabel = \"Knowledge Graphs\"\nicon = \"application-explore\"\n},\n{\npath = \"vocab\"\ndefaultLabel = \"Vocabularies\"\nicon = \"application-vocabularies\"\n},\n{\npath = \"thesaurus\"\ndefaultLabel = \"Thesauri\"\nicon = \"module-thesauri\"\n},\n{\npath = \"query\"\ndefaultLabel = \"Queries\"\nicon = \"application-queries\"\n},\n{\npath = \"integrations\"\ndefaultLabel = \"INTEGRATIONS\"\nicon = \"module-integrations\"\n}\n]\n</code></pre> <p>Note</p> <p>The \u201c<code>name\"</code>\u00a0and \u201c<code>defaultLabel</code>\u201d property should be aligned in the DataManager and DataIntegration configuration for consistency.</p> <p>A restart of DataIntegration will be required in order for the configuration change to become effective.</p>","tags":["Dashboards"]},{"location":"explore-and-author/embedding-services-via-the-integrations-module/#redash-dashboard-integration","title":"(Redash) Dashboard Integration","text":"<p>A typical (eccenca) use case for the Integrations Module is to embed redash dashboards. In order show a dashboard in a Corporate Memory make sure your redash instance use the same protocol as your Corporate Memory instance (typically https). Then open the dashboard that should be embedded and click the sharing button\u00a0. In the dialog make sure \u201cAllow public access\u201d is enabled. Copy the \u201cSecret address\u201d and paste this address into the \u201c<code>url</code>\u201d property of a tab configuration, as shown above.</p> <p></p>","tags":["Dashboards"]},{"location":"explore-and-author/graph-exploration/","title":"Graph Exploration","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#introduction","title":"Introduction","text":"<p>The\u00a0Explore\u00a0module provides a generic and extensible RDF data browser and editor. Use this\u00a0module to browse through your resources, to change between list and detail views and to edit resources.</p> <p>To open the\u00a0Explore\u00a0module, click\u00a0 Knowledge Graphs in the main menu.</p> <p>The user interface of the\u00a0Explore\u00a0module shows the following main areas:</p> <ul> <li>the header area, showing selected elements, possible actions (e.g.  create or  remove resource), a  Go to resource input field, and a  user menu</li> <li>the navigation area, showing the Graphs and the Navigation structures, (1)</li> <li>the main area, providing multiple views, depending on which resource has been selected.</li> </ul> <ol> <li>If necessary, you can toggle the\u00a0navigation area by using the      (hide) and  (show) buttons.</li> </ol>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#graphs","title":"Graphs","text":"<p>The\u00a0Graphs\u00a0box shows lists of graphs you have access to. To select a graph click the graph name in the\u00a0Graphs\u00a0box. The navigation structure of the selected graph is displayed in the Navigation box below. In the main area, the\u00a0Metadata\u00a0view of the selected graph appears, showing several tabs with metadata information.</p> <p>Note</p> <p>This default categorization is just a suggestion and can be modified by changing the workspace configuration in the CMEM Configuration graph.</p> <p>The Graphs are categorized into groups as follows:</p> <ul> <li>User: All graphs which represent user data (created manually or by build processes).\u00a0</li> <li>Vocabularies:\u00a0All graphs containing vocabularies.</li> <li>System: All graphs containing configuration data.</li> <li>All</li> </ul> <p>You can search for a specific graph with  Search.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#adding-a-new-graph","title":"Adding a new graph","text":"<p>To add a new graph to the\u00a0Graphs\u00a0list:</p> <ul> <li>Click\u00a0 Add new graph. A dialog appears.</li> <li>Select a graph type. (1)</li> <li>Provide a name and enter the graph URI (e.g.\u00a0<code>https://ns.eccenca.com</code>).</li> <li>Click Next and provide metadata (different types, require different metadata to enter).</li> <li>Click\u00a0Save\u00a0to create the new graph. </li> </ul> <ol> <li>More concrete, you select a shape here.      This can be configured in the workspace configuration as well.</li> </ol>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#downloading-a-graph","title":"Downloading a graph","text":"<p>To download a graph from the\u00a0Graphs\u00a0list:</p> <ul> <li>In the\u00a0Graphs\u00a0list, click  Download graph on the graph you want to download.</li> <li>A message box appears, stating that downloading can take a long time.</li> <li>Click\u00a0Download.</li> </ul>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#managing-a-graph","title":"Managing a graph","text":"<p>Use this function to add or replace data in the a graph.</p> <p>To update or replace data of a graph:</p> <ul> <li>In the\u00a0Graphs\u00a0box, select  Manage graph on the graph you want to update or replace.</li> <li>A dialog box appears.</li> <li>Click\u00a0Choose file\u00a0to upload a file containing the new or updated data. (1)</li> <li>Choose one of the following options:</li> <li>Update: add uploaded data to Graph.</li> <li>Replace: clear Graph and add uploaded data.</li> <li>Click\u00a0Update\u00a0to start the upload process.</li> </ul> <ol> <li>You can upload one of the following file formats: Turtle, N-Triples, RDF/XML, or JSON-LD.</li> </ol> <p>To delete a graph, select  Remove graph on the graph you want to remove and confirm deletion process.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#navigation","title":"Navigation","text":"<p>When a graph is selected in the\u00a0Graphs\u00a0box, the structure of the graph is displayed in the\u00a0Navigation\u00a0box. By default, only the top classes of the graph are listed. An arrow\u00a0( Open tree)\u00a0indicates that a class has subclasses. Click the arrow to show the subclasses.</p> <p>Use the\u00a0 Search\u00a0field of the\u00a0Navigation\u00a0box to search for an item in the navigation structure of the graph. Enter a keyword and press\u00a0Enter\u00a0to start the search. To reset the results delete the keyword and press\u00a0Enter.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#instance-list-of-a-class","title":"Instance List of a class","text":"<p>Select a class in the\u00a0Navigation\u00a0box to show all instances of this class in the main area. (1)</p> <ol> <li>The table uses a default query to list all resources with a given class.      This can be configured by adding a <code>shui:navigationListQuery</code> to the class shape.</li> </ol>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#instance-details","title":"Instance Details","text":"<p>To open the\u00a0Instance Details\u00a0of a resource click on that resource in the\u00a0Instance List. Resources are shown as grey chip buttons.</p> <p></p> <p>Warning</p> <p>When you remove the resource, all triples related to that resource are deleted as well.</p> <p>Select  Remove resource\u00a0on the upper right corner to remove the resource. A dialog box appears where you are asked to confirm the operation.</p> <p>In the Instance Details view, multiple horizontal tabs provide different views in the resource. The availability of these views depends on the context and the resource type.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#resource","title":"Resource","text":"<p>The\u00a0Resource\u00a0tab provides a view based on the shapes of the selected resource. The details of the shaped view depends on the configuration.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#properties","title":"Properties","text":"<p>The Properties tab shows all properties and objects of the selected resource independently from a shape selection.</p> <p>Use the icons on the right side to edit or delete properties. Use\u00a0SHOW IN LIST\u00a0to display objects in a list view. Select\u00a0ADD\u00a0to add a new value as an object to a property. In the dialog box, select the type from the drop-down list and enter the value. Click\u00a0SAVE\u00a0to save your changes.</p> <p>To add a new property select  Add Property . In the dialog box, enter a property, select the value type from the drop-down list and enter a value. Click\u00a0SAVE\u00a0to save your changes.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#statistics","title":"Statistics","text":"<p>Note</p> <p>This view is only available on graphs (not on resources).</p> <p>The\u00a0Statistics\u00a0tab indicates the number of classes, properties, entities and triples of the graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#graph","title":"Graph","text":"<p>Note</p> <p>This view is only available on graphs (not on resources).</p> <p>The\u00a0Graph\u00a0tab shows a visual graph representation of the graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#vocab","title":"Vocab","text":"<p>Note</p> <p>This view is only available on graphs (not on resources).</p> <p>This tab shows a graph visualization of an installed vocabulary. It displays all classes showing the class-subclass.\u00a0 You can open the class details and view the list of instances related to that class. It also allows you to\u00a0copy the resource IRI.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#references","title":"References","text":"<p>This tab shows all resources that link back to the selected resource.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/#turtle","title":"Turtle","text":"<p>This tab shows the turtle RDF representation of the raw data representing the resource. You can use this tab to edit the selected resource:</p> <ul> <li>Enter your changes in turtle.</li> <li>Click\u00a0UPDATE\u00a0to save your changes.</li> </ul> <p>Deleting the entire turtle representation deletes the resource.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/","title":"Building a customized User Interface","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#introduction","title":"Introduction","text":"<p>Working with shapes allows for creation of a customized Linked Data user interface. In addition to the standard PROPERTIES tab that shows all properties of a data resource, you can create custom \u201cform\u201d-like data interfaces. These configurable forms allow for a cleaner interface to view and author data resources. In addition, they enable integration of data from other resources that are linked to the current resource, creating a more concise view on your data.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#defining-forms","title":"Defining forms","text":"<p>You can define forms using SHACL rules. The rules state:</p> <ol> <li>What types of resources the form definition applies to. This is based on the\u00a0<code>rdf:type</code>\u00a0of a resource.</li> <li>What fields are shown in the form in which order. Field contents are retrieved from properties connected to the resource.</li> <li>Which other, linked resources are shown in the form. Linked resources can either be shown as links or as their full form.</li> <li>Which texts are used to name and describe fields, as well as the tab in the user interface.</li> </ol> <p>Forms are defined in the CMEM Shapes Catalog graph. The graph URI is\u00a0<code>https://vocab.eccenca.com/shacl/</code>.</p> <p>Form definitions are twofold:</p> <ol> <li>The form itself is defined as so called\u00a0<code>NodeShape</code>. NodeShapes define which types of resources the form applies to (the target class), and which fields are shown in the form (the Properties).</li> <li>The individual fields are defined as so called\u00a0<code>PropertyShape</code>. PropertyShapes define which property is used to retrieve data for the field (the path), the name of the field, a description, its cardinality (min and max count), its position in the form (the order), and if it should always be shown. In case of object properties, it also defines the type of the linked resource (the class). The full list of features is described in\u00a0section PropertyShapes.</li> </ol> <p>To define a new form, for example for\u00a0<code>foaf:Person</code>\u00a0resources, navigate to the CMEM Shapes Catalog graph and select\u00a0<code>NodeShape</code>\u00a0in Navigation. The list of existing NodeShapes is shown. Click \u201cCreate a new SHACL Node shape\u201d\u00a0in the upper right to create a new NodeShape. Enter a name of the resource. An empty NodeShape resource is created and shown.</p> <p></p> <p>To create the initial definition, click\u00a0\u00a0(Edit). A form is shown to you with input fields\u00a0Name,\u00a0Property Shapes,\u00a0Vocabulary,\u00a0Target class\u00a0and\u00a0Statement Annotation. The initial definition requires the name, and the target class. Fields are attached to the form later.\u00a0Target class\u00a0in particular binds the form to the resources it should cover. The\u00a0Target class\u00a0field features an auto-complete that displays all classes stored in Corporate Memory. The example form should cover resources of the type\u00a0<code>foaf:Person</code>, so enter\u00a0<code>foaf:Person</code>\u00a0in the\u00a0Target class\u00a0field. Click\u00a0SAVE\u00a0to save the NodeShape.</p> <p></p> <p>You have now created an \u201cempty\u201d form that covers\u00a0<code>foaf:Person</code>\u00a0resources with tab name \u201cPerson\u201d. Navigating to a\u00a0<code>foaf:Person</code>\u00a0resource, you see a new tab as defined. You can still see all properties of the resource in the\u00a0PROPERTIES\u00a0tab.</p> <p></p> <p>To define new fields, for example showing the email address of the person (defined as\u00a0<code>foaf:mbox</code>), navigate to the CMEM Shapes Catalog graph and select\u00a0<code>PropertyShape</code>\u00a0in Navigation. The list of existing PropertyShapes is shown. Click\u00a0CREATE NEW PROPERTYSHAPE\u00a0in the upper right to create a new PropertyShape. Enter a name of the resource. An empty PropertyShape resource is created and shown.</p> <p>Edit the form using\u00a0. A form is shown with all relevant properties of a field definition. Required in this step are:</p> <ol> <li>The name of the field, which will be displayed left of the data content or input field in the form.</li> <li>The description, which will be displayed as tooltip on the question mark to the right of the name.</li> <li>The path, which states which property the field represents. In this example, it is\u00a0<code>foaf:mbox</code>.</li> <li>The form the field should be shown in (Property of). The field provides an auto-complete, so just enter \u201cPerson\u201d and select the NodeShape resource you defined in the previous step.</li> </ol> <p>Click\u00a0SAVE\u00a0after filling out the required fields.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#using-forms","title":"Using forms","text":"<p>Once a Node Shape is created for a specific class, you are able to use the specified entry form in the Explore component of Corporate Memory.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#editing-existing-resources","title":"Editing existing resources","text":"<p>While browsing your knowledge graph, you will always see your shape in action, when you click on a resource which is an instance of the class which is linked with\u00a0<code>shacl:targetClass</code>\u00a0from your Node Shape.</p> <p>The next images demonstrate this behavior :</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/#creating-new-resources","title":"Creating new resources","text":"<p>You can also create new resources by using a shaped form. One way to achieve this, is to select the class in the navigation tree on the lower left part in the Explore component and then click the Floating Action Button at the bottom or use the context menu on upper right side.</p> <p>The next images demonstrate this\u00a0behaviour:</p> <p></p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/","title":"Datatypes","text":"<p>This is a list of supported data types in shapes.</p> <p>Warning</p> <p>Not all datatypes result in specific widgets.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#anyuri","title":"anyURI","text":"<p>The \u00b7lexical space\u00b7 of anyURI is finite-length character sequences which, when the algorithm defined in Section 5.4 of [XML Linking Language] is applied to them, result in strings which are legal URIs according to [RFC 2396], as amended by [RFC 2732]. Note:  Spaces are, in principle, allowed in the \u00b7lexical space\u00b7 of anyURI, however, their use is highly discouraged (unless they are encoded by %20).</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#anyURI</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#base64binary","title":"base64Binary","text":"<p>The lexical forms of base64Binary values are limited to the 65 characters of the Base64 Alphabet defined in [RFC 2045], i.e., a-z, A-Z, 0-9, the plus sign (+), the forward slash (/) and the equal sign (=), together with the characters defined in [XML 1.0 (Second Edition)] as white space. No other characters are allowed.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#base64Binary</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#boolean","title":"boolean","text":"<p>An instance of a datatype that is defined as \u00b7boolean\u00b7 can have the following legal literals {true, false, 1, 0}.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#boolean</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#byte","title":"byte","text":"<p>byte is \u00b7derived\u00b7 from short by setting the value of \u00b7maxInclusive\u00b7 to be 127 and \u00b7minInclusive\u00b7 to be -128. byte has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126, +100.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#byte</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#date","title":"date","text":"<p>The lexical space of date consists of finite-length sequences of characters of the form: <code>'-'? yyyy '-' mm '-' dd zzzzzz?</code> where the date and optional timezone are represented exactly the same way as they are for dateTime. The first moment of the interval is that represented by: <code>'-' yyyy '-' mm '-' dd 'T00:00:00' zzzzzz?</code> and the least upper bound of the interval is the timeline point represented (noncanonically) by: <code>'-' yyyy '-' mm '-' dd 'T24:00:00' zzzzzz?</code>.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#date</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#datetime","title":"dateTime","text":"<p>The \u00b7lexical space\u00b7 of dateTime consists of finite-length sequences of characters of the form: <code>'-'? yyyy '-' mm '-' dd 'T' hh ':' mm ':' ss ('.' s+)? (zzzzzz)?</code> For example, <code>2002-10-10T12:00:00-05:00</code> (noon on 10 October 2002, Central Daylight Savings Time as well as Eastern Standard Time in the U.S.) is <code>2002-10-10T17:00:00Z</code>, five hours later than <code>2002-10-10T12:00:00Z</code>.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#dateTime</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#datetimestamp","title":"dateTimeStamp","text":"<p>The lexical space of dateTimeStamp consists of strings which are in the \u00b7lexical space\u00b7 of dateTime and which also match the regular expression \u2018.*(Z|(+|-)[0-9][0-9]:[0-9][0-9])\u2019</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#dateTimeStamp</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#decimal","title":"decimal","text":"<p>decimal has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) separated by a period as a decimal indicator. An optional leading sign is allowed. If the sign is omitted, \u2018+\u2019 is assumed. Leading and trailing zeroes are optional. If the fractional part is zero, the period and following zeroes can be omitted. For example: -1.23, 12678967.543233, +100000.00, 210.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#decimal</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#double","title":"double","text":"<p>double values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent \u00b7must\u00b7 be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for double.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#double</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#duration","title":"duration","text":"<p>The lexical representation for duration is the ISO 8601 extended format <code>PnYnMnDTnHnMnS</code>, where <code>nY</code> represents the number of years, <code>nM</code> the number of months, <code>nD</code> the number of days, <code>T</code> is the date/time separator, <code>nH</code> the number of hours, <code>nM</code> the number of minutes and <code>nS</code> the number of seconds. The number of seconds can include decimal digits to arbitrary precision.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#duration</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#float","title":"float","text":"<p>float values have a lexical representation consisting of a mantissa followed, optionally, by the character \u2018E\u2019 or \u2018e\u2019, followed by an exponent. The exponent \u00b7must\u00b7 be an integer. The mantissa must be a decimal number. The representations for exponent and mantissa must follow the lexical rules for integer and decimal. If the \u2018E\u2019 or \u2018e\u2019 and the following exponent are omitted, an exponent value of 0 is assumed. The special values positive and negative infinity and not-a-number have lexical representations INF, -INF and NaN, respectively. Lexical representations for zero may take a positive or negative sign. For example, -1E4, 1267.43233E12, 12.78e-2, 12 , -0, 0 and INF are all legal literals for float.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#float</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gday","title":"gDay","text":"<p>The lexical representation for gDay is the left truncated lexical representation for date: <code>---DD</code> . An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gDay</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gmonth","title":"gMonth","text":"<p>The lexical representation for gMonth is the left and right truncated lexical representation for date: <code>--MM</code>. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gMonth</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gmonthday","title":"gMonthDay","text":"<p>The lexical representation for gMonthDay is the left truncated lexical representation for date: <code>--MM-DD</code>. An optional following time zone qualifier is allowed as for date. No preceding sign is allowed. No other formats are allowed. See also ISO 8601 Date and Time Formats. This datatype can be used to represent a specific day in a month. To say, for example, that my birthday occurs on the 14th of September ever year.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gMonthDay</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gyear","title":"gYear","text":"<p>The lexical representation for gYear is the reduced (right truncated) lexical representation for dateTime: <code>CCYY</code>. No left truncation is allowed. An optional following time zone qualifier is allowed as for dateTime. To accommodate year values outside the range from <code>0001</code> to <code>9999</code>, additional digits can be added to the left of this representation and a preceding <code>-</code> sign is allowed. For example, to indicate 1999, one would write: <code>1999</code>. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gYear</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#gyearmonth","title":"gYearMonth","text":"<p>The lexical representation for gYearMonth is the reduced (right truncated) lexical representation for dateTime: CCYY-MM. No left truncation is allowed. An optional following time zone qualifier is allowed. To accommodate year values outside the range from 0001 to 9999, additional digits can be added to the left of this representation and a preceding \u2018-\u2018 sign is allowed. For example, to indicate the month of May 1999, one would write: 1999-05. See also ISO 8601 Date and Time Formats (\u00b7D).</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#gYearMonth</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#hexbinary","title":"hexBinary","text":"<p>hexBinary has a lexical representation where each binary octet is encoded as a character tuple, consisting of two hexadecimal digits ([0-9a-fA-F]) representing the octet code. For example, \u20180FB7\u2019 is a hex encoding for the 16-bit integer 4023 (whose binary representation is 111110110111).</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#hexBinary</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#html","title":"HTML","text":"<p>The datatype of RDF literals storing fragments of HTML content</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#int","title":"int","text":"<p>int is \u00b7derived\u00b7 from long by setting the value of \u00b7maxInclusive\u00b7 to be 2147483647 and \u00b7minInclusive\u00b7 to be -2147483648. int has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 126789675, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#int</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#integer","title":"integer","text":"<p>integer has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39) with an optional leading sign. If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#integer</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#jinja-template-string","title":"Jinja Template String","text":"<p>Jinja is a modern and designer-friendly templating language for Python and other languages.</p> <p>IRI: <code>https://vocab.eccenca.com/shui/jinja</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#langstring","title":"langString","text":"<p>The datatype of language-tagged string values</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#langString</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#language","title":"language","text":"<p>language represents natural language identifiers as defined by by [RFC 3066] . The \u00b7value space\u00b7 of language is the set of all strings that are valid language identifiers as defined [RFC 3066] . The \u00b7lexical space\u00b7 of language is the set of all strings that conform to the pattern [a-zA-Z]{1,8}(-[a-zA-Z0-9]{1,8})* . The \u00b7base type\u00b7 of language is token.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#language</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#long","title":"long","text":"<p>long is \u00b7derived\u00b7 from integer by setting the value of \u00b7maxInclusive\u00b7 to be 9223372036854775807 and \u00b7minInclusive\u00b7 to be -9223372036854775808. long has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#long</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#markdown","title":"Markdown","text":"<p>In addition to rdf:HTML, this is the datatype of RDF literals storing fragments of markdown content. eccenca Corporate Memory user interfaces support the rendering of all basic Markdown syntax features as well as the extensions for tables, code blocks, strikethrough, task lists and footnotes.</p> <p>IRI: <code>http://ns.ontowiki.net/SysOnt/Markdown</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#name","title":"Name","text":"<p>Name represents XML Names. The \u00b7value space\u00b7 of Name is the set of all strings which \u00b7match\u00b7 the Name production of [XML 1.0 (Second Edition)]. The \u00b7lexical space\u00b7 of Name is the set of all strings which \u00b7match\u00b7 the Name production of [XML 1.0 (Second Edition)]. The \u00b7base type\u00b7 of Name is token.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#Name</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#ncname","title":"NCName","text":"<p>NCName represents XML \u2018non-colonized\u2019 Names. The \u00b7value space\u00b7 of NCName is the set of all strings which \u00b7match\u00b7 the NCName production of [Namespaces in XML]. The \u00b7lexical space\u00b7 of NCName is the set of all strings which \u00b7match\u00b7 the NCName production of [Namespaces in XML]. The \u00b7base type\u00b7 of NCName is Name.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#NCName</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#negativeinteger","title":"negativeInteger","text":"<p>negativeInteger has a lexical representation consisting of a negative sign (\u2018-\u2018) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: -1, -12678967543233, -100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#negativeInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#nmtoken","title":"NMTOKEN","text":"<p>NMTOKEN represents the NMTOKEN attribute type from [XML 1.0 (Second Edition)]. The \u00b7value space\u00b7 of NMTOKEN is the set of tokens that \u00b7match\u00b7 the Nmtoken production in [XML 1.0 (Second Edition)]. The \u00b7lexical space\u00b7 of NMTOKEN is the set of strings that \u00b7match\u00b7 the Nmtoken production in [XML 1.0 (Second Edition)]. The \u00b7base type\u00b7 of NMTOKEN is token.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#NMTOKEN</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#nonnegativeinteger","title":"nonNegativeInteger","text":"<p>nonNegativeInteger has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, the positive sign (\u2018+\u2019) is assumed. If the sign is present, it must be \u2018+\u2019 except for lexical forms denoting zero, which may be preceded by a positive (\u2018+\u2019) or a negative (\u2018-\u2018) sign. For example: 1, 0, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#nonNegativeInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#nonpositiveinteger","title":"nonPositiveInteger","text":"<p>nonPositiveInteger has a lexical representation consisting of an optional preceding sign followed by a finite-length sequence of decimal digits (#x30-#x39). The sign may be \u2018+\u2019 or may be omitted only for lexical forms denoting zero, in all other lexical forms, the negative sign (\u2018-\u2018) must be present. For example: -1, 0, -12678967543233, -100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#nonPositiveInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#normalizedstring","title":"normalizedString","text":"<p>normalizedString represents white space normalized strings. The \u00b7value space\u00b7 of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The \u00b7lexical space\u00b7 of normalizedString is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters. The \u00b7base type\u00b7 of normalizedString is string.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#normalizedString</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#plainliteral","title":"PlainLiteral","text":"<p>The class of plain (i.e. untyped) literal values, as used in RIF and OWL 2</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#PlainLiteral</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#positiveinteger","title":"positiveInteger","text":"<p>positiveInteger has a lexical representation consisting of an optional positive sign (\u2018+\u2019) followed by a finite-length sequence of decimal digits (#x30-#x39). For example: 1, 12678967543233, +100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#positiveInteger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#short","title":"short","text":"<p>short is \u00b7derived\u00b7 from int by setting the value of \u00b7maxInclusive\u00b7 to be 32767 and \u00b7minInclusive\u00b7 to be -32768. short has a lexical representation consisting of an optional sign followed by a finite-length sequence of decimal digits (#x30-#x39). If the sign is omitted, \u2018+\u2019 is assumed. For example: -1, 0, 12678, +10000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#short</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#sparqloperation","title":"sparqlOperation","text":"<p>sparql operation datatype (query or update)</p> <p>IRI: <code>https://vocab.eccenca.com/shui/sparqlOperation</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#sparqlquery","title":"sparqlQuery","text":"<p>SPARQL 1.1 Query</p> <p>IRI: <code>https://vocab.eccenca.com/shui/sparqlQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#sparqlupdate","title":"sparqlUpdate","text":"<p>SPARQL 1.1 Update</p> <p>IRI: <code>https://vocab.eccenca.com/shui/sparqlUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#string","title":"string","text":"<p>The string datatype represents character strings in XML. The \u00b7value space\u00b7 of string is the set of finite-length sequences of characters (as defined in [XML 1.0 (Second Edition)]) that \u00b7match\u00b7 the Char production from [XML 1.0 (Second Edition)]. A character is an atomic unit of communication, it is not further specified except to note that every character has a corresponding Universal Character Set code point, which is an integer.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#string</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#time","title":"time","text":"<p>The lexical representation for time is the left truncated lexical representation for dateTime: <code>hh:mm:ss.sss</code> with optional following time zone indicator. For example, to indicate 1:20 pm for Eastern Standard Time which is 5 hours behind Coordinated Universal Time (UTC), one would write: <code>13:20:00-05:00</code>. See also ISO 8601 Date and Time Formats.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#time</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#token","title":"token","text":"<p>token represents tokenized strings. The \u00b7value space\u00b7 of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The \u00b7lexical space\u00b7 of token is the set of strings that do not contain the carriage return (#xD), line feed (#xA) nor tab (#x9) characters, that have no leading or trailing spaces (#x20) and that have no internal sequences of two or more spaces. The \u00b7base type\u00b7 of token is normalizedString.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#token</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedbyte","title":"unsignedByte","text":"<p>unsignedByte is \u00b7derived\u00b7 from unsignedShort by setting the value of \u00b7maxInclusive\u00b7 to be 255. unsignedByte has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 126, 100.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedByte</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedint","title":"unsignedInt","text":"<p>unsignedInt is \u00b7derived\u00b7 from unsignedLong by setting the value of \u00b7maxInclusive\u00b7 to be 4294967295. unsignedInt has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 1267896754, 100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedInt</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedlong","title":"unsignedLong","text":"<p>unsignedLong is \u00b7derived\u00b7 from nonNegativeInteger by setting the value of \u00b7maxInclusive\u00b7 to be 18446744073709551615. unsignedLong has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678967543233, 100000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedLong</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#unsignedshort","title":"unsignedShort","text":"<p>unsignedShort is \u00b7derived\u00b7 from unsignedInt by setting the value of \u00b7maxInclusive\u00b7 to be 65535. unsignedShort has a lexical representation consisting of a finite-length sequence of decimal digits (#x30-#x39). For example: 0, 12678, 10000.</p> <p>IRI: <code>http://www.w3.org/2001/XMLSchema#unsignedShort</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/datatype-reference/#xmlliteral","title":"XMLLiteral","text":"<p>The datatype of XML literal values.</p> <p>IRI: <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/","title":"Node Shapes","text":"<p>Node Shapes are resources of type\u00a0<code>shacl:NodeShape</code>. They can be used to validate resources as well as to define custom forms for presenting and editing resources of a specific type.</p> <p>This page lists all supported properties to describe node shapes.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#naming-and-presentation","title":"Naming and Presentation","text":"<p>Info</p> <p>In this group, presentation and naming properties are collected. Most of the properties are straight forward to use.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#name","title":"Name","text":"<p>The name of the node is presented to the user only when he needs to distinguish between different shapes for the same resource.</p> <p>Used Path: <code>shacl:name</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#description","title":"Description","text":"<p>The node description should provide context information for the user when creating a new resource based on this node.</p> <p>Used Path: <code>rdfs:comment</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#navigation-list-query","title":"Navigation list query","text":"<p>This property links the node shape to a SPARQL 1.1 Query in order to provide a sophisticated user navigation list query e.g. to add specific additional columns. The query should use {{FROM}} as a placeholder for the FROM section. Additionally, {{GRAPH}} can be used to access the graph in the FROM section.</p> <p>Used Path: <code>shui:navigationListQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#depiction-image","title":"Depiction Image","text":"<p>This property links a node shape to an image in order to use this image when showing resources based on this node shape somewhere.</p> <p>Used Path: <code>http://xmlns.com/foaf/0.1/depiction</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#vocabulary","title":"Vocabulary","text":"<p>Info</p> <p>In this group, the affected vocabulary classes as well as the used property shapes are managed.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#property-shapes","title":"Property Shapes","text":"<p>The used property shapes on this node. Please note that this is NOT a link to a datatype or object property but to a SHACL property shape.</p> <p>Used Path: <code>shacl:property</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#target-class","title":"Target class","text":"<p>Class this NodeShape applies to. This is a direct link to a class resource from a vocabulary.</p> <p>Used Path: <code>shacl:targetClass</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#processing","title":"Processing","text":"<p>Info</p> <p>In this group, all shape properties are managed, have an effect on how new or existing resources are processed or created.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#uri-template","title":"URI template","text":"<p>A compact sequence of characters for describing a range of URIs through variable expansion.</p> <p>Used Path: <code>shui:uriTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#on-update-update","title":"On update update","text":"<p>A query which is executed when this nodeshape is submitted. The query should be saved in the same graph as the shape (or imported).</p> <p>The query can use these placeholders:</p> <ul> <li><code>{{shuiResource}}</code> - the resource currently shown with the node shape of this property shape,</li> <li><code>{{shuiGraph}}</code> - the currently used graph. </li> </ul> <p>Used Path: <code>shui:onUpdateUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#target-graph-template","title":"Target Graph Template","text":"<p>Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect).</p> <p>Used Path: <code>shui:targetGraphTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#statement-annotation","title":"Statement Annotation","text":"<p>Info</p> <p>Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#enable","title":"Enable","text":"<p>A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape.</p> <p>Used Path: <code>shui:enableStatementLevelMetadata</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/node-shapes/#provide-as-shape","title":"Provide as Shape","text":"<p>A value of true enables this node shape to be applied as statement annotation (reification).</p> <p>Used Path: <code>shui:isApplicableAsStatementLevelMetadata</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/","title":"Property Shapes","text":"<p>Property Shapes are resources of type\u00a0<code>shacl:PropertyShape</code>. They are used to\u00a0specify constraints and UI options that need to be met in the context of a Node Shape.</p> <p>The following Property Shape properties are supported:</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#naming-and-presentation","title":"Naming and Presentation","text":"<p>Info</p> <p>In this group, presentation and naming properties are collected. Most of the properties are straight forward to use, other properties provide more complex features, such as table reports.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#name","title":"Name","text":"<p>This name will be shown to the user.</p> <p>Used Path: <code>shacl:name</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#description","title":"Description","text":"<p>This text will be shown to the user in a tooltip. You can use new and blank lines for basic text structuring.</p> <p>Used Path: <code>shacl:description</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-table-report","title":"Query: Table Report","text":"<p>Use this property to provide a tabular read-only report of a custom SPARQL query at the place where this property shape is used in the user interface.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ;</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ;</li> <li><code>{{shuiGraph}}</code> - the currently used graph.</li> </ul> <p>Used Path: <code>shui:valueQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-table-report-hide-header","title":"Query: Table Report (hide header)","text":"<p>If set to true, the report table will be rendered without header (in case you expect only a single value).</p> <p>Used Path: <code>shui:valueQueryHideHeader</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-table-report-hide-footer","title":"Query: Table Report (hide footer)","text":"<p>If set to true, the report table will be rendered without footer (in case you expect only a single value or row).</p> <p>Used Path: <code>shui:valueQueryHideFooter</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#order","title":"Order","text":"<p>Specifies the order of the property in the UI. Ordering is separate for each group.</p> <p>Used Path: <code>shacl:order</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#group","title":"Group","text":"<p>Group to which the property belongs to.</p> <p>Used Path: <code>shacl:group</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#show-always","title":"Show always","text":"<p>Default is false. A value of true let optional properties (min count = 0) show up by default.</p> <p>Used Path: <code>shui:showAlways</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#read-only","title":"Read only","text":"<p>Default is false. A value of true means the properties are not editable by the user. Useful for displaying system properties.</p> <p>Used Path: <code>shui:readOnly</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#provide-workflow-trigger","title":"Provide Workflow Trigger","text":"<p>Integrates a workflow trigger button in order to execute workflows from or with this resource.</p> <p>Used Path: <code>shui:provideWorkflowTrigger</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#vocabulary","title":"Vocabulary","text":"<p>Info</p> <p>In this group, property paths as well cardinality restrictions are managed.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#property-of","title":"Property of","text":"<p>The node shape this property shape belongs to.</p> <p>Used Path: <code>shacl:property</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#path","title":"Path","text":"<p>The datatype or object property used in this shape.</p> <p>Used Path: <code>shacl:path</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#node-kind","title":"Node kind","text":"<p>Type of the node.</p> <p>Used Path: <code>shacl:nodeKind</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#min-count","title":"Min count","text":"<p>Min cardinality, 0 will show this property under optionals unless \u2018Show always = true\u2019</p> <p>Used Path: <code>shacl:minCount</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#max-count","title":"Max count","text":"<p>Max cardinality</p> <p>Used Path: <code>shacl:maxCount</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#datatype-property-specific","title":"Datatype Property Specific","text":"<p>Info</p> <p>In this group, all shape properties are managed, which only have effects on datatype properties.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#datatype","title":"Datatype","text":"<p>The datatype of the property.</p> <p>Used Path: <code>shacl:datatype</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#use-textarea","title":"Use textarea","text":"<p>Default is false. A value of true enables multiline editing capabilities for Literals via a <code>textarea</code> widget.</p> <p>Used Path: <code>shui:textarea</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#regex-pattern","title":"Regex Pattern","text":"<p>A XPath regular expression (Perl like) that all literal strings need to match.</p> <p>Used Path: <code>shacl:pattern</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#regex-flags","title":"Regex Flags","text":"<p>An optional string of flags for the regular expression pattern (e.g. \u2018i\u2019 for case-insensitive mode)</p> <p>Used Path: <code>shacl:flags</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#languages-allowed","title":"Languages allowed","text":"<p>This limits the given Literals to a list of languages. This property works only in combination with the datatype <code>rdf:langString</code>. Note that the expression for this property only allows for \u20182 Char ISO-639-1-Codes\u2019 only (no sub-tags).</p> <p>Used Path: <code>shui:languageIn</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#languages-unique","title":"Languages Unique","text":"<p>Default is false. A value of true enforces that no pair of Literals may use the same language tag.</p> <p>Used Path: <code>shacl:uniqueLang</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#object-property-specific","title":"Object Property Specific","text":"<p>Info</p> <p>In this group, all shape properties are managed, which only have effects on object properties.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#class","title":"Class","text":"<p>Class of the connected IRI if nodeKind == sh:IRI.</p> <p>Used Path: <code>shacl:class</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-selectable-resources","title":"Query: Selectable Resources","text":"<p>This query allows for listing selectable resources in the dropdown list for this property shape.</p> <p>Used Path: <code>shui:uiQuery</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#inverse-path","title":"Inverse Path","text":"<p>Default is false. A value of true inverts the expected / created direction of a relation.</p> <p>Used Path: <code>shui:inversePath</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#deny-new-resources","title":"Deny new resources","text":"<p>A value of true disables the option to create new resources.</p> <p>Used Path: <code>shui:denyNewResources</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#node-shape","title":"Node shape","text":"<p>The shape of the linked resource.</p> <p>Used Path: <code>shacl:node</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#processing","title":"Processing","text":"<p>Info</p> <p>In this group, all shape properties are managed, have an effect on how new or existing resources are processed or created.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#uri-template","title":"URI template","text":"<p>A compact sequence of characters for describing a range of URIs through variable expansion.</p> <p>Used Path: <code>shui:uriTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#ignore-on-copy","title":"Ignore on copy","text":"<p>Disables reusing the value(s) when creating a copy of the resource.</p> <p>Used Path: <code>shui:ignoreOnCopy</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-on-insert-update","title":"Query: On insert update","text":"<p>This query is executed when a property value is added or changed.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ;</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ;</li> <li><code>{{shuiGraph}}</code> - the currently used graph.</li> </ul> <p>Used Path: <code>shui:onInsertUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#query-on-delete-update","title":"Query: On delete update","text":"<p>This query is executed when a value is changed or removed.</p> <p>The following placeholder can be used in the query text of the SPARQL query:</p> <ul> <li><code>{{shuiMainResource}}</code> - refers to the main resource rendered in the start node shape of the currently displayed node shape tree (only relevant in case of sub-shape usage) ;</li> <li><code>{{shuiResource}}</code> - refers to the resource which is rendered in the node shape where this property shape is used (maybe a sub-shape) ;</li> <li><code>{{shuiGraph}}</code> - the currently used graph.</li> </ul> <p>Used Path: <code>shui:onDeleteUpdate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#target-graph-template","title":"Target Graph Template","text":"<p>Graph templates can be used to enforce writing statement in specific graphs rather than into the selected graph. Graph templates can be added to node and property shapes. A template on a property shape is used only for overwriting a template on a node shape (without a node shape graph template, they do not have an effect).</p> <p>Used Path: <code>shui:targetGraphTemplate</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#statement-annotation","title":"Statement Annotation","text":"<p>Info</p> <p>Statement Annotations provide a way to express knowledge about statements. This group is dedicated to properties which configure the Statement Annotation feature.</p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#enable","title":"Enable","text":"<p>A value of true enables visualisation and management capabilities of statement annotations (reification) for all statements which are shown via this shape.</p> <p>Used Path: <code>shui:enableStatementLevelMetadata</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/property-shapes/#provided-shapes","title":"Provided Shapes","text":"<p>Instead of providing all possible statement annotation node shapes for the creation of new statement annotations, this property will limit the list to the selected shapes only.</p> <p>Used Path: <code>shui:provideStatementLevelMetadataShapes</code></p>","tags":["Reference","Vocabulary"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/","title":"Workflow Trigger","text":"","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#introduction","title":"Introduction","text":"<p>Workflow Trigger allow for manual execution of data integration workflows inside of the exploration interface.</p> <p>Optionally, a reference to the resource in view on workflow execution can be sent, allowing an executed workflow to act specifically on this resource (or a specific portion of the Knowledge Graph related to it). Workflow trigger are associated to Node Shapes by defining special-purpose non-validating Property Shape resources.</p>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#setup","title":"Setup","text":"<p>Workflow Trigger can be defined and used in any active\u00a0Shape Catalog\u00a0(active means, it is imported from the main Shape Catalog). A workflow trigger resource references a data integration workflow by URI.</p> <p></p> <p>To define a workflow trigger the following information is needed:</p> <ul> <li>Label: The trigger resource needs a label (can be given in different languages), which is used for the button presentation.</li> <li>Description: The trigger resource needs a description, which is used as text that is sitting left of the button for further documentation of the activity to the user.</li> <li>Workflow: the workflow parameter defines the workflow that shall be executed upon clicking the button. The workflow can be selected from a dropdown list.</li> <li>Refresh View: can be either\u00a0<code>true</code>or\u00a0<code>false</code>.\u00a0If this value is set to\u00a0<code>true</code>, the view that contains the workflow trigger will be reloaded upon workflow completion</li> <li>Send Resource Reference: can be either\u00a0<code>true</code>or\u00a0<code>false</code>. If this value is set to\u00a0<code>true</code>, a payload that consists of the\u00a0resource IRI\u00a0that is represented in the view as well as the\u00a0graph IRI\u00a0of the graph that is currently selected.</li> </ul>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#integration","title":"Integration","text":"<p>Once a trigger resources is defined, it can be attached to a Node Shape by using a special-purpose non-validating Property Shape resources. Such\u00a0property shapes use a\u00a0<code>shui:provideWorkflowTrigger</code>\u00a0statement to define, which workflow trigger are to be represented. SHACL path statements on such Property Shape resources are meaningless and ignored, but may be provided.</p>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/building-a-customized-user-interface/workflow-trigger/#payload-structure","title":"Payload Structure","text":"<p>When\u00a0Send Resource Reference\u00a0is set to\u00a0true,\u00a0a payload is added to the call of the workflow. The payload consists of a JSON document with two attributes:</p> <p>Workflow Payload</p> <pre><code>{\n\"graphIRI\": \"http://example.org/example-graph\",\n\"resourceIRI\": \"http://example.org/example-graph/examle-resource\"\n}\n</code></pre> <ul> <li><code>graphIRI</code>\u00a0is the IRI of the graph that is currently viewed, and</li> <li><code>resourceIRI</code>is the IRI of the resource that is viewed.</li> </ul>","tags":["Workflow"]},{"location":"explore-and-author/graph-exploration/statement-annotations/","title":"Statement Annotations","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#introduction","title":"Introduction","text":"<p>Statement Annotations provide a way to express knowledge about statements.\u00a0Typical use cases for Statement Annotations include:</p> <ul> <li>the temporal validity of information,</li> <li>the origin of information, or</li> <li>just a way to annotate a specific statement with a human readable comment.</li> </ul>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#usage","title":"Usage","text":"<p>If enabled on a specific type of statement or type of resource, you see a Statement Annotation text bubble beside every annotatable statement:</p> <p></p> <p>This bubble has different status:</p> <ul> <li>A\u00a0empty text bubble\u00a0indicates, that there is no annotation on the statement, but the annotation feature is enabled for this statement.</li> <li>A\u00a0filled text bubble\u00a0indicates, that there is at least one annotation on the statement.</li> <li>No bubble\u00a0indicates, that the annotation feature is NOT enabled on this type of statement.</li> </ul> <p>Clicking on one of the text bubbles opens the Statement Annotation dialog for this specific statement:</p> <p></p> <p>In the Statement Annotation dialog, you can select the Statement Annotation Template and click Create.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#setup","title":"Setup","text":"<p>In order to have a working Statement Annotation setup, the following steps need to be done:</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#create-a-statement-annotation-graph","title":"Create a Statement Annotation Graph","text":"<p>Create a new Graph, edit its metadata and change the type to\u00a0Statement Annotation Graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#setup-and-import-the-statement-annotation-graph-in-your-data-graph","title":"Setup and import the Statement Annotation Graph in your data graph","text":"<p>In your data graph, where the resources exist which you want to annotate, import the\u00a0Statement Annotation Graph and select it as an Annotation Graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#create-a-shaped-form-which-will-be-used-to-annotate-statements","title":"Create a shaped form which will be used to annotate statements","text":"<p>In your Shape Catalog, select a Node Shape (or create one) which you want to use for statement annotations, and Enable\u00a0Statement Annotation\u00a0to\u00a0true.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#allow-statement-annotations-in-your-shaped-forms-on-specific-classes-or-properties","title":"Allow statement annotations in your shaped forms on specific Classes or Properties","text":"<p>Finally, select the Node Shape or Property Shape from your Shape Catalog, and enable annotations by setting the\u00a0Enable\u00a0option in the\u00a0Statement Annotations\u00a0group to\u00a0true.</p> <p></p> <p>This will enable the feature on the statements of all resources shown with this Node Shape or on all statements shown with this Property Shape.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#technical-background","title":"Technical Background","text":"<p>From the technical point of view, the Statement Annotation feature uses RDF Reification to annotate Statements (Triples) with additional background information. Statement resources can be annotated with custom Annotation Resources. These Annotation Resources are based on specific Shapes which are enabled as Statement Annotation shapes. Reification Resources as well as Annotation Resources are managed in a Statement Annotation Graph, which need to be configured on a Graph as well as imported to this Graph. The following illustration depicts this schema with boxes and arrows:</p> <p></p> <p>Some notes on this:</p> <ul> <li>There is one\u00a0Statement Reification Resource per Statement Annotation.</li> <li>Removing the\u00a0Statement Annotation also removes the\u00a0Statement Reification Resource.</li> <li>All annotation triples (8 triples in the image) are created in the Statement Annotation Graph, so Step 2 of the setup procedure is important.</li> </ul>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/statement-annotations/#querying-statement-annotations","title":"Querying Statement Annotations","text":"<p>In order to automate access to Statement Annotations, you can query them with SPARQL e.g. via\u00a0cmemc\u00a0or the API endpoint.</p> <p>Here is a query example to start with:</p> <pre><code># Request SPO of all Statement Annotations which annotate a\n# triple of my ResourceIRI (parameter)\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nSELECT DISTINCT ?StatementAnnotationGraph ?AnnotationResource ?p ?o\nWHERE {\nGRAPH ?StatementAnnotationGraph {\n?StatementResource a rdf:Statement .\n?StatementResource rdf:subject|rdf:predicate|rdf:object &lt;{{ResourceIRI}}&gt; .\n?StatementResource rdf:value ?AnnotationResource .\n?AnnotationResource ?p ?o\n}\n}\n</code></pre>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/","title":"Versioning of Graph Changes","text":"","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#introduction","title":"Introduction","text":"<p>This feature keeps track of changes to your Knowledge Graphs by creating change set data based on the user\u2019s editing activities.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#usage","title":"Usage","text":"<p>If enabled on a graph, all changes using shaped user interfaces will be tracked in the configured Versioning Graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#setup","title":"Setup","text":"<p>To enable this feature on a specific graph you need to setup the following steps.</p> 1. Create a Versioning Graph <p>In Exploration, create a new graph and define it as a Versioning Graph.</p> <p></p> 2. Configure a graph to use this Versioning Graph <p>In Exploration, edit this graph and add the Versioning Graph property to select the newly created Versioning Graph.</p> <p></p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/graph-exploration/versioning-of-graph-changes/#technical-background","title":"Technical Background","text":"<p>For each editing activity (\u2192 Save a Form), a ChangeSet resource will be created. This resource has some metadata (user, timestamp, label) as well as links to added and deleted Statements (using RDF Reification).</p> <p>The details of the used vocabulary are available at\u00a0the Changeset Vocabulary\u00a0page.</p>","tags":["KnowledgeGraph"]},{"location":"explore-and-author/query-module/","title":"Query Module","text":"","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#introduction","title":"Introduction","text":"<p>The\u00a0Query\u00a0module provides a user interface to store, describe, search and edit SPARQL queries. The queries are evaluated on the Knowledge Graph and provide a way to granularly aggregate semantic data as tables. These tables can then be exported as CSV, Excel or JSON documents.</p> <p>The Query module features two areas, the catalog and the editor.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#query-catalog","title":"Query catalog","text":"<p>The catalog lists all existing SPARQL queries including name, type and description.</p> <p>Use the\u00a0 Search\u00a0bar in order to look for a specific query.</p> <p></p> <p>Select the query from the Queries catalog, to open and load the query.\u00a0</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#query-editor","title":"Query editor","text":"<p>Use the Query editor to edit and execute SPARQL queries. In this view you can also browse query result previews and download full query results as CSV files.</p> <p>The Query editor provides an interface where you can write and edit your SPARQL queries. The query editor features SPARQL syntax highlighting and SPARQL validation, allowing only syntactically correct SPARQL queries to be executed.</p> <p>The Query editor allows to Run query, Download Results, Delete, Save and Save as Queries.</p> <p></p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#run-a-query","title":"Run a query","text":"<p>Click\u00a0 Run to execute the query and to display a preview of the query results under the query editor. The results are presented as a table with pagination.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#export-results","title":"Export results","text":"<p>To export the full set of results without any limits in form of a CSV file click\u00a0 Download result on the top right.\u00a0</p> <p></p> <p>Info</p> <p>The preview result ordering has no impact on the result ordering in the exported file. If you want to export some ordered query results, you need to use the\u00a0<code>ORDER BY</code>\u00a0construct in the SPARQL query itself.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#save-a-query","title":"Save a query","text":"<p>To save a query in the Query catalog click\u00a0 Save. This opens a dialog that allows you to overwrite the existing query.</p>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/query-module/#placeholders","title":"Placeholders","text":"<p>In addition to the standard SPARQL syntax, placeholders can be used to parametrize a query. Placeholders are indicated in the query using a string of the form\u00a0<code>{{placeholdername}}</code>. Multiple placeholders can be defined by changing the name inside the brackets.</p> <p>When a query contains a placeholder, the placeholder list to the right of the query editor shows a field with its name.</p> <p></p> <p>When running a query that contains placeholders, the query editor replaces the\u00a0<code>{{placeholdername}}</code>\u00a0string in the query with the respective string entered into the placeholder list. This is a direct string replacement, so placeholders can contain simple strings and literal values, URIs, variables or even sub queries.</p> <p>Running a query with a placeholder is only possible when all placeholder fields in the placeholder list have been filled.</p> <p>A typical use case is restricting a query to a specific class of objects stated by a placeholder:</p> <pre><code>SELECT * WHERE { ?classInstance a &lt;http://dbpedia.org/ontology/{{class}}&gt; .}\n</code></pre> <p>This query selects all instances of a specific DBpedia Ontology class. When you enter\u00a0<code>Person</code>\u00a0into the\u00a0<code>class</code>\u00a0placeholder field in the placeholder list the following query is executed:</p> <pre><code>SELECT * WHERE { ?classInstance a &lt;http://dbpedia.org/ontology/Person&gt; .}\n</code></pre>","tags":["SPARQL","KnowledgeGraph"]},{"location":"explore-and-author/thesauri-management/","title":"Thesauri Management","text":"","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#introduction","title":"Introduction","text":"<p>The Thesaurus module provides a user interface to create, browse and edit thesaurus resources and general taxonomical data modeled in the\u00a0Simple Knowledge Organization System (SKOS).</p> <p>A thesaurus is a reference work that lists concepts with similar meaning, containing for example synonyms, often including taxonomical relations between these concepts. Taxonomies describe classifications of concepts into categories sharing particular features and their relations to broader (parent) and narrower (child) concepts.</p> <p>An example for a taxonomy or classification is how companies can be categorized into industries, industry groups and sectors. An airport belongs to the sector Airport Services, the broader category Transportation Infrastructure, the broader category Transportation and the broader category Industrials. You can think of these relations as a hierarchical tree representing the relations as individual branches like shown in the navigation tree on the left side of the Thesaurus view. In a concept scheme Industries, a top branch in this tree, as for example the sub-industry Industrials or Health Care, is called a top concept. All branches together belong to the concept scheme Industries.</p> <p></p> <p>Info</p> <p>In order to build thesauri or taxonomies with the Thesaurus module you should be familiar with SKOS structures.</p> <p>SKOS is a convenient way to model taxonomical data. The\u00a0SKOS Reference\u00a0provides detailed documentation on the usage of SKOS. The Thesaurus module allows to create, browse and edit such structures, providing a way to structure your hierarchical data in a simple interface and make it accessible for use cases like documentation and master data management.</p> <p>Info</p> <p>Before you start working with the Thesaurus module ensure that the vocabulary\u00a0Simple Knowledge Organization System\u00a0is installed in the Vocabulary catalog (see section\u00a0Vocabulary Catalog).</p> <p>Click\u00a0 Thesauri\u00a0in the main menu, to open the Thesaurus project catalog.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#thesaurus-project-catalog","title":"Thesaurus project catalog","text":"<p>The Thesaurus project catalog lists thesaurus projects with relevant metadata in a searchable and sortable table.</p> <p>In order to get more information on a thesaurus project and edit its metadata, click\u00a0 Show more\u00a0in the table row. The view expands showing the project metadata. Click\u00a0 Edit on the right side of the row to open the edit mode, enter your changes and click\u00a0SAVE.</p> <p></p> <p>To open the detail view of a thesaurus project, click the project name in the catalog.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#creating-a-new-thesaurus-project","title":"Creating a new thesaurus project","text":"<p>In order to create a new thesaurus project, select Create new thesaurus project\u00a0on the upper right of the thesaurus project catalog. Enter a name for your thesaurus project and more metadata if required and click\u00a0SAVE. The thesaurus detail view is shown, displaying an empty thesaurus project.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#importing-an-existing-thesaurus","title":"Importing an existing thesaurus","text":"<p>You can import existing thesaurus data in a thesaurus project. To import a thesaurus make sure that the data is in Turtle format and you are on project level in the thesaurus detail view. Click the context menu\u00a0 Show more options\u00a0in the upper right of the thesaurus detail view, then select\u00a0Import data. Upload the file containing the thesaurus. Click\u00a0SAVE\u00a0to import the data.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#exporting-a-thesaurus-project","title":"Exporting a thesaurus project","text":"<p>To export a thesaurus project, go to the project level, click\u00a0the context menu\u00a0 Show more options\u00a0in the upper right of the thesaurus detail view, then select\u00a0Export project. Confirm the dialog and click\u00a0DOWNLOAD\u00a0to download the thesaurus project.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#removing-a-thesaurus-project","title":"Removing a thesaurus project","text":"<p>To remove a thesaurus project, go to the project level in the navigation tree, click\u00a0the context menu\u00a0 Show more options\u00a0in the upper right of the thesaurus detail view, then select\u00a0Remove project. Confirm the removal and click\u00a0REMOVE\u00a0to delete the thesaurus project.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#thesaurus-detail-view","title":"Thesaurus detail view","text":"<p>After selecting a thesaurus project in the Thesaurus project catalog, the thesaurus detail view is shown. This view consists of three components. The navigation tree component is displayed to the left. It displays the hierarchical structure of the thesaurus. The upmost element is the project level. Clicking\u00a0\u00a0brings you back to the Thesaurus project catalog.</p> <p>Below the project level, the concept scheme(s) and concepts are displayed. A\u00a0Concept scheme\u00a0serves as a meaningful aggregation of a number of concepts. A concept itself constitutes a unit of thought, a conceptual class or category of objects you want to describe. For example, all concepts on a particular topic or domain could belong to the same concept scheme.</p> <p>If a concept in the navigation tree has narrower (child) concepts, you can expand this branch of the tree by clicking the arrow displayed in front of the concept. Arrows are only shown for concepts for which narrower concepts exist. Clicking the name of a concept scheme or concept updates both the detail view as well as the concept list to the right of the navigation tree.</p> <p>The tab\u00a0Statistics\u00a0shows statistical information about the content of the thesaurus project.</p> <p>A list of concept schemes of the thesaurus project is shown in the lower part as a searchable table.</p> <p>To edit project metadata in the thesaurus detail view click\u00a0 Edit.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#concept-detail-view","title":"Concept detail view","text":"<p>Clicking on a\u00a0Concept scheme\u00a0or a\u00a0Concept\u00a0in the navigation tree displays information on that resource in the concept detail view to the right.</p> <p>The concept detail view displays the concept\u2019s preferred and alternative labels in the\u00a0Overview\u00a0tab, as well as definitions and other metadata. A list of top concepts of a concept scheme or narrower (child) concepts of a concept is shown below these metadata as a searchable table called concept list.</p> <p>To find specific sub concepts of a concept, you can use the search bar in the concept list. Click any result row to display more information on the concept, like its labels, definition and notation. Click the concept name in the row itself to open the concept detail view for this concept. Selecting a concept in the concept list updates the navigation tree to highlight the concept\u2019s position in the tree.</p> <p>In case of concept schemes, the\u00a0Statistics\u00a0tab can be used to learn more about the number of concepts in this concept scheme, as well as their relations.</p> <p>The\u00a0Triples\u00a0tab shows the RDF source code of the concept as\u00a0Turtle\u00a0serialization. The triples in this view are editable, but be aware that changes in the triple editor can cause major errors in the Thesaurus module. When you are not an expert in working with triples do not use the triple view as an editor.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#adding-new-concepts-and-concept-schemes","title":"Adding new concepts and concept schemes","text":"<p>Concept schemes can be added on project level in an opened thesaurus project. Click on the project name in the navigation tree, then click the context menu\u00a0 Show more options\u00a0and select\u00a0Create new concept scheme. You can return to this level by clicking the name of the thesaurus project above the navigation tree.</p> <p>Enter at least one preferred label naming the concept scheme. Select the language of the preferred label to the right of the label field. Click  under the label field to set multiple labels. Create the concept scheme by clicking\u00a0SAVE. A new concept scheme will appear in the concept list as well as the navigation tree.</p> <p>Top concepts can be added in the same way. Instead of starting on project level, select an existing concept scheme first. Click the context menu\u00a0 Show more options\u00a0and a new option\u00a0Create new top concept\u00a0is available. Use it to create a new top concept.</p> <p>Selecting a normal concept brings up the option\u00a0Create new concept\u00a0in the same menu. Using this option creates a new concept and automatically add it as a narrower concept to the concept you created it on. It also automatically adds the\u00a0<code>broader</code>\u00a0back link on the newly created concept.</p> <p>To edit a concept or concept scheme, click\u00a0 Edit\u00a0to get a form displaying all available fields, like labels and definition.</p> <p>Info</p> <p>For each concept, you have to enter one preferred label per language. Be aware that SKOS allows only one preferred label per language. If you want to enter more labels use alternative or hidden labels as specified by SKOS.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#creating-relations-between-concepts","title":"Creating relations between concepts","text":"<p>Besides the narrower relation automatically generated when a new concept is created, you can add further relations between concepts. You can add, for example, a second broader concept for an existing concept or a related concept to indicate associative relations.</p> <p>To add relations, select the concept in the navigation tree. In the detail view, click\u00a0 Edit to open the edit mode.</p> <ul> <li>To add an associative relation to another concept, enter the concept name in the field\u00a0Related concept.</li> <li>To add a further broader relation, enter the name of the broader concept in the field\u00a0Broader concepts.</li> </ul> <p>You can only choose from existing concepts. Click\u00a0SAVE\u00a0to confirm your changes.</p> <p>In the same way you can also add a top concept to a second concept scheme. Use therefore the field\u00a0Top concept of\u00a0in the editing mode of a top concept.</p> <p>When adding relations the inverse relation is automatically added, too.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/thesauri-management/#removing-concepts-and-concept-schemes","title":"Removing concepts and concept schemes","text":"<p>Warning</p> <p>Be aware that removing a concept or concept scheme with child elements (top concepts or narrower concepts) means that the complete substructure, i.e.\u00a0all childs, are also deleted regardless whether they are used in another concept scheme.</p> <p>To remove concepts or concept schemes, select the resource in the navigation tree, click the context menu\u00a0 Show more options\u00a0and select the Remove option. Confirm the dialog and click\u00a0REMOVE.</p>","tags":["Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/","title":"Vocabulary Catalog","text":"","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/#introduction","title":"Introduction","text":"<p>Vocabularies are the foundation for semantic data lifting activities.This module shows the list of all managed vocabularies in Corporate Memory that are accessible for the user. The table represents the list of known vocabularies. Installed vocabularies are indicated by the orange switch in the column\u00a0<code>Installed</code>.</p> <p></p>","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/#add-new-vocabulary","title":"Add new vocabulary","text":"<p>Click\u00a0\u00a0to register a new vocabulary.</p> <p>A new form will be shown, fill it an add the file to import the vocabulary to Corporate Memory.</p> <p>Use the\u00a0Search\u00a0bar to find vocabularies based on name or other metadata.</p>","tags":["KnowledgeGraph","Vocabulary"]},{"location":"explore-and-author/vocabulary-catalog/#extended-information-and-options","title":"Extended information and options","text":"<p>Each table row provides a menu\u00a0with more options clicking on\u00a0\u00a0or in the\u00a0<code>Vocabulary</code>\u00a0column.</p> <p>A vocabulary which is known and available but not installed, looks like this:</p> <p></p> <p>Example of extended information of uninstalled Vocabulary Catalog</p> <ul> <li>Use\u00a0Install\u00a0or the switch in the column\u00a0<code>Installed</code>\u00a0to install the Catalog.</li> <li>Use\u00a0View\u00a0to access the Vocabulary.</li> </ul> <p>A vocabulary which is installed looks like this</p> <p></p> <p>Example of extended information of installed Vocabulary Catalog</p> <ul> <li>Use\u00a0Uninstall\u00a0to remove an installed vocabulary or\u00a0Install\u00a0to install a vocabulary.</li> <li>Use\u00a0View\u00a0to access the Vocabulary.</li> <li>Use\u00a0Upload\u00a0to install or overwrite the vocabulary from a file.</li> </ul>","tags":["KnowledgeGraph","Vocabulary"]},{"location":"getting-started/","title":"Getting Started","text":"","tags":["BeginnersTutorial"]},{"location":"getting-started/#introduction","title":"Introduction","text":"<p>This page describes how to work with Corporate Memory and shortly outlines all functionalities of the user interface. For the installation and configuration of Corporate Memory refer to the\u00a0\u2606 Deploy and Configure\u00a0section.</p> <p>Info</p> <p>Functions described in this manual for adding, editing or deleting data depend on the write permissions that are assigned to you. In case you only have read permission these functions are not available for you.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#about","title":"About","text":"<p>eccenca Corporate Memory is a semantic data management software that accelerates analytics and reporting projects by transforming the way enterprises understand, align, prepare and access their data.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#main-features","title":"Main features","text":"<p>The main features of Corporate Memory include:</p> <ul> <li>Flexible metadata and schema layer based on knowledge graphs</li> <li>Data virtualization and analytics</li> <li>Data integration and indexing</li> <li>Dataset and vocabulary management</li> <li>Thesaurus and taxonomy management</li> <li>Big data scalability</li> <li>Access control</li> </ul>","tags":["BeginnersTutorial"]},{"location":"getting-started/#minimal-requirements","title":"Minimal requirements","text":"<p>For the best user experience, we recommend to use the newest version of Google Chrome or Mozilla Firefox. Corporate Memory is tested with the following browsers:</p> <ul> <li>Google Chrome 83 or later</li> <li>Mozilla Firefox 78 or later</li> <li>Microsoft Edge 83 (on Windows) or later</li> </ul>","tags":["BeginnersTutorial"]},{"location":"getting-started/#login-and-logout","title":"Login and Logout","text":"<p>To start eccenca Corporate Memory:</p> <ol> <li>Enter the URL in your web browser.</li> <li>Select your workspace and click\u00a0CONTINUE WITH LOGIN.</li> <li>Enter your credentials and click\u00a0LOG IN.</li> </ol> <p>After you logged in to your Corporate Memory instance, the main application view appears.</p> <p>To log out, open the menu\u00a0 in the Module bar and click\u00a0Logout.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#workspaces","title":"Workspaces","text":"<p>A workspace is an endpoint of an eccenca DataPlatform identified by a workspace name and the DataPlatform URL. The specific configuration of the application defines which options are available here, i.e. whether you can select one of several workspaces, access only a default workspace or are allowed to create own workspaces. These options are configured by the system administrator.</p> <p>For more information on workspace configuration refer to the system manual of eccenca DataManager.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#selecting-a-workspace","title":"Selecting a workspace","text":"<p>To select an existing workspace open the drop-down list and click the workspace you want to open. The name and the DataPlatform URL of the selected workspace are shown under\u00a0Workspace Configuration. Click\u00a0CONTINUE WITH LOGIN\u00a0and enter your credentials to log in.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#adding-a-new-workspace","title":"Adding a new workspace","text":"<p>Info</p> <p>Whether this option is available depends on the configuration of Corporate Memory that is defined by the system administrator.</p> <p>To add a new workspace, open the drop-down list on the Workspaces window and click\u00a0Add New Workspace.</p> <p></p> <p>Under\u00a0Workspace Configuration\u00a0enter a Workspace Name and the DataPlatform URL.</p> <p>Click\u00a0SHOW OPTIONS\u00a0to display extended configuration options.</p> <p>Note</p> <p>Refer to the system manual of eccenca DataManager to get more information on all the options that can be configured here.</p> <p>Click on\u00a0CONTINUE WITH LOGIN\u00a0to save your entries.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#deleting-a-workspace","title":"Deleting a workspace","text":"<p>Note</p> <p>This option is only available for workspaces created by users themselves.</p> <p>To delete a workspace, select the workspace from the drop-down list on the Workspace screen and click\u00a0DELETE.</p> <p>The workspace is removed from the drop-down list.</p> <p>Note</p> <p>When you delete a workspace, the graph data is not deleted.</p> <p>This section describes the main elements of the graphical user interface of eccenca Corporate Memory.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#user-interface-and-modules","title":"User interface and modules","text":"<p>The user interface of Corporate Memory usually consists of two sections:</p> <ol> <li>The module bar providing access to the various modules of Corporate Memory and to a menu with further options</li> <li>The main section for operating the software functions</li> </ol> <p></p> <p>Each module provides a set of functionalities and views for specific use cases. To access a module, click the module name. The active module is highlighted.</p> <p>By default, Corporate Memory provides the following modules:</p> <ul> <li>EXPLORE - for Knowledge Graph browsing and exploration, specifically<ul> <li>Knowledge Graphs\u00a0- a generic and extensible RDF data browser and editor</li> <li>Vocabularies\u00a0- for vocabulary management</li> <li>Thesauri\u00a0- for managing thesauri and taxonomies based on SKOS</li> <li>Queries\u00a0- a SPARQL query interface</li> </ul> </li> <li>BUILD\u00a0- for creating and integrating Knowledge Graphs, with specific links to<ul> <li>Projects - the BUILD Projects level</li> <li>Datasets - the Datasets across all BUILD Projects</li> <li>Workflows - the Workflows across all BUILD Projects</li> <li>Activities - activities overview and monitoring</li> </ul> </li> </ul> <p>Note</p> <p>Depending on your specific product package, more or fewer modules can be available.</p> <p>Use the provided search field(s) in each module to search for specific keywords or strings in names and labels of resources.</p> <p>The\u00a0EXPLORE\u00a0module provides more search fields (e.g. in the Graph box, Navigation box, etc.) where you can limit your search to specific graphs or resources.</p> <p></p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#settings-menu-for-table-views","title":"Settings menu for table views","text":"<p>Each table view of Corporate Memory provides a setting menu (3) to adjust the table according to your requirements. You can change the sorting of columns, show or hide specific columns or set filters. To open the settings menu click\u00a0.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#context-help","title":"Context help","text":"<p>Corporate Memory provides a context help (4) for the main functions of modules. To open the context help click\u00a0\u00a0in the upper right corner. To close the context help, click\u00a0.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#used-icons","title":"Used Icons","text":"<p>This section provides an overview of icons and their functionality in Corporate Memory.</p>","tags":["BeginnersTutorial"]},{"location":"getting-started/#icons-of-main-interaction","title":"Icons of main interaction","text":"Icon Description Add a new element. Edit an element. Remove data or an element. Note that there is no \u2018Trash\u2019 or \u2018Recycle Bin\u2019. Open the context specific user help.","tags":["BeginnersTutorial"]},{"location":"getting-started/#icons-for-navigation","title":"Icons for navigation","text":"Icon Description Navigate back. Navigate one page backwards. Navigate one page forward. Go to the first page. Go to the last page.","tags":["BeginnersTutorial"]},{"location":"getting-started/#icons-for-table-views","title":"Icons for table views","text":"Icon Description Adjust settings for a table view. Change filter of table columns. Indicates no specific ordering. Order this column ascending. Order this column descending.","tags":["BeginnersTutorial"]},{"location":"getting-started/#editor-specific-icons","title":"Editor specific icons","text":"Icon Description Object mappings. Value mappings.","tags":["BeginnersTutorial"]},{"location":"release-notes/corporate-memory-19-10/","title":"Corporate Memory 19.10","text":"<p>Corporate Memory 19.10 is the third release in 2019.</p> <p>The highlights of this release are:</p> <ul> <li>Switched from an own OAuth 2.0 authorization server implementation to a more capable and supported solution based on Keycloak.</li> <li>Largely enhanced JDBC / SQL support:</li> <li>Overall performance improvements.</li> <li>Allowing hierarchical mappings to write to JDBC/SQL datasets.</li> <li>Support for Oracle SQL databases.</li> <li>Input validation of mandatory fields in shaceline resource details views.</li> <li>Resource Table results can be directly downloaded in Excel and other formats.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataPlatform configuration must be adapted according to the migration notes below.</p> <p>Consequently this release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v19.10</li> <li>eccenca DataIntegration v19.10</li> <li>eccenca DataManager v19.10</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#eccenca-dataintegration-v1910","title":"eccenca DataIntegration v19.10","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Write support for hierarchical data via JDBC.</li> <li>Allow arbitrary column names on <code>SqlEndpoint</code> datasets.</li> <li>Support for Oracle SQL.</li> <li>JSON Rest endpoint that allows to evaluate portions of a linking rules.</li> <li>SPARQL 1.1 endpoint each <code>RdfDataset</code> thereby allowing instantaneous SPARQL access via REST and SPARQL SERVICE keyword.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Add parameter that decides how empty values are handled by the concat transformer.</li> <li>Add \u2018ZIP file regex\u2019 parameter to all bulk resource datasets that allows to filter resources inside the bulk resource container (currently ZIP files only).</li> <li>Simplify Hive Serialization, refactor SQL utility methods.</li> <li>SPARQL Update operator</li> <li>Do not read from input data source when the SPARQL Update template is static, i.e. when it always runs the same static SPARQL Update query exactly once.</li> <li>Add SPARQL Update execution report containing various statistics, e.g. number of queries, query throughput etc.</li> <li>Support Apache Velocity Engine based templates. This adds logic like conditional branching and loops to the templates. For more information visit https://velocity.apache.org</li> <li>SPARQL Select operator generates an execution report with various statistics, e.g. rows processed, runtime etc.</li> <li>SPARQL dataset generates execution report when executing SPARQL Update queries e.g. from the SPARQL Update operator with statistics like remaining queries, time estimation etc.</li> <li>Extended SQL Endpoint documentation.</li> <li>Added local execution to JDBC dataset. The execution is more efficient and pushes limits and group-by columns into the database.</li> <li>While workspace is initializing, subsequent requests will timeout.<ul> <li>Config parameter <code>workspace.timeouts.waitForWorkspaceInitialization</code> (in milliseconds, default: 5000ms).</li> </ul> </li> <li>Scheduler: Added \u2018Stop On Error\u2019 parameter. If enabled this will stop a scheduler on the first execution error. Default: <code>false</code>.</li> <li>Added <code>addMarkdownDocumentation</code> parameter to <code>/plugins</code> and <code>/plugins/:pluginType</code> REST endpoints in order to request the optional Markdown documentation for plugins.</li> <li>Added SPARQL query timeout parameter in order to limit query execution times to \u2018Knowledge Graph\u2019 and \u2018SPARQL endpoint\u2019 datasets and to the SPARQL Select operator.</li> <li>RDF serialization: Tasks referenced in RDF serialization with <code>di:output</code> and <code>di:task</code> use the correct project task URIs instead of artificial URIs or literals.</li> <li>Complete Zip Stream support (replacing reliability on zip files only).</li> <li>MultiCsvZip are now BulkResourceDatasets.</li> <li>JDBC dataset: Removed database parameter. If required, the database needs to be specified as part of the JDBC URL, e.g., <code>jdbc:mysql://localhost:port/databaseName</code>. Appending connection parameters to the URL is supported now.</li> <li>Safer defaults:<ul> <li>SPARQL Endpoint and Knowledge Graph dataset: Do not clear graphs before execution to prevent accidental deletion of graphs.</li> <li>Knowledge Graph dataset: Increase page size from 1000 to 100,000, because small page sizes lead to suboptimal execution performance.</li> <li>SPARQL Update operator: Decrease batch size from 10 to 1.</li> </ul> </li> <li>Upgraded to Apache Spark 2.3.3</li> <li>SQL datasets: If a URI attribute is specified, the URI will be added as a new column of that name.</li> <li>All projects are loaded first, before any cache or any other autorun activity is started. This improves loading when both the workspace provider and some caches load from the same database.</li> <li>Improved JDBC dataset performance for MySQL and MariaDB by using the \u2018Load Data\u2019 command to load a local CSV file into the database.</li> <li>Shipping with MariaDB JDBC driver.</li> <li>Redirect to original request URL instead of the start page after authenticating a user and logging in.</li> <li>Revised generation of table names in JDBC dataset (see documentation). Keep letter case of table names.</li> <li>Schedulers are always started by the super user, if the super user is configured.</li> <li>On start-up if DataPlatform is configured, DI will wait for DP to become healthy for a configurable amount of time. Parameters:<ul> <li><code>eccencaDataPlatform.health.waitingTimeInSeconds</code>: Overall time in seconds DI should wait for DP to be up and healthy. Setting this to 0 will disable this check. Default: 60.</li> <li><code>eccencaDataPlatform.health.delayBetweenRetriesInSeconds</code>: Amount of time in seconds to wait between retries. Default: 5.</li> </ul> </li> <li>Enhanced <code>JdbcDataset</code> and <code>SqlEndpoint</code> parameters and improve their descriptions.</li> <li>Suppress case changes in <code>SqlEndpoint</code> table names.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#eccenca-datamanager-v1910","title":"eccenca DataManager v19.10","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>New module <code>task</code><ul> <li>Offers a direct resource actions. Interfaces only available by URL. See documentation for more details.</li> <li>Path <code>/task/resource/create</code> allows to create a new resource by given graph and type.</li> </ul> </li> <li>General<ul> <li>Config parameter <code>js.config.api.defaultTimeout</code> for default UI queries timeout.</li> <li>Config parameter <code>js.config.resourceTable.timeoutDownload</code> for Resource Table timeout on download requests on Explore and Query modules.</li> <li>Validation of mandatory fields in <code>shacline</code> view.</li> <li>Add new property <code>shui:onUpdateUpdate</code> for <code>sh:NodeShape</code>.</li> </ul> </li> <li>Module Explore<ul> <li>Config parameter <code>js.config.modules.explore.graphlist.whiteList</code> to filter specific graphs.</li> <li>Config parameter <code>js.config.modules.explore.graphlist.internalGraphs</code> to hide specific graphs.</li> <li>Config parameter <code>js.config.modules.explore.navigation.itemsPerPage</code> show items per page in navigation box.</li> <li>Support for inverse property relations.</li> </ul> </li> <li>Module Query<ul> <li>Config parameter <code>js.config.modules.query.timeout</code> for manual queries.</li> <li>Config parameter <code>js.config.modules.query.graph</code> to define the graph were data is saved and requested.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Default pagination size of 20 elements for all Resource Tables.</li> <li>Allow datatype <code>xsd:anyURI</code> for literals.</li> <li>Upgraded to react 16.</li> </ul> </li> <li>Module Explore<ul> <li>Merged graph view <code>RDFDoc</code> into \u2018resource details view\u2019.</li> <li>Renamed global search label.</li> <li>Graph creation will add the type <code>void:Dataset</code> instead of <code>owl:Ontology</code>.</li> <li>Use the label of the type of the instances for the name of the CSV file downloaded from the Resource Table.</li> <li>Display the context graph in <code>properties</code> and <code>references</code> tables.</li> </ul> </li> <li>Module Dataset<ul> <li>Adjusted position and tooltip of parameter <code>uriProperty</code> in \u2018Add data stepper\u2019.</li> </ul> </li> <li>Module Query<ul> <li>Use the dataset label for the name of the CSV file downloaded from the Resource Table.</li> </ul> </li> <li>Module Login<ul> <li>Renew tokens when they expire.</li> </ul> </li> <li>Module Administration<ul> <li>Allow to search in IRIs for list of readable and writeable graphs.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Module Explore<ul> <li>Config parameter <code>js.config.modules.explore.graphlist.listQuery</code> which is now obsolete.</li> <li>Config parameter <code>js.config.modules.explore.details.history</code> which is now obsolete as the feature is no longer supported.</li> <li>\u2018History\u2019 tab.</li> </ul> </li> <li>Module Sync also known as <code>SubscriptionManagement</code>.</li> </ul> <p>In addition to that, multiple stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#eccenca-dataplatform-v1910","title":"eccenca DataPlatform v19.10","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>SPARQL 1.1 Query endpoint<ul> <li>An <code>in-iris</code> property to the JSON <code>search</code> parameter to enable search over IRIs.</li> <li>A <code>timeout</code> parameter which allows to configure the maximal amount of milliseconds that a query execution can run.</li> <li>Support for Microsoft Excel (<code>.xlsx</code>) file download for <code>SELECT</code> queries.</li> </ul> </li> <li>SPARQL 1.1 Update endpoint<ul> <li>A <code>timeout</code> parameter which allows to configure the maximal amount of milliseconds that an update execution can run (Stardog only).</li> </ul> </li> <li>SPARQL 1.1 Graph Store Protocol<ul> <li><code>multipart/form-data</code> support for HTTP PUT.</li> <li>Added the <code>timeout</code> parameter, which allows to configure the maximal amount of milliseconds that a request execution should run.</li> <li>Documentation for content negotiation by <code>format</code> query parameter.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Data Sharing: A WebSub based Publish-Subscribe service for RDF named graphs.</li> <li>IoT Permissions Plugin: A plugin which enables the usage of the IoT Permissions Service API 2.</li> <li>OAuth 2.0 authorization server: Issues access tokens to a client after successfully authenticating a user.</li> <li>Authentication: User management via authentication providers as it was only needed by the OAuth 2.0 authorization server.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Stardog<ul> <li>Upgraded support to version 7.0.2.</li> <li>Versioning does no longer work with Stardog 7.</li> <li>Legacy versioning support for Stardog 6 (deprecated).</li> </ul> </li> <li>OAuth 2.0: Resource protection is now mandatory (can no longer be disabled, use anonymous access instead).</li> <li>SPARQL 1.1 Query endpoint<ul> <li>The value of the <code>string</code> property of the JSON <code>search</code> parameter is now tokenized which means that each token will be searched separately. Only results matching all tokens will be returned.</li> <li>Updated Spring Boot version from 1.5.21 to 1.5.22.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-19-10/#migration-notes","title":"Migration Notes","text":"<p>With the removal of the OAuth 2.0 authorization server capability, many configuration properties have been changed.</p> <ul> <li>Removed<ul> <li>The properties\u00a0<code>oauth2.clients.*</code>\u00a0have been removed.</li> <li>The properties\u00a0<code>authentication.*</code>\u00a0have been removed.</li> </ul> </li> <li>Moved<ul> <li>The property\u00a0<code>oauth2.jwt.signing.verificationKey</code>\u00a0has been moved to\u00a0<code>security.oauth2.resource.jwt.keyValue</code>\u00a0.</li> <li>The property\u00a0<code>oauth2.anonymous</code>\u00a0has been moved to\u00a0<code>security.oauth2.resource.anonymous</code>\u00a0.</li> <li>The claims mapping properties under\u00a0<code>oauth2.resourceServer.claimsMapping.*</code>\u00a0have been moved to\u00a0<code>security.oauth2.resource.jwt.claims.*</code>\u00a0.</li> <li>The properties\u00a0<code>oauth2.authorizeRequests.*</code>\u00a0to configure the resources to be protected by the resource server have been moved to\u00a0<code>security.oauth2.resource.authorizeRequests.*</code>\u00a0.</li> </ul> </li> <li>Added<ul> <li>The value of the property\u00a0<code>security.oauth2.resource.id</code>\u00a0\u00a0(defaults to\u00a0<code>dataplatform</code>) must be part of the\u00a0<code>aud</code>\u00a0(audience) claim in the JWT used to access a protected resource.</li> </ul> </li> </ul> <p>Don\u2019t forget to update your configuration accordingly. For instance, assuming you have the following old configuration:</p> <pre><code>oauth2:\nanonymous: true\nclients:\n- id: client\nsecret: secret\ngrantTypes:\n- authorization_code\nredirectUris:\n- http://example.org/oauth/client\njwt:\nenabled: true\nsigning:\nverificationKey: |\n-----BEGIN PUBLIC KEY-----\n...\n-----END PUBLIC KEY-----\nresourceServer:\nclaimsMapping:\nusername: 'preferred_username'\nclientId: 'azp'\ngroups:\nkey: 'groups'\n</code></pre> <p>The migrated properties should look like this:</p> <pre><code>security:\noauth2:\nresource:\nanonymous: true # optional, defaults to `false`\njwt:\nkeyValue: |\n-----BEGIN PUBLIC KEY-----\n...\n-----END PUBLIC KEY-----\nclaims:\nusername: preferred_username # optional, defaults to `preferred_username`\ngroups: groups # optional, defaults to `groups`\nclientId: azp # optional, defaults to `azp`\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/","title":"Corporate Memory 20.03","text":"<p>Corporate Memory 20.03 is the first release in 2020.</p> <p>The highlights of this release are:</p> <ul> <li>DataIntegration supports resources to be stored in an AWS S3 buckets.</li> <li>Rich SHACL forms can be used for the creation of new resources.</li> <li>New BUILD module is introduced in DataManager to provide an experts shortcut to DataIntegration.</li> <li>SPARQL queries can now be used to define arbitrary result tables directly in SHACL views.</li> <li>Object properties can be switched between chips and resource table view in SHACL views.</li> <li>cmemc, our Corporate Memory Command Line Interface is now generally available</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.03</li> <li>eccenca DataIntegration v20.03</li> <li>eccenca DataManager v20.03</li> <li>eccenca Corporate Memory Control (cmemc) v20.03</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-dataintegration-v2003","title":"eccenca DataIntegration v20.03","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Support for additional value types for mapping targets (XML Schema date/time types, duration, etc.).</li> <li>More date types to <code>DateTypeParser</code>.</li> <li>Script operator can also be used in local execution mode.</li> <li>Operator search in mapping rule editor.</li> <li>Safe-mode that prevents access to external data systems, e.g. JDBC, SPARQL dataset:<ul> <li>Data access in executed workflows is not affected by the safe-mode.</li> <li>Safe-mode can be toggled on and off at runtime in the UI.</li> <li>To enable safe-mode, set following parameter in the config: <code>config.production.safeMode = true</code>.</li> </ul> </li> <li>Config parameter <code>caches.config.enableAutoRun</code>, to enable/disable automatic execution of caches (default: <code>true</code>).</li> <li>Knowledge Graph File Upload Operator: Lets the user upload N-Triples files from the file repository into a DataPlatform graph.</li> <li>Support for file resource repositories on S3.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Improved password encryption<ul> <li>Using AES-256 instead of AES-128.</li> <li>If no valid key has been configured in production mode, application does not start.</li> <li>Better error messages, if key is invalid.</li> <li>Secret AES-256 key is generated from the configured key using SHA256 hashing, allowing for arbitrarily long keys.</li> </ul> </li> <li>Improved SQL writing performance for MariaDB and MySQL.</li> <li>Rework of the dataset view:<ul> <li>If a dataset is opened, the SPARQL (for RDF datasets) or table view (other datasets) is directly opened.</li> <li>Added Material Design formatting.</li> <li>Added scrollbars to tables with many columns.</li> </ul> </li> <li>Active learning UI uses Material Design cards.</li> <li>the config endpoint <code>/core/config</code> is no longer available when running in production mode.</li> <li>If a mapping reads from a CSV column that does not exist, the mapping still executes successfully, but a warning is displayed in the execution report.</li> <li>With XML dataset in streaming mode default URIs are now created by using the row and column numbers of the XML element instead of a hash value.</li> <li>Reduce memory foot print of linking evaluation and execution.</li> <li>We are now sorting tasks in workspace by label.</li> <li>Now displaying the modification date in resource dialog.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-datamanager-v2003","title":"eccenca DataManager v20.03","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General<ul> <li>Blank nodes are filtered in shacline views.</li> <li>Open external links in a new browser window.</li> <li><code>shui:valueQuery</code> for tabular representation<ul> <li>load of pre-defined queries as a <code>shui:valueQuery</code>.</li> </ul> </li> <li><code>ResourceTable</code> now allow to resolve labels on download results.</li> </ul> </li> <li>Access Control<ul> <li>Allow to create user and groups providing just a label.</li> </ul> </li> <li>Module Explore<ul> <li>Shacl views now allow to switch object property links between chip and <code>ResourceTable</code> view</li> <li>allow to add additional columns, search and filter using a <code>ResourceTable</code>.</li> <li><code>sh:path</code> is no longer mandatory on Shacl. One of both <code>sh:path</code> or <code>shui:valueQuery</code> is now mandatory.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Layout make better use of widescreen estate.</li> <li>Show existing resources linked by an object property in a <code>ResourceTable</code> in edit mode.</li> </ul> </li> <li>Module Explore<ul> <li>Navigation box uses search query only when a search term is present.</li> <li>Creation of new resources can now make use of rich shacline forms.</li> <li>Add new config parameter <code>modules.explore.navigation.defaultClass</code> that selects a default class EXPLORE should start with when <code>modules.explore.graphlist.defaultGraph</code> is defined.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Datasets management<ul> <li>Config parameter <code>includeOAuthToken</code> is no longer used. DataIntegration authentication will be done in an iFrame instead.</li> </ul> </li> <li>Access Control<ul> <li>Support for parameter <code>Requires client</code> has been removed from Access Control module.</li> </ul> </li> </ul> <p>In addition to that, multiple stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-dataplatform-v2003","title":"eccenca DataPlatform v20.03","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>SPARQL 1.1 Query endpoint<ul> <li>Support for non-string literals when using the <code>contains</code>, <code>startsWith</code> and <code>endsWith</code> filter functions.</li> <li>Server side label resolution by using <code>resolveLabels</code>, which allows <code>NONE</code> and <code>LABEL</code> for resolving IRIs to literals.</li> <li>The search parameter utilizes Stardog\u2019s built-in text match instead of <code>SPARQL CONTAINS</code> if a Stardog database is used. The search string is cleaned from special characters and english stop words and conjuncts all search terms.</li> </ul> </li> <li>SPARQL 1.1 Update endpoint<ul> <li><code>owl:imports</code> resolution on <code>USING</code>/<code>USING NAMED</code> clauses.</li> </ul> </li> <li><code>/info</code> and <code>/health</code> in addition to defaults <code>/actuator/info</code> and <code>/actuator/health</code> for backward compatibility.</li> <li>Show Redis status in application health if used as cache.</li> <li>The property <code>spring.security.oauth2.resourceserver.jwt.issuerUri</code> or <code>spring.security.oauth2.resourceserver.jwt.jwk-set-uri</code> must now be set in order to allow for JWT (signature) validation (see migration notes below).</li> <li>Access Conditions<ul> <li>Allow embedded creation of elements of type <code>eccauth:Account</code> or <code>eccauth:Group</code>.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Upgraded Stardog support to version 7.1.1.</li> <li>The default value of the property <code>spring.security.oauth2.resourceserver.jwt.claims.clientId</code> has been changed from <code>azp</code> to <code>clientId</code>.</li> <li>The properties under <code>security.oauth2.resource.jwt.claims.*</code> have been moved to <code>spring.security.oauth2.resourceserver.jwt.claims.*</code>.</li> <li>The property <code>security.oauth2.resource.anonymous</code> has been moved to <code>spring.security.oauth2.resourceserver.anonymous</code>.</li> <li>The property <code>http.cors.allowOriginRegex</code> has been moved to <code>http.cors.allowedOrigins</code>.</li> <li>The property <code>http.cors.allowMethods</code> has been moved to <code>http.cors.allowedMethods</code>.</li> <li>The property <code>http.cors.allowHeaders</code> has been moved to <code>http.cors.allowedHeaders</code>.</li> <li>The property <code>http.cors.exposeHeaders</code> has been moved to <code>http.cors.exposedHeaders</code>.</li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>Versioning support has been removed.</li> <li>Access Conditions<ul> <li>Support for <code>eccauth:requiresProtocol</code> and <code>eccauth:requiresClient</code> has been removed.</li> </ul> </li> <li>The properties <code>security.oauth2</code> have been removed.</li> <li>The property <code>http.cors.enabled</code> has been removed.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#eccenca-corporate-memory-control-cmemc-v2003","title":"eccenca Corporate Memory Control (cmemc) v20.03","text":"<p>This version of eccenca Corporate Memory Control (cmemc) adds the following new features:</p> <ul> <li><code>config</code> command group, to <code>list</code>, <code>edit</code> and <code>check</code> configurations</li> <li><code>graph</code> command group, to <code>list</code>, <code>import</code>, <code>export</code>, <code>delete</code> and <code>open</code> graphs</li> <li><code>project</code> command group, to <code>list</code>, <code>import</code>, <code>export</code>, <code>create</code> and <code>delete</code> projects</li> <li><code>query</code> command group, to <code>list</code> and <code>execute</code> local and remote SPARQL queries</li> <li><code>workflow</code> command group, to <code>list</code>, <code>execute</code>, <code>open</code> or <code>inspect</code> workflows</li> <li><code>workspace</code> command group, to <code>import</code> and <code>export</code> the workspace</li> <li>ability to work with SSL enabled deployments (add CA certs)</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#dataintegration","title":"DataIntegration","text":"<p>With v20.03 the following changes need to be made in your dataintegration.conf file when upgrading from v19.10:</p> <ul> <li>Remove the <code>play.crypto.secret</code> property, it has been deprecated with v20.03.</li> <li>Two properties need to be added: <code>play.http.secret.key</code> and <code>plugin.parameters.password.crypt.key</code><ul> <li>both take an arbitrary alpha numerical string of minimum 16 characters length</li> <li>depending on your deployment set them in your <code>production.conf</code> or <code>application.conf</code> DataIntegration configuration file</li> </ul> </li> </ul> <pre><code>...\nplay.http.secret.key = \"uiodshfoun78qwg8asd7gfasdasddfgn87gsn8fdsngasdfsngf8ds\"\n...\nplugin.parameters.password.crypt.key = \"uiodshfoun78qwg8\"\n...\n</code></pre> <p>Note</p> <p>In case you are deploying based on the DataIntegration docker images eccenca provides a <code>production.conf</code> configuration file needs to be used, the <code>dataintegration.conf</code> cannot be used to set the <code>play.http.secret.key</code> parameter.</p> <p>Warning</p> <p>The property <code>plugin.parameters.password.crypt.key</code> is used to encrypt / decrypt the passwords stored with you project configuration (e.g. JDBC passwords). When you set or change this property, all passwords in your DataIntegration projects need to be re-entered.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#datamanager","title":"DataManager","text":"<p>With v20.03 a the new BUILD module is introduced. In order to enable and configure it add the following section to you <code>application.yml</code>:</p> DataManager application.yml BUILD module configuration<pre><code>js.config.modules.build:\nenable: true\nurl: \"&lt;DI-BASE_URI&gt;/workspace\"\n</code></pre> <p>Where <code>&lt;DI-BASE-URI&gt;</code> need to point to the DataIntegration URI (e.g. <code>https://host.domain.com/dataintegration</code>).</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-03/#dataplatform","title":"DataPlatform","text":"<p>With v20.03 the following changes need to be made in your <code>application.yml</code> file when upgrading from v19.10:</p> <ul> <li>the key <code>http.cors.enabled</code> has been removed</li> <li>the key <code>http.cors.allowOriginRegex</code> has been renamed to <code>http.cors.allowedOrigins</code> and takes now a list of origins:</li> </ul> DataPlatform application.yml http.cors configuration<pre><code>http:\ncors:\nallowedOrigins: # optional, defaults to allow all: \"*\"\n- \"http://docker.local\"\n- \"https://docker.local\"\n</code></pre> <ul> <li>the key <code>security.oauth2.resource.jwt.keyValue</code> has been removed</li> <li>the key <code>spring.security.oauth2.resourceserver.jwt.jwk-set-uri</code> need to be specified.<ul> <li>Refer to your keycloaks Corporate Memory (cmem) realm \u201cOpenID Endpoint Configuration\u201d details where the relevant uri is listed as <code>jwks_uri</code>:</li> </ul> </li> </ul> DataPlatform application.yml spring.security configuration<pre><code>spring:\n## OAuth2Properties\nsecurity:\noauth2:\nresourceserver:\njwt:\njwk-set-uri: http://keycloak:8080/auth/realms/cmem/protocol/openid-connect/certs\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/","title":"Corporate Memory 20.06","text":"<p>Corporate Memory 20.06 is the second release in 2020.</p> <p>The highlights of this release are:</p> <ul> <li>Jinja template support in DataIntegration workflows</li> <li>Physical unit normalization and distance measure operators</li> <li>Versioning of user edits in SHACL based forms</li> <li>Named query API to get data without SPARQL know-how</li> <li>OpenAPI compliant DataPlatform API specification and UI</li> <li>Preview/Beta release of the upcoming DataIntegration Workspace</li> <li>Preview/Beta support for GraphDB as triple store backend \u2192 Vendor Homepage</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration, DataManager and DataPlatform configuration must be adapted according to the migration notes below. In addition to that, cmemc has some changed default outputs.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.06</li> <li>eccenca DataIntegration v20.06</li> <li>eccenca DataManager v20.06</li> <li>eccenca Corporate Memory Control (cmemc) v20.06</li> <li>eccenca Corporate Memory PowerBI Connector v20.06</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-dataintegration-v2006","title":"eccenca DataIntegration v20.06","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Workflow operator that evaluates a user-defined template on entities.<ul> <li>Jinja templating language is supported.</li> <li>Can be used after a transformation or directly after datasets that output a single table, such as CSV or Excel.</li> <li>For each input entity, an output entity is generated that provides a single output attribute, which contains the evaluated template.</li> <li><code>DataIntegration</code> transformation tasks can be used as Jinja filters.</li> </ul> </li> <li>Operators to normalize and compare physical quantities.<ul> <li>Transform rule operator to normalize physical quantities to a base unit.</li> <li>Distance measure to compare two physical quantities.</li> </ul> </li> <li>The transform evaluate tab allows the selection of the mapping rule to be evaluated.</li> <li>Rule operator for generating UUIDs.</li> <li>Complete rework of the workspace UI</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Removed deprecated SQL query strategies.</li> <li>Updated eccenca logo and favicon.</li> <li>Consistent resource deletion behavior for resource repositories:<ul> <li>For resource repositories that do not share resources between projects, resources are removed on project deletion.</li> <li>For resource repositories that do share resources between projects, resources are NOT removed on project deletion.</li> <li>In both cases, the user is informed in the UI about the behavior of the configured resource repository.</li> </ul> </li> <li>RDF Workspace Provider: Improved reading of project data if Graph Store protocol is supported by RDF endpoint.</li> <li>RDF Workspace Provider: Improved import of projects if Graph Store protocol is supported by RDF endpoint.</li> <li>More consistent labels for tasks, operators and their parameters.</li> <li>If active learning is started with an existing linkage rule, it\u2019s also used to generate the unlabeled pool.</li> <li><code>ExcelMapTransformer</code> reloads the referenced resource if it\u2019s modification time changed. For performance reasons the check may be deferred by some seconds.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-datamanager-v2006","title":"eccenca DataManager v20.06","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Query Module<ul> <li>The results of CONSTRUCT queries can now be downloaded.</li> </ul> </li> <li>General<ul> <li>Support to send base64 encoded queries in SPARQL and framed requests (configure with <code>js.config.api.sparqlQueryBase64Enconded</code>).</li> <li>Add new helper for unify datatype info (as regex)</li> </ul> </li> <li>Build module<ul> <li>Show/hide build module based on user (ACL) action <code>urn:eccenca:di</code>.</li> </ul> </li> <li>Shacline<ul> <li>Allow sh:name and sh:description to used multiple times - in different languages - in the SHAPE definitions.</li> <li>Support for the <code>xsd</code> formats <code>gYearMonth</code>, <code>gYear</code>, <code>gMonthDay</code>, <code>gDay</code>, <code>gMonth</code> and <code>duration</code>.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Upgrade node, npm and yarn.</li> <li>Allow <code>sh:pattern</code> and <code>sh:flags</code> for shapes definition of literals of type string, numeric and dates.</li> </ul> </li> <li>Shacline</li> <li>Shacl-groups without content are now hidden.</li> <li>ReadOnly properties are not available for adding on edit mode.</li> <li>Use languages from <code>js.config.titleHelper.languages</code> from config file as default languages.</li> </ul> <p>In addition to that, multiple stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-dataplatform-v2006","title":"eccenca DataPlatform v20.06","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>SPARQL 1.1 Query &amp; Update endpoint<ul> <li>Support for base64 encoded query strings.</li> </ul> </li> <li>Manual Edit Endpoint<ul> <li>Fine grained access control schemes for edits performed on SHACL shapes.</li> <li>Basic versioning of those edits.</li> <li>Shapes that support the configuration of those features directly in DataManager</li> </ul> </li> <li>OpenAPI 3 compliant API documentation<ul> <li>OpenAPI compliant documentation of all DataPlatform APIs</li> <li>Swagger UI based browser interface for interactive learning and experiments with the APIs</li> </ul> </li> <li>Backend Support<ul> <li>Support for GraphDB was added.</li> </ul> </li> <li>Additional APIs<ul> <li>Title Helper (<code>/api/explore/title</code>): Finds short labels for Resources.</li> <li>Named Query (<code>/api/queries</code>): Allows passing the identifier of the query and a parameterization to generate a CSV report</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>OpenAPI 3 compliant API documentation<ul> <li>All REST controllers have been annotated with OpenAPI metadata annotations</li> <li><code>SecurityConfiguration</code> has been modified to allow access to the <code>/v3/api-docs</code>(<code>.yaml</code>) and <code>/swagger-ui</code> endpoints.</li> </ul> </li> </ul> <p>The following features have been removed in this release:</p> <ul> <li>OpenAPI 3 compliant API documentation<ul> <li>RAML documentation</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-corporate-memory-control-cmemc-v2006","title":"eccenca Corporate Memory Control (cmemc) v20.06","text":"<p>This version of eccenca Corporate Memory Control (cmemc) adds the following new features:</p> <ul> <li>Shipped with a MacOSX binary (installable as copy deployment or via the homebrew package manager).</li> <li>Support for base64 encoded SPARQL queries and updates (<code>--base64</code>) - this needs eccenca DataPlatform v20.06.</li> <li>Extension of the query list command to list labels and parameters of queries in the query catalog (also the <code>--id-only</code> parameter to get an ID only list).</li> <li>Support for parameterized SPARQL queries: the query execute command now has the option <code>-p</code> / <code>--parameter</code> to provide parameter/value pairs for the query execution.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>query execute<ul> <li>default result format is now <code>text/csv</code> for tables and text/turtle for graphs (was <code>*</code> before)</li> <li>Migration Note: use <code>--accept</code> in case you need the old defaults</li> </ul> </li> <li>query list<ul> <li>now has a tabularized output</li> <li>Migration Note: use <code>--id-only</code> option to get the old URI only list</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved:</p> <ul> <li>cmemc now supports all SPARQL query types when executing a query from a file (there were errors on <code>ASK</code>, <code>DESCRIBE</code> and <code>CONSTRUCT</code> before)</li> <li>all tab completion results are now correctly sorted as well as filtered case-insensitively</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#eccenca-corporate-memory-powerbi-connector-v2006","title":"eccenca Corporate Memory PowerBI Connector (v20.06)","text":"<p>This is the first release of our PowerBI Connector which enables PowerBI users to retrieve data into PowerBI, based on collected queries in the Corporate Memory Query Catalog.</p> <p>The feature of this release are:</p> <ul> <li>add and delete eccenca Corporate Memory data sources</li> <li>get data out of SELECT queries from the Query Catalog</li> </ul> <p>We provided a tutorial for this new component: Consuming Graphs in Power BI</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#dataintegration","title":"DataIntegration","text":"<p>With v20.06 the API has been improved:</p> <ul> <li>The JSON format of transform, linking and workflow tasks has changed and is now consistent with dataset and custom tasks, i.e. all config parameters are now under property <code>parameters</code>.</li> <li>The JSON format of transform tasks has in addition following changes:<ul> <li><code>output</code> instead of <code>outputs</code> property that is a single string value instead of an array of strings.</li> <li><code>mappingRule</code> instead of <code>root</code> for the property of the mapping rule.</li> </ul> </li> <li>The JSON format of linking tasks had in addition following change:<ul> <li><code>output</code> instead of <code>outputs</code> property that is a single string value instead of an array of strings.</li> </ul> </li> <li>The JSON format for the <code>/plugins</code> endpoint has changed. The <code>type</code> attribute is now correctly specifying the JSON schema data type, e.g. <code>string</code> or <code>object</code>.</li> <li>The actual, more specific, parameter type has been renamed to <code>parameterType</code>.</li> <li>JSON format of resources have no relative and absolute path anymore.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#datamanager","title":"DataManager","text":"<p>With v20.06 a new title helper configuration section is introduced. It is used to define you language preferences in the data:</p> DataManager application.yml BUILD module configuration<pre><code>js.config.titleHelper:\nlanguages:\n- en\n- ''\n</code></pre> <p>In addition to that, we introduced a new Access Condition Action which represents the right to use the EXPLORE tab. Please have a look at the access condition documentation and add <code>urn:eccenca:ExploreUserInterface</code> to already existing conditions if needed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-06/#cmemc","title":"cmemc","text":"<p>With v20.06 the following changed need to be made:</p> <ul> <li><code>query execute</code> default result format has changed, to keep the previous behavior change your cmemc <code>query execute</code> calls to:<ul> <li><code>cmemc query execute --accept '*'</code></li> </ul> </li> <li><code>query list</code> has a different default output, to return to the previous behavior change your cmemc <code>query list</code> calls to:<ul> <li><code>cmemc query list --id-only</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/","title":"Corporate Memory 20.10","text":"<p>Corporate Memory 20.10 is the third release in 2020.</p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Release of the new DataIntegration workspace.</li> <li>Support for statement annotations, in order to express knowledge about specific statements.</li> <li>Support for tracking change sets for all shape based editing activities.</li> <li>Support for automation of vocabulary and dataset management with cmemc.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc has a change default behaviour.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.10.1</li> <li>eccenca DataIntegration v20.10</li> <li>eccenca DataManager v20.10.1</li> <li>eccenca Corporate Memory Control (cmemc) v20.10</li> <li>eccenca Corporate Memory PowerBI Connector v20.10</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-dataintegration-v20101","title":"eccenca DataIntegration v20.10.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Improvements to new Workspace UI:<ul> <li>New Workspace UI allows to export projects with and without file resources.</li> <li>Basic support for multiple languages in the New Workspace UI. Initially English and German are supported and plugins are not translated yet.</li> <li>Multi-step Project import in new workspace UI.</li> <li>Multi-step, asynchronous project import REST API.</li> <li>Profiling UI component to start dataset profiling and show profiling information in the dataset preview.</li> <li>Navigation menu in new workspace UI.</li> <li>In link tables, clicking on an entity redirects to the corresponding resource in DataManager, if the entity is coming from an RDF dataset.</li> </ul> </li> <li>New/improved operators:<ul> <li>New transform operator to retrieve lat/long of a location from a specified API in order to normalize location data.</li> <li>New operator to scale similarity values in linking rules by a specified factor.</li> <li>Email operator improvements:<ul> <li>multiple recipients in TO, CC and BCC</li> <li>CC and BCC recipients</li> <li>Timeout parameter</li> <li>SSL support</li> </ul> </li> </ul> </li> <li>Improvements to datasets<ul> <li>CSV Dataset supports UTF-8-BOM encoding for writing CSVs that open correctly in Excel.</li> <li>Support for #id and #text paths in JSON sources.</li> </ul> </li> <li>API improvements<ul> <li>Task activities API that allows to fetch a list of task activities with optional project and status filter.</li> <li>Profiling data is available via the API.</li> </ul> </li> <li>Global vocabulary cache that holds all installed vocabularies from the DataPlatform.<ul> <li>REST endpoint to trigger cache updates.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Vocabulary caches are not persisted between reboots and workspace reloads</li> <li>Disable geo location data type detector by default via plugin.blacklist parameter</li> <li>Item search API returns plugin IDs where available</li> <li>Expose some Amazon S3 client configuration. Can be changed in the Dataintegration configuration now</li> <li>Improvements to Spark execution engine<ul> <li>Entities are stored in DataFrames instead of RDDs</li> <li>Performance improvements</li> <li>Bugfixes</li> </ul> </li> <li>Check for usages of resources in all tasks, before deleting them. This was checked only in datasets before</li> <li>File management improvements<ul> <li>Allow multi file uploads</li> <li>Ask to replace existing files</li> <li>Allow to delete uploaded files in upload dialog</li> <li>When deleting files check for usages of resources in all items, before deleting them, e.g. transform tasks. This was checked only in datasets before</li> <li>When deleting files that are in use, link the dependent items</li> <li>Upload modal does not close when clicking outside of the modal</li> <li>If the limit parameter of the itemSearch API is set to 0, it will now return all search results instead of none</li> <li>Frontend initialisation endpoint returns initial language preference and configured DM base URL</li> </ul> </li> </ul> <p>Finally, the following performance and stability issues were solved:</p> <ul> <li>Regression: the output of a transformation is lost after reloading</li> <li>Added warning to the CSV datasets \u2018maxCharsPerColumn\u2019 parameter to make it clear that it affects the heap size</li> <li>Fixed reading of JSON files that contain Unicode byte order marks (BOMs)</li> <li>Workflow not interrupted on invalid XML from Triple-store</li> <li>Fixed generating paths for JSON files that contain keys with special characters, such as spaces. Those will be encoded now</li> <li>Project\u2019s rdfs:label uses project ID instead of label</li> <li>Generate consistent URIs for object mappings on JSON files</li> <li>Caches have not been written if the XML workspace provider was used</li> <li>Do not recreate caches on every run</li> <li>In link tables, the header shows the task labels instead of the task ids</li> <li>Fixed search field in link tables (did not work with characters that need to be URL encoded)</li> <li>Meta data description does not maintain whitespace formatting in XML serialisation</li> <li>New workspace UI has invalid favicon</li> <li>Creating a new project with description does not store the description in the new workspace UI</li> <li>XML Dataset: Values that include HTML entities are not retrieved</li> <li>Support for MS Internet Explorer 11 in new workspace</li> <li>Logout action not working. Should perform a global logout</li> <li>Deleting S3 backed resources broken due to a slash added to filenames</li> <li>Update PostgreSQL driver to v42.2.14 because of security vulnerability</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-datamanager-v20101","title":"eccenca DataManager v20.10.1","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General<ul> <li>Add translations and i18n language selection (and ship english and german translations)</li> <li>Allow for Annotation of Statements with additional meta data</li> <li>Integrate with the new DataIntegration workspace (Data Integration Tab)</li> </ul> </li> <li>Shacline<ul> <li>Add support for \u2018sh:languageIn\u2019 (as multiple values) in literal properties</li> </ul> </li> <li>Resource Tables<ul> <li>Allow Lucene syntax in the search field of any resource table (Query Syntax)<ul> <li>This search will be applied to the label(s) configured in proxy.labelProperties (cf. DataPlatform); by default the search will only be applied to the first column, the labels of the selected resource</li> </ul> </li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Shacline<ul> <li>Use the new resource/shaped API to generate / save shacl forms.</li> <li>Rendering empty fields on every change</li> <li>Add class triple to save only if class is a string.</li> <li>Prevent labels to be cloned on adding a new block.</li> <li>Nested Table query now defines default graph</li> <li><code>{graph}</code> can now be used as a placeholder in RFC6570 URI Template string</li> </ul> </li> <li>ResourceTable<ul> <li>Download data does not retain column order</li> <li>Add pagination/limit on config file</li> <li>Lock Drag and Drop while adding columns to prevent collision</li> <li>Update default pagination limit to 25 and default pagination interval to 5, 10, 25, 100, 500, 1000</li> </ul> </li> <li>General<ul> <li>use new backend API to retrieve labels.</li> <li>use new backend API to retrieve facets (possible columns)</li> <li>DEPRECATE titleHelper configuration parameters</li> <li>BREAKING remove support for Internet Explorer 11.</li> <li>Disable Datasets module, moved to Data Integration</li> <li>Disable Build module, moved to Data Integration</li> </ul> </li> <li>ResourceSelect<ul> <li>Wait until click on it to load values.</li> </ul> </li> <li>Explore<ul> <li>Cyclic references on Tabs content crash the app</li> <li>modules.explore.navigation.topQuery changed in order to list configured graph classes (<code>shui:managedClasses</code>)</li> <li>Update Navigation pagination limit to 15</li> <li>Load ResourceTable pagination limit from config file</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-dataplatform-v2010","title":"eccenca DataPlatform v20.10","text":"<p>This version of eccenca DataPlatform adds the following new features:</p> <ul> <li>Custom endpoint<ul> <li>Create custom json endpoints by defining a query for retrieving the data and a template for transforming the result.</li> </ul> </li> <li>Concise Boundary Description retrieval depth is adjustable.</li> <li>New submodule <code>:src:it</code> for integration tests</li> <li>Statement Annotations/Metadata<ul> <li>APIs for providing access and managing existing relations</li> </ul> </li> <li>Additional APIs<ul> <li>Explore Facets (<code>/api/explore/facets</code>): Lists the properties of a class or query.</li> <li>Graph List (<code>/api/graphs/list</code>): Returns a list of graphs readable by the current user, optionally including OWL imports.</li> <li>Graph List Detailed (<code>/api/graphs/list-detailed</code>): Like the previous one, but adding details of triples, classes and instances counts.</li> <li>Added openapi.server.urls env variable in order to define custom baseUrl to be used in</li> <li>Added resource shaping to the backend, this includes<ul> <li>Resource (<code>/api/resources</code>) api for getting information about individual resources</li> <li>Shape (<code>/api/shapes</code>) api for applying shape information onto the graph</li> <li>Statement Level Metadata (<code>/api/statementmetadata/</code>) management for adding statement annotations.</li> </ul> </li> <li>Added Caching to internal handling of prefixes, vocabularies and shapes lists. Caches are invalidated by updates.</li> <li>Added Showcase (<code>/api/admin/showcase</code>) endpoint, which inserts a scalable test dataset into the configured endpoint.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-corporate-memory-control-cmemc-v2010","title":"eccenca Corporate Memory Control (cmemc) v20.10","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>A <code>dataset</code> command group, enabling users to <code>create</code>, <code>delete</code> and <code>update</code> datasets as well as <code>upload</code> and <code>download</code> dataset file resources.</li> <li>A <code>vocabulary</code> command group, enabling users to manage vocabularies similar to the vocabulary catalog.</li> <li>The <code>query execute</code> command has some new options for limit, offset distinct and timeout settings.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Added:<ul> <li>The <code>workflow status</code> command has a <code>--project</code> option</li> </ul> </li> <li>Changed:<ul> <li>The <code>graph import</code> command outputs a replace/add status message per graph.</li> <li>Much faster <code>workflow status</code> retrieval by using a new activity API</li> <li>The <code>dataset export</code> command default file template changed to <code>{{date}}-{{connection}}-{{id}}.project</code></li> <li>The <code>query execute</code> command now uses POST instead of GET requests for SPARQL queries</li> </ul> </li> <li>Fixed:<ul> <li>The <code>graph import --replace</code> command  does not re-replace a same graph with a different file anymore.</li> <li>The completion of <code>--filename-template</code> resulted in files with wrong chars.</li> <li>The python version is disabled in completion mode.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#eccenca-corporate-memory-powerbi-connector-v2010","title":"eccenca Corporate Memory PowerBI Connector (v20.10)","text":"<p>This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#dataintegration","title":"DataIntegration","text":"<ul> <li>XML serialization for meta data elements is not forward compatible, i.e. projects exported with this version cannot be imported in older DataIntegration versions.</li> <li>The logout URL needs to be set to make sure that DataIntegration also triggers a logout inside the Keycloak instance:     <pre><code>oauth.logoutRedirectUrl = ${DEPLOY_BASE_URL}\"/auth/realms/cmem/protocol/openid-connect/logout?redirect_uri=\"${DEPLOY_BASE_URL}\n</code></pre></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#datamanager","title":"DataManager","text":"<ul> <li>The <code>graphInfo</code> flag in the explore module is now enabled by default.</li> <li>Due to the introduction of the new DataIntegration workspace these changes need to be applied:<ul> <li>The modules <code>build</code> as well as <code>datasets</code> are disabled now by default.</li> <li>The module <code>explore</code> is the default first entry point (<code>startsWith</code>).</li> <li>This section needs to be added to each workspace configuration: <code>yaml     DIWorkspace:       enable: true       url: /dataintegration/workbench</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-10/#cmemc","title":"cmemc","text":"<ul> <li>If your automation scripts rely on the created file name of the project export command, you need to change your scripts and set the old export name explicitly with <code>-t {{id}}</code>.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/","title":"Corporate Memory 20.12","text":"<p>Corporate Memory 20.12 is the fourth release in 2020.</p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: With the integration of all main views of the Data Integration build workbench, building Knowledge Graphs was never so smooth and streamlined.</li> <li>Automate: With cmemc\u2019s workflow io command, execution of workflows with variable file payload (input) as well as receiving data from a workflows (output) was never so easy before. Please refer to the corresponding tutorial.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataManager configurations have to be adapted according to the migration notes below. In addition to that, cmemc deprecates a command which will be removed in the next release.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v20.12</li> <li>eccenca DataIntegration v20.12</li> <li>eccenca DataManager v20.12</li> <li>eccenca Corporate Memory Control (cmemc) v20.12</li> <li>eccenca Corporate Memory PowerBI Connector v20.12</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-dataintegration-v2012","title":"eccenca DataIntegration v20.12","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Improvements to new Workspace UI<ul> <li>Existing views (e.g. editors and reporting interfaces) are now integrated.<ul> <li>Knowledge graph datasets show an embedded query and explore view.</li> </ul> </li> <li>Quick search dialog: The hotkey / allows to quickly switch between projects and tasks.</li> </ul> </li> <li>Optimized DataPlatform entity retrieval strategy for stability and performance.</li> <li>RDF workspace backend does write additional convenience properties for transform tasks and linking tasks<ul> <li><code>di:usedSourceClass</code>: lists all classes for which entities are read by this task.</li> <li><code>di:usedSourceProperty</code>: lists all properties that are read by this task.</li> <li><code>di:usedTargetClass</code>: lists all classes for which entities are written by this task.</li> <li><code>di:usedTargetProperty</code>: lists all properties that are written by this task.</li> </ul> </li> <li>Project resource download button.</li> <li>Persisted execution reports:<ul> <li>Added a configurable report manager that persists execution reports and allows to retrieve previous reports.</li> <li>Configurable retention time. Reports older than that will be deleted.</li> </ul> </li> <li>Error output for transformations:<ul> <li>Transformations have a new parameter \u201cerror output\u201d. When executing the workflow, all erroneous entities together with the error description are written to the configured dataset.</li> </ul> </li> <li>Add JSON sink</li> <li>Simple variable workflow execution REST endpoint<ul> <li>Allows to execute simple variables workflow (at most one variable input and/or output dataset)</li> <li>Input is provided via query parameters or directly in the request body</li> <li>Output is defined by the <code>ACCEPT</code> header and is output in the corresponding MIME type in the response body</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>ExcelMap operator:<ul> <li>If there are multiple values for a given key, it now returns all values instead of just the last one</li> </ul> </li> <li>Project file multi-upload:<ul> <li>Upload one file after the other instead of all at once</li> </ul> </li> <li>The configuration parameter <code>workbench.showHeader</code> is no longer supported. Instead, a URL parameter is used to decide whether the header should be shown.</li> <li>Data preview for XML, CSV and JSON now automatically loaded and shown on the dataset details page.</li> <li>After deleting an item the user is now redirected to the project page (previously to the workbench page).</li> <li>In addition to that, multiple performance and stability issues were solved.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-datamanager-v2012","title":"eccenca DataManager v20.12","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General:<ul> <li>Support for datatype <code>rdf:HTML</code> in object view.</li> </ul> </li> <li>Data Integration:<ul> <li>Add config parameter <code>DIWorkspace.baseURL</code> for logout.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped</p> <ul> <li>General:<ul> <li>Add error message if browser is not compatible: Safari, IE.</li> <li>Override default config language with workspace language if user did not select a language before.</li> </ul> </li> <li>Shacline:<ul> <li>Redirect to new resource after use the clone resource feature.</li> <li>Load graph permissions directly from the data request.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-dataplatform-v2012","title":"eccenca DataPlatform v20.12","text":"<p>This version of eccenca DataPlatform ships the following changes:</p> <ul> <li>Spring libraries:<ul> <li>Spring Boot version upgraded to <code>2.2.10.RELEASE</code>, Spring Cloud version upgraded to <code>Hoxton.RELEASE</code>.</li> </ul> </li> <li>Bootstrap &amp; Vocabularies:<ul> <li>cmem ontologies and graphs can now be updated using the bootstrap endpoint.</li> </ul> </li> <li>Statement Annotation:<ul> <li>additional endpoints for browsing and bulk editing</li> </ul> </li> <li>Graph List:<ul> <li>enriched endpoint with additional information</li> </ul> </li> <li>Label Resolution:<ul> <li>per default, in case no language matches, the precedence is ignored an any one value of the defined properties is taken as fallback.</li> <li>in case of multiple matches, the alphabetically first entry is chosen.</li> </ul> </li> <li>Localization / i18n (20.10.1):<ul> <li>language selection in titlehelper, shapes and facets API</li> <li>uses <code>shui:languageIn</code> instead of <code>sh:languageIn</code></li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-corporate-memory-control-cmemc-v2012","title":"eccenca Corporate Memory Control (cmemc) v20.12","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>The <code>workflow io</code> command was added to allow for executing workflows with variable file payload (input) and receive data from a workflow (output).<ul> <li>This feature is described in the advanced tutorial Processing data with variable input workflows.</li> <li>In addition to that, the concepts of io workflows is described in Workflow execution and orchestration.</li> </ul> </li> <li>The <code>admin</code> command group was added and includes the following commands:<ul> <li><code>bootstrap</code> - Update/Import bootstrap data.</li> <li><code>showcase</code> - Create showcase data.</li> <li><code>status</code> - Output health and version information.</li> </ul> </li> <li>The <code>template</code> option of the <code>graph export</code> command allows for using the <code>{{iriname}}</code> placeholder now.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>The <code>config check</code> command outputs a deprecation warning now (use the <code>admin status</code> command instead).</li> <li>cmemc now sends a User-Agent header with every call, currently: <code>cmemc/20.12 (Python 3.7.7)</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#eccenca-corporate-memory-powerbi-connector-v2012","title":"eccenca Corporate Memory PowerBI Connector v20.12","text":"<p>This release of our PowerBI Connector does not introduce new features or relevant changes. We provided a tutorial on how to use this component: Consuming Graphs in Power BI.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#dataintegration","title":"DataIntegration","text":"<ul> <li>No Forward Compatibility of Dataintegration Projects Exports<ul> <li>Due to a change in the internal XML serialization DI projects from this version can not be imported into instances running older version of DataIntegration.</li> <li>Please try the following workaround if this is something you need to perform. Contact our support team in case this procedure does not work in your case:</li> </ul> </li> </ul> <p>Info</p> <ol> <li>download your project resources (if needed)</li> <li>export the project using cmemc:<ul> <li>cmemc project export \u2013type rdfTurtle </li> </ul> </li> <li>import the project at the back-level instance using cmemc:<ul> <li>cmemc project import .project.ttl </li> </ul> </li> <li>upload your file resources to the back-level instance (if needed)</li> </ol> <ul> <li>Configuration<ul> <li>Remove the following configuration parameter: <code>workbench.showHeader</code></li> <li>In order to enable the new Execution Report Manager you need to configure the respective plugin, see \u201cExecution Report Manager\u201d in DataIntegration for Details.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#datamanager","title":"DataManager","text":"<ul> <li>In your workspaces configuration add <code>DIWorkspace.baseUrl</code> (mostly this will be <code>\"/dataintegration\"</code>): <pre><code>js.config.workspaces:\ndefault:\n...\nDIWorkspace:\n...\nbaseUrl: /dataintegration\n</code></pre></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-20-12/#cmemc","title":"cmemc","text":"<ul> <li>The <code>config check</code> command has been deprecated, please use the <code>admin status</code> command instead.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/","title":"Corporate Memory 21.02","text":"<p>Corporate Memory 21.02 is the first release in 2021.</p> <p></p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: Our re-designed mapping suggestion wizard make bootstrapping of your transformations a quick and easy exercise.</li> <li>Explore: interactively and visually browse through your graph with our integrated GRAPH visualization.</li> <li>Automate: The command line has never been more colorful, enjoyable and helpful (guessing your wishes as typos might happen). Experience the UX centric redesign of our command line client cmemc.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration, DataManager, DataPlatform configuration and cmemc command behavior has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.02.1</li> <li>eccenca DataIntegration v21.02.1</li> <li>eccenca DataManager v21.02</li> <li>eccenca Corporate Memory Control (cmemc) v21.02</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-dataintegration-v21021","title":"eccenca DataIntegration v21.02.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>REST request operator:<ul> <li>added \u201cAccept all SSL certificates\u201d parameter in advanced section. Default: <code>false</code>.</li> </ul> </li> <li>If the option <code>eccencaDataPlatform.writeGraphType</code> is set to true the a graph in CMEM generated by Dataintegration will contain a triple (<code>&lt;graph&gt; rdf:type di:Dataset</code>) indicating that this graph was written by DataIntegration.</li> <li>Improved mapping suggestion:<ul> <li>Allows to select from multiple matching candidates.</li> <li>The user can switch between data source and target vocabulary view.</li> <li>A sub-set of the vocabularies can be selected.</li> <li>The user can pick any (non-matched) property from the target vocabularies via search.</li> <li>Improved filtering and sorting.</li> <li>Added multi word text search filter.</li> <li>Allows to choose am existing or custom URI prefix for the auto-generated target properties.</li> <li>Added tooltip for source paths with e.g. example values.</li> <li>Added tooltip for target property selection with meta data and a link to DataManager for that property resource.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>The \u201cTimestamp to date\u201d and \u201cDate to timestamp\u201d transformers support configurable time units and full <code>xsd:dateTime</code> values.</li> <li>Updated build to Spark 2.4, Scala 2.12 and sbt 1.x</li> <li>If a transformer inside a linkage rule throws a validation error, it will no longer fail the entire linking task, but will not generate a value for that transformer.</li> <li>In the linking evaluation view and the reference links view, validation errors are shown in the evaluation tree.</li> <li>Allow to persist caches between restarts in order to reduce application start-up time.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-datamanager-v2102","title":"eccenca DataManager v21.02","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Explore<ul> <li>Add a tab with the Ontodia tool to explore graph detail view.</li> <li>Make the Ontodia tab configurable: js.modules.explore.details.ontodia.enable</li> </ul> </li> <li>Shacline<ul> <li>Display rdfs:comment of the selected node shape in the shacl form view.</li> <li>Add group:comment to the header of groups.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Explore<ul> <li>Remove js.modules.explore.graphlist.internalGraphs from DataManager config since DataPlatform is now providing this information.</li> <li>Rename tab: ontodia (Data@en, Daten@de).</li> <li>Rename tab: visualization (Vocab@en, Vocab@de).</li> <li>Move ontodia tab next to visualization tab.</li> </ul> </li> <li>Vocabulary<ul> <li>Fix format to new endpoint with Titles already loaded.</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-dataplatform-v21021","title":"eccenca DataPlatform v21.02.1","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>GraphDB datastore implementation<ul> <li>Added <code>spring.servlet.multipart.max-file-size: \"20GB\"</code> in properties files and dist file.</li> <li>Added <code>useDirectTransfer</code> to use native Graph Store operation without shared folder</li> </ul> </li> <li>Admin Endpoint for listing currently running queries.</li> <li>Graph API<ul> <li>New endpoints for owl:imports resolution.</li> </ul> </li> <li>Bootstrap Data<ul> <li>Shapes for managing prefix declarations</li> <li>Shape catalogs have prefix declarations as managed classes.</li> </ul> </li> <li>Imports resolution<ul> <li><code>owl:imports</code> self references ignored.</li> <li><code>owl:imports</code> are now resolved for graph uris in request parameters.</li> </ul> </li> <li>Vocabulary List<ul> <li>Title resolve according to standard i18n settings.</li> </ul> </li> <li>Shapes Endpoint<ul> <li>Comments of shapes included.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Dist Config<ul> <li><code>skos:prefLabel</code> now preferred over <code>rdfs:label</code>.</li> </ul> </li> <li>Query Logging<ul> <li>Query logging uses console appender only.</li> <li>Query logging level set to <code>DEBUG</code>.</li> <li>Environment variables no longer needed:<ul> <li><code>QUERY_LOGGING_DIR</code></li> <li><code>QUERY_LOGGING_MAX_FILE_SIZE</code></li> <li><code>QUERY_LOGGING_TOTAL_SIZE_CAP</code></li> <li><code>QUERY_LOGGING_MAX_HISTORY</code></li> </ul> </li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#eccenca-corporate-memory-control-cmemc-v2102","title":"eccenca Corporate Memory Control (cmemc) v21.02","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>New configuration option <code>OAUTH_GRANT_TYPE=prefetched_token</code></li> <li>New <code>admin token</code> command, fetch and output an access token</li> <li>New <code>project open</code> command, open projects in the browser</li> <li><code>graph list</code> command<ul> <li>Added table output with graph type and label.</li> <li><code>--id-only</code> option added: get only graph IRIs.</li> </ul> </li> <li><code>project list</code> command<ul> <li>Added table output with project ID and label.</li> <li><code>--id-only</code> option added: get only project IDs.</li> <li><code>--raw</code> option added: get raw JSON.</li> </ul> </li> <li><code>workflow list</code> command<ul> <li>Added table output with workflow ID and label.</li> <li><code>--id-only</code> option added: get only graph IRIs.</li> <li><code>--raw</code> option added: get raw JSON.</li> </ul> </li> <li><code>graph list</code> command<ul> <li><code>--filter imported-by IRI</code> added: filter to all graphs imported recursively by a graph.</li> </ul> </li> <li><code>graph tree</code> command added, output <code>owl:imports</code> tree for each selected graph.</li> <li><code>graph export</code> and <code>graph delete</code> command<ul> <li><code>--include-imports</code> option added: work with selected graph(s) and all graphs which are imported from the selected graph(s).</li> </ul> </li> <li><code>workflow list</code> command<ul> <li><code>--filter</code> option added: filter by project or io command capability (input, output, both, any).</li> </ul> </li> <li><code>graph export</code> command<ul> <li><code>--create-catalog</code> added: create a Protege XML catalog for import resolution.</li> </ul> </li> <li><code>query status</code> command, list and view still running and executed queries.</li> <li>output coloring<ul> <li>json output is highlighted</li> <li>help texts are colored (red terms indicate writing commands, possible dangerous to your data)</li> <li>table headers are colored</li> </ul> </li> <li>git-like did-you-mean command suggestion for misspelled commands</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>docker image: now based on <code>debian:stable-20201209-slim</code>.</li> <li><code>graph list</code> command<ul> <li>default output changed.</li> <li>use the <code>--id-only</code> or <code>CMEMC_GRAPH_LIST_ID_ONLY=true</code> to get the IRI list.</li> </ul> </li> <li><code>project list</code> command<ul> <li>default output changed.</li> <li>use the <code>--id-only</code> or <code>CMEMC_PROJECT_LIST_ID_ONLY=true</code> to get the ID list.</li> </ul> </li> <li><code>workflow list</code> command<ul> <li>default output changed.</li> <li>use the <code>--id-only</code> or <code>CMEMC_WORKFLOW_LIST_ID_ONLY=true</code> to get the ID list.</li> </ul> </li> <li><code>graph list</code> command<ul> <li>filter option changed.</li> <li>use <code>--filter access readonly</code>|<code>writeable</code> instead of <code>--filter readonly</code>|<code>writeable</code></li> </ul> </li> <li><code>workflow open</code> command<ul> <li>URL changed for new workbench</li> </ul> </li> <li><code>admin status</code> command, now warns you if cmemc is too new for the current backend</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#dataintegration","title":"DataIntegration","text":"<ul> <li>Timestamp to date operator changed default behavior<ul> <li>The \u201cTimestamp to date\u201d now assumes milliseconds instead of seconds by default. In addition, it generates full xsd:dateTime values instead of simple dates.<ul> <li>To makes sure that existing usages don\u2019t break, please open {DataIntegration}/api/core/usages/plugins/timeToDate and check all usages.</li> <li>In order to revert to the previous behavior, the following changes have to be made to each usage:<ul> <li>Change the unit to \u201cseconds\u201d.</li> <li>Change the format to \u201cyyyy-MM-dd\u201d</li> </ul> </li> </ul> </li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#datamanager","title":"DataManager","text":"<ul> <li>In your application.yml the following config property can be removed, if existing. This information is now internally provided by Datalatform: <code>js.modules.explore.graphlist.internalGraphs</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#dataplatform","title":"DataPlatform","text":"<ul> <li>in case you used Query Logging of DataPlatform you can remove the following environment variables as they are no longer needed: <code>QUERY_LOGGING_DIR</code>, <code>QUERY_LOGGING_MAX_FILE_SIZE</code>, <code>QUERY_LOGGING_TOTAL_SIZE_CAP</code>, <code>QUERY_LOGGING_MAX_HISTORY</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-02/#cmemc","title":"cmemc","text":"<ul> <li><code>workspace import</code>|<code>export</code>|<code>reload</code> commands are deprecated now<ul> <li>use <code>admin workspace import</code>|<code>export</code>|<code>reload</code> commands instead</li> </ul> </li> <li>Many commands have new default output:<ul> <li><code>graph list</code>  command, use the <code>--id-only</code> or <code>CMEMC_GRAPH_LIST_ID_ONLY=true</code> to get the IRI list.</li> <li><code>project list</code> command, use the <code>--id-only</code> or <code>CMEMC_PROJECT_LIST_ID_ONLY=true</code> to get the ID list.</li> <li><code>workflow list</code> command, use the <code>--id-only</code> or <code>CMEMC_WORKFLOW_LIST_ID_ONLY=true</code> to get the ID list.</li> <li><code>graph list</code> command, use <code>--filter access readonly</code>|<code>writeable</code> instead of <code>--filter</code> <code>readonly</code>|<code>writeable</code></li> </ul> </li> <li>The command config check  was removed (was deprecated in v20.12)</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/","title":"Corporate Memory 21.04","text":"<p>Corporate Memory 21.04 is the second release in 2021.</p> <p></p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: The mapping editor now allows for auto-completion of paths on any level in multi-hop paths, including source type specific paths with special semantics, e.g. <code>#idx</code> for CSV datasets. This feature lowers the barrier for new Corporate Memory users and allows for much master mapping creation.</li> <li>Explore: Manual authoring of resources via SHACL-shape based customized user interfaces is now supported with client-side datatype validation (in addition to store-based validation). This feature provides instant user feedback while typing Literals and therefor allows faster data entry.</li> <li>Automate: The new vocabulary import command of cmemc adds a turtle file as a vocabulary to Corporate Memory (upload and create catalog entry). This allows for automation of CI/CD pipeline which depend on vocabularies managed in a Git Repository.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataManager configuration as well as cmemc command behavior has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.04</li> <li>eccenca DataIntegration v21.04</li> <li>eccenca DataManager v21.04</li> <li>eccenca Corporate Memory Control (cmemc) v21.04</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-dataintegration-v2104","title":"eccenca DataIntegration v21.04","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>A new mapping parameter enables the user to specify whether single or multiple values are written by a particular mapping:<ul> <li>Supported by both value and object mappings.</li> <li>Replaces the \u201cis Attribute\u201d parameter on value mappings, which has been specific to XML.</li> <li>Generates a validation error if multiple values are written if single values are configured.</li> <li>Datasets may adapt the written schema based on the chosen option (see help text in mapping editor).</li> </ul> </li> <li>Improved auto-completion value path mapping field:<ul> <li>Allows auto-completion of paths on any level in multi-hop paths.</li> <li>Supports auto-completion of properties inside of property filters.</li> <li>Proposes data source specific paths with special semantics, e.g. <code>#idx</code> for CSV.</li> <li>Validates the syntax of the value path and highlights errors to the user.</li> </ul> </li> <li>Tasks can be copied between projects.</li> <li>A new transform task parameter (<code>abortIfErrorsOccur</code>) specifies whether the execution will fail if a validation error occurs.</li> <li>Added URI literal type for writing <code>xsd:anyURI</code> values to Knowledge Graphs.</li> <li>Added option for \u201cRegex extract\u201d transformer for extracting all matches.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Support editing of auto-completed values in mapping editor.</li> <li>Mapping editor auto-complete element:<ul> <li>Support editing of auto-completed values.</li> <li>added support for multi-word search highlighting.</li> <li>Show label and value of the selected value, e.g. label and URI for target property.</li> </ul> </li> <li>Improvements to execution reports<ul> <li>Added an overview section, which displays general information about the execution:<ul> <li>Final workflow status</li> <li>Start, finish and cancellation times</li> <li>Users account which started and canceled (if any) the execution</li> </ul> </li> <li>All operator executions are persisted. For instance, workflow reports will contain separate task reports for writing and reading a dataset.</li> <li>For each operator execution, an optional operation label can be shown (e.g., read, write, generate queries).</li> <li>The order of the task reports is stable now and reflects the order of execution.</li> <li>Added a scrollbar to the task list, if it is too large to be displayed.</li> </ul> </li> <li>Improved JSON writing support:<ul> <li>A rewritten implementation supports arbitrarily large JSON files by using a memory-mapped key-value store.</li> <li>An optional template may be specified to customize the written JSON.</li> <li>Values are only wrapped in JSON arrays if the multiple value option is set in the mapping.</li> </ul> </li> <li>Transform Evaluation now shows evaluation of the URI rule.</li> <li>Improved workflow saving:<ul> <li>On loading, the save button is only enabled after the workflow has been loaded to prevent the user from saving an empty workflow.</li> <li>A spinner is shown while the workflow is saved.</li> <li>The save button is disabled while the workflow is saved.</li> <li>A confirm dialog is shown, if the user tries to leave while a save is in progress.</li> </ul> </li> <li>The behavior of linking rules in case of missing values has been improved:<ul> <li>The <code>required</code> attribute has been removed, because it has lead to unexpected and sometimes inconsistent behavior.</li> <li>Boolean aggregators have been reworked to interpret missing values as false.</li> <li>The \u201cHandle missing values\u201d aggregator has been added to handle cases in which missing values should default to a user specified score (For instance, if missing value should be interpreted as true).</li> <li>The \u201cDefault value\u201d transformer has been added to generate default values for missing values.</li> </ul> </li> <li>Improved inline documentation using markdown rather than text description.</li> <li>Workflow progressbars now show the task labels instead of their internal identifiers.</li> <li>Page header contents are now created directly in the artifact view templates instead being an independent component pulling all information via holistic approach.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-datamanager-v2104","title":"eccenca DataManager v21.04","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Shacl<ul> <li>Add datatype validation for all supported datatypes</li> </ul> </li> <li>Configuration<ul> <li>Add a configurable link to account settings in keycloak:<ul> <li><code>js.config.modules.accountSettings.enable: true</code></li> <li><code>js.config.modules.accountSettings.url: http://docker.local/auth/realms/cmem/account/?referrer={{REFERRER}}&amp;referrer_uri={{REFERRER_URI}}</code></li> </ul> </li> <li>General<ul> <li>Global error handling to display errors preventing most grey screens</li> </ul> </li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General         - Use redux store to manage notifications in DataManager (MessageHandler) and improve error parse / handle         - Use redux store to manage main application state.         - Change value of <code>js.config.modules.explore.overallSearchQuery</code> and <code>js.config.modules.explore.navigation.searchQuery</code> to use the <code>\"\"\"\"</code> SPARQL string separator.         -   BREAK please use <code>\"\"\"</code> if you use custom queries for that values</li> <li>Development<ul> <li>Switch to GUI elements repository from Github</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-dataplatform-v2104","title":"eccenca DataPlatform v21.04","text":"<p>These followin changes are shipped:</p> <ul> <li>General<ul> <li>Virtuoso is now using the custom build-in function to list graphs faster.</li> </ul> </li> <li>Bootstrap Data<ul> <li>ucum removed as default vocab in the vocabulary catatog</li> <li>qudt added as a default vocab in the vocabulary catalog</li> <li>all vocabularies are provided via download.eccenca.com now</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p> <ul> <li>Health Endpoint<ul> <li>race condition in the health condition that leads to rare cases of wrongly reporting DOWN</li> </ul> </li> <li>HTTP Connection Pool<ul> <li>size increased to increase parallelism and resilience</li> </ul> </li> <li>Statement-Level Metadata<ul> <li>works now on inverse properties</li> </ul> </li> <li>Graph List<ul> <li>incorrect type statements are ignored</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#eccenca-corporate-memory-control-cmemc-v2102","title":"eccenca Corporate Memory Control (cmemc) v21.02","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li>new <code>config get</code> command<ul> <li>get the value of certain configuration key (such as <code>DP_API_ENDPOINT</code>)</li> </ul> </li> <li>new <code>dataset open</code> command<ul> <li>similar to the other open commands, opens a dataset in the browser</li> </ul> </li> <li><code>graph export</code> command<ul> <li><code>--mime-type</code> option added to specify requested mime type</li> <li>the default mime type is still <code>application/n-triples</code></li> </ul> </li> <li>new <code>vocabular import</code> command<ul> <li>Import a turtle file as a vocabulary (upload and create catalog entry)</li> </ul> </li> <li><code>project export</code> command<ul> <li><code>--extract</code> option added in order to export projects to directories</li> <li><code>--help-types</code> option added to get a list of export formats</li> </ul> </li> <li><code>project import</code> command<ul> <li>add support for importing projects from extracted directories</li> <li>add <code>--overwrite</code> option to import files/directory to an existing project</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>default values for <code>OAUTH_USER</code> and <code>OAUTH_PASSWORD</code> is <code>None</code> except for grant type password</li> <li>docker base image forwarded to <code>debian:stable-20210408-slim</code></li> <li><code>graph export</code> command<ul> <li>does not load the result in memory anymore but stream the result to the file</li> </ul> </li> <li><code>graph import</code> command<ul> <li>does not load the payload in memory anymore but stream it to the endpoint</li> </ul> </li> <li><code>project export</code> command<ul> <li>command now fails early if a non-existing project is requested</li> </ul> </li> <li><code>project create</code> command<ul> <li>command now fails early if a project is already there</li> </ul> </li> <li><code>project delete</code> command<ul> <li>command now fails early if a non-existing project is requested</li> </ul> </li> <li><code>project import</code> command<ul> <li>now uses the new project import API</li> </ul> </li> <li>removed reference to config keys<ul> <li><code>CMEM_BASE_PROTOCOL</code> and <code>CMEM_BASE_DOMAIN</code> were never used</li> </ul> </li> <li>remove <code>workspace</code> command group from root<ul> <li>was in 21.02 deprecated</li> <li>now removed from root and available as <code>admin workspace</code> command group</li> </ul> </li> <li>fix: <code>project export</code> command<ul> <li>command now fails correctly with exit code 1 if a non-existing project is requested</li> </ul> </li> <li>fix: <code>project import</code> command<ul> <li>command now fails correctly with exit code 1 in case of an import to an existing project</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#dataintegration","title":"DataIntegration","text":"<ul> <li>The behavior of linking rules in case of missing values has been changed:<ul> <li>Now, boolean aggregations (AND, OR) interpret missing values as \u201cfalse\u201d.</li> <li>Non-boolean aggregations will returns \u201c-1\u201d if values for at least one input are missing.</li> <li>If another behavior is expected, the \u201cHandle missing values\u201d aggregation or the \u201cdefault value\u201d transformer can be used in both cases.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#datamanager","title":"DataManager","text":"<ul> <li>To allow special characters to be search in <code>js.config.modules.explore.overallSearchQuery</code> and <code>js.config.modules.explore.navigation.searchQuery</code>, use <code>\"\"\"\"</code> instead of just <code>\"</code> to delimitate strings with search placeholders. e.g: <code>regex(str(?resource),\"{{QUERY}}\",\"i\")</code> should be written as <code>regex(str(?resource),\"\"\"{{QUERY}}\"\"\",\"i\")</code></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#dataplatform","title":"DataPlatform","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-04/#cmemc","title":"cmemc","text":"<ul> <li>The exit code values of <code>project import</code> and <code>export</code> commands are fixed (in case of failure) so you may have to change these calls in your scripts.</li> <li>The deprecated <code>workspace</code> command group is now only available as <code>admin workspace</code> command group so you have to change these calls in scripts.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/","title":"Corporate Memory 21.06","text":"<p>Corporate Memory 21.06 is the third release in 2021.</p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: The Data Integration workflow editor got a complete remake based on a more flexible and better extensible drawing engine. Workflow tasks use the same icons and tags from the workspace now and are better integrated in the build user interface.</li> <li>Explore: The new Data Manager vocabulary viewer visualises classes and its relations (subclasses, domain/range relations) from an installed vocabulary.</li> <li>Automate: cmemc is now able to fetch credentials from external processes in order to integrate with company wide or personal password infrastructure.</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and cmemc configuration and behaviour has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.06</li> <li>eccenca DataIntegration v21.06.1</li> <li>eccenca DataManager v21.06.3</li> <li>eccenca Corporate Memory Control (cmemc) v21.06</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#eccenca-dataintegration-v21061","title":"eccenca DataIntegration v21.06.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>New workflow editor:<ul> <li>Completely rewritten workflow editor that replaces the old editor.</li> <li>The old workflow editor can be re-enabled by setting the following configuration value: <code>workbench.tabs.legacyWorkflowEditor = true</code>.</li> <li>Added API endpoint to fetch workflow node (input) port configurations.</li> </ul> </li> <li>Enable the creation and execution of nested workflows.<ul> <li>Workflows can be nested within other workflows.</li> <li>Nested workflow reports can be viewed in the execution report.</li> <li>Already nested workflows cannot be used in other nested workflows to protect from too complex projects.</li> </ul> </li> <li>Added script transform operators<ul> <li>Python and Scala are supported as scripting languages.</li> <li>Need to be enabled in the configuration.</li> </ul> </li> <li>Template transform operator.</li> <li>Synonym-based mapping suggestion<ul> <li>Added global vocabulary synonym cache that extracts synonyms for vocabulary properties from existing mapping rules.</li> <li>Use synonyms in mapping suggestion so more properties can be suggested to the user based on existing mapping rules that map similarly named attributes/properties.</li> <li>Config parameters:</li> <li><code>mapping.suggestion.features.extractSynonymsFromExistingMappingRules.enabled</code>: Enables the synonym based mapping suggestion. Default: <code>true</code></li> <li><code>mapping.suggestion.features.extractSynonymsFromExistingMappingRules.timeBetweenRefreshes</code>: The minimum time in milliseconds between synonym cache refreshes. Default: 10 seconds</li> <li><code>mapping.suggestion.features.extractSynonymsFromExistingMappingRules.waitForCacheToFinish</code>: The max. time to wait for a new cache value during a mapping suggestion request if the current value has gotten stale. Default: 50ms</li> </ul> </li> <li>New OpenAPI based documentation of HTTP API:<ul> <li>Replaces the previous RAML-based documentation.</li> <li>Can be viewed live in the UI at <code>{DI_URL}/doc/api</code>.</li> </ul> </li> <li>New Neo4j dataset, which supports writing into Neo4j graphs and reading them back.</li> <li>Coalesce transform operator that forwards the first input that has any value/s.</li> <li>Add concrete item type, e.g. \u2018CSV\u2019 or \u2018Transform\u2019, to search result and recently viewed items and make it searchable.</li> <li>Add tooltip to search item if item description is too long to show in a single line.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Mapping suggestion improvements:<ul> <li>Support source path column filters: show only already mapped source paths, show only unmapped source paths.</li> <li>Do not show filters in from-vocabulary view that cannot be applied, e.g. show auto-generated only.</li> <li>Shortened source paths are shown as tooltip in full length on hover.</li> <li>Show source path type (data type or object) in source info box.</li> <li>For object source paths show their direct sub-paths.</li> <li>Improve error reporting in mapping suggestion and mapping rule example view.</li> </ul> </li> <li>Prefix management improvements:<ul> <li>Validate prefix name and value in the UI</li> <li>Ask before updating existing prefix names. Also change button to \u2018Update\u2019 when prefix name matches an existing prefix.</li> </ul> </li> <li>Allow object rule mappings with empty target property and non-empty source path in order to change the source resource, but stay on the target resource.</li> <li>The configuration of plugins (in particular the blacklist) has been improved.<ul> <li>Plugin configuration has been grouped under a common root</li> <li>Plugins can be enabled/disabled individually</li> <li>See breaking changes for details</li> </ul> </li> <li>E-mail operator extensions and improvements:<ul> <li>Allow to send multiple e-mails with different configurations (from, to, subject, content, cc, bcc).</li> <li>Add e-mail execution report</li> <li>Add retry mechanism</li> </ul> </li> <li>XML dataset (streaming mode):<ul> <li>Allow property filters on attributes in object paths</li> </ul> </li> <li>The RDF file dataset now autocompletes formats.</li> <li>Centralized error handling.</li> <li>Upgrade to Play 2.8.</li> <li>(21.06.1) Added a retry mechanism if connections on S3 are interrupted (CMEM-3675)<ul> <li>Per default, at most 10 retries are attempted.</li> <li>Number of retries can be changed by setting the configuration parameter <code>retryCount</code> (available on <code>workspace.repository.s3</code> and <code>workspace.repository.projectS3</code>).</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#eccenca-datamanager-v21065","title":"eccenca DataManager v21.06.5","text":"<p>This version of eccenca DataManager fixes the following issues:</p> <ul> <li>Linkrules<ul> <li>several stability improvements for GraphDB backends</li> </ul> </li> <li>Explore<ul> <li>New Vocabulary Visualization Component to the graph explore view.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Vocabs<ul> <li>Show spinner during installing/uninstalling vocab</li> <li>Install new vocabularies failed if preferredNamespace is not defined.</li> </ul> </li> <li>ObjectView<ul> <li>Render images without knowing the relation by parsing the value. Images are provided as URI: <code>&lt;data:image_svg+xml......&gt;</code></li> </ul> </li> <li>Linkrules<ul> <li>Fix of missing dcterms prefix in a query</li> <li>Fix of the publish/unpublish query</li> <li>Restore Joint.js common styles</li> <li>A warning about saving published rule should be shown once</li> <li>Layout position is preserved for newly added operators</li> <li>Evaluation is presented inline without conflicting with the manual placement</li> <li>Empty text fields are no longer reset to the placeholder value</li> <li>wrong order of selection items in templates</li> <li>ID generation for operators, in case multiple prototypical pipelines are presents</li> <li>Link Rules Type Error</li> <li>Failed DI calls not catched in DM</li> <li>Broken visual connection after evaluation in LinkRuleEditor</li> </ul> </li> <li>General<ul> <li>prevent errors in login</li> <li>Bearer Token not persistent</li> <li>DM Query fails because endpoint id and token are not set</li> </ul> </li> <li>Query<ul> <li>Search in labels does not work</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#eccenca-corporate-memory-control-cmemc-v2106","title":"eccenca Corporate Memory Control (cmemc) v21.06","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>graph import</code> command</li> <li>new option <code>--skip-existing</code> will not touch graphs which are already there</li> <li><code>admin token</code> command<ul> <li>new option <code>--decode</code> shows content of the decoded auth token</li> <li>in combination with <code>--raw</code> it outputs the decoded token as json</li> </ul> </li> <li>configuration<ul> <li>new configuration keys to fetch credentials from external processes</li> <li>use the parameter <code>OAUTH_PASSWORD_PROCESS</code>, <code>OAUTH_CLIENT_SECRET_PROCESS</code> and <code>OAUTH_ACCESS_TOKEN_PROCESS</code> to setup an external executable</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>docker base image is now <code>debian:stable-20210721-slim</code></li> <li>support for <code>OAUTH_PASSWORD_ENTRY</code> and <code>OAUTH_CLIENT_SECRET_ENTRY</code> removed</li> <li>fix: freeze click dependency to 7.1.2</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#dataintegration","title":"DataIntegration","text":"<ul> <li>All script operators are disabled by default now and need to be re-enabled by configuration.</li> <li>The (not-working) spotlight transform operator is disabled by default now.</li> <li>Optional: When loading existing workflows in the new workflow editor, the operators might overlap and may need to be re-arranged manually.<ul> <li>This does not influence the actual execution of the workflows in any way.</li> <li>An auto-layouting feature will be added in the future</li> </ul> </li> <li>Plugin configuration has been changed. The \u2018plugin.blacklist\u2019 has been deprecated and will be removed in future versions. See example below for new format: <pre><code>pluginRegistry {\n  # External plugins are loaded from this folder\n  pluginFolder = ${elds.home}\"/etc/dataintegration/plugins/\"\n\n  # Configuration of individual plugins.\n  plugins {\n    pluginId1.enabled = false\n    ...\n  }\n}\n</code></pre></li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#datamanager","title":"DataManager","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#dataplatform","title":"DataPlatform","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-06/#cmemc","title":"cmemc","text":"<ul> <li>The configuration keys <code>*_ENTRY</code> are not supported anymore. In case you used them, switch to <code>*_PROCESS</code> configuration</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/","title":"Corporate Memory 21.11","text":"<p>Corporate Memory 21.11 is the fourth release in 2021.</p> <p></p> <p></p> <p>The highlights of this release are:</p> <ul> <li>Build: The workflow editor user interface allows for undo/redo of you activities now, as well as shows inline (live) progress and statistics of a running workflow.</li> <li>Explore: The Explore interface is now adapted to our new look and feel + the Knowledge Graph list component can be configured to show multiple lists of named graphs e.g. to distinguish between user, vocabulary and system graphs.</li> <li>Automate: cmemc can now interact with workflow scheduler (disable, enable, inspect, list, open) as well as dataset resources (delete, inspect, usage, list).</li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataIntegration and DataPlatform configuration and behaviour has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v21.11.1</li> <li>eccenca DataIntegration v21.11</li> <li>eccenca DataManager v21.11.5</li> <li>eccenca Corporate Memory Control (cmemc) v21.11.4</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataintegration-v2111","title":"eccenca DataIntegration v21.11","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Remote Excel Dataset using Google Drive Spreadsheets as resources.</li> <li>Remote Office 365 Spreadsheets Support.</li> <li>Workflow editor improvements:<ul> <li>View navigation via clicking and dragging the mouse on the mini-map.</li> <li>Config input port is hidden by default and can be enabled via menu entry.</li> <li>Inline (live) progress and statistics.</li> </ul> </li> <li>Support mapping of RDF literals in object mappings.<ul> <li>Literals are handled as entities and can be mapped in object mappings.</li> <li>Special path #text that allows to access the lexical value of the mapped resource. This allows to access the value of a mapped literal in an object mapping.</li> </ul> </li> <li>RDF datasets:<ul> <li>Special path #lang that allows to access the language tag of a language tagged RDF literal.</li> </ul> </li> <li>Extensions to the Excel dataset:<ul> <li>Excel columns may be addressed by their letter code (<code>#A</code>, <code>#B</code>, etc.) as well.</li> <li>A new parameter \u2018hasHeader\u2019 allows handling of pure data sheets with no table header.</li> </ul> </li> <li>Support selecting multiple vocabularies with auto-completion support.</li> <li>Transform URI pattern improvements:<ul> <li>Validation of URI patterns in the UI and backend, i.e. it checks that URI templates generate valid URIs.</li> <li>Auto-completion support in URI pattern input component.</li> <li>URI pattern validation endpoint</li> <li>URI pattern auto-completion endpoint</li> <li>Change initial URI pattern of complex URI rules from <code>/</code> to <code>{}/&lt;OBJECT_RULE_ID&gt;</code></li> </ul> </li> <li>REST endpoint to fetch an activity execution error report as JSON or Markdown.</li> <li>Activity integration into task detail pages:<ul> <li>Primary, running, failed and all related caching activities are shown on the task detail pages.</li> <li>Caching activities are grouped and have additional information and controls like \u2018refresh all caches\u2019, last update etc.</li> <li>For failed activities it is possible to see and download an execution error report.</li> </ul> </li> <li>Support for requesting task parameter values in the item search API.</li> <li>New workflow operator to stop the current workflow execution (without failing) if a specified condition has been met.<ul> <li>Add global cache for URI patterns that stores all URI patterns extracted from transform tasks.</li> <li>Add API endpoint to fetch all URI patterns used for given target class URIs.</li> <li>Allow to select from existing URI patterns (related to the same target classes) in object mapping rule form.</li> </ul> </li> <li>Support setting the URI pattern during creation of an object mapping rule.</li> <li>Application version info in user menu (sidebar on the right side).</li> <li>JDBC dataset does support token-based authentication for MS SQL server now.</li> <li>Undo/redo support in workflow editor.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Improved task search with highlighting for nodes in the canvas.</li> <li>Added \u201cDefault Value\u201d Transformation to recommended category.</li> <li>OLE2-based Excel documents (e.g., .xls) are now supported in non-streaming mode.</li> <li>Enabled the Excel \u2018LEFT\u2019 transform function.</li> <li>Mapping Rule Editor will show the rule label (if any) and the mapping target.</li> <li>The JSON dataset supports streaming.<ul> <li>The change applies to reading JSON, writing was already streamed.</li> <li>If streaming is enabled, files won\u2019t be loaded into memory, allowing to read large JSON files without running into OutOfMemory errors.</li> </ul> </li> <li>Allow to open the value mapping rule formula editor from the create/edit value mapping rule form.</li> <li>Improvements to Template operators:<ul> <li>Added option to forward input attributes.</li> <li>Allow tests in conditions (e.g., <code>if input1 is sequence</code>).</li> <li>Updated Jinja library to latest bugfix release.</li> </ul> </li> <li>Render markdown (links only) in meta data preview and search item description instead of markdown markup text.</li> <li>Moved <code>Endpoint</code> parameter of Knowledge Graph dataset into advanced section</li> <li>JDBC dataset will retry failed queries due to interrupted connections. Retries will start at the offset of the previously read row.</li> <li>Include query URLs in result from REST operator (multi input).</li> <li>Workflow editor: Adapt node height based on number of inputs.</li> <li>Project import does not show the error message.</li> <li>Added rail navigation bar, replacing the old navigation.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-datamanager-v21115","title":"eccenca DataManager v21.11.5","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>General<ul> <li>Main navigation and application header<ul> <li>New header was enabled</li> <li>showing the title of the current view or actually shown resource</li> <li>includes main actions for the page</li> </ul> </li> <li>Main navigation was moved to the right sidebar<ul> <li>can get expanded permanently</li> <li>offers option to get expanded in a reduced form by hovering it with the cursor</li> <li>we now have various section in the main nav, modules with main navigation items can be configured via <code>subSection</code> parameter (order of sections need to be defined in <code>Navigationbar</code> component, currently we have <code>timetracker</code>, <code>explore</code>, <code>build</code> and <code>other</code> as options, if not set it is automatically organized into <code>explore</code> or <code>other</code>)</li> <li>Deprecation notice: configuration variables <code>windowTitle</code> and <code>headerName</code> are now deprecated, please use <code>companyName</code>, <code>productName</code> and <code>applicationName</code> from <code>appPresentation</code></li> </ul> </li> </ul> </li> <li>Query Module<ul> <li>New Query Module v2</li> <li>Activated per default</li> <li>Improved catalog functionality</li> <li>Richer editor based on yasqe</li> <li>Integrated prefix handling</li> </ul> </li> <li>Vocabs Module<ul> <li>Allow create new empty ontology without uploading a file.</li> <li>Check if graph exist and show an error while creating a new vocab.</li> </ul> </li> <li>Explore<ul> <li>Allow hide / show the vocab viz module via configuration <code>details.visualization.enable</code></li> <li>Center automatically load vocab viz on load</li> <li>Show precise tooltips for controls of vocab viz</li> <li><code>NavigationListQuery</code> now accepts <code>{{GRAPH}}</code> as placeholder</li> <li>Selectbox load values on focus rather than on click to allow using keyboard.</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Explore<ul> <li>bug producing runtime errors in vocab viz</li> <li>Extended existing <code>externalTools</code> mechanism for custom iframes</li> </ul> </li> <li>Build<ul> <li>simplify configuration object DIWorkspace removing unused <code>url</code> value, letting only <code>enable: BOOLEAN</code> and <code>baseUrl: URL_TO_DATAINTEGRATION</code></li> </ul> </li> <li>Shacl<ul> <li>Reload form if language changes</li> <li>Load form data using the current selected language</li> <li>Show group header if there are visible elements with no value in shacline</li> <li>Display selected value in options in select boxes when it is no multi select</li> <li>Don\u2019t send empty datatypes to saveShaped as are interpreted as <code>&lt;file:///data/&gt;</code>.</li> </ul> </li> <li>Query Module<ul> <li>Improve search on query module</li> <li>Order prefixes alphabetically</li> <li>URL parameters: use <code>query</code> to open a query by IRI or <code>queryString</code> to open a query by query string</li> </ul> </li> <li>General<ul> <li>Reorder modules. Move Explore to position 1, query to position 4 and manage to the last</li> </ul> </li> <li>Explore<ul> <li>Group values if only origin graph differs in shacl view.</li> <li>replace <code>markdown-it</code> with <code>react-markdown</code> library and refactor usage</li> <li>update navigation queries <code>navigation.topQuery</code> and <code>navigation.subQuery</code></li> </ul> </li> <li>Vocab<ul> <li>Use relative paths properly when requesting DI login prefixes or cache endpoints.</li> </ul> </li> <li>Thesaurus<ul> <li>Fix links in detail view to point to the selected concept</li> <li>Fix button \u201cSee more in list\u201d to point to explore instance view</li> <li>Prevent infinite loop displaying tabs in detail view</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataplatform-v21112","title":"eccenca DataPlatform v21.11.2","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>Prometheus and Spring metrics endpoints are now exposed per default, i.e. <code>./actuator/prometheus</code> or <code>actuator/metrics</code> for list and, exemplarily, <code>./actuator/metrics/cache.size</code> for the metric of interest, see the spring doc for more information.<ul> <li>you can deactivate them using the configuration properties in <code>application.yml</code> (or any other spring config) application.yml<pre><code>endpoint:\nprometheus:\nenabled: false\nmetrics:\nenabled: false\n</code></pre></li> <li>Users roles need to match values of <code>authorization.abox.adminGroup</code> or <code>authorization.abox.metricsGroup</code> role definition for accessing those endpoints. <code>authorization.abox.metricsGroup</code> defaults to <code>metrics</code>, therefore in keycloak a user needs to <code>metrics</code> added as role, for example via a group and groupmapping.</li> </ul> </li> <li>graphdb lucene index support<ul> <li>the index is used for example in the explore section to allow fast and userfriendly access</li> </ul> </li> <li>Graph List<ul> <li>The graph list query is now configurable, using the parameter <code>proxy.graphListQuery</code> with a default value of <code>SELECT distinct ?g {graph ?g {?s ?p ?o}}</code></li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>Middleware Upgrades<ul> <li>Upgraded Stardog support to version 7.8.3.</li> <li>Upgraded GraphDB support to version 9.10.2</li> </ul> </li> <li>Change of proxied graph store get endpoint <code>/proxy/{id}/graph</code><ul> <li>Removal of support for timeout and ETags</li> <li>Usage of underlying store graph store endpoints (if available) for performance</li> </ul> </li> <li>Upgraded Stardog support to version 7.8.3.</li> <li>Upgraded GraphDB support to version 9.10.2</li> <li>stop words in search expression are no longer removed</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-dataplatform-v21111","title":"eccenca DataPlatform v21.11.1","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>Prefixes are now used in TURTLE serializations<ul> <li>Prefixes defined in the Vocaulary catalog are used.</li> </ul> </li> <li>Added support for all shacl:path expressions<ul> <li><code>shui:inversePath</code> is still supported, however please use <code>sh:inversePath</code> wherever possible.</li> </ul> </li> <li>Property usage analytics endpoints <code>api/vocabusage/*</code> for both explicit Vocab definitions and usage information. Please refer to the OpenAPI definitions for more information.</li> <li>Explicitly defined supported-submit-methods property to enable / disable \u201cTry I Out\u201d button in Swagger UI.</li> <li>Server side UI configuration Support<ul> <li>Shapes for <code>WorkspaceConfiguration</code> added</li> <li>Configuration endpoint <code>api/conf/workspace</code> exposes workspace specific information about graph lists.</li> <li><code>graphs/list</code> endpoint includes graph list association for customization of the navigation list.</li> </ul> </li> <li>Shape-intgrated Workflow triggering<ul> <li>Shapes extended for linking a Shape to DI workdlows</li> <li>extension of the <code>/api/resource/shaped</code> endpoint to include this information</li> </ul> </li> <li>Bootstrap Data:<ul> <li>example configuration the default workspace\u2019s navlist</li> </ul> </li> <li>Admins can now manually flush all caches per api call.</li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>Apache Jena 3.17 is now used.<ul> <li>SPARQL requests to virtuoso are now executed over HTTP (see migration notes)</li> </ul> </li> <li>Graph exports now sorted by subject resource</li> <li>IT tests:<ul> <li>Updated IT test GraphDB Docker image to v9.9.0-1-se</li> <li>parametrization</li> <li>DefaultGraphIT</li> <li>Resources refactoring</li> </ul> </li> <li>Bootstrap Data:<ul> <li>upgrade some vocabulary references to new versions (fibo, org, qudt, schema, gist, time)</li> <li>Updates Spring Security to mitigate potential security issue: CVE-2021-22119</li> <li>Improved API documentation for APIs offering multiple HTTP methods (i.e. <code>GET</code> and <code>POST</code>). Improves OpenAPI client generation</li> </ul> </li> <li>GSP Endpoint:<ul> <li>read content type from multipart files and use only file extension only as backup</li> </ul> </li> <li>Jinja templates:<ul> <li>templates no fail on unknown tokens, allowing easier useage of SPARQL <code>OPTIONAL</code> values in templates.</li> </ul> </li> <li>Removed:<ul> <li>Virtuoso Provisioned Authorization is no longer supported. Use <code>FROM</code> authorization instead.</li> </ul> </li> <li>Errors in the configuration graph gracefully fall back to the default config</li> <li>GraphDB FROM Authorization is correctly initialized for the SPARQL proxy</li> <li>Type fetching on stardog could lead to NullPointerException</li> <li>Facets on graphs without imports on Stardog</li> <li>Graph type query was optimized to only fetch types of interest</li> <li>Errors in the configuration graph gracefully fall back to the default config</li> <li>GraphDB FROM Authorization is correctly initialized for the SPARQL proxy</li> <li>Type fetching on stardog could lead to NullPointerException</li> <li>Facets on graphs without imports on Stardog</li> <li>Graph type query was optimized to only fetch types of interest</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#eccenca-corporate-memory-control-cmemc-v21114","title":"eccenca Corporate Memory Control (cmemc) v21.11.4","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>dataset resource</code> command group with the following resource commands:<ul> <li><code>delete</code> - Delete file resources.</li> <li><code>inspect</code> -Display all meta data of a file resource.</li> <li><code>list</code> - List available file resources.</li> <li><code>usage</code> - Display all usage data of a file resource.</li> </ul> </li> <li>scheduler command group with the following workflow scheduler commands:<ul> <li><code>disable</code> - Disable a scheduler.</li> <li><code>enable</code> - Enable a scheduler.</li> <li><code>inspect</code> - Display meta data of a scheduler.</li> <li><code>list</code> - List available schedulers.</li> <li><code>open</code> - Open scheduler in the browser.</li> </ul> </li> <li><code>config eval</code> command<ul> <li>shell environment preparation for configuration</li> </ul> </li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>docker base image is now <code>debian:stable-20211011-slim</code></li> <li>workflow execute commands now uses <code>ExecuteDefaultWorkflow</code> as activity (instead of <code>ExecuteLocalWorkflow</code>)</li> <li><code>admin token --decode</code> table now sorted by Key column</li> <li><code>dataset inspect</code> table now sorted by Key column</li> <li><code>query status</code> table now sorted by Key column</li> <li><code>vocabulary cache list</code> table now sorted by IRI column</li> <li><code>dataset create --type</code> completes now plugin IDs as well</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#dataintegration","title":"DataIntegration","text":"<ul> <li>The fix of \u2018XML dataset ignores base path when type URI is set.\u2019, could break existing projects that are relying on the previously broken behaviour.<ul> <li>This may affect all use cases with XML datasets that have the \u2018base path\u2019 parameter set to a non-empty value AND where a transformation or linking task overwrites this path by setting a type path/URI.</li> <li>Removing the \u2018base path\u2019 value from the affected XML datasets should solve the issue.</li> </ul> </li> <li>Generating URIs for entities failed if XML tags contained dots</li> <li>The JSON dataset uses streaming by default now, which does not support backward paths.</li> <li>Using backward paths will fail and the error message will contain a suggestion to change the streaming parameter to false.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#datamanager","title":"DataManager","text":"<p>No migration notes</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#dataplatform","title":"DataPlatform","text":"<ul> <li>Jinja templates will no longer fail on unknown tokens. If this was used for signaling errors or fail-fast evaluation, this has to be implemented in regular conditional checks.</li> <li>Virtuoso config requires adjustments, its HTTP port needs to be configured.<ul> <li>Please ensure, that the configured user has the same access rights in virtuoso via ODBC and HTTP application.yml (old)<pre><code>sparqlEndpoints:\nvirtuoso:\n- id: \"default\"\nauthorization: NONE\nhost: \"store\"\nport: \"1111\"\nusername: \"dba\"\npassword: \"dba\"\n</code></pre> becomes application.yml (new)<pre><code>sparqlEndpoints:\nvirtuoso:\n- id: \"default\"\nauthorization: NONE\nhost: \"store\"\nport: \"1111\"\nhttpPort: \"80\"\nusername: \"dba\"\npassword: \"dba\"\n</code></pre></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-21-11/#cmemc","title":"cmemc","text":"<p>No migration notes.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/","title":"Corporate Memory 22.1","text":"<p>Corporate Memory 22.1 is the first release in 2022.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>The all new linking editor offering a new level of user experience in the linking process supercharged with inline preview and inline validation, improved operator search and much more</li> <li>Python plugin SDK (workflow and transformation plugins)</li> </ul> </li> <li>Explore:<ul> <li>Shacl: Customizable workflow execute button in Property Shapes allows for declarative embedding of</li> <li>Backend: Support for Amazon Neptune as primary Knowledge Graph Store incl. bulk loading of large files via Amazon S3</li> </ul> </li> <li>Automate:<ul> <li>new commands and command groups making the Corporate Memory swiss-command-line-army-knife - cmemc - even more useful</li> <li>Python plugin command group adds capabilities for managing python plugins in your build workspace (admin workspace python)</li> <li>Store command group adds managing commands on quad store level (admin store)</li> <li>Metrics command groups allows for inspecting of server metrics (admin metrics, DataPlatform metrics only at the moment)</li> </ul> </li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataPlatform configuration and behavior has changed and have to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v22.1</li> <li>eccenca DataIntegration v22.1</li> <li>eccenca DataManager v22.1.1</li> <li>eccenca Corporate Memory Control (cmemc) v22.1.1</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-dataintegration-v221","title":"eccenca DataIntegration v22.1","text":"<p>This version of eccenca DataIntegration adds the following new features:</p> <ul> <li>Artifact creation dialogs now allows to set the parent project</li> <li>Added a parameter to JDBC datasets to allow clearing the table before workflow execution.</li> <li>Support for custom project and task identifiers at item creation time.</li> <li>Menu option to copy item ID to clipboard.</li> <li>Value type to represent geometry as WKT literals.</li> <li>Python plugin support<ul> <li>Initial support for workflow and transform plugins.</li> <li>Plugins are executed in a Python 3 environment.</li> <li>Check documentation for details: Python Plugins</li> </ul> </li> <li>User-defined tags on projects and tasks.</li> <li>REST endpoint to fetch rule operator plugins (<code>/api/core/ruleOperatorPlugins</code>).</li> <li><code>convertToComplex</code> query parameter to GET transform rule REST endpoint to always request a complex value transform rule.</li> <li>Add new route to display a task view plugin all by itself without headers, side bar etc.<ul> <li>Route: <code>/workbench/projects/:projectId/item/:pluginId/:taskId/view/:viewId</code></li> </ul> </li> <li>REST endpoint to evaluate a linking rule against the reference links only.</li> <li>New linking and transform editors:<ul> <li>User created layout and auto-layouting support.</li> <li>Multi-word rule operator search with highlighting.</li> <li>New node actions: select nodes, move selection</li> <li>New edge actions: Connect edge to first free input port via dragging over node, Swap edges</li> <li>Delete selected node(s) or edge via Backspace</li> <li>Select nodes via select box (press Shift + left mouse button &amp; draw rectangle) or multi select (press Alt or Cmd + left clicks): Delete, move clone selection</li> <li>Filter out \u2018Excel\u2019 category of operators when <code>hideGreyListedParameters</code> query parameter is set to true.</li> <li>Supports read-only mode. Query parameter <code>readOnly</code> sets the editor in permanent read-only mode.</li> <li>Allow to edit rule node parameters in a larger modal, so complex parameter values can be more easily edited.<ul> <li>All changes done in that modal can be updated in a single transaction (wrt. UNDO/REDO) or cancelled.</li> </ul> </li> <li>Inline evaluation<ul> <li>Show operator output values and evaluation scores directly inside the linking rule editor nodes</li> <li>Support to show link to external reference links UI via <code>referenceLinksUrl</code> query parameter.</li> <li>Support for real-time evaluation when reference links are available</li> </ul> </li> </ul> </li> <li>Simple path operator auto-completion</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>Support writing large XML files by using a memory-mapped key-value store internally.</li> <li>Request N-Triples instead of Turtle for SPARQL Construct queries in all RDF datasets, because some RDF stores run into memory problems when requesting Turtle.</li> <li>Improved performance of listing project resources on S3.</li> <li>The maximum size of the internal key value store can be configured now.<ul> <li>Default value is: <code>caches.persistence.maxSize=10GB</code></li> </ul> </li> <li>Improved writing to PostgresQL by using CSV import.</li> <li>If an operator writes multiple tables into a dataset that cannot hold multiple tables, an error is thrown now.<ul> <li>Example: A hierarchical transformation writes into a CSV file.</li> <li>Previously, the last table has been written.</li> </ul> </li> <li>Workflows can also be executed asynchronously using the \u201csimple\u201d workflow execution endpoints.</li> <li>If a sub-workflow is running, the workflow editor will display the full sub-workflow report.</li> <li>Keep sort config and page size when switching filters in faceted search views.</li> <li>In generated transform object rules for nested data sources (XML, JSON and RDF) keep the source path even for paths pointing at literal values, e.g. strings.</li> <li>Use DataPlatform\u2019s <code>facets</code> and <code>vocabusage</code> endpoints to fetch available properties for Knowledge Graph datasets in order to improve performance and load.</li> <li>Generated project &amp; task identifiers: Put label-part as prefix and shorten and simplify generated random string.</li> <li>Show labels and links for dependent tasks in delete modal.</li> <li>Added parameter to \u201clower than\u201d and \u201cgreater than\u201d metrics to choose order.</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-datamanager-v2211","title":"eccenca DataManager v22.1.1","text":"<p>This version of eccenca DataManager adds the following new features:</p> <ul> <li>Shacl: Customizable workflow execute button in Property Shapes.</li> <li>Explore: New Graph List component with configurable lists.</li> </ul> <p>In addition to that, these changes are shipped:</p> <ul> <li>General<ul> <li>Updated <code>typescript</code> to <code>^4.5.2</code> and <code>@reduxjs/toolkit</code> to <code>^1.6.2</code>.</li> <li>Use session cookie to authenticate in DI requests.</li> <li>Removed Save graph void stats from the Statistics tab.</li> </ul> </li> <li>Shacl<ul> <li>Allow HTML in Markdown of Shacl descriptions and ObjectView</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-dataplatform-v221","title":"eccenca DataPlatform v22.1","text":"<p>This version of eccenca DataPlatform ships the following new features:</p> <ul> <li>Admin endpoints for zip backup / restore of all graphs</li> <li>New endpoints for storage, analysis and upload of files (rdf files and zip/tgz)<ul> <li><code>/api/upload/</code> for storing and analyzing stored files</li> <li><code>/api/upload/transfer</code> for transferring stored files to rdf triple store</li> <li>stored files on system are removed in housekeeping maintenance job</li> </ul> </li> <li>Integrated Neptune as a triple store including bulk loading of large files via Amazon Simple Storage Service (Amazon S3)</li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>Changed proxied graph store to get endpoint <code>/proxy/{id}/graph</code><ul> <li>Removed support for timeout and ETags.</li> <li>Used underlying store graph store endpoints (if available) for performance.</li> </ul> </li> <li>Middleware Upgrades<ul> <li>Upgraded Stardog support to version 7.9.0</li> <li>Upgraded GraphDB support to version 9.10.2</li> </ul> </li> <li>Library Upgrades<ul> <li>Upgrade to Java 11</li> <li>Upgrades of several libraries including Spring Boot 2.6.6 has been done.</li> <li>Upgrade of Apache Jena 4.4 implies usage of JDK11 http client library instead of apache http client.</li> </ul> </li> <li>Query Monitor<ul> <li>New fields added to output of <code>/api/admin/currentQueries</code> endpoint: <code>user</code> (Executing user in form of IRI), <code>type</code> (Query type - one of <code>ASK</code>, <code>SELECT</code>, <code>CONSTRUCT</code>, <code>DESCRIBE</code>, <code>UNKNOWN</code>), <code>traceId</code> (Trace id of call to Dataplatform - this id bundles 1-n child ids for each query call to backend store)</li> <li>Introduced Spring Sleuth tracing for generation of query IDs and tracing IDs of requests</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#eccenca-corporate-memory-control-cmemc-v2211","title":"eccenca Corporate Memory Control (cmemc) v22.1.1","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>admin workspace python</code> command group<ul> <li><code>install</code> - Install a python package to the workspace</li> <li><code>list</code> - List installed python packages</li> <li><code>list-plugins</code> - List installed workspace plugins</li> <li><code>uninstall</code> - Uninstall a python package from the workspace</li> </ul> </li> <li><code>project import</code> command<ul> <li>output warnings in case there are failed tasks errors</li> </ul> </li> <li><code>graph export</code> command<ul> <li>the <code>--filename-template</code> / <code>-t</code> option has now a completion of common examples</li> </ul> </li> <li><code>query replay</code> command<ul> <li>replay query logs from the query status command</li> </ul> </li> <li><code>admin status</code> command<ul> <li>Output of DataManager version and status</li> <li>Output of ShapesCatalog version and status</li> </ul> </li> <li><code>query status</code> command<ul> <li>Type filter allows for filtering by query type</li> <li>status filter accepts value <code>error</code> to filter for non-successful queries</li> </ul> </li> <li><code>admin store</code> command group<ul> <li><code>export</code> - backup all knowledge graphs to a ZIP archive</li> <li><code>import</code> - restore graphs from a ZIP archive</li> <li><code>bootstrap</code> - was <code>admin bootstap</code></li> <li><code>showcase</code> - was <code>admin showcase</code></li> </ul> </li> <li><code>admin metrics</code> command group<ul> <li><code>get</code> - Get sample data of a metric</li> <li><code>inspect</code> - Inspect a metric</li> <li><code>list</code> - List metrics for a specific job</li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are shipped:</p> <ul> <li>docker base image is now <code>python:3.9-slim</code></li> <li>graph tree <code>--id-only</code> option<ul> <li>this option now outputs a flat, de-duplicated list of existing graphs</li> <li>the old output was similar to the default tree output and not useful for piping</li> </ul> </li> <li>tested and build python version is now 3.9</li> <li>cmempy tests for python 2.7 are now disabled</li> <li><code>graph list</code> command</li> <li>SPARQL 1.1 Service Description namespace now recognised as <code>sd:</code> (e.g. in virtuoso used)</li> <li><code>query list</code> command<ul> <li>newly introduced query types are treated correctly now</li> <li>additional types: <code>DELETE</code>, <code>DROP</code> and <code>INSERT</code></li> </ul> </li> <li>docker image has now an empty <code>config.ini</code> in order to avoid warnings when using cmemc with environment variables only</li> </ul> <p>The following commands are deprecated:</p> <ul> <li><code>admin bootstap</code> command<ul> <li>is now in admin store command group, will be removed with the next release</li> </ul> </li> <li><code>admin showcase</code> command     is now in admin store command group, will be removed with the next release</li> </ul> <p>In addition to that, multiple performance and stability issues were solved.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#migration-notes","title":"Migration Notes","text":"","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#dataintegration","title":"DataIntegration","text":"<ul> <li>Writing hierarchical transformations into a CSV dataset or any other datasets that are single tables will lead to an error.</li> <li>The following plugin IDs have been renamed. Old projects can still be loaded with this DI version, but reading projects written with this version using DI releases older than 22.1 can result in project loading errors:<ul> <li>Substring comparison: ID <code>substring</code> to <code>substringDistance</code></li> <li>Constant distance measure: ID <code>constant</code> to <code>constantDistance</code></li> <li>Negate transformer: ID <code>negate</code> to <code>negateTransformer</code></li> </ul> </li> <li>DI uses new DataPlatform endpoints for schema extraction from knowledge graphs by default, i.e. some functionality will break when run with an older DataPlatform.<ul> <li>Set <code>eccencaDataPlatform.sparqlSource.retrievePathsViaDpEndpoints</code> to false in order to use the old approach.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#datamanager","title":"DataManager","text":"<ul> <li>New Graph List component with configurable lists.<ul> <li>The lists can be configured in the cmem config graph.</li> <li>Configurations in the <code>&lt;datamanager&gt;/application.yml</code> are ignored any customizations need to be migrated.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#dataplatform","title":"DataPlatform","text":"<ul> <li>While updating, property <code>spring.profiles=PROFILE</code> needs to be replaced by <code>spring.config.activate.on-profile</code>. For further information, please see this blog post.</li> <li>Removed custom redirect for Swagger UI under <code>/swagger-ui</code>. Swagger UI only accessible under <code>/swagger-ui.html</code> (Spring Boot Default)</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-1/#cmemc","title":"cmemc","text":"<ul> <li>docker image usage<ul> <li>the cmemc docker image is now built to run as user <code>cmem</code> (id: <code>999</code>)</li> <li>the container internally used config file has changed<ul> <li>old: <code>/root/.config/cmemc/config.ini</code></li> <li>new: <code>/config/cmemc.ini</code></li> <li>This means, mounted config volumes need to be changed!</li> </ul> </li> </ul> </li> <li>deprecated commands<ul> <li><code>admin bootstrap</code>|<code>showcase</code> are deprecated</li> <li>use <code>admin store bootstrap</code>|<code>showcase</code> instead</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/","title":"Corporate Memory 22.2.3","text":"<p>Corporate Memory 22.2.3 is the third patch release in the 22.2 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>The all new Active (Link) Learning UI</li> <li>Extended Python Plugin SDK</li> </ul> </li> <li>Explore:<ul> <li>New graph exploration module EasyNav</li> </ul> </li> <li>Automate:<ul> <li>Tag filter, better status monitoring and complete query management</li> </ul> </li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataPlatform configuration and behavior has changed and needs to be adapted according to the migration notes below.</p> <p>Warning</p> <p>With this release of Corporate Memory the (DataIntegration) Python plugin SDK contains the <code>ExecutionContext</code> class. This results in a changed signature of the SDK API functions and causes a breaking change to your exisitng code. Your python SDK based plugins need to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v22.2.2</li> <li>eccenca DataIntegration v22.2.1</li> <li>eccenca DataManager v22.2.3</li> <li>eccenca Corporate Memory Control (cmemc) v22.2</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-dataintegration-v2221","title":"eccenca DataIntegration v22.2.1","text":"<p>v22.2.1 of eccenca DataIntegration adds the following new features:</p> <ul> <li>Rule and workflow editors:<ul> <li>Support automatic scrolling when moving beyond the editor canvas borders on a all drag and edge connect/update operations.</li> </ul> </li> <li>Added \u201csort words\u201d transform operator, which sorts words in each value.</li> </ul> <p>In addition to that, these changes are included in v22.2.1 of eccenca DataIntegration:</p> <ul> <li>Rule editors (linking, transform):<ul> <li>On tab change do not remove the search text, instead select the text to easily overwrite it.</li> <li>Allow to search for input paths in the <code>All</code> tab.</li> </ul> </li> <li>If a long-running workflow is executed manually, the same workflow can be started by a scheduler in the background.</li> <li>Executing workflows did not occupy a slot in the thread pool (i.e., unlimited workflows could be executed concurrently).</li> <li>Generating links could lead to a deadlock, if no slot in the thread pool is available.</li> <li>Entering an invalid URI as path input in the linking editor with a knowledge graph as input results in the rule being broken in the editor.</li> <li>Linking editor: Show the same property labels in the input path auto-completion as in the tab auto-completion.</li> </ul> <p>v22.2 of eccenca DataIntegration adds the following new features:</p> <ul> <li>New active learning UI</li> <li>Python plugins: Added context objects that allow accessing context dependent functionalities, such as:<ul> <li>The current OAuth token</li> <li>Updating the execution report (for workflows)</li> <li>DI version</li> <li>Current project and task identifiers</li> <li>Requires <code>cmem-plugin-base &gt;=2.0.0</code></li> </ul> </li> <li>Workflows search link in main navigation</li> <li>Linking rule editor<ul> <li>Advanced parameter toggle that shows/hides advanced parameters like <code>weight</code> and advanced section in rule parameter modal</li> </ul> </li> <li>Support for sticky notes in both linking and workflow editors</li> <li>Parameter <code>profiling.defaults.noEntities</code> to configure the default entity limit for profiling operations</li> <li>Parameter <code>org.silkframework.runtime.activity.concurrentExecutions</code> to set the max. concurrent activity instances</li> <li>Support for the <code>URI attribute</code> parameter of datasets</li> <li>Support for auto-configuration in create/update dialog</li> <li>Config parameters:<ul> <li><code>profiling.defaults.noEntities</code> to configure the default entity limit for profiling operations</li> <li><code>org.silkframework.runtime.activity.concurrentExecutions</code> to set the max. concurrent activity instances</li> <li><code>cors.enabled</code>, <code>cors.config.allowOrigins</code> and <code>cors.config.allowCredentials</code> to configure CORS settings</li> </ul> </li> </ul> <p>In addition to that, these changes are included in v22.2:</p> <ul> <li>Move <code>outputTemplate</code> parameter to advanced section of XML dataset plugin</li> <li>Improved performance of conversions to floating point numbers</li> <li>Improved linking performance</li> <li>Show report on linking execution tab</li> <li>When the evaluation fails because of missing paths in the cache give specific error message with node highlighting instead of generic error notification</li> <li>Errors in invalid Python packages are recorded and returned, instead of failing</li> <li>Size of the activity thread pool can be configured</li> <li>Linking rule editor<ul> <li>Show linking rule label above toolbar when in integrated mode</li> <li>Handle \u201creversible\u201d comparators, e.g. \u201cGreater than\u201d, by allowing to switch source/target inputs instead of setting the \u2018reverse\u2019 parameter</li> </ul> </li> <li>DataPlatform API timeout is configurable now</li> <li>Workflow progress information was moved to node footer that is displayed empty when no information is available</li> <li>Docker image base: <code>debian:bullseye-20220912-slim</code></li> <li>Return 503 error before exceeding the concurrent activity execution limit instead of discarding a running activity instance</li> <li>Do not execute empty object mapping rules to improve performance</li> <li>Remove root (start) page:<ul> <li>Redirect to workbench project search page</li> <li>Remove legacy workspace link from user menu</li> <li>Add \u201cload example project\u201d action to user menu</li> </ul> </li> <li>Show activity labels instead of IDs in task activity overview</li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-datamanager-v2223","title":"eccenca DataManager v22.2.3","text":"<p>v22.2.3 of eccenca DataManager has the following fixes:</p> <ul> <li>LinkRules<ul> <li>Rule Setup: Fix display of filter</li> </ul> </li> </ul> <p>v22.2.2 of eccenca DataManager has the following fixes:</p> <ul> <li>General<ul> <li>Logout in DM also triggers logout in DI</li> </ul> </li> <li>LinkRules<ul> <li>Rule Setup: Rule filter correctly displays OneOf and NoneOf</li> <li>Rule is correctly serialized after editing, preventing the rule contents to be deleted</li> </ul> </li> </ul> <p>v22.2.1 of eccenca DataManager has the following fixes:</p> <ul> <li>LinkRules<ul> <li>Fixed trigger of refetching data after an update</li> <li>Display of negative Reference Links</li> </ul> </li> </ul> <p>v22.2 of eccenca DataManager adds the following new features:</p> <ul> <li>Navigation<ul> <li>Add DataIntegration workflows link to main navigation</li> </ul> </li> <li>Vocabulary Catalog<ul> <li>Inline vocabulary metadata via (editable) shape</li> <li>Ability to activate git synchronization of changes<ul> <li>Change history with diff view and ability to revert to a specific commit</li> </ul> </li> </ul> </li> <li>Explore<ul> <li>New (Shacl) Template based graph creation wizard<ul> <li>Supporting different methods to define / select graph IRIs</li> <li>Support for bulk add via <code>.zip</code> archives containing multiple RDF files</li> </ul> </li> </ul> </li> <li>i18n<ul> <li>French translation</li> </ul> </li> <li>EasyNav<ul> <li>New graph visualization module</li> <li>With search filter configuration</li> <li>Bulk node search and bulk add</li> <li>Ability to save, load and share explorations</li> </ul> </li> </ul> <p>In addition to that, these changes are included in v22.2 of eccenca DataManager:</p> <ul> <li>Increase height of Turtle editor in the resource details view.</li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-dataplatform-v2222","title":"eccenca DataPlatform v22.2.2","text":"<p>v22.2.2 of eccenca DataPlatform has the following changes:</p> <ul> <li>Fixed<ul> <li>reintroduced support for IRI templates in node shapes, with only the uuid placeholder.</li> <li>Prevent buffer overflow for large query results streaming to client</li> </ul> </li> <li>Changed<ul> <li>Maintenance: Updated Spring Boot to 2.7.8</li> </ul> </li> </ul> <p>v22.2.1 of eccenca DataPlatform has the following fixes:</p> <ul> <li>Update of dependencies because of vulnerabilities i.e. Spring Boot.</li> <li>Addition of logstash runtime dependency as to enable json logging.</li> <li>GraphDb indices are created without facet option causing problems.</li> <li>Fix of memory leak in query monitor causing high heap usage.</li> <li>Refactoring of spring integration tests (IT) and inclusion of most tests in the cucumber subproject.</li> </ul> <p>v22.2 of eccenca DataPlatform ships the following new features:</p> <ul> <li>Added support for manual query/update cancellation:<ul> <li>active for graphdb, stardog, neptune</li> <li>DELETE <code>/api/admin/currentQueries/{queryId}</code></li> <li>Neptune updates cannot be cancelled because queryId header not processed</li> </ul> </li> <li>Added support for creation of configured graphdb repository on DP startup<ul> <li><code>store.graphdb.createRepositoryOnStartup</code>: Flag if repository shall be created on startup (default: false)</li> </ul> </li> <li>Added support for selective invalidation of caches (graph list, shapes) via Update parsing / GraphDb Change Tracking<ul> <li><code>proxy.cache-selective-invalidation</code>: true if activated, false otherwise full flush on every write (default: true)</li> <li><code>store.graphdb.graphDbChangeTrackingActive</code>: Whether change tracking for updates is active - better results for cache invalidation (default: true)</li> <li><code>store.graphdb.graphDbChangeTrackingMaxQuadMemory</code>: Amount of quads as a result of an update which are loaded into memory for analyzing consequences for caches (default: 1000)</li> </ul> </li> <li>Automatic creation of default <code>application.yml</code> and gradle tasks for generation of markdown documentation</li> <li>Added endpoints for supporting easynav graph visualizations<ul> <li>search and resource listing via <code>/api/search</code></li> <li>managing of persisted visualisations via <code>/api/navigate</code> endpoints</li> </ul> </li> <li>Added provisioning of jinja templates with provided substitution map for endpoint <code>/api/custom/{slug}</code></li> <li>Added property <code>proxy.descriptionProperties</code> (analogous to <code>proxy.labelProperties</code>) for defining search relevant description properties</li> <li>Extend query monitor<ul> <li>Added fields per entry<ul> <li><code>timeout</code>: value in ms of the query/update timeout</li> <li><code>timedOut</code>: boolean value on whether the query timed out or not</li> <li><code>cancelled</code>: boolean value on whether the query has been cancelled manually</li> <li><code>running</code>: boolean value on whether the query is currently still being executed</li> <li><code>affectedGraphs</code>: on successfully finished query/update the affected graphs are shown (if possible to determine)</li> </ul> </li> <li>Added property for memory bound consumption in MB for query monitor list<ul> <li><code>proxy.queryMonitorMaxMemoryInMb</code> (Default: 30)</li> </ul> </li> <li>Added fields to prometheus metrics endpoint<ul> <li><code>querymonitor_memoryusage_total</code>: memory usage of query queue in MB</li> <li><code>querymonitor_queuesize_total</code>: query queue size</li> </ul> </li> </ul> </li> <li>Extend actuator info endpoint with store backend properties, <code>/actuator/info</code>:<ul> <li>fields under store:<ul> <li><code>type</code>: same as <code>store.type</code> property (MEMORY, HTTP, GRAPHDB, STARDOG, VIRTUOSO, NEPTUNE)</li> <li><code>version</code>: if possible / otherwise UNKNOWN</li> <li><code>host</code>: if applicable otherwise N/A</li> <li><code>repository</code>: if applicable otherwise N/A</li> <li><code>user</code>: if applicable otherwise N/A</li> </ul> </li> </ul> </li> <li>Add non-transactional git sync of graph changes<ul> <li>graphs can be configured via graph configuration for bi-directional git sync</li> <li>cf. config properties under <code>gitSync.*</code></li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are included in v22.2 of eccence DataPlatform:</p> <ul> <li>New store configuration properties, see below for migration notes</li> <li>Changed property for defining select query for graphList<ul> <li>setting is store dependant and not valid for some stores</li> <li>property <code>proxy.graphListQuery</code> (<code>proxy.graph_list_query</code>) moved to store settings:<ul> <li><code>store.stardog.graphListQuery</code></li> <li><code>store.neptune.graphListQuery</code></li> </ul> </li> </ul> </li> <li>Changed property for scheduled cache invalidation<ul> <li><code>proxy.cacheInvalidationCron</code>: Spring boot cron entry cf. (default: <code>* */30 * * * *</code>)</li> <li>https://docs.spring.io/spring-framework/docs/current/reference/html/integration.html#scheduling-cron-expression</li> </ul> </li> <li>Library updates including Spring Boot / Stardog</li> <li>Changed property for DP query system timeout<ul> <li><code>proxy.queryTimeoutGeneral</code> -&gt; <code>store.queryTimeoutGeneral</code> in ISO 8601 duration format (default: <code>PT1H</code>)</li> </ul> </li> <li>Changed loading of model entities i.e. shapes cache<ul> <li>load model entities using GSP requests instead of construct queries</li> <li>Changed property for base IRI: <code>files.defaultBaseIri</code> to <code>proxy.defaultBaseIri</code> (default: <code>http://localhost/</code>)</li> </ul> </li> </ul> <p>The following functionalities have been discontinued:</p> <ul> <li>Support for provisioned store authorization</li> <li>Command line options create-config, update-war</li> <li>WAR build target and support for WAR servlet deployment</li> <li>Property for DP query system timeout check interval<ul> <li><code>proxy.queryTimeoutCheckCron</code> not necessary anymore</li> </ul> </li> <li>Support for multiple endpoints</li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#eccenca-corporate-memory-control-cmemc-v222","title":"eccenca Corporate Memory Control (cmemc) v22.2","text":"<p>This version of cmemc adds the following new features:</p> <ul> <li><code>project reload</code> command<ul> <li>Reload all tasks of a project from the workspace provider</li> </ul> </li> <li><code>admin workspace python list-plugins</code> command<ul> <li>New option <code>--package-id-only</code> to output only package IDs</li> </ul> </li> <li><code>admin workspace python install</code> command completion<ul> <li>now also provides plugin packages published on pypi.org</li> </ul> </li> <li><code>query status</code> command<ul> <li>New filter <code>query</code>:<ul> <li><code>graph</code> - List only queries which affected a certain graph (URL)</li> <li><code>regex</code> - List only queries which query text matches a regular expression</li> <li><code>trace-id</code> - List only queries which have the specified trace ID</li> <li><code>user</code> - List only queries executed by the specified account (URL)</li> </ul> </li> <li>New values for filter <code>status</code>:<ul> <li><code>cancelled</code>: List only queries which were cancelled</li> <li><code>timeout</code>: List only queries which ran into a timeout</li> </ul> </li> </ul> </li> <li><code>query cancel</code> command<ul> <li>cancel a running query - this stops the execution in the backend</li> <li>Depending on the backend store, this will result in a broken result stream (stardog, neptune and virtuoso) or a valid result stream with incomplete results (graphdb)</li> </ul> </li> <li><code>dataset list</code>|<code>delete</code> commands<ul> <li>New option <code>--filter</code> with the following concrete filter<ul> <li><code>project</code> - filter by project ID</li> <li><code>regex</code> - filter by regular expression on the dataset label</li> <li><code>tag</code> - filter by tag label</li> <li><code>type</code> - filter by dataset type</li> </ul> </li> </ul> </li> <li><code>workflow list</code> command<ul> <li>New option <code>--filter</code> with the following concrete filter<ul> <li><code>project</code> - filter by project ID</li> <li><code>regex</code> - filter by regular expression on the dataset label</li> <li><code>tag</code> - filter by tag label</li> <li><code>io</code> - filter by io type</li> </ul> </li> </ul> </li> <li><code>admin status</code> command<ul> <li>overall rewrite</li> <li>new table output</li> <li>new option <code>--raw</code> to output collected status / info values</li> <li>new option <code>--key</code> to output only specific values</li> <li>new option <code>--enforce-table</code> to enforce table output of <code>--key</code></li> </ul> </li> <li><code>vocabular import</code> command<ul> <li>new option <code>--namespace</code>: In case the imported vocabulary file does not include a preferred namespace prefix, you can manually add a namespace prefix</li> </ul> </li> <li><code>workflow io</code> command<ul> <li>new flag <code>--autoconfig</code> / <code>--no-autoconfig</code> for input dataset auto configuration</li> </ul> </li> </ul> <p>In addition to that, these changes and fixes are included:</p> <ul> <li><code>admin workspace python list-plugins</code> command<ul> <li>Additionally outputs the Package ID</li> </ul> </li> <li><code>project import</code> command<ul> <li>The project id is now optional when importing project files</li> </ul> </li> <li><code>admin status</code> command<ul> <li>new table output (similar to the other tables)</li> <li><code>status</code> filter with <code>error</code> value<ul> <li>only execution errors are listed</li> <li>this specifically means no cancelled and timeouted queries (they have there own status now)</li> </ul> </li> </ul> </li> <li>Add pysocks dependency to cmempy<ul> <li>This allows for using the <code>all_proxy</code> evironment variable</li> </ul> </li> <li><code>dataset list --raw</code> output<ul> <li>output was not a JSON array and not filtered correctly</li> </ul> </li> <li>cmempy get graph streams<ul> <li>stream enabled</li> </ul> </li> <li><code>admin status</code> command<ul> <li>command will now always return, even if a component is down</li> </ul> </li> </ul> <p>The following commands are discontinued:</p> <ul> <li><code>admin bootstap</code> command<ul> <li>was deprecated in 22.1, use <code>admin store bootstrap</code> command instead</li> </ul> </li> <li><code>admin showcase</code> command<ul> <li>was deprecated in 22.1, use <code>admin store showcase</code> command instead</li> </ul> </li> <li><code>dataset list</code>|<code>delete</code> command<ul> <li><code>--project</code> option, use <code>--filter projext XXX</code> instead</li> </ul> </li> </ul> <p>In addition to that, multiple performance and stability issues are addressed.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#migration-notes","title":"Migration Notes","text":"<p>Warning</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v22.2 into DataIntegration v22.1 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v22.1 can be imported into DataIntegration v22.2.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#dataintegration","title":"DataIntegration","text":"<ul> <li>CSV attributes specified via the <code>properties</code> parameter had inconsistent encoding rules. For CSV datasets where the <code>properties</code> parameter is used this can lead to changed source paths.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#python-plugins","title":"Python plugins","text":"<p>Due to the added context classes, the signature of a number of functions has been changed. The following changes need to be made for the implementation of these classes:</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#workflowplugin","title":"WorkflowPlugin","text":"<ul> <li>The execute function has a new parameter <code>context</code>:<ul> <li><code>def execute(self, inputs: Sequence[Entities], context: ExecutionContext)</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#parametertype","title":"ParameterType","text":"<ul> <li>The <code>project_id</code> parameters of the label and the autocompletion functions have been replaced by the PluginContext:<ul> <li><code>def autocomplete(self, query_terms: list[str], context: PluginContext) -&gt; list[Autocompletion]</code></li> <li><code>def label(self, value: str, context: PluginContext) -&gt; Optional[str]</code></li> <li>The project identifier can still be accessed via <code>context.project_id</code></li> </ul> </li> <li>The <code>fromString</code> function has a new parameter <code>context</code>:<ul> <li><code>def from_string(self, value: str, context: PluginContext) -&gt; T</code></li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#dataplatform","title":"DataPlatform","text":"<p>Due to the removed multiple endpoint support the store configuration properties have changed. Please revise your store configuration section(s) in your DataPlatform <code>application.yml</code>. The new configuration properties are:</p> <ul> <li>Type of store (general settings)<ul> <li><code>store.type</code>: MEMORY, HTTP, GRAPHDB, STARDOG, VIRTUOSO, NEPTUNE</li> <li><code>store.authorization</code>: NONE, REWRITE_FROM</li> </ul> </li> <li>MEMORY:<ul> <li><code>store.memory.files</code>: List of files loaded on startup</li> </ul> </li> <li>HTTP:<ul> <li><code>store.http.queryEndpointUrl</code>: SPARQL Query endpoint (mandatory)</li> <li><code>store.http.updateEndpointUrl</code>: SPARQL Update endpoint (mandatory)</li> <li><code>store.http.graphStoreEndpointUrl</code>: SPARQL GSP endpoint (optional but highly recommended)</li> <li><code>store.http.username</code>: Username (optional)</li> <li><code>store.http.password</code>: Password (optional)</li> </ul> </li> <li>GRAPHDB:<ul> <li><code>store.graphdb.host</code>: host of graphdb backend (i.e. localhost)</li> <li><code>store.graphdb.port</code>: port of graphdb backend (i.e. 7200)</li> <li><code>store.graphdb.ssl-enabled</code>: flag if ssl (https) is enabled (default: false)</li> <li><code>store.graphdb.repository</code>: name of repository (i.e. cmem)</li> <li><code>store.graphdb.username</code>: Username (optional)</li> <li><code>store.graphdb.password</code>: Password (optional)</li> <li><code>store.graphdb.useDirectTransfer</code>: flag if direct GSP endpoints of graphdb shall be used instead of workbench upload (default: true)</li> <li><code>store.graphdb.importDirectory</code>: Import directory to be utilized in the \u201cworkbench import with shared folder\u201d approach.</li> <li><code>store.graphdb.graphDbChangeTrackingActive</code>: Whether change tracking for updates is active - better results for cache invalidation (default: true)</li> <li><code>store.graphdb.graphDbChangeTrackingMaxQuadMemory</code>: Amount of quads as a result of an update which are loaded into memory for analyzing consequences for caches (default: 1000)</li> </ul> </li> <li>STARDOG:<ul> <li><code>store.stardog.host</code>: host of stardog backend (i.e. localhost)</li> <li><code>store.stardog.port</code>: port of stardog backend (i.e. 5820)</li> <li><code>store.stardog.ssl-enabled</code>: flag if ssl (https) is enabled (default: false)</li> <li><code>store.stardog.repository</code>: name of repository (i.e. cmem)</li> <li><code>store.stardog.username</code>: Username (optional)</li> <li><code>store.stardog.password</code>: Password (optional)</li> <li><code>store.stardog.userPasswordSalt</code>: salt for generated user password (optional)</li> <li><code>store.stardog.updateTimeoutInMilliseconds</code>: Timeout in ms for updates (default: 0 = deactivated)</li> <li><code>store.stardog.graphListQuery</code>: Query for graph list - graph must be bound to variable ?g</li> </ul> </li> <li>NEPTUNE:<ul> <li><code>store.neptune.host</code>: host of neptune backend (i.e. neptune-cluster123.eu-central-1.neptune.amazonaws.com)</li> <li><code>store.neptune.port</code>: port of neptune backend (i.e. 8182)</li> <li><code>store.neptune.graphListQuery</code>: Query for graph list - graph must be bound to variable ?g</li> <li>Settings under store.neptune.aws (mandatory):<ul> <li><code>store.neptune.aws.region</code>: AWS region where the configured neptune cluster is located (e.g. eu-central-1)</li> <li><code>store.neptune.aws.authEnabled</code>: Flag on whether authentication is enabled on neptune cluster (default: true)</li> </ul> </li> <li>Settings under <code>store.neptune.s3</code> for upload of large files (&gt;150MB uncompressed) (optional):<ul> <li><code>store.neptune.s3.bucketNameOrAPAlias</code>: Name of bucket or access point for S3 bulk load</li> <li><code>store.neptune.s3.iamRoleArn</code>: ARN of role under which neptune cluster loads from S3</li> <li><code>store.neptune.s3.bulkLoadThresholdInMb</code>: Load threshold in MB for GSP access, if graph data greater than S3 upload is used (default: 150)</li> <li><code>store.neptune.s3.bulkLoadParallelism</code>: Degree of parallelism for neptune S3 bulk loader (LOW (default), MEDIUM, HIGH, OVERSUBSCRIBE)</li> </ul> </li> </ul> </li> <li>VIRTUOSO:<ul> <li><code>store.virtuoso.host</code>: host of virtuoso backend (i.e. localhost)</li> <li><code>store.virtuoso.port</code>: http port of virtuoso backend (i.e. 8080)</li> <li><code>store.virtuoso.databasePort</code>: database port of virtuoso backend (i.e. 1111)</li> <li><code>store.virtuoso.ssl-enabled</code>: flag if ssl (https) is enabled (default: false)</li> <li><code>store.virtuoso.username</code>: Username (optional)</li> <li><code>store.virtuoso.password</code>: Password (optional)</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-22-2/#cmemc","title":"cmemc","text":"<ul> <li><code>dataset list</code>|<code>delete command</code><ul> <li>option <code>--project</code> is removed</li> <li>Please use <code>--filter project XXX</code> instead</li> </ul> </li> <li><code>admin status</code> command<ul> <li>in case you piped the normal output of this command and reacted on that, you need to use the <code>--key</code> command now</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/","title":"Corporate Memory 23.1","text":"<p>Corporate Memory 23.1 is the first release in the 23.1 release line.</p> <p> </p> <p>The highlights of this release are:</p> <ul> <li>Build:<ul> <li>Support for global variables in dataset and task parameters.</li> <li>Extensions to the Python Plugin API, including autocompleted parameter type and password parameter type.</li> </ul> </li> <li>Explore:<ul> <li>Workspaces are now selectable at runtime.</li> <li>Enhanced editing capabilities in the EasyNav editor.</li> </ul> </li> <li>Automate:<ul> <li>New <code>admin user</code> command group for managing user accounts in the Keycloak CMEM realm.</li> </ul> </li> </ul> <p>Warning</p> <p>With this release of Corporate Memory the DataManager configuration has changed and needs to be adapted according to the migration notes below.</p> <p>This release delivers the following component versions:</p> <ul> <li>eccenca DataPlatform v23.1</li> <li>eccenca DataIntegration v23.1</li> <li>eccenca DataIntegration Python Plugins v3.0.0</li> <li>eccenca DataManager v23.1.1</li> <li>eccenca Corporate Memory Control (cmemc) v23.1</li> </ul> <p>More detailed release notes for these versions are listed below.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration-v231","title":"eccenca DataIntegration v23.1","text":"<p>We\u2019re excited to bring you the latest update to DataIntegration v23.1, featuring numerous enhancements, bug fixes, and deprecations. This release introduces global variables support, Python Plugin API extensions, improved handling of replaceable datasets, and much more.</p> <p>v23.1 of eccenca DataIntegration adds the following new features:</p> <ul> <li>Support for global variables:<ul> <li>Dataset and task parameters can be set to Jinja templates.</li> <li>Templates may access configured global variables. User-defined variables will be added later.</li> <li>Global variable resolution is supported by the \u2018Evaluate template\u2019 transform operator.</li> <li>Disabled by default.</li> </ul> </li> <li>Extensions to the Python Plugin API:<ul> <li>Autocompleted parameter types may declare dependent parameters.</li> <li>Password plugin parameter type.</li> <li>Custom parameter types can be registered</li> <li>For details, see changelog of the cmem-plugin-base module.</li> </ul> </li> <li>REST endpoint to search for properties in the global vocabulary cache:<ul> <li>GET /api/workspace/vocabularies/property/search</li> <li>Warn of invisible characters in input fields and offer action to remove them from the input string.</li> </ul> </li> <li>Autocompletion of graph parameters.</li> <li>Auto-completion support to linking rule \u2018link type\u2019 parameter.</li> <li>Improve handling of replaceable datasets:<ul> <li>Datasets that can be replaced/configured in a workflow at API request time can be set in the workflow editor.</li> <li>This allows for the execution of workflows with mock data, which has not been possible with \u2018Variable dataset\u2019 tasks.</li> </ul> </li> <li>Allow to config datasets as read-only to prevent accidentally writing into them.</li> <li>New resource endpoints to replace the deprecated resource endpoints. See deprecation section for more details.</li> <li>Allow to force start activity.</li> <li>Rewritten linking evaluation view.</li> </ul> <p>v23.1 of eccenca DataIntegration introduces the following changes:</p> <ul> <li>Check token expiration (&gt; 5s left) before sending a request to prevent unnecessary request retries.</li> <li>\u2018Concatenate\u2019 and \u2018Concatenate multiple values\u2019 transformer:<ul> <li>In \u2018glue\u2019 parameter value support <code>\\t</code>, <code>\\n</code> and <code>\\\\</code> as escaped characters.</li> </ul> </li> <li>Indexing of levenshtein comparisons can be configured now.</li> <li>Rename \u2018Constant\u2019 comparison operator to \u2018Constant similarity value\u2019.</li> <li>Neo4j improvements:<ul> <li>Support for paths when reading entities (forward and backward operators).</li> <li>Using a relation at the end of a path will return the URI of the node.</li> <li>The <code>#id</code> special path will return the internal node id.</li> </ul> </li> <li>CSV dataset auto-configuration now supports detecting more encodings for the Charset parameter.</li> <li>Changed search behavior in most places to search after typing stops instead of needing to hit the ENTER key:<ul> <li>In the \u2018Create new item\u2019 dialog hitting the Enter key now has the same effect as clicking the \u2018Add\u2019 button.</li> </ul> </li> <li>Show value type label primarily instead of ID.</li> <li>Show default URI pattern example in a object rule mapping form when the source path is non-empty.</li> <li>Response body of a failed REST operator request is also added to the workflow report in addition to being logged.</li> <li>Linking execution report has a warning message when the link limit was reduced because of the config of <code>linking.execution.linkLimit.max</code>.</li> <li>Disable streaming in \u2018Parse JSON\u2019 operator, so backward paths can be used against it.</li> <li>Improved online documentation of many rule operators:<ul> <li>Distance measures: Added information if a measure is either boolean, normalized or unbounded.</li> <li>Distance measures: Clarified what happens with multiple values for single value measures.</li> <li>Transformers, Distance measures and Aggregators: Added examples</li> </ul> </li> </ul> <p>v23.1 of eccenca DataIntegration ships following fixes:</p> <ul> <li>Layout breaks on small screens on detail pages of the workspace.</li> <li>Mapping suggestion list is empty when there is no matching response even though source paths exist.</li> <li>Active Learning shows incorrect entity values.</li> <li>Add notes dialog keeps focus when workflow is executed and running.</li> <li>Race condition in project/task tag selection.</li> <li>Dataset auto-configure parameter changes not set for parameters that support auto-completion.</li> <li>Label and description of existing root/object rules cannot be changed.</li> <li>DI writes invalid XML, if the last segment of a URI starts with a number.</li> <li>Optimize peak endpoint if only one path is requested.</li> <li>Python Plugin Environment: package dependencies can not update the base requirements anymore.</li> <li>Spinner is being shown eternally when no comparison pairs have been found in the link learning.</li> <li>Value path auto-completion can suggest wrong paths if backward paths exist in the paths cache.</li> <li>Show spinner while transform examples are requested from the backend.</li> <li>Abort a not fully consumed S3 input stream instead of closing it which leads to warnings.</li> <li>Date parser fails when no input/output pattern is selected even though an alternative input/output pattern is given.</li> <li>Dependent parameter auto-completion using default values of other parameters.</li> <li>Support replaceable/variable datasets in nested workflows.</li> <li>Display info message when a parameter is disabled because it depends on other parameters to be set.</li> <li>\u2018Fix URI\u2019 operator trims the URI before fixing it and tries better to maintain the original URI with only the invalid characters encoded.</li> <li>Task completion message is shown without executing the transformation.</li> <li>Evaluation in mapping rule editor does not work when inside object mappings.</li> <li>Show error message when project import fails because of errors detected in the backend instead of closing the project import modal.</li> <li>Linking editor evaluation toolbar component issues.</li> <li>Levensthein indexing slow if combined conjunctively.</li> <li>Transform execution tab layout issues.</li> </ul> <p>v23.1 of eccenca DataIntegration introduced the following deprecations:</p> <ul> <li>Resource endpoints:<ul> <li>All resources endpoints that have the file path (<code>workspace/projects/:project/resources/:name</code>) encoded in the URL path are now deprecated. The files endpoints using a query parameter for the path should be used now.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration-python-plugins-v300","title":"eccenca DataIntegration Python Plugins v3.0.0","text":"<p>Corporate Memory v23.1 includes the DataIntegration Python Plugins support in version 3.0.0.</p> <p>v3.0.0 of eccenca DataIntegration Python Plugins adds the following new features:</p> <ul> <li> <p>Autocompleted parameter types may declare dependent parameters. For instance, a parameter <code>city</code> may declare that its completed values depend on another parameter \u2018country\u2019:</p> <pre><code>class CityParameterType(StringParameterType):\nautocompletion_depends_on_parameters: list[str] = [\"country\"]\ndef autocomplete(self,\nquery_terms: list[str],\ndepend_on_parameter_values: list[Any],\ncontext: PluginContext) -&gt; list[Autocompletion]:\n# 'depend_on_parameter_values' contains the value of the country parameter\nreturn ...\n</code></pre> </li> <li> <p>Password plugin parameter type. Passwords will be encrypted in the backend and not shown to users:</p> <pre><code>@Plugin(label=\"My Plugin\")\nclass MyTestPlugin(TransformPlugin):\ndef __init__(self, password: Password):\nself.password = password\n# The decrypted password can be accessed using:\nself.password.decrypt()\n</code></pre> </li> <li> <p>Custom parameter types can be registered. See implementation of <code>PasswordParameterType</code> for an example.</p> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-datamanager-v2311","title":"eccenca DataManager v23.1.1","text":"<p>We are excited to announce the latest update to DataManager v23.1, which introduces new features, improvements and bug fixes. This release brings enhancements to workspaces, editing capabilities in the EasyNav editor, and updates to the authentication system.</p> <p>v23.1.1 of eccenca DataManager ships following fixes:</p> <ul> <li>Fixes link rules creation dialogue setting a target property.</li> </ul> <p>v23.1 of eccenca DataManager adds the following new features:</p> <ul> <li>Workspaces are selectable at runtime.</li> <li>Routes can include a workspace selection.</li> <li>Added Editing capabilities to the EasyNav editor.</li> </ul> <p>v23.1 of eccenca DataManager introduces the following changes:</p> <ul> <li>Configuration is now fully retrieved from DataPlatform, the included Spring Boot based backend is solely delivering the javascript frontend.</li> <li>The configuration can be changed at runtime using a frontend in the <code>/admin</code> Module. Changes are visible with the next full browser reload.</li> <li>Authentication is now based on the OAuth2 Code Flow.</li> </ul> <p>v23.1 of eccenca DataManager ships following fixes:</p> <ul> <li>Removed session token from URL.</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataplatform-v231","title":"eccenca DataPlatform v23.1","text":"<p>We\u2019re excited to announce the latest update to DataPlatform v23.1, featuring significant improvements in caching, user rights management, and workspace configuration. This update also includes various bug fixes and the removal of deprecated properties. Here\u2019s an overview of the changes:</p> <p>v23.1 of eccenca DataPlatform adds the following new features:</p> <ul> <li>Added ability to use dynamic access conditions</li> <li>Added graph for infos about logged in users (iri, login):<ul> <li>Can be (de)activated using property <code>authorization.userInfoGraph.active</code> (default: true)</li> </ul> </li> <li>Workspace Selection and Configuration:<ul> <li>Activate OAuth 2.0 client role permanently</li> <li>Redirect login page to (exactly) one configured resource provider</li> <li>REST endpoints for workspace configuration</li> </ul> </li> </ul> <p>v23.1 of eccenca DataPlatform introduces the following changes:</p> <ul> <li>Integrate infinispan as sole cache provider:<ul> <li>Enables clustering of DataPlatform instances<ul> <li>clustering can be activated by <code>spring.cache.infinispan.mode=CLUSTER</code></li> </ul> </li> <li>Removed property <code>files.maintenanceCron</code> (housekeeping done by infinispan)</li> <li>Added property <code>files.storageDirectory</code> for configuring shared directory between multiple DataPlatform instances</li> <li>Replaced property <code>proxy.cacheInvalidationCron</code> with <code>proxy.cacheExpiration</code> (no scheduled flush anymore but cache expiration as default)</li> </ul> </li> <li>Changed logic of resolving user rights through access conditions - performance optimized</li> </ul> <p>v23.1 of eccenca DataPlatform ships following fixes:</p> <ul> <li>Prevent injection of formulas in Excel/CSV exports</li> <li>Diagnostic store operations / query rewrite log on logging topic <code>com.eccenca.elds.backend.sparql.query.diagnostic</code> - must be set to TRACE:<ul> <li>Activated update result statistics in existing query result logger</li> </ul> </li> <li>Missing access condition action resource for EasyNav added</li> </ul> <p>v23.1 of eccenca DataPlatform removed the following features and configurations:</p> <ul> <li>Deprecated properties under authorization.accessConditions<ul> <li><code>authorization.accessConditions.graph</code>: used graph is always the default graph from bootstrap</li> <li><code>authorization.accessConditions.url</code>: url as source for access condition not supported anymore</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-corporate-memory-control-cmemc-v231","title":"eccenca Corporate Memory Control (cmemc) v23.1","text":"<p>We are excited to announce the latest updates to eccenca Corporate Memory Control v23.1, which brings new features and improvements. This release introduces new command functionalities, configuration options, and a change in the project structure.</p> <p>v23.1 of eccenca Corporate Memory Control adds the following new features:</p> <ul> <li><code>admin status</code> command:<ul> <li>option <code>--exit-1</code> to specify, when to return non-zero exit code</li> <li>currently set to <code>never</code>, this will be changed to <code>always</code> in the future</li> </ul> </li> <li><code>admin user</code> command group:<ul> <li><code>create</code> command - add a user account to the keycloak CMEM realm</li> <li><code>delete</code> command - remove a user account from the keycloak CMEM realm</li> <li><code>list</code> command - list user accounts in the keycloak CMEM realm</li> <li><code>password</code> command - change the accounts password</li> <li><code>update</code> command - change a user account in the keycloak CMEM realm</li> </ul> </li> <li>optional <code>KEYCLOAK_BASE_URI</code> config environment</li> <li>optional <code>KEYCLOAK_REALM_ID</code> config environment</li> </ul> <p>v23.1 of eccenca Corporate Memory Control introduced the following deprecations:</p> <ul> <li><code>admin status</code> command <code>--exit-1</code> option default<ul> <li>currently set to <code>never</code>, this will be changed to <code>always</code> in a future release</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#migration-notes","title":"Migration Notes","text":"<p>Warning</p> <p>We do not guarantee forward compatibility for configuration, data or projects. I.e. importing a project created with DataIntegration v23.1 into DataIntegration v22.2 (or older) might not work.</p> <p>Backward compatibility will be ensured or migration paths explained. I.e. projects created with DataIntegration v22.2 can be imported into DataIntegration v23.1.</p>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration","title":"eccenca DataIntegration","text":"<ul> <li>Resource endpoints:<ul> <li>All resources endpoints that have the file path (<code>workspace/projects/:project/resources/:name</code>) encoded in the URL path are now deprecated.</li> <li>Use corresponding endpoints starting with <code>workspace/projects/:project/files</code> instead, using a query parameter for the file path.</li> </ul> </li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataintegration-python-plugins","title":"eccenca DataIntegration Python Plugins","text":"<p>The signature of the autocomplete function has been changed. All autocomplete implementations need to be updated to the following signature:</p> <pre><code>def autocomplete(self, query_terms: list[str], depend_on_parameter_values: list[Any], context: PluginContext) -&gt; list[Autocompletion]\n</code></pre> <p>Parameters using the old signature will continue to work for one release, but a warning will be printed in the log.</p> <p>The same applies to the label function that has been updated to the following signature:</p> <pre><code>def label(self, value: str, depend_on_parameter_values: list[Any], context: PluginContext) -&gt; Optional[str]\n</code></pre>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-datamanager","title":"eccenca DataManager","text":"<ul> <li>A manual migration for the graph based configuration of the EasyNav configuration and the graph list configuration of the explore module is necessary.</li> <li>A manual migration for the <code>.yml</code> based DataManager configuration is necessary.</li> <li>The new web based configuration tool can be used to migrate, create and manage your DataManager (workspace) configuration</li> </ul>","tags":["ReleaseNote"]},{"location":"release-notes/corporate-memory-23-1/#eccenca-dataplatform","title":"eccenca DataPlatform","text":"<ul> <li>Deprecated properties under <code>authorization.accessConditions</code> have been removed. The used graph is always the default graph from bootstrap, and URL as a source for access conditions is not supported anymore.</li> </ul>","tags":["ReleaseNote"]},{"location":"tutorials/","title":"Tutorials","text":"<p>Info</p> <p>This list of Tutorial style content can be used to learn by example and by following step-by-step guides.</p> <p>If in doubt, start with the Getting Started Tutorial.</p>"},{"location":"tutorials/#advancedtutorial","title":"AdvancedTutorial","text":"<ul> <li>Lift data from JSON and XML source</li> <li>Populate Data to Neo4j</li> </ul>"},{"location":"tutorials/#beginnerstutorial","title":"BeginnersTutorial","text":"<ul> <li>Active Learning of Linking Rules</li> <li>Lift data from tabular data such as CSV, XSLX or database tables</li> <li>Getting Started</li> </ul>"},{"location":"tutorials/#experttutorial","title":"ExpertTutorial","text":"<ul> <li>Processing Data with Variable Input Workflows</li> <li>Extracting data from a Web API</li> <li>Loading JDBC datasets incrementally</li> <li>Consuming Graphs with SQL Databases</li> <li>Provide Data in any Format via a Custom API</li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#advancedtutorial","title":"AdvancedTutorial","text":"<ul> <li>Lift data from JSON and XML source</li> <li>Populate Data to Neo4j</li> </ul>"},{"location":"tags/#api","title":"API","text":"<ul> <li>Provide Data in any Format via a Custom API</li> <li>cmempy - Python API</li> <li>DataIntegration APIs</li> <li>DataPlatform APIs</li> </ul>"},{"location":"tags/#automate","title":"Automate","text":"<ul> <li>cmemc (Overview)</li> <li>cmemc: Command Group - workflow scheduler</li> <li>cmemc: Using Github Actions</li> <li>cmemc: Using Gitlab Pipelines</li> <li>cmemc: SPARQL Scripts</li> <li>cmemc: Workflow Execution and Orchestration</li> <li>Processing Data with Variable Input Workflows</li> <li>Scheduling Workflows</li> <li>Populate Graphs to Apache Kafka</li> </ul>"},{"location":"tags/#beginnerstutorial","title":"BeginnersTutorial","text":"<ul> <li>Active Learning of Linking Rules</li> <li>Lift data from tabular data such as CSV, XSLX or database tables</li> <li>Getting Started</li> </ul>"},{"location":"tags/#bestpractice","title":"BestPractice","text":"<ul> <li>Cool IRIs</li> <li>Define Prefixes / Namespaces</li> </ul>"},{"location":"tags/#cmemc","title":"cmemc","text":"<ul> <li>cmemc (Overview)</li> <li>cmemc: Command Reference</li> <li>cmemc: Command Group - admin</li> <li>cmemc: Command Group - admin metrics</li> <li>cmemc: Command Group - admin store</li> <li>cmemc: Command Group - admin user</li> <li>cmemc: Command Group - admin workspace</li> <li>cmemc: Command Group - admin workspace python</li> <li>cmemc: Command Group - config</li> <li>cmemc: Command Group - dataset</li> <li>cmemc: Command Group - dataset resource</li> <li>cmemc: Command Group - graph</li> <li>cmemc: Command Group - project</li> <li>cmemc: Command Group - query</li> <li>cmemc: Command Group - vocabulary</li> <li>cmemc: Command Group - vocabulary cache</li> <li>cmemc: Command Group - workflow</li> <li>cmemc: Command Group - workflow scheduler</li> <li>cmemc: Configuration</li> <li>cmemc: Certificate handling and SSL verification</li> <li>cmemc: Command-Line Completion</li> <li>cmemc: Environment-based Configuration</li> <li>cmemc: File-based Configuration</li> <li>cmemc: Getting Credentials from External Processes</li> <li>cmemc: Installation</li> <li>cmemc: Invocation</li> <li>cmemc: Using the Docker Image</li> <li>cmemc: Using Github Actions</li> <li>cmemc: Using Gitlab Pipelines</li> <li>cmemc: SPARQL Scripts</li> <li>cmemc: Troubleshooting and Caveats</li> <li>cmemc: Workflow Execution and Orchestration</li> <li>Processing Data with Variable Input Workflows</li> <li>Access Conditions</li> </ul>"},{"location":"tags/#configuration","title":"Configuration","text":"<ul> <li>cmemc: Command Group - config</li> <li>DataIntegration</li> <li>DataManager</li> <li>DataPlatform</li> <li>Docker Orchestration</li> <li>Keycloak</li> <li>Changing Passwords and Keys</li> <li>Configure Corporate Memory with an external Keycloak</li> <li>Production-Ready Settings</li> <li>Quad Store Configuration</li> <li>Reverse Proxy</li> </ul>"},{"location":"tags/#dashboards","title":"Dashboards","text":"<ul> <li>Consuming Graphs in Power BI</li> <li>Consuming Graphs in Redash</li> <li>Embedding Services via the Integrations Module</li> </ul>"},{"location":"tags/#docker","title":"Docker","text":"<ul> <li>cmemc: Using the Docker Image</li> <li>Docker Orchestration</li> </ul>"},{"location":"tags/#experttutorial","title":"ExpertTutorial","text":"<ul> <li>Processing Data with Variable Input Workflows</li> <li>Extracting data from a Web API</li> <li>Loading JDBC datasets incrementally</li> <li>Consuming Graphs with SQL Databases</li> <li>Provide Data in any Format via a Custom API</li> </ul>"},{"location":"tags/#java","title":"Java","text":"<ul> <li>Accessing Graphs with Java Applications</li> </ul>"},{"location":"tags/#keycloak","title":"Keycloak","text":"<ul> <li>cmemc: Command Group - admin user</li> <li>Keycloak</li> <li>Changing Passwords and Keys</li> <li>Configure Corporate Memory with an external Keycloak</li> </ul>"},{"location":"tags/#knowledgegraph","title":"KnowledgeGraph","text":"<ul> <li>cmemc: Command Group - graph</li> <li>Cool IRIs</li> <li>Define Prefixes / Namespaces</li> <li>Lift data from JSON and XML source</li> <li>Lift data from tabular data such as CSV, XSLX or database tables</li> <li>Populate Graphs to Apache Kafka</li> <li>Consuming Graphs in Power BI</li> <li>Consuming Graphs in Redash</li> <li>EasyNav Module</li> <li>Graph Exploration</li> <li>Building a Customized User Interface</li> <li>Statement Annotations</li> <li>Versioning of Graph Changes</li> <li>Query Module</li> <li>Vocabulary Catalog</li> </ul>"},{"location":"tags/#plugin","title":"Plugin","text":"<ul> <li>Python Plugins: Installation and Usage</li> </ul>"},{"location":"tags/#project","title":"Project","text":"<ul> <li>cmemc: Command Group - project</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>cmemc: Command Group - admin workspace python</li> <li>cmempy - Python API</li> <li>Python Plugins: Overview</li> <li>Python Plugins: Development</li> <li>Python Plugins: Installation and Usage</li> <li>Python Plugins: Setup and Configuration</li> </ul>"},{"location":"tags/#pythonplugin","title":"PythonPlugin","text":"<ul> <li>Populate Graphs to Apache Kafka</li> </ul>"},{"location":"tags/#reference","title":"Reference","text":"<ul> <li>cmemc: Command Reference</li> <li>Rule Operators</li> <li>DataIntegration: Activity Reference</li> <li>DataIntegration: Plugin Reference</li> <li>GraphResourcePattern</li> <li>Datatypes</li> <li>Node Shape Reference</li> <li>Property Shapes</li> </ul>"},{"location":"tags/#releasenote","title":"ReleaseNote","text":"<ul> <li>Corporate Memory 19.10</li> <li>Corporate Memory 20.03</li> <li>Corporate Memory 20.06</li> <li>Corporate Memory 20.10</li> <li>Corporate Memory 20.12</li> <li>Corporate Memory 21.02</li> <li>Corporate Memory 21.04</li> <li>Corporate Memory 21.06</li> <li>Corporate Memory 21.11</li> <li>Corporate Memory 22.1</li> <li>Corporate Memory 22.2.3</li> <li>Corporate Memory 23.1</li> </ul>"},{"location":"tags/#security","title":"Security","text":"<ul> <li>cmemc: Command Group - admin user</li> <li>cmemc: Certificate handling and SSL verification</li> <li>cmemc: Getting Credentials from External Processes</li> <li>Access Conditions</li> <li>Keycloak</li> <li>Changing Passwords and Keys</li> <li>Configure Corporate Memory with an external Keycloak</li> <li>Production-Ready Settings</li> </ul>"},{"location":"tags/#sparql","title":"SPARQL","text":"<ul> <li>cmemc: Command Group - admin store</li> <li>cmemc: Command Group - query</li> <li>cmemc: SPARQL Scripts</li> <li>Provide Data in any Format via a Custom API</li> <li>Query Module</li> </ul>"},{"location":"tags/#video","title":"Video","text":"<ul> <li>cmemc: Command-Line Completion</li> <li>Scheduling Workflows</li> </ul>"},{"location":"tags/#vocabulary","title":"Vocabulary","text":"<ul> <li>cmemc: Command Group - vocabulary</li> <li>cmemc: Command Group - vocabulary cache</li> <li>Datatypes</li> <li>Node Shape Reference</li> <li>Property Shapes</li> <li>Thesauri Management</li> <li>Vocabulary Catalog</li> </ul>"},{"location":"tags/#workflow","title":"Workflow","text":"<ul> <li>cmemc: Command Group - workflow</li> <li>cmemc: Workflow Execution and Orchestration</li> <li>Processing Data with Variable Input Workflows</li> <li>Scheduling Workflows</li> <li>Workflow Reconfiguration</li> <li>Workflow Trigger</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Info</p> <p>This list of Tutorial style content can be used to learn by example and by following step-by-step guides.</p> <p>If in doubt, start with the Getting Started Tutorial.</p>"},{"location":"tutorials/#advancedtutorial","title":"AdvancedTutorial","text":"<ul> <li>Lift data from JSON and XML source</li> <li>Populate Data to Neo4j</li> </ul>"},{"location":"tutorials/#beginnerstutorial","title":"BeginnersTutorial","text":"<ul> <li>Active Learning of Linking Rules</li> <li>Lift data from tabular data such as CSV, XSLX or database tables</li> <li>Getting Started</li> </ul>"},{"location":"tutorials/#experttutorial","title":"ExpertTutorial","text":"<ul> <li>Processing Data with Variable Input Workflows</li> <li>Extracting data from a Web API</li> <li>Loading JDBC datasets incrementally</li> <li>Consuming Graphs with SQL Databases</li> <li>Provide Data in any Format via a Custom API</li> </ul>"}]}